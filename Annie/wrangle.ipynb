{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c279306",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68e7f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Optional, Union, cast\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "\n",
    "from env import github_token, github_username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae79dddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make a github personal access token.\n",
    "#     1. Go here and generate a personal access token: https://github.com/settings/tokens\n",
    "#        You do _not_ need select any scopes, i.e. leave all the checkboxes unchecked\n",
    "#     2. Save it in your env.py file under the variable `github_token`\n",
    "# TODO: Add your github username to your env.py file under the variable `github_username`\n",
    "# TODO: Add more repositories to the `REPOS` list below.\n",
    "\n",
    "\n",
    "#----------ACQUIRE----------------------------\n",
    "headers = {\"Authorization\": f\"token {github_token}\", \"User-Agent\": github_username}\n",
    "\n",
    "if headers[\"Authorization\"] == \"token \" or headers[\"User-Agent\"] == \"\":\n",
    "    raise Exception(\n",
    "        \"You need to follow the instructions marked TODO in this script before trying to use it\"\n",
    "    )\n",
    "\n",
    "#### FUNCTION TO GET REPO LINKS\n",
    "def get_repo_links() -> List[str]:\n",
    "    '''\n",
    "    NOTE!!! VERY SLOW. IF DON'T HAVE A JSON FILE MAKE SURE TO RUN THIS FUNCTION AT LEAST FOR 1 HR\n",
    "    \n",
    "    Scraps the links of the repositories and saves them to the list\n",
    "    '''\n",
    "    filename = 'README_REPOS.json'\n",
    "    REPOS=[]\n",
    "    #headers = {\"Authorization\": f\"token {github_token}\", \"User-Agent\": github_username}\n",
    "    languages = ['JavaScript', 'Python']\n",
    "    # if the json file is available\n",
    "    if os.path.isfile(filename):\n",
    "        # read from json file\n",
    "        with open(filename, \"r\") as json_file:\n",
    "            REPOS = json.load(json_file)\n",
    "    else:\n",
    "        for i in range(1, 101):\n",
    "            print(i)\n",
    "            if i == 1:\n",
    "                start_link = 'https://github.com/search?o=desc&q=stars:%3E1&s=forks&type=Repositories'\n",
    "            else:\n",
    "                start_link = f'https://github.com/search?o=desc&q=stars:%3E1&s=forks&type=Repositories'\n",
    "\n",
    "            response = requests.get(start_link, headers=headers)\n",
    "            if response.status_code != 200:\n",
    "                print('problem' + str(response.status_code))\n",
    "                sleep(20)\n",
    "                response = requests.get(start_link, headers=headers)\n",
    "            print(response.status_code)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            all_blocks = soup.find_all('li', class_='repo-list-item hx_hit-repo d-flex flex-justify-start py-4 public source')\n",
    "            if type(all_blocks) == None:\n",
    "                print('all blocks fail')\n",
    "                sleep(30)\n",
    "                all_blocks = soup.find_all('li', class_='repo-list-item hx_hit-repo d-flex flex-justify-start py-4 public source')\n",
    "            for block in all_blocks:\n",
    "                try:\n",
    "                    language = block.find('span', itemprop='programmingLanguage').text\n",
    "                except:\n",
    "                    continue\n",
    "                if language in languages:\n",
    "                    link = block.find('a', class_='v-align-middle')['href'][1:]\n",
    "                    REPOS.append(link)\n",
    "            sleep(20)\n",
    "        \n",
    "        with open(filename, \"w\") as outfile:\n",
    "            json.dump(REPOS, outfile)\n",
    "    return REPOS\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def github_api_request(url: str) -> Union[List, Dict]:\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response_data = response.json()\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\n",
    "            f\"Error response from github api! status code: {response.status_code}, \"\n",
    "            f\"response: {json.dumps(response_data)}\"\n",
    "        )\n",
    "    return response_data\n",
    "\n",
    "\n",
    "def get_repo_language(repo: str) -> str:\n",
    "    url = f\"https://api.github.com/repos/{repo}\"\n",
    "    repo_info = github_api_request(url)\n",
    "    if type(repo_info) is dict:\n",
    "        repo_info = cast(Dict, repo_info)\n",
    "        if \"language\" not in repo_info:\n",
    "            raise Exception(\n",
    "                \"'language' key not round in response\\n{}\".format(json.dumps(repo_info))\n",
    "            )\n",
    "        return repo_info[\"language\"]\n",
    "    raise Exception(\n",
    "        f\"Expecting a dictionary response from {url}, instead got {json.dumps(repo_info)}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def get_repo_contents(repo: str) -> List[Dict[str, str]]:\n",
    "    url = f\"https://api.github.com/repos/{repo}/contents/\"\n",
    "    contents = github_api_request(url)\n",
    "    if type(contents) is list:\n",
    "        contents = cast(List, contents)\n",
    "        return contents\n",
    "    raise Exception(\n",
    "        f\"Expecting a list response from {url}, instead got {json.dumps(contents)}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def get_readme_download_url(files: List[Dict[str, str]]) -> str:\n",
    "    \"\"\"\n",
    "    Takes in a response from the github api that lists the files in a repo and\n",
    "    returns the url that can be used to download the repo's README file.\n",
    "    \"\"\"\n",
    "    for file in files:\n",
    "        if file[\"name\"].lower().startswith(\"readme\"):\n",
    "            return file[\"download_url\"]\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def process_repo(repo: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Takes a repo name like \"gocodeup/codeup-setup-script\" and returns a\n",
    "    dictionary with the language of the repo and the readme contents.\n",
    "    \"\"\"\n",
    "    contents = get_repo_contents(repo)\n",
    "    readme_download_url = get_readme_download_url(contents)\n",
    "    if readme_download_url == \"\":\n",
    "        readme_contents = \"\"\n",
    "    else:\n",
    "        readme_contents = requests.get(readme_download_url).text\n",
    "    return {\n",
    "        \"repo\": repo,\n",
    "        \"language\": get_repo_language(repo),\n",
    "        \"readme_contents\": readme_contents,\n",
    "    }\n",
    "\n",
    "\n",
    "def scrape_github_data() -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    WARNING!!! VERY SLOW. IF DON'T HAVE A JSON FILE MAKE SURE TO RUN THIS FUNCTION AT LEAST FOR 1 HR\n",
    "\n",
    "    Loop through all of the repos and process them. Returns the processed data.\n",
    "    \"\"\"\n",
    "    if os.path.isfile('data.json'):\n",
    "        # read from json file\n",
    "        with open('data.json', \"r\") as json_file:\n",
    "            data = json.load(json_file)\n",
    "    else:\n",
    "        REPOS = get_repo_links()\n",
    "        data = [process_repo(repo) for repo in REPOS]\n",
    "        with open('data.json', \"w\") as outfile:\n",
    "            json.dump(data, outfile)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4d22eb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_clean_df\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    Acquires the data from acquire helper file, saves it into a data frame.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    Cleans columns by appying cleaning functions from this file.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    Return:\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m        df: pd.DataFrame -> cleaned data frame\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# acquire a data from inshorts.com website\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "def get_clean_df() -> pd.DataFrame:\n",
    "    '''\n",
    "    Acquires the data from acquire helper file, saves it into a data frame.\n",
    "    Cleans columns by appying cleaning functions from this file.\n",
    "    Return:\n",
    "        df: pd.DataFrame -> cleaned data frame\n",
    "    '''\n",
    "\n",
    "    # acquire a data from inshorts.com website\n",
    "    df = pd.DataFrame(acquire.scrape_github_data())\n",
    "    # news_df transformations\n",
    "    # rename columns\n",
    "    df.rename({'readme_contents':'original'}, axis=1, inplace=True)\n",
    "    # create a column 'first_clean' hlml and markdown removed\n",
    "    df['first_clean'] = df.original.apply(clean_html_markdown)\n",
    "    # create a column 'clean' lower case, ascii, no stopwords\n",
    "    df['clean'] = df.first_clean.apply(basic_clean).apply(tokenize).apply(remove_stopwords,extra_words=[\"'\", 'space'])\n",
    "    # only stems\n",
    "    #df['stemmed'] = news_df.clean.apply(stem)\n",
    "    # only lemmas\n",
    "    df['lemmatized'] = df.clean.apply(lemmatize)\n",
    "    # ENGINEER FEATURES BASED ON THE CLEAN TEXT COLUMN\n",
    "    sia = nltk.sentiment.SentimentIntensityAnalyzer()\n",
    "    # adds counpound sentiment score\n",
    "    df['sentiment'] = df['clean'].apply(lambda doc: sia.polarity_scores(doc)['compound'])\n",
    "    # numerical\n",
    "    df['lem_length'] = df.lemmatized.str.len()\n",
    "    df['original_length'] = df.original.str.len()\n",
    "    df['clean_length'] = df.clean.str.len()\n",
    "    df['length_diff'] = df.original_length - df.clean_length\n",
    "    # categorical\n",
    "    df['has_#9'] = np.where(df.clean.str.contains('&#9;'), 1, 0)\n",
    "    df['has_parts'] = np.where((df.clean.str.contains(' part ')) | (df.clean.str.contains('parts')), 1, 0)\n",
    "    df['has_fix'] = np.where(df.clean.str.contains(' fix '), 1, 0)\n",
    "    df['has_tab'] = np.where(df.clean.str.contains(' tab '), 1, 0)\n",
    "    df['has_x'] = np.where(df.clean.str.contains(' x '), 1, 0)\n",
    "    df['has_v'] = np.where(df.clean.str.contains(' v '), 1, 0)\n",
    "    df['has_codeblock'] = np.where(df.clean.str.contains('codeblock'), 1, 0)\n",
    "    df['has_image'] = np.where(df.clean.str.contains('image'), 1, 0)\n",
    "    # change language to category\n",
    "    df.language = pd.Categorical(df.language)\n",
    "    # drop repo column\n",
    "    df.drop('repo', axis=1, inplace=True)\n",
    "    # drop 'clean_length' columns, as it is part of length_diff column\n",
    "    df.drop('clean_length', axis=1, inplace=True)\n",
    "    # reorder columns\n",
    "    new_order = ['original', 'first_clean', 'clean', 'lemmatized', 'sentiment', 'lem_length',\n",
    "        'original_length', 'length_diff', 'has_#9', 'has_tab',\\\n",
    "        'has_parts', 'has_fix', 'has_x', 'has_v',\\\n",
    "       'has_codeblock', 'has_image', 'language']\n",
    "    df = df[new_order]\n",
    "    return df\n",
    "\n",
    "####### PREPARATIONS FOR THE MODELING\n",
    "\n",
    "def scale_numeric_data(X_train, X_validate, X_test):\n",
    "    '''\n",
    "    Scales numerical columns.\n",
    "    Parameters:\n",
    "        train, validate, test data sets\n",
    "    Returns:\n",
    "    train, validate, test data sets with scaled data\n",
    "    '''\n",
    "    # features to scale\n",
    "    to_scale = ['sentiment', 'lem_length', 'original_length',  'length_diff']\n",
    "    # create a scaler\n",
    "    sc = MinMaxScaler()\n",
    "    sc.fit(X_train[to_scale])\n",
    "    # transform data\n",
    "    X_train[to_scale] = sc.transform(X_train[to_scale])\n",
    "    X_validate[to_scale] = sc.transform(X_validate[to_scale])\n",
    "    X_test[to_scale] = sc.transform(X_test[to_scale])\n",
    "    \n",
    "    return X_train, X_validate, X_test\n",
    "\n",
    "####### SPLITTING FUNCTIONS\n",
    "def split_3(df):\n",
    "    '''\n",
    "    This function takes in a dataframe and splits it into 3 data sets\n",
    "    Test is 20% of the original dataset, validate is .30*.80= 24% of the \n",
    "    original dataset, and train is .70*.80= 56% of the original dataset. \n",
    "    The function returns, in this order, train, validate and test dataframes. \n",
    "    '''\n",
    "    explore_columns = ['original', 'first_clean', 'clean', 'lemmatized', 'sentiment', 'lem_length',\\\n",
    "        'original_length', 'length_diff', 'language']\n",
    "    df = df[explore_columns]\n",
    "    #split_db class verision with random seed\n",
    "    train_validate, test = train_test_split(df, test_size=0.2, \n",
    "                                            random_state=seed, stratify=df[target])\n",
    "    train, validate = train_test_split(train_validate, test_size=0.3, \n",
    "                                       random_state=seed, stratify=train_validate[target])\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42473e5-5dee-4aee-8ea7-2a28bb54cf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "bag_of_words = cv.fit_transform(X_train.lemmatized)\n",
    "bow = pd.DataFrame(bag_of_words.todense())\n",
    "bow.columns = cv.get_feature_names()\n",
    "bow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a0e606-d98d-44af-97f4-9ec4bed62be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow.apply(lambda row: row / row.sum(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2adee69-26db-4b55-a671-0e04576cce16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
