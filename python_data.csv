,repo_name,url,language,readme_content
0,TheAlgorithms/Python,https://github.com/TheAlgorithms/Python/blob/master/README.md,Python,          The Algorithms - Python                                                                  All algorithms implemented in Python - for educationImplementations are for learning purposes only. They may be less efficient than the implementations in the Python standard library. Use them at your discretion.Getting StartedRead through our Contribution Guidelines before you contribute.Community ChannelsWe are on Discord and Gitter! Community channels are a great way for you to ask questions and get help. Please join us!List of AlgorithmsSee our directory for easier navigation and a better overview of the project.
1,google/it-cert-automation-practice,https://github.com/google/it-cert-automation-practice/blob/master/README.md,Python,Google IT Automation with Python Professional Certificate - Practice filesThis repository contains the practice files used throughout the courses that arepart of the Google IT Automation with Python Professional CertificateThere's a separate folder for each course.
2,huggingface/transformers,https://github.com/huggingface/transformers/blob/main/README.md,Python,"                                                                                                                    English |        简体中文 |        繁體中文 |        한국어 |        Español |        日本語 |        हिन्दी        State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow    🤗 Transformers provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio.These models can be applied on:📝 Text, for tasks like text classification, information extraction, question answering, summarization, translation, text generation, in over 100 languages.🖼️ Images, for tasks like image classification, object detection, and segmentation.🗣️ Audio, for tasks like speech recognition and audio classification.Transformer models can also perform tasks on several modalities combined, such as table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.🤗 Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments.🤗 Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch and TensorFlow — with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other.Online demosYou can test most of our models directly on their pages from the model hub. We also offer private model hosting, versioning, & an inference API for public and private models.Here are a few examples:In Natural Language Processing:Masked word completion with BERTName Entity Recognition with ElectraText generation with GPT-2Natural Language Inference with RoBERTaSummarization with BARTQuestion answering with DistilBERTTranslation with T5In Computer Vision:Image classification with ViTObject Detection with DETRSemantic Segmentation with SegFormerPanoptic Segmentation with MaskFormerDepth Estimation with DPTVideo Classification with VideoMAEUniversal Segmentation with OneFormerIn Audio:Automatic Speech Recognition with Wav2Vec2Keyword Spotting with Wav2Vec2Audio Classification with Audio Spectrogram TransformerIn Multimodal tasks:Table Question Answering with TAPASVisual Question Answering with ViLTZero-shot Image Classification with CLIPDocument Question Answering with LayoutLMZero-shot Video Classification with X-CLIP100 projects using TransformersTransformers is more than a toolkit to use pretrained models: it's a community of projects built around it and theHugging Face Hub. We want Transformers to enable developers, researchers, students, professors, engineers, and anyoneelse to build their dream projects.In order to celebrate the 100,000 stars of transformers, we have decided to put the spotlight on thecommunity, and we have created the awesome-transformers page which lists 100incredible projects built in the vicinity of transformers.If you own or use a project that you believe should be part of the list, please open a PR to add it!If you are looking for custom support from the Hugging Face team    Quick tourTo immediately use a model on a given input (text, image, audio, ...), we provide the pipeline API. Pipelines group together a pretrained model with the preprocessing that was used during that model's training. Here is how to quickly use a pipeline to classify positive versus negative texts:>>> from transformers import pipeline# Allocate a pipeline for sentiment-analysis>>> classifier = pipeline('sentiment-analysis')>>> classifier('We are very happy to introduce pipeline to the transformers repository.')[{'label': 'POSITIVE', 'score': 0.9996980428695679}]The second line of code downloads and caches the pretrained model used by the pipeline, while the third evaluates it on the given text. Here the answer is \""positive\"" with a confidence of 99.97%.Many tasks have a pre-trained pipeline ready to go, in NLP but also in computer vision and speech. For example, we can easily extract detected objects in an image:>>> import requests>>> from PIL import Image>>> from transformers import pipeline# Download an image with cute cats>>> url = \""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample.png\"">>> image_data = requests.get(url, stream=True).raw>>> image = Image.open(image_data)# Allocate a pipeline for object detection>>> object_detector = pipeline('object-detection')>>> object_detector(image)[{'score': 0.9982201457023621,  'label': 'remote',  'box': {'xmin': 40, 'ymin': 70, 'xmax': 175, 'ymax': 117}}, {'score': 0.9960021376609802,  'label': 'remote',  'box': {'xmin': 333, 'ymin': 72, 'xmax': 368, 'ymax': 187}}, {'score': 0.9954745173454285,  'label': 'couch',  'box': {'xmin': 0, 'ymin': 1, 'xmax': 639, 'ymax': 473}}, {'score': 0.9988006353378296,  'label': 'cat',  'box': {'xmin': 13, 'ymin': 52, 'xmax': 314, 'ymax': 470}}, {'score': 0.9986783862113953,  'label': 'cat',  'box': {'xmin': 345, 'ymin': 23, 'xmax': 640, 'ymax': 368}}]Here we get a list of objects detected in the image, with a box surrounding the object and a confidence score. Here is the original image on the left, with the predictions displayed on the right:        You can learn more about the tasks supported by the pipeline API in this tutorial.In addition to pipeline, to download and use any of the pretrained models on your given task, all it takes is three lines of code. Here is the PyTorch version:>>> from transformers import AutoTokenizer, AutoModel>>> tokenizer = AutoTokenizer.from_pretrained(\""bert-base-uncased\"")>>> model = AutoModel.from_pretrained(\""bert-base-uncased\"")>>> inputs = tokenizer(\""Hello world!\"", return_tensors=\""pt\"")>>> outputs = model(**inputs)And here is the equivalent code for TensorFlow:>>> from transformers import AutoTokenizer, TFAutoModel>>> tokenizer = AutoTokenizer.from_pretrained(\""bert-base-uncased\"")>>> model = TFAutoModel.from_pretrained(\""bert-base-uncased\"")>>> inputs = tokenizer(\""Hello world!\"", return_tensors=\""tf\"")>>> outputs = model(**inputs)The tokenizer is responsible for all the preprocessing the pretrained model expects, and can be called directly on a single string (as in the above examples) or a list. It will output a dictionary that you can use in downstream code or simply directly pass to your model using the ** argument unpacking operator.The model itself is a regular Pytorch nn.Module or a TensorFlow tf.keras.Model (depending on your backend) which you can use as usual. This tutorial explains how to integrate such a model into a classic PyTorch or TensorFlow training loop, or how to use our Trainer API to quickly fine-tune on a new dataset.Why should I use transformers?Easy-to-use state-of-the-art models:High performance on natural language understanding & generation, computer vision, and audio tasks.Low barrier to entry for educators and practitioners.Few user-facing abstractions with just three classes to learn.A unified API for using all our pretrained models.Lower compute costs, smaller carbon footprint:Researchers can share trained models instead of always retraining.Practitioners can reduce compute time and production costs.Dozens of architectures with over 60,000 pretrained models across all modalities.Choose the right framework for every part of a model's lifetime:Train state-of-the-art models in 3 lines of code.Move a single model between TF2.0/PyTorch/JAX frameworks at will.Seamlessly pick the right framework for training, evaluation and production.Easily customize a model or an example to your needs:We provide examples for each architecture to reproduce the results published by its original authors.Model internals are exposed as consistently as possible.Model files can be used independently of the library for quick experiments.Why shouldn't I use transformers?This library is not a modular toolbox of building blocks for neural nets. The code in the model files is not refactored with additional abstractions on purpose, so that researchers can quickly iterate on each of the models without diving into additional abstractions/files.The training API is not intended to work on any model but is optimized to work with the models provided by the library. For generic machine learning loops, you should use another library (possibly, Accelerate).While we strive to present as many use cases as possible, the scripts in our examples folder are just that: examples. It is expected that they won't work out-of-the box on your specific problem and that you will be required to change a few lines of code to adapt them to your needs.InstallationWith pipThis repository is tested on Python 3.8+, Flax 0.4.1+, PyTorch 1.10+ and TensorFlow 2.6+.You should install 🤗 Transformers in a virtual environment. If you're unfamiliar with Python virtual environments, check out the user guide.First, create a virtual environment with the version of Python you're going to use and activate it.Then, you will need to install at least one of Flax, PyTorch or TensorFlow.Please refer to TensorFlow installation page, PyTorch installation page and/or Flax and Jax installation pages regarding the specific installation command for your platform.When one of those backends has been installed, 🤗 Transformers can be installed using pip as follows:pip install transformersIf you'd like to play with the examples or need the bleeding edge of the code and can't wait for a new release, you must install the library from source.With condaSince Transformers version v4.0.0, we now have a conda channel: huggingface.🤗 Transformers can be installed using conda as follows:conda install -c huggingface transformersFollow the installation pages of Flax, PyTorch or TensorFlow to see how to install them with conda.NOTE:  On Windows, you may be prompted to activate Developer Mode in order to benefit from caching. If this is not an option for you, please let us know in this issue.Model architecturesAll the model checkpoints provided by 🤗 Transformers are seamlessly integrated from the huggingface.co model hub where they are uploaded directly by users and organizations.Current number of checkpoints: 🤗 Transformers currently provides the following architectures (see here for a high-level summary of each them):ALBERT (from Google Research and the Toyota Technological Institute at Chicago) released with the paper ALBERT: A Lite BERT for Self-supervised Learning of Language Representations, by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut.ALIGN (from Google Research) released with the paper Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision by Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig.AltCLIP (from BAAI) released with the paper AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities by Chen, Zhongzhi and Liu, Guang and Zhang, Bo-Wen and Ye, Fulong and Yang, Qinghong and Wu, Ledell.Audio Spectrogram Transformer (from MIT) released with the paper AST: Audio Spectrogram Transformer by Yuan Gong, Yu-An Chung, James Glass.Autoformer (from Tsinghua University) released with the paper Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting by Haixu Wu, Jiehui Xu, Jianmin Wang, Mingsheng Long.Bark (from Suno) released in the repository suno-ai/bark by Suno AI team.BART (from Facebook) released with the paper BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension by Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov and Luke Zettlemoyer.BARThez (from École polytechnique) released with the paper BARThez: a Skilled Pretrained French Sequence-to-Sequence Model by Moussa Kamal Eddine, Antoine J.-P. Tixier, Michalis Vazirgiannis.BARTpho (from VinAI Research) released with the paper BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese by Nguyen Luong Tran, Duong Minh Le and Dat Quoc Nguyen.BEiT (from Microsoft) released with the paper BEiT: BERT Pre-Training of Image Transformers by Hangbo Bao, Li Dong, Furu Wei.BERT (from Google) released with the paper BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.BERT For Sequence Generation (from Google) released with the paper Leveraging Pre-trained Checkpoints for Sequence Generation Tasks by Sascha Rothe, Shashi Narayan, Aliaksei Severyn.BERTweet (from VinAI Research) released with the paper BERTweet: A pre-trained language model for English Tweets by Dat Quoc Nguyen, Thanh Vu and Anh Tuan Nguyen.BigBird-Pegasus (from Google Research) released with the paper Big Bird: Transformers for Longer Sequences by Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.BigBird-RoBERTa (from Google Research) released with the paper Big Bird: Transformers for Longer Sequences by Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.BioGpt (from Microsoft Research AI4Science) released with the paper BioGPT: generative pre-trained transformer for biomedical text generation and mining by Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon and Tie-Yan Liu.BiT (from Google AI) released with the paper Big Transfer (BiT): General Visual Representation Learning by Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, Neil Houlsby.Blenderbot (from Facebook) released with the paper Recipes for building an open-domain chatbot by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.BlenderbotSmall (from Facebook) released with the paper Recipes for building an open-domain chatbot by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.BLIP (from Salesforce) released with the paper BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation by Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi.BLIP-2 (from Salesforce) released with the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi.BLOOM (from BigScience workshop) released by the BigScience Workshop.BORT (from Alexa) released with the paper Optimal Subarchitecture Extraction For BERT by Adrian de Wynter and Daniel J. Perry.BridgeTower (from Harbin Institute of Technology/Microsoft Research Asia/Intel Labs) released with the paper BridgeTower: Building Bridges Between Encoders in Vision-Language Representation Learning by Xiao Xu, Chenfei Wu, Shachar Rosenman, Vasudev Lal, Wanxiang Che, Nan Duan.ByT5 (from Google Research) released with the paper ByT5: Towards a token-free future with pre-trained byte-to-byte models by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel.CamemBERT (from Inria/Facebook/Sorbonne) released with the paper CamemBERT: a Tasty French Language Model by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Suárez*, Yoann Dupont, Laurent Romary, Éric Villemonte de la Clergerie, Djamé Seddah and Benoît Sagot.CANINE (from Google Research) released with the paper CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation by Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting.Chinese-CLIP (from OFA-Sys) released with the paper Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese by An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, Chang Zhou.CLAP (from LAION-AI) released with the paper Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation by Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, Shlomo Dubnov.CLIP (from OpenAI) released with the paper Learning Transferable Visual Models From Natural Language Supervision by Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever.CLIPSeg (from University of Göttingen) released with the paper Image Segmentation Using Text and Image Prompts by Timo Lüddecke and Alexander Ecker.CodeGen (from Salesforce) released with the paper A Conversational Paradigm for Program Synthesis by Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong.Conditional DETR (from Microsoft Research Asia) released with the paper Conditional DETR for Fast Training Convergence by Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng, Houqiang Li, Yuhui Yuan, Lei Sun, Jingdong Wang.ConvBERT (from YituTech) released with the paper ConvBERT: Improving BERT with Span-based Dynamic Convolution by Zihang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, Shuicheng Yan.ConvNeXT (from Facebook AI) released with the paper A ConvNet for the 2020s by Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie.ConvNeXTV2 (from Facebook AI) released with the paper ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders by Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, Saining Xie.CPM (from Tsinghua University) released with the paper CPM: A Large-scale Generative Chinese Pre-trained Language Model by Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin, Yusheng Su, Haozhe Ji, Jian Guan, Fanchao Qi, Xiaozhi Wang, Yanan Zheng, Guoyang Zeng, Huanqi Cao, Shengqi Chen, Daixuan Li, Zhenbo Sun, Zhiyuan Liu, Minlie Huang, Wentao Han, Jie Tang, Juanzi Li, Xiaoyan Zhu, Maosong Sun.CPM-Ant (from OpenBMB) released by the OpenBMB.CTRL (from Salesforce) released with the paper CTRL: A Conditional Transformer Language Model for Controllable Generation by Nitish Shirish Keskar*, Bryan McCann*, Lav R. Varshney, Caiming Xiong and Richard Socher.CvT (from Microsoft) released with the paper CvT: Introducing Convolutions to Vision Transformers by Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, Lei Zhang.Data2Vec (from Facebook) released with the paper Data2Vec:  A General Framework for Self-supervised Learning in Speech, Vision and Language by Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, Michael Auli.DeBERTa (from Microsoft) released with the paper DeBERTa: Decoding-enhanced BERT with Disentangled Attention by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.DeBERTa-v2 (from Microsoft) released with the paper DeBERTa: Decoding-enhanced BERT with Disentangled Attention by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.Decision Transformer (from Berkeley/Facebook/Google) released with the paper Decision Transformer: Reinforcement Learning via Sequence Modeling by Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch.Deformable DETR (from SenseTime Research) released with the paper Deformable DETR: Deformable Transformers for End-to-End Object Detection by Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai.DeiT (from Facebook) released with the paper Training data-efficient image transformers & distillation through attention by Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Hervé Jégou.DePlot (from Google AI) released with the paper DePlot: One-shot visual language reasoning by plot-to-table translation by Fangyu Liu, Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Wenhu Chen, Nigel Collier, Yasemin Altun.DETA (from The University of Texas at Austin) released with the paper NMS Strikes Back by Jeffrey Ouyang-Zhang, Jang Hyun Cho, Xingyi Zhou, Philipp Krähenbühl.DETR (from Facebook) released with the paper End-to-End Object Detection with Transformers by Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko.DialoGPT (from Microsoft Research) released with the paper DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation by Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan.DiNAT (from SHI Labs) released with the paper Dilated Neighborhood Attention Transformer by Ali Hassani and Humphrey Shi.DINOv2 (from Meta AI) released with the paper DINOv2: Learning Robust Visual Features without Supervision by Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, Piotr Bojanowski.DistilBERT (from HuggingFace), released together with the paper DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2 into DistilGPT2, RoBERTa into DistilRoBERTa, Multilingual BERT into DistilmBERT and a German version of DistilBERT.DiT (from Microsoft Research) released with the paper DiT: Self-supervised Pre-training for Document Image Transformer by Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, Furu Wei.Donut (from NAVER), released together with the paper OCR-free Document Understanding Transformer by Geewook Kim, Teakgyu Hong, Moonbin Yim, Jeongyeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, Seunghyun Park.DPR (from Facebook) released with the paper Dense Passage Retrieval for Open-Domain Question Answering by Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih.DPT (from Intel Labs) released with the paper Vision Transformers for Dense Prediction by René Ranftl, Alexey Bochkovskiy, Vladlen Koltun.EfficientFormer (from Snap Research) released with the paper EfficientFormer: Vision Transformers at MobileNetSpeed by Yanyu Li, Geng Yuan, Yang Wen, Ju Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, Jian Ren.EfficientNet (from Google Brain) released with the paper EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks by Mingxing Tan, Quoc V. Le.ELECTRA (from Google Research/Stanford University) released with the paper ELECTRA: Pre-training text encoders as discriminators rather than generators by Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning.EnCodec (from Meta AI) released with the paper High Fidelity Neural Audio Compression by Alexandre Défossez, Jade Copet, Gabriel Synnaeve, Yossi Adi.EncoderDecoder (from Google Research) released with the paper Leveraging Pre-trained Checkpoints for Sequence Generation Tasks by Sascha Rothe, Shashi Narayan, Aliaksei Severyn.ERNIE (from Baidu) released with the paper ERNIE: Enhanced Representation through Knowledge Integration by Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, Hua Wu.ErnieM (from Baidu) released with the paper ERNIE-M: Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora by Xuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang.ESM (from Meta AI) are transformer protein language models.  ESM-1b was released with the paper Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences by Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, and Rob Fergus. ESM-1v was released with the paper Language models enable zero-shot prediction of the effects of mutations on protein function by Joshua Meier, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu and Alexander Rives. ESM-2 and ESMFold were released with the paper Language models of protein sequences at the scale of evolution enable accurate structure prediction by Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Sal Candido, Alexander Rives.Falcon (from Technology Innovation Institute) by Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme.FLAN-T5 (from Google AI) released in the repository google-research/t5x by Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason WeiFLAN-UL2 (from Google AI) released in the repository google-research/t5x by Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason WeiFlauBERT (from CNRS) released with the paper FlauBERT: Unsupervised Language Model Pre-training for French by Hang Le, Loïc Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen, Benoît Crabbé, Laurent Besacier, Didier Schwab.FLAVA (from Facebook AI) released with the paper FLAVA: A Foundational Language And Vision Alignment Model by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela.FNet (from Google Research) released with the paper FNet: Mixing Tokens with Fourier Transforms by James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon.FocalNet (from Microsoft Research) released with the paper Focal Modulation Networks by Jianwei Yang, Chunyuan Li, Xiyang Dai, Lu Yuan, Jianfeng Gao.Funnel Transformer (from CMU/Google Brain) released with the paper Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le.GIT (from Microsoft Research) released with the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, Lijuan Wang.GLPN (from KAIST) released with the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Doyeon Kim, Woonghyun Ga, Pyungwhan Ahn, Donggyu Joo, Sehwan Chun, Junmo Kim.GPT (from OpenAI) released with the paper Improving Language Understanding by Generative Pre-Training by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever.GPT Neo (from EleutherAI) released in the repository EleutherAI/gpt-neo by Sid Black, Stella Biderman, Leo Gao, Phil Wang and Connor Leahy.GPT NeoX (from EleutherAI) released with the paper GPT-NeoX-20B: An Open-Source Autoregressive Language Model by Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, Samuel WeinbachGPT NeoX Japanese (from ABEJA) released by Shinya Otani, Takayoshi Makabe, Anuj Arora, and Kyo Hattori.GPT-2 (from OpenAI) released with the paper Language Models are Unsupervised Multitask Learners by Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei** and Ilya Sutskever**.GPT-J (from EleutherAI) released in the repository kingoflolz/mesh-transformer-jax by Ben Wang and Aran Komatsuzaki.GPT-Sw3 (from AI-Sweden) released with the paper Lessons Learned from GPT-SW3: Building the First Large-Scale Generative Language Model for Swedish by Ariel Ekgren, Amaru Cuba Gyllensten, Evangelia Gogoulou, Alice Heiman, Severine Verlinden, Joey Öhman, Fredrik Carlsson, Magnus Sahlgren.GPTBigCode (from BigCode) released with the paper SantaCoder: don't reach for the stars! by Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, Joel Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco De Toni, Bernardo García del Río, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries, Leandro von Werra.GPTSAN-japanese released in the repository tanreinama/GPTSAN by Toshiyuki Sakamoto(tanreinama).Graphormer (from Microsoft) released with the paper Do Transformers Really Perform Bad for Graph Representation? by Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, Tie-Yan Liu.GroupViT (from UCSD, NVIDIA) released with the paper GroupViT: Semantic Segmentation Emerges from Text Supervision by Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, Xiaolong Wang.Hubert (from Facebook) released with the paper HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed.I-BERT (from Berkeley) released with the paper I-BERT: Integer-only BERT Quantization by Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W. Mahoney, Kurt Keutzer.ImageGPT (from OpenAI) released with the paper Generative Pretraining from Pixels by Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, Ilya Sutskever.Informer (from Beihang University, UC Berkeley, Rutgers University, SEDD Company) released with the paper Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting by Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang.InstructBLIP (from Salesforce) released with the paper InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning by Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi.Jukebox (from OpenAI) released with the paper Jukebox: A Generative Model for Music by Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, Ilya Sutskever.LayoutLM (from Microsoft Research Asia) released with the paper LayoutLM: Pre-training of Text and Layout for Document Image Understanding by Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou.LayoutLMv2 (from Microsoft Research Asia) released with the paper LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding by Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, Lidong Zhou.LayoutLMv3 (from Microsoft Research Asia) released with the paper LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking by Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, Furu Wei.LayoutXLM (from Microsoft Research Asia) released with the paper LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding by Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Furu Wei.LED (from AllenAI) released with the paper Longformer: The Long-Document Transformer by Iz Beltagy, Matthew E. Peters, Arman Cohan.LeViT (from Meta AI) released with the paper LeViT: A Vision Transformer in ConvNet's Clothing for Faster Inference by Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Hervé Jégou, Matthijs Douze.LiLT (from South China University of Technology) released with the paper LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding by Jiapeng Wang, Lianwen Jin, Kai Ding.LLaMA (from The FAIR team of Meta AI) released with the paper LLaMA: Open and Efficient Foundation Language Models by Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample.Llama2 (from The FAIR team of Meta AI) released with the paper Llama2: Open Foundation and Fine-Tuned Chat Models by Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushka rMishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing EllenTan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom.Longformer (from AllenAI) released with the paper Longformer: The Long-Document Transformer by Iz Beltagy, Matthew E. Peters, Arman Cohan.LongT5 (from Google AI) released with the paper LongT5: Efficient Text-To-Text Transformer for Long Sequences by Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, Yinfei Yang.LUKE (from Studio Ousia) released with the paper LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention by Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, Yuji Matsumoto.LXMERT (from UNC Chapel Hill) released with the paper LXMERT: Learning Cross-Modality Encoder Representations from Transformers for Open-Domain Question Answering by Hao Tan and Mohit Bansal.M-CTC-T (from Facebook) released with the paper Pseudo-Labeling For Massively Multilingual Speech Recognition by Loren Lugosch, Tatiana Likhomanenko, Gabriel Synnaeve, and Ronan Collobert.M2M100 (from Facebook) released with the paper Beyond English-Centric Multilingual Machine Translation by Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, Armand Joulin.MarianMT Machine translation models trained using OPUS data by Jörg Tiedemann. The Marian Framework is being developed by the Microsoft Translator Team.MarkupLM (from Microsoft Research Asia) released with the paper MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document Understanding by Junlong Li, Yiheng Xu, Lei Cui, Furu Wei.Mask2Former (from FAIR and UIUC) released with the paper Masked-attention Mask Transformer for Universal Image Segmentation by Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, Rohit Girdhar.MaskFormer (from Meta and UIUC) released with the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation by Bowen Cheng, Alexander G. Schwing, Alexander Kirillov.MatCha (from Google AI) released with the paper MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering by Fangyu Liu, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Yasemin Altun, Nigel Collier, Julian Martin Eisenschlos.mBART (from Facebook) released with the paper Multilingual Denoising Pre-training for Neural Machine Translation by Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, Luke Zettlemoyer.mBART-50 (from Facebook) released with the paper Multilingual Translation with Extensible Multilingual Pretraining and Finetuning by Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, Angela Fan.MEGA (from Meta/USC/CMU/SJTU) released with the paper Mega: Moving Average Equipped Gated Attention by Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer.Megatron-BERT (from NVIDIA) released with the paper Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper and Bryan Catanzaro.Megatron-GPT2 (from NVIDIA) released with the paper Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper and Bryan Catanzaro.MGP-STR (from Alibaba Research) released with the paper Multi-Granularity Prediction for Scene Text Recognition by Peng Wang, Cheng Da, and Cong Yao.mLUKE (from Studio Ousia) released with the paper mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models by Ryokan Ri, Ikuya Yamada, and Yoshimasa Tsuruoka.MMS (from Facebook) released with the paper Scaling Speech Technology to 1,000+ Languages by Vineel Pratap, Andros Tjandra, Bowen Shi, Paden Tomasello, Arun Babu, Sayani Kundu, Ali Elkahky, Zhaoheng Ni, Apoorv Vyas, Maryam Fazel-Zarandi, Alexei Baevski, Yossi Adi, Xiaohui Zhang, Wei-Ning Hsu, Alexis Conneau, Michael Auli.MobileBERT (from CMU/Google Brain) released with the paper MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices by Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou.MobileNetV1 (from Google Inc.) released with the paper MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications by Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam.MobileNetV2 (from Google Inc.) released with the paper MobileNetV2: Inverted Residuals and Linear Bottlenecks by Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen.MobileViT (from Apple) released with the paper MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer by Sachin Mehta and Mohammad Rastegari.MobileViTV2 (from Apple) released with the paper Separable Self-attention for Mobile Vision Transformers by Sachin Mehta and Mohammad Rastegari.MPNet (from Microsoft Research) released with the paper MPNet: Masked and Permuted Pre-training for Language Understanding by Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu.MPT (from MosaiML) released with the repository llm-foundry by the MosaicML NLP Team.MRA (from the University of Wisconsin - Madison) released with the paper Multi Resolution Analysis (MRA) for Approximate Self-Attention by Zhanpeng Zeng, Sourav Pal, Jeffery Kline, Glenn M Fung, Vikas Singh.MT5 (from Google AI) released with the paper mT5: A massively multilingual pre-trained text-to-text transformer by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel.MusicGen (from Meta) released with the paper Simple and Controllable Music Generation by Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi and Alexandre Défossez.MVP (from RUC AI Box) released with the paper MVP: Multi-task Supervised Pre-training for Natural Language Generation by Tianyi Tang, Junyi Li, Wayne Xin Zhao and Ji-Rong Wen.NAT (from SHI Labs) released with the paper Neighborhood Attention Transformer by Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and Humphrey Shi.Nezha (from Huawei Noah’s Ark Lab) released with the paper NEZHA: Neural Contextualized Representation for Chinese Language Understanding by Junqiu Wei, Xiaozhe Ren, Xiaoguang Li, Wenyong Huang, Yi Liao, Yasheng Wang, Jiashu Lin, Xin Jiang, Xiao Chen and Qun Liu.NLLB (from Meta) released with the paper No Language Left Behind: Scaling Human-Centered Machine Translation by the NLLB team.NLLB-MOE (from Meta) released with the paper No Language Left Behind: Scaling Human-Centered Machine Translation by the NLLB team.Nyströmformer (from the University of Wisconsin - Madison) released with the paper Nyströmformer: A Nyström-Based Algorithm for Approximating Self-Attention by Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, Vikas Singh.OneFormer (from SHI Labs) released with the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov, Humphrey Shi.OpenLlama (from s-JoL) released in Open-Llama.OPT (from Meta AI) released with the paper OPT: Open Pre-trained Transformer Language Models by Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen et al.OWL-ViT (from Google AI) released with the paper Simple Open-Vocabulary Object Detection with Vision Transformers by Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby.Pegasus (from Google) released with the paper PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu.PEGASUS-X (from Google) released with the paper Investigating Efficiently Extending Transformers for Long Input Summarization by Jason Phang, Yao Zhao, and Peter J. Liu.Perceiver IO (from Deepmind) released with the paper Perceiver IO: A General Architecture for Structured Inputs & Outputs by Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier Hénaff, Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, João Carreira.PhoBERT (from VinAI Research) released with the paper PhoBERT: Pre-trained language models for Vietnamese by Dat Quoc Nguyen and Anh Tuan Nguyen.Pix2Struct (from Google) released with the paper Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding by Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, Kristina Toutanova.PLBart (from UCLA NLP) released with the paper Unified Pre-training for Program Understanding and Generation by Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, Kai-Wei Chang.PoolFormer (from Sea AI Labs) released with the paper MetaFormer is Actually What You Need for Vision by Yu, Weihao and Luo, Mi and Zhou, Pan and Si, Chenyang and Zhou, Yichen and Wang, Xinchao and Feng, Jiashi and Yan, Shuicheng.ProphetNet (from Microsoft Research) released with the paper ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang and Ming Zhou.PVT (from Nanjing University, The University of Hong Kong etc.) released with the paper Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions by Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao.QDQBert (from NVIDIA) released with the paper Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation by Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev and Paulius Micikevicius.RAG (from Facebook) released with the paper Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela.REALM (from Google Research) released with the paper REALM: Retrieval-Augmented Language Model Pre-Training by Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat and Ming-Wei Chang.Reformer (from Google Research) released with the paper Reformer: The Efficient Transformer by Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya.RegNet (from META Platforms) released with the paper Designing Network Design Space by Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, Piotr Dollár.RemBERT (from Google Research) released with the paper Rethinking embedding coupling in pre-trained language models by Hyung Won Chung, Thibault Févry, Henry Tsai, M. Johnson, Sebastian Ruder.ResNet (from Microsoft Research) released with the paper Deep Residual Learning for Image Recognition by Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun.RoBERTa (from Facebook), released together with the paper RoBERTa: A Robustly Optimized BERT Pretraining Approach by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.RoBERTa-PreLayerNorm (from Facebook) released with the paper fairseq: A Fast, Extensible Toolkit for Sequence Modeling by Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli.RoCBert (from WeChatAI) released with the paper RoCBert: Robust Chinese Bert with Multimodal Contrastive Pretraining by HuiSu, WeiweiShi, XiaoyuShen, XiaoZhou, TuoJi, JiaruiFang, JieZhou.RoFormer (from ZhuiyiTechnology), released together with the paper RoFormer: Enhanced Transformer with Rotary Position Embedding by Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu.RWKV (from Bo Peng), released on this repo by Bo Peng.SegFormer (from NVIDIA) released with the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping Luo.Segment Anything (from Meta AI) released with the paper Segment Anything by Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alex Berg, Wan-Yen Lo, Piotr Dollar, Ross Girshick.SEW (from ASAPP) released with the paper Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition by Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.SEW-D (from ASAPP) released with the paper Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition by Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.SpeechT5 (from Microsoft Research) released with the paper SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing by Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei.SpeechToTextTransformer (from Facebook), released together with the paper fairseq S2T: Fast Speech-to-Text Modeling with fairseq by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Dmytro Okhonko, Juan Pino.SpeechToTextTransformer2 (from Facebook), released together with the paper Large-Scale Self- and Semi-Supervised Learning for Speech Translation by Changhan Wang, Anne Wu, Juan Pino, Alexei Baevski, Michael Auli, Alexis Conneau.Splinter (from Tel Aviv University), released together with the paper Few-Shot Question Answering by Pretraining Span Selection by Ori Ram, Yuval Kirstain, Jonathan Berant, Amir Globerson, Omer Levy.SqueezeBERT (from Berkeley) released with the paper SqueezeBERT: What can computer vision teach NLP about efficient neural networks? by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer.SwiftFormer (from MBZUAI) released with the paper SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications by Abdelrahman Shaker, Muhammad Maaz, Hanoona Rasheed, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan.Swin Transformer (from Microsoft) released with the paper Swin Transformer: Hierarchical Vision Transformer using Shifted Windows by Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo.Swin Transformer V2 (from Microsoft) released with the paper Swin Transformer V2: Scaling Up Capacity and Resolution by Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, Baining Guo.Swin2SR (from University of Würzburg) released with the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Marcos V. Conde, Ui-Jin Choi, Maxime Burchi, Radu Timofte.SwitchTransformers (from Google) released with the paper Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity by William Fedus, Barret Zoph, Noam Shazeer.T5 (from Google AI) released with the paper Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu.T5v1.1 (from Google AI) released in the repository google-research/text-to-text-transfer-transformer by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu.Table Transformer (from Microsoft Research) released with the paper PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents by Brandon Smock, Rohith Pesala, Robin Abraham.TAPAS (from Google AI) released with the paper TAPAS: Weakly Supervised Table Parsing via Pre-training by Jonathan Herzig, Paweł Krzysztof Nowak, Thomas Müller, Francesco Piccinno and Julian Martin Eisenschlos.TAPEX (from Microsoft Research) released with the paper TAPEX: Table Pre-training via Learning a Neural SQL Executor by Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou.Time Series Transformer (from HuggingFace).TimeSformer (from Facebook) released with the paper Is Space-Time Attention All You Need for Video Understanding? by Gedas Bertasius, Heng Wang, Lorenzo Torresani.Trajectory Transformer (from the University of California at Berkeley) released with the paper Offline Reinforcement Learning as One Big Sequence Modeling Problem by Michael Janner, Qiyang Li, Sergey LevineTransformer-XL (from Google/CMU) released with the paper Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context by Zihang Dai*, Zhilin Yang*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.TrOCR (from Microsoft), released together with the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, Furu Wei.TVLT (from UNC Chapel Hill) released with the paper TVLT: Textless Vision-Language Transformer by Zineng Tang, Jaemin Cho, Yixin Nie, Mohit Bansal.UL2 (from Google Research) released with the paper Unifying Language Learning Paradigms by Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, Donald MetzlerUMT5 (from Google Research) released with the paper UniMax: Fairer and More Effective Language Sampling for Large-Scale Multilingual Pretraining by Hyung Won Chung, Xavier Garcia, Adam Roberts, Yi Tay, Orhan Firat, Sharan Narang, Noah Constant.UniSpeech (from Microsoft Research) released with the paper UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael Zeng, Xuedong Huang.UniSpeechSat (from Microsoft Research) released with the paper UNISPEECH-SAT: UNIVERSAL SPEECH REPRESENTATION LEARNING WITH SPEAKER AWARE PRE-TRAINING by Sanyuan Chen, Yu Wu, Chengyi Wang, Zhengyang Chen, Zhuo Chen, Shujie Liu, Jian Wu, Yao Qian, Furu Wei, Jinyu Li, Xiangzhan Yu.UPerNet (from Peking University) released with the paper Unified Perceptual Parsing for Scene Understanding by Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, Jian Sun.VAN (from Tsinghua University and Nankai University) released with the paper Visual Attention Network by Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, Shi-Min Hu.VideoMAE (from Multimedia Computing Group, Nanjing University) released with the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Zhan Tong, Yibing Song, Jue Wang, Limin Wang.ViLT (from NAVER AI Lab/Kakao Enterprise/Kakao Brain) released with the paper ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision by Wonjae Kim, Bokyung Son, Ildoo Kim.Vision Transformer (ViT) (from Google AI) released with the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.VisualBERT (from UCLA NLP) released with the paper VisualBERT: A Simple and Performant Baseline for Vision and Language by Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang.ViT Hybrid (from Google AI) released with the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.ViTMAE (from Meta AI) released with the paper Masked Autoencoders Are Scalable Vision Learners by Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick.ViTMSN (from Meta AI) released with the paper Masked Siamese Networks for Label-Efficient Learning by Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes, Pascal Vincent, Armand Joulin, Michael Rabbat, Nicolas Ballas.ViViT (from Google Research) released with the paper ViViT: A Video Vision Transformer by Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lučić, Cordelia Schmid.Wav2Vec2 (from Facebook AI) released with the paper wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.Wav2Vec2-Conformer (from Facebook AI) released with the paper FAIRSEQ S2T: Fast Speech-to-Text Modeling with FAIRSEQ by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Sravya Popuri, Dmytro Okhonko, Juan Pino.Wav2Vec2Phoneme (from Facebook AI) released with the paper Simple and Effective Zero-shot Cross-lingual Phoneme Recognition by Qiantong Xu, Alexei Baevski, Michael Auli.WavLM (from Microsoft Research) released with the paper WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing by Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Furu Wei.Whisper (from OpenAI) released with the paper Robust Speech Recognition via Large-Scale Weak Supervision by Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever.X-CLIP (from Microsoft Research) released with the paper Expanding Language-Image Pretrained Models for General Video Recognition by Bolin Ni, Houwen Peng, Minghao Chen, Songyang Zhang, Gaofeng Meng, Jianlong Fu, Shiming Xiang, Haibin Ling.X-MOD (from Meta AI) released with the paper Lifting the Curse of Multilinguality by Pre-training Modular Transformers by Jonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James Cross, Sebastian Riedel, Mikel Artetxe.XGLM (From Facebook AI) released with the paper Few-shot Learning with Multilingual Language Models by Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, Xian Li.XLM (from Facebook) released together with the paper Cross-lingual Language Model Pretraining by Guillaume Lample and Alexis Conneau.XLM-ProphetNet (from Microsoft Research) released with the paper ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang and Ming Zhou.XLM-RoBERTa (from Facebook AI), released together with the paper Unsupervised Cross-lingual Representation Learning at Scale by Alexis Conneau*, Kartikay Khandelwal*, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov.XLM-RoBERTa-XL (from Facebook AI), released together with the paper Larger-Scale Transformers for Multilingual Masked Language Modeling by Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, Alexis Conneau.XLM-V (from Meta AI) released with the paper XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models by Davis Liang, Hila Gonen, Yuning Mao, Rui Hou, Naman Goyal, Marjan Ghazvininejad, Luke Zettlemoyer, Madian Khabsa.XLNet (from Google/CMU) released with the paper ​XLNet: Generalized Autoregressive Pretraining for Language Understanding by Zhilin Yang*, Zihang Dai*, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.XLS-R (from Facebook AI) released with the paper XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale by Arun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman Goyal, Kritika Singh, Patrick von Platen, Yatharth Saraf, Juan Pino, Alexei Baevski, Alexis Conneau, Michael Auli.XLSR-Wav2Vec2 (from Facebook AI) released with the paper Unsupervised Cross-Lingual Representation Learning For Speech Recognition by Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, Michael Auli.YOLOS (from Huazhong University of Science & Technology) released with the paper You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Yuxin Fang, Bencheng Liao, Xinggang Wang, Jiemin Fang, Jiyang Qi, Rui Wu, Jianwei Niu, Wenyu Liu.YOSO (from the University of Wisconsin - Madison) released with the paper You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling by Zhanpeng Zeng, Yunyang Xiong, Sathya N. Ravi, Shailesh Acharya, Glenn Fung, Vikas Singh.Want to contribute a new model? We have added a detailed guide and templates to guide you in the process of adding a new model. You can find them in the templates folder of the repository. Be sure to check the contributing guidelines and contact the maintainers or open an issue to collect feedbacks before starting your PR.To check if each model has an implementation in Flax, PyTorch or TensorFlow, or has an associated tokenizer backed by the 🤗 Tokenizers library, refer to this table.These implementations have been tested on several datasets (see the example scripts) and should match the performance of the original implementations. You can find more details on performance in the Examples section of the documentation.Learn moreSectionDescriptionDocumentationFull API documentation and tutorialsTask summaryTasks supported by 🤗 TransformersPreprocessing tutorialUsing the Tokenizer class to prepare data for the modelsTraining and fine-tuningUsing the models provided by 🤗 Transformers in a PyTorch/TensorFlow training loop and the Trainer APIQuick tour: Fine-tuning/usage scriptsExample scripts for fine-tuning models on a wide range of tasksModel sharing and uploadingUpload and share your fine-tuned models with the communityCitationWe now have a paper you can cite for the 🤗 Transformers library:@inproceedings{wolf-etal-2020-transformers,    title = \""Transformers: State-of-the-Art Natural Language Processing\"",    author = \""Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush\"",    booktitle = \""Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\"",    month = oct,    year = \""2020\"",    address = \""Online\"",    publisher = \""Association for Computational Linguistics\"",    url = \""https://www.aclweb.org/anthology/2020.emnlp-demos.6\"",    pages = \""38--45\""}"
3,shadowsocks/shadowsocks,https://github.com/shadowsocks/shadowsocks/blob/rm/README.md,Python,Removed according to regulations.
4,zero-to-mastery/start-here-guidelines,https://github.com/zero-to-mastery/start-here-guidelines/blob/master/README.md,Python,"One rule of this community:We don't care if you break things. This is a playground, and we encourage failing often. Use this as a practice ground, and enjoy contributing to projects you create with your fellow students. Many students have gained real-world experience \""working in teams\"" by working on these projects.A Guide to Get Started (used to be the 4 step guide)Check out Andrei's videos on github if you haven't watched it already.On the GitHub page for this repository, click on the button \""Fork.\""Clone your forked repository to your computer:For example, run this command inside your terminal:git clone https://github.com/<your-github-username>/start-here-guidelines.gitReplace <your-github-username>!Learn more about forking and cloning a repo.Move to project directory:cd start-here-guidelinesBefore you make any changes, keep your fork in sync to avoid merge conflicts:git remote add upstream https://github.com/zero-to-mastery/start-here-guidelines.gitgit pull upstream masterIf you run into a merge conflict, you have to resolve the conflict. There are a lot of guides online, or you can watch this tutorial.After adding the upstream and checking that all files are up to date, we now will create new branch before editing any files. There are two ways to do so:git checkout -b <branch-name>git branch <branch-name>git switch <branch-name>On your computer, open your text editor, and add your name to the CONTRIBUTORS.md file.⚠️ IMPORTANT NOTE #1: Add your name somewhere in the middle. Not at the top or bottom in order to avoid the chance of you getting a merge conflict!⚠️ IMPORTANT NOTE #2: Please do NOT edit or remove other people from the list, even to fix their indentation etc. This will likely prevent your PR from being merged.Add the changes with git add, git commit (write a good commit message, if possible):git add CONTRIBUTORS.mdgit commit -m \""Add <your-github-username>\""Replace <your-github-username>!Push your changes to your repository:git push origin <branch-name>Go to the GitHub page of your fork, and make a pull request:Read more about pull requests on the GitHub help pages.Wait until Zerobot or one of the maintainers merges your pull request. If there are any conflicts, you will get a notification and be required to resolve the conflict.Go join a project and start contributing or create your own group apps. Don't be shy and enjoy creating things together (We have over 20 projects for all levels of programmers)! Check out this guide for more information on selecting a project.To see the Zero to Mastery Icon in your GitHub profile, follow these steps (you must complete steps 1 and 2 for this to work).Anatomy of an open-source project:Every open-source community is different.Spending years on one open-source project means you’ve gotten to know one open-source project. Move to a different project, and you might find the vocabulary, norms, and communication styles are completely different.That being said, many open-source projects follow a similar organizational structure. Understanding the different community roles and overall process will help you get quickly oriented to any new project.A typical open-source project has the following types of people:Author: The person(s) or organization that created the project.Owner: The person(s) who has administrative ownership over the organization or repository (not always the same as the original author).Maintainers: Contributors who are responsible for driving the vision and managing the organizational aspects of the project (may also be authors or owners of the project).Contributors: Everyone who has contributed something back to the project.Community Members: People who use the project. They might be active in conversations or express their opinion on the project’s direction.Bigger projects may also have subcommittees or working groups focused on different tasks, such as tooling, triage, community moderation, and event organizing. Look on a project’s website for a “team” page or in the repository for governance documentation to find this information.A project also has documentation. These files are usually listed in the top level of a repository.LICENSE: By definition, every open-source project must have an open-source license. If the project does not have a license, it is not open source.README: The README is the instruction manual that welcomes new community members to the project. It explains why the project is useful and how to get started.CONTRIBUTING: Whereas READMEs help people use the project, contributing docs help people contribute to the project. It explains what types of contributions are needed and how the process works. While not every project has a CONTRIBUTING file, its presence signals that this is a welcoming project to contribute to.CODE_OF_CONDUCT: The code of conduct sets ground rules for participants’ behavior and helps to facilitate a friendly, welcoming environment. While not every project has a CODE_OF_CONDUCT file, its presence signals that this is a welcoming project to contribute to.Other documentation: There might be additional documentation such as tutorials, walkthroughs, or governance policies, especially on bigger projects.Finally, open-source projects use the following tools to organize discussion. Reading through the archives will give you a good picture of how the community thinks and works.Issue tracker: Where people discuss issues related to the project.Pull requests: Where people discuss and review changes that are in progress.Discussion forums or mailing lists: Some projects may use these channels for conversational topics (for example, “How do I…“ or “What do you think about…“ instead of bug reports or feature requests). Others use the issue tracker for all conversations.Synchronous chat channel: Some projects use chat channels (such as Discord or IRC) for casual conversation, collaboration, and quick exchanges.Get all the ZTM Courses, for one monthly subscription here."
5,Ebazhanov/linkedin-skill-assessments-quizzes,https://github.com/Ebazhanov/linkedin-skill-assessments-quizzes/blob/main/README.md,Python,"Linkedin Skill assessments - Answers⚠️ DISCLAIMER: The owners of this repository are not liable for any illegal usage of the content provided in this repository. The content is provided for informational and educational purposes only, and any actions taken by users of this repository are the responsibility of the user. By accessing this repository, you agree to hold the owners harmless from any claims, damages, or expenses arising from the use of the information provided.[ Go to see the last contributor ]🙏 PLEASEAlways add explanation (or reference link) to your answers. Use online grammar checker.That would help anyone to better learn new concepts!🎉 AnnouncementsColumn Translation have links to quizzes in different languages like Es, Fr, It and De.If you want to meet each other or discuss quiz related problems or maybe ask for skills endorsement just join the Discord chat.Playground before taking quiz using:MD2Practice (Web App)Skill Assessments Quizzes (Web App)LinkedIn Quizzed with Kodyfire (Terminal)Want to contribute? Here is the source code.❓ Need help?Open new issue🔥 Open in VS Code view here or thereTable of ContentsLinkedin-quiz-questionsPassed/FailedTranslated  in ...QuestionsAnswersYour resource for answers. In case you have doubts please contact this person or add them to review your PR.Accounting❗needs updating  5049@tujinwei, @mervynteo, @johnfelipeAdobe-Acrobat  2722Adobe-Illustrator❗needs updating  7674Adobe-InDesign❗needs updating  4240Adobe-Lightroom❗needs updating  2020Adobe-Photoshop❗needs updating  9393@declarckAdobe Premiere Pro  4836Adobe XD  1613After Effects❗needs updating  2413Agile Methodologies❗needs updating  116116@its0x08Android  7272@surajsahani, @mr-shoxruxbek, @ItSNeverLateAngular  7965@vanekbr, @aamita96ArcGIS Products55AutoCAD❗needs updating  7775@djayorAutodesk Fusion 360❗needs updating  3725@djayor, @tm-sanjayAutodesk Maya3030@marifogluAWS  9999@jokerkeny, @Amsal1AWS-Lambda❗needs updating  5149Bash  7877@D4RIO, @Amsal1C#6161@LiviuSosu, @RamonMartinezNieto, @declarckC++❗needs updating7373@Amsal1, @Amsal1C (Programming Language)8383@makifay, @Amsal1, @its0x08CSS122116@BHARGAVPATEL1244Cybersecurity❗needs updating10196Django7171@PROCW.NET Framework6359@declarckEclipse❗needs updating3628Front-end Development6868@vanekbr, @ShankS3, @declarckGit134134@Emanuele-emGo (Programming Language)4040@ruslanbes, @monkrusGoogle Ads2925Google Analytics8282Google Cloud Platform (GCP)5250@antra0497Hadoop7154HTML129128@declarckIT Operations5454@asahioceanJava130130@sumanas27, @ruslanbes, @PROCWJavascript131131@taletski, @PROCW, @msteiner96, @declarckjQuery8477@declarckJSON❗needs updating8786@iHamzaKhanzadaKeynote140Kotlin7878@ItSNeverLate, @HusseinhjLinux8278@D4RIO, @Amsal1Logic Pro8278Machine Learning9898@aaronwangj, @antra0497MATLAB7070@tm-sanjayMaven5350Microsoft Access3028@drmegalomaniacMicrosoft Azure5553@tomtreffke, @ziasistaniMicrosoft Excel❗needs updating109107@gazihasanrahmanMicrosoft Outlook7956Microsoft Power Automate1402@mervynteoMicrosoft Power BI8180@vittorio-giattiMicrosoft Power Point8577@ckulloMicrosoft Project❗needs updating4443Microsoft Word❗needs updating7877MongoDB7777MySQL9797@ruslanbesnode.js7976@pbachmanNoSQL5655objective-c4038OOP10282@declarck, @gaurovgiriPHP8979@ruslanbes, @msteiner96Pro Tools22Python176176@tik9, @Amsal1, @declarck, @TSG405QuickBooks❗needs updating6739R5252@gregglindReact.js100100@RobTables @bandinoplaREST API6565Revit❗needs updating140Ruby on Rails5959@gudataRust3232@BobbyByrne @Emanuele-emScala5248Search Engine Optimization (SEO)8181SharePoint❗needs updating5338Sketchup22SOLIDWORKS❗needs updating5757@BHARGAVPATEL1244Spring Framework6767Swift6767Transact-SQL (T-SQL)4542@beefydog, @BenVlodgiUnity❗needs updating4746@uno-sebastianVisual Basic for Applications (VBA)❗needs updating3634@AdamKaczor6250Visio3535Windows Server6857WordPress8073@ruslanbes, @Amsal1XML4342@ruslanbesContributors ✨Thanks goes to these wonderful people (emoji key):            Evgenii💻 🖋      Sergei Stadnik💻 🔍 🤔 📖      Santhosh💻      Jacob Dsa💻 🖋      Aaron Meese💻 🖋      arqarq💻 🖋      Amit Yadav💻 🖋              Javokhir Nazarov💻 🖋      saurav kumar🖋      Chetan🖋      Amir Hossein Shekari🎨 🖋 💻      SergDaut🎨      Nilotpal Pramanik🎨 💻 🖋 💼 📖 🔣 💡      Abhishek Kumar🎨              Monu Gupta🎨      KARTIKEYA GUPTA💻 🖋      kenkyusha💻 🖋      juandavidtowers💻 🖋      cyber-netics💻 🖋      jtrisw💻 🖋      Renato Regalado💻 🖋              Matthew💻 🖋      Jan S.💻 🖋      Manoli💻 🖋      Faraz tanveer💻 🖋      mohnishkarri💻 🖋 🎨      andyzhu💻 🖋      Vishal Kushwah💻 🖋              Yurii Yakymenko💻 🖋      Swetabh Suman💻 🖋      AJAY DANDGE💻 🖋      Mehmet Yesin🎨      Lok Chun Wai🎨      Adria de Juan🎨      GL-Man🎨              Jheel Patel🎨      Sameer Waskar🎨      Alexander Andrews🎨      Alexander Maxwell🎨      Slava🎨      Mayur Khatri🎨      Mascantosh💻 🖋 📢 🤔              Kivanc Enes🎨      Ritika Das🎨      Zer07793🎨      Andrew Cheung🎨      Sadha🎨      tainenko🎨 💻      github-star-coder🎨              Danilo Oliveira🎨      lordeko🎨      Shubham Kumar🎨 💻      testtree🎨      Cheryl Murphy🎨 💻      Bipin Thomas🎨      Abdulrahman Hisham🎨              Dakshitha Dissanayaka🎨      BADR KACIMI🎨      Alex Wang🎨      Maxim🎨      GordonGrant🎨 💻      Ephrem Demelash🎨      JonOrcutt🎨              topdev10🎨      cookwellwebsite🎨      xren935🎨      Nemo Frenkel🎨      MD SAIF ALAM🎨      Boris López Araya🎨      Larry Chiem🎨              Muhammad Bilal Ilyas🎨      AliMilani🎨 💻      Suraj Sahani🎨      FlyingSquirrel🎨      Erick Tijero🎨      Jaskaran Kukreja🎨      MichaelL🎨              MagicLegend🎨      Dereck Bearsong🎨      Pappu Kumar Pashi🎨      Venkata Kishore Tavva🎨      Rafat Touqir Rafsun🎨      Snehesh Dutta🎨      Timo Körner🎨 💻              alexxxan🎨      GGJason🎨      LeeAnna Ewing🎨 🤔      kamal Jyotwal🎨      Bob-Johns🎨 💻 🖋      yunussalmanlyit🎨 💻      chilcot🎨 💻              Jacky Li💻 🖋 🎨      Sarthak Trivedi🎨      Ayush Aggarwal🎨 💻      Nic Ballarini🎨      Luigi Zambetti🎨 💻      govindhaswin🎨      Addy Roy💻 🎨              Akshat Tamrakar🎨 💻      Sai Bhargava Ramu🎨      Gurkan💻      Spencer Hayes-Laverdiere💻      Aniket Soni💻      tanmay5792💻      Dina Taklit💻 🎨 🖋              Dushyant Singh💻      Ravi Prakash Singh💻      Nihal Joshi💻      Guy Klages💻      Arvind🎨 💻      mujeeb91💻      joserca🎨 💻              Prateek Agrawal💻      Teoh Tze Chuin(サラ)💻 🎨      Jayant Jain💻      Ayush Sahu💻      Hridya Krishna R💻 🎨      Rahul Bali💻 🎨      S.ZHeng🎨 💻 💼              Shriya Madan🎨 💻      mahalrupi🎨      Lucas Lermagne🎨      Jeff Deutsch🎨 💻      Betoxx1🎨      Wingman4l7🎨      Martin Espericueta🎨              Mh-Tahir🎨      Zdravko Šplajt🎨 💻      Ms3105🎨 💻 🖋      Ambika Sidhesware💻      mundoguero💻      Darkus24🖋      Sou-786🖋 🎨              Banurekha🖋      ShiraStarL🎨      Ilya Komarov🎨      DemigodMs🖋 📖      Mekha Hridya🎨 🔍      Andrey Safonov🎨 🔍      Tommaso🎨 💻              Jessica Salbert💻 🎨      JAYANTH DOLAI💻 🎨      silverstroom💻 🎨 💼      Furkan Sayım💻 🎨      Sukumar Chandrasekaran🎨      Yejin Park🎨 💻      Ali Nooshabadi🎨 💻              imitavor🎨 💻      Salih Kilicli🎨 💻      Marcelo Meneses🎨 💻      Anton Krekotun🎨 🚧 🖋 💻 📖 💼      Arnav Sarma💻 💡 🎨      meghatiku💻 🎨      Anshu Trivedi🎨              Taylor Dorsett💻 🖋 🎨      Havit Rovik💻      pushpapune💻 🎨      Ramtin Radfar🎨 🤔 💼 💵 💻 🖋 💬      Abdulmajeed Isa💻 🎨      vikassaxena02🎨      RobTables🎨 💻 💼              Daniel🎨 💻 💼 🔍      Zahid Ali💻 🎨      Chad Chai💻 🎨      Marco Biedermann💻 🎨 💼 🤔      Srinidhi Murthy🎨      Miao Cai💻 🎨      Dionicio Diaz🎨 💻              Mir Monoarul Alam🎨      Shawn Ohn💻 🎨      Amanbolat Balabekov🎨 💻      black-mamba-code💻      Jian-forks🎨 💻      shivani patel🎨      Akash Chowrasia🎨              yairg98🎨      Jay Gajjar🎨      coolerbooler💻      Md Zinnatul Islam Morol🎨      shresthashok550🎨 📖      Alan Pallath📖      Adrian Wong💻              vsDizzy💻 🎨      Frex Cuadillera🎨 💻      ashish570💻 🎨      ruchpeanuts💻 🎨      Artmasque🎨 💻      Amirhossein Mojiri Foroushani🎨      for💻 🎨              Luke🎨 💻      Hector Espinoza🎨      Adrián Buenfil🎨 💻      Amit Kumar🎨      schoppfe🎨 💻      Sofiyal C🎨 💻      spitlisk💻 🎨              PRAVIN SHARMA🎨      NIDZAAA1🎨 💻      John Mai🎨 💻      kimsoyeong🎨      Dona Ghosh💻      Ryan Hill🎨 💻      j42z🎨 💻              Ashish Sangale🎨 💻      Derek Yang🎨 💻      mohsinmsm🎨 💻      Gokulkrish2302💻      Bhaavishek💻 🎨      Louis Liao🎨      sengc92🎨 💻              Alex Marvin🎨      Balkrishna Bhatt🎨 💻      Evaldas Lavrinovičius🎨 💻      Adam Erchegyi🎨 💻      Truman Hung🎨 💻      rzamora11🎨      gaurav0224🎨              Lee GyeongJun🎨      Mirek🎨 💻      surajm245🎨      ArisLaode🎨 💻      RaviDhoriya🎨 💻      sarai-84🎨 💻      Vishnu🎨 💻              Muhammad Minhaj💻      Chandrika Deb🎨 💻      Gitgit101-bit💻 🎨      Hedi Sellami💻 🎨      saurabhvaish93💻 🎨      Nikola Begovic💻 🎨      Wang💻 🎨              Manuel Eusebio de Paz Carmona🎨      Basim Al-Jawahery🎨 💻      RAJA AHMED🎨 💻      Abhik Lodh💻      Md. Pial Ahamed💻 🎨      Hassan Shahzad💻 🎨      Christian Sosa Gago💻              Hasnain Rasheed💻 🎨      T-Radford💻      dahiyashish💻 🎨      RahulSharma468💻 🎨      Jumpod Plekhongthu💻 🎨      Thomas Young-Audet💻 🎨      VinayagamBabu💻 🎨              Deniz Koç💻 🎨      Azhar Khan💻 🎨 🖋 📖 🔣 🚧      Jacob Short💻 🎨      Uchimura85💻 🎨      Leo Nugraha💻 🎨 📖      Mujtaba Mehdi📖 🖋      Jim-ds💻 🎨              Sreehari K💻 🎨      Florian Martinez💻 🎨      Aaron💻 🎨      apoage🎨      Ignacio Guillermo Martinez 💻 🎨      AirlineDog🎨 💻      Mekel🎨 💻              hmosharrof🎨 💻      Ben Emamian💻 🎨      babeshark💻 🎨      Leonardo Jaques💻 🎨      Stefanos Apkarian💻 🎨      Ayhan Albayrak💻 🎨      KidusMT💻 🎨              hectormarroquin20💻 🎨      Edelweiss35💻 🎨      MihaiD💻 🎨      AnveshReddyAnnem💻 🎨      Hyunjae Park💻 🎨      Rajiv Albino💻 🎨      Atishay💻              Yusuf Naheem🎨      Windu🎨 💻      Superv1sor💻 🎨      Karine (:🎨 💻      Eduard Pech🎨 💻      jjeshwani🎨 💻      Steve🎨 💻              Aleigh Ohslund💻      Abhinav Suman🎨 💻      Hamza Ehtesham Farooq🎨 💻      IamNotPeterPan💻 💵 🎨      Cetger🎨      pkonopacki🎨      Yang Yang🎨 💻              Muhammad Shoaib Sarwar💻      Murilo Henrique💻 🎨      emilianoalvz🎨 💻      Sumana Saha🎨 💻      Yurii17K🎨 💻      Rupesh Bhandari🎨 💻      salmos3718💻              John Baker🎨 💻      SanjaySathiraju🎨 💻      Donat Kabashi🎨      Arul Prasad J🎨 💻      Qi Chen🎨 💻      Maksym Dmyterko🎨 💻      ilovepullrequests💻              Samira Maleki🎨 💻      NIKITA MAHOVIYA💻      jesuisdev.Net🎨 💻      Ashraf Nazar🎨      Naveed Ahmad🎨      Ajmain Naqib🎨 💻      Avinash Tingre💻 🎨              nicktids🎨      Keith Dinh💻 🎨      André Ferreira💻 🎨      eliottkespi💻 🎨      praveenpno💻 🎨      vitowidigdo💻 🎨      Devesh Pratap Singh💻 🎨              Dario Rodriguez💻 🎨      charmander_didi💻 🎨      PHBasin💻 🎨      Ritvik Singh Chauhan💻 🎨      Riya P Mathew💻 🎨      Stephanie Cherubin💻 🎨      BenitesGui💻 🎨              FarikBear💻 🎨      Dmytro Havrilov💻 🎨      Parvesh Monu💻 🎨      Dipen Panchasara💻 🎨      gudata🎨 💻      gawadeditor💻 🎨      Kirill Taletski🎨 💻              Saajan🎨 💻      Kushagra S🎨 💻      Oanh Le🎨 💻      Frane Medvidović🎨 💻      Yorman🎨 💻      Bill Chan🎨 💻      Pratik Lomte🎨 💻              LOC LAM🎨 💻      TUSAR RANJAN MAHAPATRA💻      BhargavKanjarla💻      Karel De Smet💻 🎨      sidisan🎨      ygnzayarphyo🎨 💻      svansteelandt💻              Kebechet🎨      Daniel Selvan D🎨 💻      Mahdi Razavi🎨 💻      Niklas Tiede💻 🎨      narutubaderddin💻 🎨      dylandhood💻      Dheeraj Gupta💻              Pieter Claerhout💻 🎨      Shivam Agnihotri💻      RanjithReddy-Narra💻      Nikita Wadhwani🎨 💻      rsholokh💻 🎨      Ayaan Hossain💻 🎨      Rajesh Swarna💻              Deniz Etkar🎨 💻      pro335💻 🎨      Jakub Radzik💻 🎨      Hamza Khanzada💻      ARNON🎨      Vikram Singh💻      Shoxruxbek💻 🎨              Amit Khatri💻 🎨      Wali Ullah🎨 💻      Amit11794💻 🎨      metis-macys-66898💻 🎨      Faisal Maqbool🎨 💻      Kumar Neeraj💻 🎨      Maurizio Marini🎨 💻              Saket Kothari🎨 💻      Szymon Zborowski🎨 💻      iks3000🎨 💻      Ehsan Seyedi🎨 💻      vanekbr🎨 💻      Princy_M🎨 💻      Shijie Zhou🎨 💻              lakshyamcs16🎨 💻      Filippo Facco🎨 💻      mendel5🎨 💻      Patryk🎨 💻      VishwaSangani🎨 💻      Alvin Zhao🎨 💻      Lazar Gugleta🎨 💻              vmicho🎨 💻      Sikandar Ali🎨 💻      Raja Babu🎨 💻      faizajahanzeb💻      Guil_AiT🎨 💻      Kushal Das🎨 💻      Luis Bonilla🎨 💻              jovan1013🎨 💻      Damian🎨 💻      Yash Gupta💻      lolcatnip🎨 💻      Ikko Ashimine🎨 💻      Farukh🎨 💻      Moksedul💻 🎨              Navneet Kumar🎨 💻      Saqib AlMalik💻      fahimrahman🎨 💻      vaibhav patil🎨 💻      Rahul Madan🎨 💻      kartik Kaklotar🎨 💻      ASAHI OCEAN🎨 💻              Daniel Jungbluth🎨 💻      Rajdeep Singh Borana🎨 💻      ankitha19💻      Linh Tran💻      islamarr💻 🎨      Mohamed Sabith🎨 💻      Miguel Angel Cruz Acosta🎨 💻              Adebayo Ilerioluwa 🎨      Markus🎨 💻      dkonyayev🎨 💻      Kevin A Mathew🎨 💻      David Melo🎨 🔣      DFW1N🎨 💻      Sohaib Ayub🎨 💻              Navvy🎨 💻      bloodiator2🎨 💻      Hanji🎨 💻      arthur74🎨 💻      Sri Subathra Devi B🎨 💻      Akif Aydogmus🎨 💻      Umer Javaid🎨 💻              Norio Umata🎨 💻      Gazi Hasan Rahman🎨 💻      Keith Nguyen🎨 💻      Megalomaniac🎨 💻      ShankS3🎨 💻      Farhad Alishov🎨 💻      Ronak J Vanpariya🎨 💻              azrael0learza🎨 💻      Pavel Rahman🎨 💻      chuabern🎨 💻      Rahul Tirkey🎨 💻      Ruslan Bes🎨 💻 💡 🚧 🖋 🔣 🚇      Bohdan🎨 💻      Juzdzewski🎨 💻              Grigor Minasyan🎨 💻      alvintwc🎨 💻      Anand Natarajan🎨 💻      Kashan Ali🎨 💻      Thomas Meshail🎨 💻      Son Pham🎨      Michael French💡              Yash Mishra📖      Miguel Rodriguez🎨 💻      Philipp Bachmann🎨 💻      sunny🎨 💻      Siddharth Chatterjee🎨 💻      Michael Naghavipour🎨 💻      Sahil Garg🎨 💻              MicroLion🎨 💻      wctwc🎨 💻      Rohan Sharma🔣      AshishBodla🎨 💻      Taras Pysarskyi🎨 💻      Luqman Bello O.🎨 💻      DyingDown🎨 💻              Diego Chapedelaine🎨 💻      Richlee🎨 💻      Asif Habib🎨 💻      Mazharul Hossain🎨 💻      toni🎨 💻      Pragyanshu Rai🎨 💻      Matthew Eller🎨 💻              AbhiBiju🎨 💻      Roman Zhornytskiy🎨 💻      Lucas Camino🎨 💻      João Vitor Casarin🎨 💻      Evgeniy Shay🎨 💻      Ehsan Barkhordar🎨 💻      Gabriel🎨 💻              Shibu Mohapatra🎨 💻      Pavel Kirkovsky🎨 💻      Tahir Gul🎨 💻      imDevSalman🎨 💻      Jordan Donaldson🎨 💻      js-venus🎨 💻      Faisal Shaikh🎨 💻              ashishbpatil🎨 💻      Tri Le🎨 💻      tomtreffke🎨 💻      Salah Eddine Lalami🎨 💻      Mattias Xu🎨 💻      Manas Gupta🎨 💻      wolfsong62🎨 💻              Mehdi Mirzaei🎨 💻      Van Ba Khanh🎨 💻      Sel Embee🎨 💻      Suvradip Paul🎨 💻      Sharique🎨      Seabass🎨 💻      Penny Liu🎨 💻              jatinder bhola🎨 💻      misterqbit🎨 💻      Daniel-VS9🎨 💻      Shruthi🎨 💻      beefydog🎨 💻      Suraj Kumar🎨 💻      hrishikeshps🎨 💻              Sudarshan🎨 💻      Divyansh💻 🎨      Zyaire🎨 💻      Omar Belkady🎨 💻      alexiismua🎨 💻      Eduarda Alves🎨      pycoach🎨 💻              Ruhul🎨 💻      pmoustopoulos🎨 💻      Lee Hui Ting💻 🎨      bodi1981🎨 💻      Devaraat Joshi🎨 💻      Johnny🎨 💻      rogue-coder🎨 💻              viiktr🎨      Lalit Mohan💻      João Sousa💻      言葉之靈💻 🎨      RJLABS💻      brittney0522🎨 💻      sham🎨 💻              Glenn Goossens💻 🎨      Cyber Hawk🎨 💻 🖋 💼      Ankit Yadav🎨 💻      verbality💻      Mohammed Siddiqui🎨 💻      AdamKaczor6250🎨 💻      Ramón Martinez Nieto🎨 💻              Grzegorz Dziubak🎨 💻      Ayoub BERDEDDOUCH🎨 💻      nikola-fadv🎨 💻      Akarsh Agrawal🎨 💻      Mitra Mirshafiee🎨 💻      Parker Stephens🎨 💻      alrenee99💻              Karthick Vankayala💻      Iryna 🎨 💻      palanugrah💻      Gwinbleind🎨 💻      Randy Bobandy🎨 💻      Bek Rozikoff💻      davnguye🎨 💻              Neel Patel💻      ehudbehar🎨 💻      nicholas-cod3r🎨 💻      michaelfranki🎨      Esther White🎨 💻      prathmeshpb🎨 💻      Victor Lin🎨 💻              Christine C. Yin🎨 💻      GitLearner-begin🎨 💻      Mesrop Andreasyan🎨 💻      Nathan Garcia🎨      commonsw04🎨 💻      Md. Rashad Tanjim🎨 💻      Ali Malek💻              PAODLT🎨 💻      Nikhil Bobade🎨 💻      hyuckjin21💻      Itasha Modi🎨 💻      Nikitha Reddy🎨 💻      Mahshooq Zubair🎨 💻      Subham Das💻              Onkar Birajdar🎨 💻      Nick Titomichelakis🎨 💻      Christian Leo-Pernold🎨      Matthew Marquise🎨 💻      baronfac🎨 💻      Abhishek Tilwar🎨 💻      DavidsDvm🎨 💻              Parth Parikh🎨 💻      Hector Castro🎨 💻      Rikky Arisendi🎨 💻      Ali HamXa🎨 💻      Frank.wu🎨 💻      Jatin Kumar🎨 💻 📖      masterHAWK99🎨 💻              Pushp Jain🎨 💻      Ashutosh Rout🎨 💻      Atharva Deshpande🎨 💻      Teodor Ciripescu🎨 💻      Anmol Bansal🎨 💻      Nikhil Kumar Macharla🎨 💻      Dexter🎨 💻              Aaron🎨 💻      Yogita Jaswani🎨 💻 📖 🖋      StoryDev🎨 💻      Mesut Doğansoy🎨 💻      Paras Dhawan🎨 💻      Emanuel Zhupa🎨 💻      Aaradhyaa717🎨 💻              jaacko-torus🎨 💻      mBlack💻      kalrayashwin📖 🖋 🎨 💻      Seraph💻 🎨      ZhiHong Chua🎨 💻      Amsal Khan🎨 💻 📖 🖋      Raghav Rastogi🎨 💻              Tzila📖      Shahriar Nasim Nafi📖      AG🎨 💻      Mojtaba Kamyabi🎨 💻      Ahmad Abdulrahman🎨 💻      Eclipse🎨 💻      Anshu Pal🎨 💻              Denis🎨 💻      mehmet sayin📖      WebDEV🎨 💻      Sam Komesarook🎨 💻      Kiran Ghimire🎨 💻      Joshua Davis🎨 💻      Muhammad-Huzaifa-Siddiqui💻              tobeornottobeadev🎨 💻      VAIBHAV SINGHAL🎨 💻      Keiran Pillman🎨 💻      Max Donchenko🎨 💻      sgonsal🎨 💻      diksha137🎨 💻      Vignesh🎨 💻              Gabriel França🎨 💻      Joseph🎨 💻      Bruno Rafael🎨 💻      vcamarre🎨 💻      thibault ketterer🎨 💻 🚧      VictorGonzalezToledo🎨 💻      1911510996🎨 💻              invidu🎨 💻      Nurul Furqon🎨 💻      David Asbill🎨 💻      Niko Birbilis🎨 💻      Mugundan Kottursuresh🎨      agrsachin81🎨 💻      Othmane El Alami🎨 💻              Syed Atif Ali🎨 💻      lakhanjindam🎨 💻      youssef hamdane🎨 💻      starfaerie🎨 💻      rodrigo0107🎨 💻      Michał Gralak🎨 💻      Jewel Mahmud🎨 💻              cwilson830🎨 💻      buun1030🎨 💻      Reda-ELOUAHABI🎨 💻      saad-aksa🎨 💻      Emdadul Haque🎨 💻      PROCW🎨 💻      cccppp1🎨 💻              Joanna Baile🎨 💻      Ahmed Saber🎨 💻      Masoud Keshavarz🎨 💻      mortazavian🎨 💻      Aniket Pandey🎨 💻      Vijay Nirmal🎨 💻      Daniel Carvallo💻              menaechmi🎨 💻      azenyx🎨 💻      Ahmet Özrahat🎨 💻      Abdulrahman Abouzaid🎨 💻      jmgnorbec🎨 💻      palinko91🎨 💻      Laisson R. Silveira🎨 💻              BHARGAVPATEL1244🎨 💻      Candide U🎨 💻      Sitansh Rajput🎨 💻      Houda Mouttalib🎨 💻      MumuTW🎨 💻      Suave Bajaj🎨 💻      Mehdi Parsaei🎨 💻              Dinko Osrecki🎨 💻      Dhia Djobbi🎨 💻      Mahmoud Galal🎨 💻      Anh Minh🎨 💻      Suvesh K🎨 💻      Petar Todorov🎨 💻      Alexander Nguyen🎨 💻              Morteza Jalalvand🎨 💻      Claudson Martins🎨 💻      Matt Jacobson🎨 💻      Rafael Belokurows🎨 💻       Thomas Gamauf🎨 💻      Rishabh Mahajan🎨 💻      rakeshpdgupta23🎨 💻              Shashidharknaik🎨 💻      taleleuma🎨 💻      Florian Bühler🎨 💻      Raihan Bin Wahid🎨 💻      MOHAMMED NASSER🎨 💻      federico🎨 💻      Andre Violante🎨 💻              tcunningham98🎨 💻      Jan Grießer🎨 💻      Serkan Alc🎨 💻 🖋      Jez McKean🎨 💻      meisam alifallahi🎨 💻      Mehul Thakkar🎨 💻      Saksham Soni🎨 💻              Pedro Peregrina🎨 💻      Mintu Choudhary🎨 💻      lucianmoldovanu🎨 💻      John C. Scott🎨 💻      Mia D.🎨 💻      EwenBernard🎨 💻      M. Reza Nasirloo🎨 💻              Jay Agrawal🎨 💻      DeShay🎨 💻      Jay206-Programmer🎨 💻      Elender🎨 💻 🖋      Bobby Byrne🎨 💻      Pirci🎨 💻      Hasanuzzaman🎨 💻              Josh Kautz🎨 💻      Brofar🎨 💻      Mina Karam🎨 💻      Duncan O N🎨 💻      Sean Tumulak-Nguyen🎨 💻      Artur Trześniewski🎨 💻      JJaammeessM🎨 💻              shubham agarwal🎨 💻      Michele Righi🎨 💻      Panagiotis Kontos🎨 💻      sumitbathla🎨 💻      Deepak Mathur🎨 💻      Juho Nykänen🎨 💻      Santiago González Siordia🎨 💻              SRIJITA MALLICK🎨 💻      Samriddhi B🎨 💻      Nitzan Papini🎨 💻      Mario Sanz🎨 💻      Crab^4🎨 💻      Pablo🎨 💻      Gordon Pham-Nguyen🎨 💻              Kristoffer🎨 💻      chrisblach🎨 💻      Gábor🎨 💻      Lina🎨 💻      Harrison Watts🎨 💻      Mario Petričko🎨 💻      Ben8120🎨 💻              Giovanna🎨 💻      Minal Ahuja🎨 💻      mossfarmer🎨 💻      ThaC0derDre🎨 💻      itware🎨 💻      Michael Walker🎨 💻      Tom Jacob Chirayil🎨 💻              Sachin Kumar🎨 💻      adi-ray🎨 💻      Dr-Blank-alt🎨 💻      Bogdan Cazacu🎨 💻      Gilson Urbano🎨 💻      Nina🎨 💻      Anthony🎨 💻              manushimjani🎨 💻      Michael Reyes🎨 💻      Rachel Kennelly🎨 💻      Aakash Garg🎨 💻      Daniel Livingston🎨 💻      alexrojco🎨 💻      Minh Nguyen🎨 💻              Mahesh Dattatraya Babar🎨 💻      Jin Zihang🎨 💻      Bikramjit Ganguly🎨 💻      QuestionableGuise🎨 💻      liq19ch🎨 💻      Bruno Rocha🎨 💻      Anand Dyavanapalli💻 🖋              crucian-afk🎨 💻      0xgainz🎨 💻      weirdfsh🎨 💻      Valan Baptist Mathuranayagam🎨 💻      Paul Kaefer🎨 💻      Yu-Hsiang Wang🎨 💻      Javad Adib🎨 💻              davidliu0930🎨 💻      Achilleas John Yfantis🎨 💻      Omkar Shivadekar🎨 💻 🖋 🐛      ToanTran🎨 💻      Gautam Naik🎨 💻      Marc🎨 💻      twix20🎨 💻              Kristian S.🎨 💻      Aleksey Khoroshilov🎨 💻      arjunsrsr🎨 💻      Ali Haider🎨 💻      Trisha Dring🎨 💻      Andre Marzulo🎨 💻      Krishna Modi🎨 💻              Rosemary Li🎨 💻      Alex Weller🎨 💻      Tam Nguyen🎨 💻      aquintelaoliveira🎨 💻      Norbert Brett🎨 💻      rocsogd🎨 💻      0nyr🎨 💻              rethkevin🎨 💻      RickHeadle🎨 💻      Leandre🎨 💻      Natnael Sisay🎨 💻      sbbu🎨 💻      wael🎨 💻      Fabricio Tramontano Pirini🎨 💻              Alexander Stoyanov🎨 💻      Dezx20🎨 💻      southparkkids🎨 💻      bmstar🎨 💻      kiagam🎨 💻      Juan Castillo🎨 💻      FFenne🎨 💻              Jose Toledo🎨 💻      Pat McGhen🎨 💻      Eiko Wagenknecht💻 🖋 🔣      Alan Chalmers🎨 💻      Jean Didier🎨 💻      Andy🎨 💻      pestadieu🎨 💻              Kanishka Chakraborty🎨 💻      Nandha🎨 💻      Vahid Mafi🎨 💻 🔣 🖋 💼      Akshay Ashok🎨 💻      0x08🎨 💻      Sandeep Mishra🎨 💻      Evann Regnault🎨 💻              Lenny Zeitoun🎨 💻      Eden Boaron🎨 💻      TroyBTC🎨 💻      Aby Sebastian🎨 💻      Matthew Dunn🎨 💻      ckullo🎨 💻 🖋 🔣      Mohamed Mamdouh🎨 💻              Youssef Bazina🎨 💻      Frederico Kückelhaus💻      Nushan Kodikara💻      Zach Cooper💻      Roy🎨 💻      Saurav Panchal🎨 💻      totallynotdavid🎨 💻              goosepirate🎨 💻 💡 💼      KAUTH🎨 💻      Hari Kiran Vusirikala🎨 💻      Sounak Dey🎨 💻      zia💼 🎨 💻      Reza Davari🎨 💻      AkshayAjaykumar🎨 💻              x24870🎨 💻      Ko Phone🎨 💻      Nabstar3🎨 💻      Mateusz🎨 💻      Yunus Emre Emik💻      Abhinav Sinha🎨 💻      Hung Nguyen🎨 💻              Maselino💻      Shuktika Mahanty💻      Mikołaj Gawroński🎨 💻      Hussein Habibi Juybari🎨 💻      Sean-McArthur🎨 💻      Osman F Bayram🎨 💻      Benjamin Thomas Blodgett🎨 💻              Chuanlong-Zang🎨 💻      julian🎨 💻      francisco🎨 💻      aalihhiader9211🎨 💻      Muhammad Zunair🎨 💻      Liya🎨 💻      BegadTarek🎨 💻              etorobot🎨 💻      Hussam Khan🎨 💻      Saikat Chakraborty🎨 💻      Nicholas Quisler🎨 💻      Evang Poul🎨 💻      Gregg Lind🎨 💻      Deepak Kumar🎨 💻              Callum Leslie🎨 💻      Curtis Barnard Jr.🎨 💻      Deepanshukaim🎨 💻      Manthan Ank🎨 💻      hossein varmazyar🎨 💻      Brayan Muñoz V.🎨 💻      Kamil Rasheed Siddiqui💻 🎨              mutt0-ds🎨 💻      egbertjk🎨 💻      Majid Zojaji🎨 💻      Sean Chen🎨 💻      Herbert Milhomme🎨 💻      A3🎨 💻      Killian🎨 💻              Coakeow🎨 💻      ྅༻ Ǭɀħ ༄༆ཉ🎨 💻      Pratik Solanki🎨 💻      Sunny🎨 💻      ssge🎨 💻      Bernat Frangi🎨 💻      Jeevan Rupacha🎨 💻              amirandap🎨 💻      Deepakshi Mittal🎨 💻      Abhijeet Parida🎨 💻      Khaled Riyad🎨 💻      Pratap parui🎨 💻      Prajit Panday🎨 💻      PipeSierra🎨 💻              Collins Oden🎨 💻      Kshitij Dwivedi🎨 💻      Bernardia Vitri Arumsari🎨 💻      Ömer Faruk Taşdemir🎨 💻      Spencer Stith🎨 💻      Porsche Rodjanasak🎨 💻      Shakeel Sharif🎨 💻              Victoria Cheng🎨 💻      Denis🎨 💻      Anand Prakash Tiwari🎨 💻      danijeljw-rpc🎨 💻      Ahmed H Ebrahim🎨 💻      Virginia Gardner🎨 💻      Jhironsel Diaz A.🎨 💻              Yunus Kidem🎨 💻      MT🎨 💻      Dinesh Zaldekar🎨 💻      adi🎨 💻      Farhan Shaikh🎨 💻      Elvis Salvatierra🎨 💻      Kaushik-Iyer🎨 💻              HocAndres🎨 💻      VictorHugoAguilarAguilar🎨 💻      Murat Can Abay🎨 💻      Chris🎨 💻      Shivam7-1🎨 💻      Paipai13🎨 💻      Shambles-io🎨 💻              Abhishek K M🎨 💻      Ezequiel Cuevas🎨 💻      Plamen Ivanov🎨 💻      Yuji🎨 💻      Jean-Philippe Lebœuf🎨 💻 🔣      Naufan🎨 💻      jadnov🎨 💻              vaxtangens🎨 💻      subashkonar13🎨 💻      Rushi Javiya🎨 💻      Mert Gül🎨 💻      Lily🎨 💻      Kalinoff🎨 💻      Joel Tony🎨 💻              Peter🎨 💻      Roozbeh Zarei🎨 💻      Shen🎨 💻      Joonsoo.LEE🎨 💻      Fede.Breg🎨 💻      Rui Costa🎨 💻      João Gustavo Bispo🎨 💻              Sami-I🎨 💻      Tsvetoslav Tsvetkov🎨 💻      Olabode Olaniyi David🎨 💻      theRuslan🎨 💻      leighboz🎨 💻      Frank Sossi🎨 💻      Tomasz Adamski🎨 💻              Mansoor M. Sathir🎨 💻      Golamrabbi Azad🎨 💻      Nahian Ahmed🎨 💻      Rafael de Jesus Silva Monteiro🎨 💻      Odionyebuchukwu Jude🎨 💻      The Nithin Balaji🎨 💻      Knackii🎨 💻              vittorio-giatti🎨 💻      Guilherme de Carvalho Lima Rebouças🎨 💻      aaref shami🎨 💻      Andrey Dryupin🎨 💻      Muhanned Noman🎨 💻      Jan Silva🎨 💻      emanuele-em🎨 💻 🖋              Sanjay TM🎨 💻      Joe Markberg / code editor🎨 💻      Julien Quiaios🎨 💻      Eric Ramirez Santis🎨 💻      M🎨 💻      Malcata🎨 💻      Athul Muralidharan🎨 💻              Dariusz Ochota🎨 💻      CHANDAN CHOUDHURY🎨 💻      Deep🎨 💻      Ahmet İstemihan ÖZTÜRK🎨 💻      TIM🎨 💻      jakeg814🎨 💻      Leonidos🎨 💻              Abhinandu V Nair🎨 💻      charafeddine01🎨 💻      Jasper🎨 💻      Manish Goyal🎨 💻      SATYAM_SINGH🎨 💻      Four🎨 💻      Vaishnavi Amira Yada🎨 💻              ShriKrushna Bhagwat🎨 💻      Rohit Nandagawali🎨 💻      felipe🎨 💻 🚧 🖋 ✅ 🧑‍🏫      Saurabh Mudgal🎨 💻      szenadam🎨 💻      Shubhendra Singh🎨 💻      Yoosuf Sayyid💻 🎨              Güven Çetinerler🎨 💻      Luke Jefferies🎨 💻      Chris🎨 💻      Lúcio Aguiar💻      Enuma029💻      yktsang01💻      maximumn3rd🎨 💻              Jon Galletero🎨 💻      Thaddeus  Thomas🎨 💻      Aakash Kumar💻 🎨      Ali M🎨 💻      OskyEdz🎨 💻      Ravi Gupta🎨 💻      Rafa Raizer🎨 💻              Abdullah Al Muzaki🎨 💻      Rahul Faujdar🎨 💻      Abhishek Verma🎨 💻      Ashutosh Shinde🎨 💻      Ganesh Rai🎨 💻      StefanTrpkovic🎨 💻      Erik Blanca🎨 💻              Vedant Madane🎨 💻      Antra Tripathi🎨 💻      Ethan Knights🎨 💻      Alexandru Boncut🎨 💻      Pablo Bandinopla🎨 💻 🚧 🖋      Robz-99🎨 💻      Harpal Singh🎨 💻              paulboundy99🎨 💻      Mubashir Ahmed🎨 💻      Rohan Hari🎨 💻      Erik Henrique 🎨 💻      Leandro Matheus🎨 💻      Deepak🎨 💻      AlishaSingh🎨 💻              Lynn Latt Yati🎨 💻      San Shwe🎨 💻      SKR🎨 💻      msbunnyjaguar🎨 💻      Mohamad Zabiulla🎨 💻      Hatim Zahid🎨 💻      Rauzan Sumara🎨 💻              Hosein1358🎨 💻      Mohit🎨 💻      Ali🎨 💻      Avinash1765🎨 💻      Sai Teja Madha🎨 💻      Monsur Ahmed Shafiq🎨 💻      xuxianjin-dev🎨 💻              chetna🎨 💻      Gul Zaib🎨 💻      Natalia🎨 💻      Dionísio Braga🎨 💻      Pritish Rajpurohit🎨 💻      incanlove🎨 💻      Innocent🎨 💻              Devin Almonor🎨 💻      antonyveyre🎨 💻      Beltz Anhxton🎨 💻      Mehdi🎨 💻      Muhammad Usman🎨 💻      Patrick Dantas🎨 💻      Tak Vannak🎨 💻              Ramzi RADDAOUI🎨 💻      Konstantin-Glukhov🎨 💻      uguroban🎨 💻      Humberto Alves🎨 💻      JuangZendrato🎨 💻      James Oluwaleye🎨 💻      Wasi Sadman🎨 💻              Pavle Mijatovic🎨 💻      Luiz H. S. Bispo🎨 💻      Сухас Дхолз🎨 💻      Alvaro Trujillo🎨 💻      Everton 🎨 💻      jfrozas🎨 💻      Shuaaib Badran🎨 💻              Shivam Jha🎨 💻      Mohamed Tayeh🎨 💻      Makendran G🎨 💻      mayank singh tomar🎨 💻      hossam sadany🎨 💻      Harshbardhan Singh💻 🎨      Fawad Jawaid Malik🎨 💻              Tina Lacatis🎨 💻      TeddyCuoreDolce🎨 💻      bchooxg🎨 💻      Alisha Takkar🎨 💻      Gianluigi🎨 💻      Mehran Javaherian🎨 💻      Benjamin Ololade Adedokun🎨 💻              Md. Abdul Mutalib🎨 💻      Aadil Arsh.S.R🎨 💻      J. Nathan Allen🎨 💻      Kieran Krug🎨 💻      Seth Addo🎨 💻      Satvik Singh Rathore🎨 💻      dangoth🎨 💻              Maxim🎨 💻      Phuong-Cat Ngo🎨 💻      Frenchtoast0🎨 💻      Rakshith🎨 💻      Vaibhav Arora🎨 💻      zghp🎨 💻      Bedovan🎨 💻              chiaramistro🎨 💻      him2016🎨 💻      HarshitSachdeva🎨 💻      Sadaf Saleem🎨 💻      Aaroh Srivastava🎨 💻      eloygplaza🎨 💻      Gaurav Kumar Verma🎨 💻              AndreaCUS🎨 💻      Simran🎨 💻      Prashant Bhapkar🎨 💻      mhaendler🎨 💻      Gauri Maheshwari🎨 💻      4Lajf🎨 💻      Tanmoy Sengupta🎨 💻              Sharad Tripathi🎨 💻      Niraj Chavan🎨 💻      Luisa Gualda🎨 💻      Monika-Sivakumar-3🎨 💻      harryfensome🎨 💻      Shubham Choubey🎨 💻      Ashwini Patil🎨 💻              cleversonlira🎨 💻      Nurmukhammed🎨 💻      workspace-utkarsh🎨 💻      Santosh Phadtare🎨 💻      Prashant Warghude🎨 💻      Umang Dakh🎨 💻      Shalini Chavan🎨 💻              vinit gurjar🎨 💻      Vishal Kumar🎨 💻      Wonhyeong Seo🎨 💻      Achwale Prajwal Namdevrao🎨 💻      Ankan Banerjee🎨 💻      bhaumikankan🎨 💻      JamesMacroZhang🎨 💻              Pedro Lopes🎨 💻      dia🎨 💻      tayyabhussain2910🎨 💻      Rajdeep Shrivastava 🎨 💻      Mukul Kumar🎨 💻      Mayank N🎨 💻      jdelucca🎨 💻              Sneha Mittal🎨 💻      Sarika Kushwaha🎨 💻      farzad-khb🎨 💻      Elijah Shackelford🎨 💻      The-Only-Raminator🎨 💻      Keerthana Kasthuril🎨 💻      Viachaslau Auchynnikau🎨 💻              Mohammad Osman Rasooli🎨 💻      mvedovato🎨 💻      Sonali Rajput🎨 💻      Isha Dhek🎨 💻      Ramshad Cheriyeri Peediyakkal🎨 💻      Micah🎨 💻      gauravshukla2203🎨 💻              sndmurthy🎨 💻      Shivam-Singh🎨 💻      M. Ammar Khan🎨 💻      chandolakul🎨 💻      bhatnagar221🎨 💻      Adrian Nieściur🎨 💻      nezi311🎨 💻              scottajevans🎨 💻      Marcelo Antunes Soares Fantini🎨 💻      Axel De Acetis🎨 💻      Drishti Sah🎨 💻      VipulDhillon🎨 💻      Urmi Jana🎨 💻      Ayush Mokal🎨 💻              Damola Olutoke🎨 💻      Max🎨 💻      Lakshmi N🎨 💻      ArtemReva🎨 💻      Ujjwal Aggarwal🎨 💻      Mo🎨 💻      Brian🎨 💻              chamley🎨 💻      Simone Baptiste🎨 💻      Shekhar Thakur🎨 💻      Smith🎨 💻      codernoob1🎨 💻      lok84🎨 💻      Tobias Riemenschneider🎨 💻              Tharsanan1🎨 💻      ANURAG SINGH🎨 💻      Yash Sant🎨 💻      Krishiv Patel🎨 💻      GGGalaxy🎨 💻      pardeepdhillon661🎨 💻      anujd64🎨 💻              Pedro Pereira🎨 💻      Master_Saptak🎨 💻      SURANJAN DAS🎨 💻      Tripura kant🎨 💻      shabzkhan🎨 💻      Mustafa Poya🎨 💻      Roshan Jha🎨 💻              GuillaumeLarue🎨 💻      Tomasz Rodak🎨 💻      Junil Kim🎨 💻      Surbhi Mayank🎨 💻      Nemanja Lekic🎨 💻      HemantMalokar🎨 💻      Felipe M. López🎨 💻              bibliofilo🎨 💻      GauthamG2🎨 💻      02_t🎨 💻      Yusuf Abdul-razaq🎨 💻      Vladimir🎨 💻      Sai Chandra K🎨 💻      Soroush Bonab🎨 💻              Giide0n🎨 💻      GG🎨 💻      Dáger Zúñiga🎨 💻      rsk2🎨 💻      Storozhev DJ🎨 💻      Jeevan🎨 💻      Andy Johnson🎨 💻              Aníbal Pozo🎨 💻      Jovane de Castro🎨 💻      Muhammad Hamza Amir🎨 💻      tharaka-mts🎨 💻      Ali KHYAR🎨 💻      Caio Araujo🎨 💻      Oscar Dyremyhr🎨 💻              arteality🎨 💻      Daniel Drexlmaier🎨 💻      Marco Monti🎨 💻      mikeycrystal🎨 💻      Veljanovskii🎨 💻      Ivan Gorbachev🎨 💻      Sahil Rawat🎨 💻              Hasitha Suneth🎨 💻      Yerko Vera Lezama🎨 💻      Ivan Penchev🎨 💻      Tanver Islam Tonmoy🎨 💻      Xun Cao🎨 💻      Nayan Babariya🎨 💻      Priyanshu Maurya🎨 💻              Dylan Tintenfich🎨 💻      Ron Strauss🎨 💻      Mohammed AlBanna🎨 💻      Mukund M🎨 💻      Franklin Ohaegbulam🎨 💻      Nisarg Shah🎨 💻      Unik Dahal🎨 💻              Readily🎨 💻      Alexandre Poitevin🎨 💻      Scaramir🎨 💻      Pruthvi🎨 💻      Kalmanq🎨 💻      Alfatah Nesab🎨 💻      arudesai🎨 💻              Adryenne🎨 💻      El mehdi oudaoud🎨 💻      Jayant Goel🎨 💻      Tsuki🎨 💻      Peter Lemanski🎨 💻      Annurag-byte🎨 💻      Anthony Vu🎨 💻              Vitaly Nikolaychuk🎨 💻      Nathan🎨 💻      Evgenii Petukhov🎨 💻      Loris Guerra🎨 💻      fakhriaunur🎨 💻      Mehdi HYANI🎨 💻      Sarvex Jatasra🎨 💻              santimanuelr🎨 💻      Evgeniy Rezanov🎨 💻      Sonia M🎨 💻      Grzegorz Kmita🎨 💻      Manuel Carita🎨 💻      Felipe Cisternas Alvarez🎨 💻      Guo Ci🎨 💻              Marcos Silva🎨 💻      KK🎨 💻      Shubhanjan Medhi🎨 💻      ArthurFerreiraRodrigues🎨 💻      PabloHermun🎨 💻      disha-baldawa🎨 💻      StaroMoon🎨 💻              Amila T Kumarasekara🎨 💻      Amoh Prince🎨 💻      AngeloGC🎨 💻      Ebube Glory Ogbonda🎨 💻      Prahalad Belavadi📖      Antoni Sarnowski-Trypka🎨 💻      Alberto Pasqualetto🎨 💻              Amir Babaei🎨 💻      Syed Abdul Hannan🎨 💻      Srajan Rai🎨 💻      Clarence Moore🎨 💻      Nguyen Anh Tuan🎨 💻      dar2dar2🎨 💻      Ameer Ibrahim🎨 💻              Tiago Lugatto🎨 💻      raremiroir🎨 💻      Moobie🎨 💻      AlicanDursun🎨 💻      bbalsam🎨 💻      Luboš Hájek🎨 💻      mrshahzeb7🎨 💻              Wesley Scholl🎨 💻      Lawrence Turcotte🎨 💻      Michael DiPaolo🎨 💻      Smart-Codi🎨 💻      Vivek Kumar🎨 💻      Igor Moiseev🎨 💻      Bård Pedersen🎨 💻              HOA PHAN🎨 💻      GaborModra🎨 💻      vivek-114🎨 💻      Robin🎨 💻      Alex🎨 💻      John Ehrlinger🎨 💻      Roman Zhuravlov🎨 💻              Jordan Moss🎨 💻      RaeShelly🎨 💻      gmollard🎨 💻      Md Kaif Khan🎨 💻      Pablo Romera🎨 💻      Erik Bustos🎨 💻      trogfield🎨 💻              simon-aichhorn🎨 💻      Tufan GÜLEÇ🎨 💻      Uğur Berkecan Ünlü🎨 💻      Revanth Naik🎨 💻      Lia Pires🎨 💻      Igor Mestechkin🎨 💻      Anirudh Karanth🎨 💻              KBobovskiy🎨 💻      zhatiayua🎨 💻 🖋      David Cardona🎨 💻      Paulo Castilho🎨 💻      Sebastiano Picchi🎨 💻      pjotar🎨 💻      Rimel CHERIF💻              Arsal uddin🖋      Dmitry Kasporsky💻      SoftwareDev1014🎨 💻      @Robvred🎨 💻      Kasun Shanaka💻      Ahmad M.🎨 💻      Alex Kozin🎨 💻              Mandy Meindersma🎨 💻      LEGALISE PIRACY🎨 💻      Alex Logvin🎨 💻      Aria Dahl🎨 💻      Mustafa Arifoglu🎨 💻      Yevhen Leshchenko🎨 💻      Anubhav Adhikari🎨 💻              Noah Tatko🎨 💻      Mohit Gadhavi🎨 💻      Pedro Basílio🎨 💻      RealSanjeev🎨 💻      Akash Hazra🎨 💻      Christoph Dahlen🎨 💻      Vincent du Plessis🎨 💻              Karen Tamrazyan🎨 💻      Mirza Younus Baig🎨 💻      Ashish Kumar🎨 💻      Unknown6334🎨 💻      flowaz🎨 💻      zi-aikra🎨 💻      PAYAL PM🎨 💻              Lennart Lösche🎨 💻      Yummy-Yums🎨 💻      Njuacha Hubert Mikulowski🎨 💻      Hussein Esmail🎨 💻      Bilgehan Bezir🎨 💻      Muhammed Shittu🎨 💻      Clément FERNANDES🎨 💻              JaCKoP619🎨 💻      userutf8🎨 💻      Mohamed Ubaid🎨 💻      Justin Yates🎨 💻      mohammad ali🎨 💻      Madhav Singh🎨 💻      RgbMouse69🎨 💻              Nicholas Leask🎨 💻      parthav0🎨 💻      Sigma🎨 💻      Evelina Becheva🎨 💻      Akshit Gulyan🎨 💻      Arpita Jana🎨 💻      Praveen Kumar🎨 💻              Mohammad Sami🎨 💻      eddiestefanescu🎨 💻      Ramesh Yadav🎨 💻      Sarthak Joshi🎨 💻      Nikhil12300🎨 💻      Yevgen🎨 💻      Leo🎨 💻              laurent b🎨 💻      Mettchen🎨 💻      Ali Mahdavi🎨 💻      Lucas Dondo🎨 💻      Siddhesh Agarwal🎨 💻      slimerPuncher🎨 💻      saritashh🎨 💻              Iulian-Valeriu Cioată🎨 💻      Szabolcs Nagy🎨 💻      Jarle Kvile🎨 💻      劉耀升 Vic Liu🎨 💻      Suryansh🎨 💻      Matthew Oosthuyse🎨 💻      Florin Zamfir🎨 💻              Melek🎨 💻      moesocio🎨 💻      Alan James🎨 💻      Mai Thanh Phương🎨 💻      Neville Dabre🎨 💻      Maksym🎨 💻      tamanna900🎨 💻              Adithya Awati🎨 💻      This project follows the all-contributors specification.Contributions of any kind welcome![ Go back to the top of the page ]Contributor Over TimeStargazers over timeVisualisation of this repository by Gourcehttps://www.youtube.com/watch?v=24cZVytc5D4"
6,ageitgey/face_recognition,https://github.com/ageitgey/face_recognition/blob/master/README.md,Python,"Face RecognitionYou can also read a translated version of this file in Chinese 简体中文版 or in Korean 한국어 or in Japanese 日本語.Recognize and manipulate faces from Python or from the command line withthe world's simplest face recognition library.Built using dlib's state-of-the-art face recognitionbuilt with deep learning. The model has an accuracy of 99.38% on theLabeled Faces in the Wild benchmark.This also provides a simple face_recognition command line tool that letsyou do face recognition on a folder of images from the command line!FeaturesFind faces in picturesFind all the faces that appear in a picture:import face_recognitionimage = face_recognition.load_image_file(\""your_file.jpg\"")face_locations = face_recognition.face_locations(image)Find and manipulate facial features in picturesGet the locations and outlines of each person's eyes, nose, mouth and chin.import face_recognitionimage = face_recognition.load_image_file(\""your_file.jpg\"")face_landmarks_list = face_recognition.face_landmarks(image)Finding facial features is super useful for lots of important stuff. But you can also use it for really stupid stufflike applying digital make-up (think 'Meitu'):Identify faces in picturesRecognize who appears in each photo.import face_recognitionknown_image = face_recognition.load_image_file(\""biden.jpg\"")unknown_image = face_recognition.load_image_file(\""unknown.jpg\"")biden_encoding = face_recognition.face_encodings(known_image)[0]unknown_encoding = face_recognition.face_encodings(unknown_image)[0]results = face_recognition.compare_faces([biden_encoding], unknown_encoding)You can even use this library with other Python libraries to do real-time face recognition:See this example for the code.Online DemosUser-contributed shared Jupyter notebook demo (not officially supported): InstallationRequirementsPython 3.3+ or Python 2.7macOS or Linux (Windows not officially supported, but might work)Installation Options:Installing on Mac or LinuxFirst, make sure you have dlib already installed with Python bindings:How to install dlib from source on macOS or UbuntuThen, make sure you have cmake installed:brew install cmakeFinally, install this module from pypi using pip3 (or pip2 for Python 2):pip3 install face_recognitionAlternatively, you can try this library with Docker, see this section.If you are having trouble with installation, you can also try out apre-configured VM.Installing on an Nvidia Jetson Nano boardJetson Nano installation instructionsPlease follow the instructions in the article carefully. There is current a bug in the CUDA libraries on the Jetson Nano that will cause this library to fail silently if you don't follow the instructions in the article to comment out a line in dlib and recompile it.Installing on Raspberry Pi 2+Raspberry Pi 2+ installation instructionsInstalling on FreeBSDpkg install graphics/py-face_recognitionInstalling on WindowsWhile Windows isn't officially supported, helpful users have posted instructions on how to install this library:@masoudr's Windows 10 installation guide (dlib + face_recognition)Installing a pre-configured Virtual Machine imageDownload the pre-configured VM image (for VMware Player or VirtualBox).UsageCommand-Line InterfaceWhen you install face_recognition, you get two simple command-lineprograms:face_recognition - Recognize faces in a photograph or folder full forphotographs.face_detection - Find faces in a photograph or folder full for photographs.face_recognition command line toolThe face_recognition command lets you recognize faces in a photograph orfolder full  for photographs.First, you need to provide a folder with one picture of each person youalready know. There should be one image file for each person with thefiles named according to who is in the picture:Next, you need a second folder with the files you want to identify:Then in you simply run the command face_recognition, passing inthe folder of known people and the folder (or single image) with unknownpeople and it tells you who is in each image:$ face_recognition ./pictures_of_people_i_know/ ./unknown_pictures//unknown_pictures/unknown.jpg,Barack Obama/face_recognition_test/unknown_pictures/unknown.jpg,unknown_personThere's one line in the output for each face. The data is comma-separatedwith the filename and the name of the person found.An unknown_person is a face in the image that didn't match anyone inyour folder of known people.face_detection command line toolThe face_detection command lets you find the location (pixel coordinatates)of any faces in an image.Just run the command face_detection, passing in a folder of imagesto check (or a single image):$ face_detection  ./folder_with_pictures/examples/image1.jpg,65,215,169,112examples/image2.jpg,62,394,211,244examples/image2.jpg,95,941,244,792It prints one line for each face that was detected. The coordinatesreported are the top, right, bottom and left coordinates of the face (in pixels).Adjusting Tolerance / SensitivityIf you are getting multiple matches for the same person, it might be thatthe people in your photos look very similar and a lower tolerance valueis needed to make face comparisons more strict.You can do that with the --tolerance parameter. The default tolerancevalue is 0.6 and lower numbers make face comparisons more strict:$ face_recognition --tolerance 0.54 ./pictures_of_people_i_know/ ./unknown_pictures//unknown_pictures/unknown.jpg,Barack Obama/face_recognition_test/unknown_pictures/unknown.jpg,unknown_personIf you want to see the face distance calculated for each match in orderto adjust the tolerance setting, you can use --show-distance true:$ face_recognition --show-distance true ./pictures_of_people_i_know/ ./unknown_pictures//unknown_pictures/unknown.jpg,Barack Obama,0.378542298956785/face_recognition_test/unknown_pictures/unknown.jpg,unknown_person,NoneMore ExamplesIf you simply want to know the names of the people in each photograph but don'tcare about file names, you could do this:$ face_recognition ./pictures_of_people_i_know/ ./unknown_pictures/ | cut -d ',' -f2Barack Obamaunknown_personSpeeding up Face RecognitionFace recognition can be done in parallel if you have a computer withmultiple CPU cores. For example, if your system has 4 CPU cores, you canprocess about 4 times as many images in the same amount of time by usingall your CPU cores in parallel.If you are using Python 3.4 or newer, pass in a --cpus <number_of_cpu_cores_to_use> parameter:$ face_recognition --cpus 4 ./pictures_of_people_i_know/ ./unknown_pictures/You can also pass in --cpus -1 to use all CPU cores in your system.Python ModuleYou can import the face_recognition module and then easily manipulatefaces with just a couple of lines of code. It's super easy!API Docs: https://face-recognition.readthedocs.io.Automatically find all the faces in an imageimport face_recognitionimage = face_recognition.load_image_file(\""my_picture.jpg\"")face_locations = face_recognition.face_locations(image)# face_locations is now an array listing the co-ordinates of each face!See this exampleto try it out.You can also opt-in to a somewhat more accurate deep-learning-based face detection model.Note: GPU acceleration (via NVidia's CUDA library) is required for goodperformance with this model. You'll also want to enable CUDA supportwhen compliling dlib.import face_recognitionimage = face_recognition.load_image_file(\""my_picture.jpg\"")face_locations = face_recognition.face_locations(image, model=\""cnn\"")# face_locations is now an array listing the co-ordinates of each face!See this exampleto try it out.If you have a lot of images and a GPU, you can alsofind faces in batches.Automatically locate the facial features of a person in an imageimport face_recognitionimage = face_recognition.load_image_file(\""my_picture.jpg\"")face_landmarks_list = face_recognition.face_landmarks(image)# face_landmarks_list is now an array with the locations of each facial feature in each face.# face_landmarks_list[0]['left_eye'] would be the location and outline of the first person's left eye.See this exampleto try it out.Recognize faces in images and identify who they areimport face_recognitionpicture_of_me = face_recognition.load_image_file(\""me.jpg\"")my_face_encoding = face_recognition.face_encodings(picture_of_me)[0]# my_face_encoding now contains a universal 'encoding' of my facial features that can be compared to any other picture of a face!unknown_picture = face_recognition.load_image_file(\""unknown.jpg\"")unknown_face_encoding = face_recognition.face_encodings(unknown_picture)[0]# Now we can see the two face encodings are of the same person with `compare_faces`!results = face_recognition.compare_faces([my_face_encoding], unknown_face_encoding)if results[0] == True:    print(\""It's a picture of me!\"")else:    print(\""It's not a picture of me!\"")See this exampleto try it out.Python Code ExamplesAll the examples are available here.Face DetectionFind faces in a photographFind faces in a photograph (using deep learning)Find faces in batches of images w/ GPU (using deep learning)Blur all the faces in a live video using your webcam (Requires OpenCV to be installed)Facial FeaturesIdentify specific facial features in a photographApply (horribly ugly) digital make-upFacial RecognitionFind and recognize unknown faces in a photograph based on photographs of known peopleIdentify and draw boxes around each person in a photoCompare faces by numeric face distance instead of only True/False matchesRecognize faces in live video using your webcam - Simple / Slower Version (Requires OpenCV to be installed)Recognize faces in live video using your webcam - Faster Version (Requires OpenCV to be installed)Recognize faces in a video file and write out new video file (Requires OpenCV to be installed)Recognize faces on a Raspberry Pi w/ cameraRun a web service to recognize faces via HTTP (Requires Flask to be installed)Recognize faces with a K-nearest neighbors classifierTrain multiple images per person then recognize faces using a SVMCreating a Standalone ExecutableIf you want to create a standalone executable that can run without the need to install python or face_recognition, you can use PyInstaller. However, it requires some custom configuration to work with this library. See this issue for how to do it.Articles and Guides that cover face_recognitionMy article on how Face Recognition works: Modern Face Recognition with Deep LearningCovers the algorithms and how they generally workFace recognition with OpenCV, Python, and deep learning by Adrian RosebrockCovers how to use face recognition in practiceRaspberry Pi Face Recognition by Adrian RosebrockCovers how to use this on a Raspberry PiFace clustering with Python by Adrian RosebrockCovers how to automatically cluster photos based on who appears in each photo using unsupervised learningHow Face Recognition WorksIf you want to learn how face location and recognition work instead ofdepending on a black box library, read my article.CaveatsThe face recognition model is trained on adults and does not work very well on children. It tends to mixup children quite easy using the default comparison threshold of 0.6.Accuracy may vary between ethnic groups. Please see this wiki page for more details.Deployment to Cloud Hosts (Heroku, AWS, etc)Since face_recognition depends on dlib which is written in C++, it can be tricky to deploy an appusing it to a cloud hosting provider like Heroku or AWS.To make things easier, there's an example Dockerfile in this repo that shows how to run an app built withface_recognition in a Docker container. With that, you should be able to deployto any service that supports Docker images.You can try the Docker image locally by running: docker-compose up --buildThere are also several prebuilt Docker images.Linux users with a GPU (drivers >= 384.81) and Nvidia-Docker installed can run the example on the GPU: Open the docker-compose.yml file and uncomment the dockerfile: Dockerfile.gpu and runtime: nvidia lines.Having problems?If you run into problems, please read the Common Errors section of the wiki before filing a github issue.ThanksMany, many thanks to Davis King (@nulhom)for creating dlib and for providing the trained facial feature detection and face encoding modelsused in this library. For more information on the ResNet that powers the face encodings, check outhis blog post.Thanks to everyone who works on all the awesome Python data science libraries like numpy, scipy, scikit-image,pillow, etc, etc that makes this kind of stuff so easy and fun in Python.Thanks to Cookiecutter and theaudreyr/cookiecutter-pypackage project templatefor making Python project packaging way more tolerable."
7,swisskyrepo/PayloadsAllTheThings,https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/README.md,Python,"Payloads All The ThingsA list of useful payloads and bypasses for Web Application Security.Feel free to improve with your payloads and techniques !I ❤️ pull requests :)You can also contribute with a 🍻 IRL, or using the sponsor buttonAn alternative display version is available at PayloadsAllTheThingsWeb.  📖 DocumentationEvery section contains the following files, you can use the _template_vuln folder to create a new chapter:README.md - vulnerability description and how to exploit it, including several payloadsIntruder - a set of files to give to Burp IntruderImages - pictures for the README.mdFiles - some files referenced in the README.mdYou might also like the Methodology and Resources folder :Methodology and ResourcesActive Directory Attack.mdCloud - AWS Pentest.mdCloud - Azure Pentest.mdCobalt Strike - Cheatsheet.mdLinux - Evasion.mdLinux - Persistence.mdLinux - Privilege Escalation.mdMetasploit - Cheatsheet.mdMethodology and enumeration.mdNetwork Pivoting Techniques.mdNetwork Discovery.mdReverse Shell Cheatsheet.mdSubdomains Enumeration.mdWindows - AMSI Bypass.mdWindows - DPAPI.mdWindows - Download and Execute.mdWindows - Mimikatz.mdWindows - Persistence.mdWindows - Privilege Escalation.mdWindows - Using credentials.mdYou want more ? Check the Books and Youtube videos selections.👨‍💻 ContributionsBe sure to read CONTRIBUTING.md  Thanks again for your contribution! ❤️🧙‍♂️ SponsorsThis project is proudly sponsored by these companies."
8,xtekky/gpt4free,https://github.com/xtekky/gpt4free/blob/main/README.md,Python,"By using this repository or any code related to it, you agree to the legal notice. The author is not responsible for any copies, forks, or reuploads made by other users. This is the author's only account and repository. To prevent impersonation or irresponsible actions, you may comply with the GNU GPL license this Repository uses.This (quite censored) New Version of gpt4free, was just released, it may contain bugs, open an issue or contribute a PR when encountering one, some features were disabled.Docker is for now not available but I would be happy if someone contributes a PR. The g4f GUI will be uploaded soon enough.Newpypi package:pip install -U g4fTable of Contents:Getting StartedPrerequisitesSetting up the projectUsageThe g4f Packageinterference openai-proxy apiModelsgpt-3.5 / gpt-4Other ModelsRelated gpt4free projectsContributeChatGPT cloneCopyrightCopyright NoticeStar HistoryGetting StartedPrerequisites:Download and install Python (Version 3.x is recommended).Setting up the project:Install using pypipip install -U g4forClone the GitHub repository:git clone https://github.com/xtekky/gpt4free.gitNavigate to the project directory:cd gpt4free(Recommended) Create a virtual environment to manage Python packages for your project:python3 -m venv venvActivate the virtual environment:On Windows:.\\venv\\Scripts\\activateOn macOS and Linux:source venv/bin/activateInstall the required Python packages from requirements.txt:pip install -r requirements.txtCreate a test.py file in the root folder and start using the repo, further Instructions are belowimport g4f...UsageThe g4f Packageimport g4fprint(g4f.provider.Ails.params)  # supported args# Automatic selection of provider# streamed completionresponse = g4f.ChatCompletion.create(    model=\""gpt-3.5-turbo\"",    messages=[{\""role\"": \""user\"", \""content\"": \""Hello world\""}],    stream=True,)for message in response:    print(message, flush=True, end='')# normal responseresponse = g4f.ChatCompletion.create(    model=g4f.models.gpt_4,    messages=[{\""role\"": \""user\"", \""content\"": \""hi\""}],)  # alterative model settingprint(response)# Set with providerresponse = g4f.ChatCompletion.create(    model=\""gpt-3.5-turbo\"",    provider=g4f.provider.DeepAi,    messages=[{\""role\"": \""user\"", \""content\"": \""Hello world\""}],    stream=True,)for message in response:    print(message)providers:from g4f.provider import (    Acytoo,    Aichat,    Ails,    AiService,    AItianhu,    Bard,    Bing,    ChatgptAi,    ChatgptLogin,    DeepAi,    GetGpt)# usage:response = g4f.ChatCompletion.create(..., provider=ProviderName)interference openai-proxy api (use with openai python package)get requirements:pip install -r interference/requirements.txtrun server:python3 -m interference.appimport openaiopenai.api_key = \""\""openai.api_base = \""http://localhost:1337\""def main():    chat_completion = openai.ChatCompletion.create(        model=\""gpt-3.5-turbo\"",        messages=[{\""role\"": \""user\"", \""content\"": \""write a poem about a tree\""}],        stream=True,    )    if isinstance(chat_completion, dict):        # not stream        print(chat_completion.choices[0].message.content)    else:        # stream        for token in chat_completion:            content = token[\""choices\""][0][\""delta\""].get(\""content\"")            if content != None:                print(content, end=\""\"", flush=True)if __name__ == \""__main__\"":    main()Modelsgpt-3.5 / gpt-4WebsiteProvidergpt-3.5gpt-4StreamingStatusAuthwww.aitianhu.comg4f.provider.AItianhu✔️❌❌❌chat.acytoo.comg4f.provider.Acytoo✔️❌❌❌aiservice.vercel.appg4f.provider.AiService✔️❌❌❌chat-gpt.orgg4f.provider.Aichat✔️❌❌❌ai.lsg4f.provider.Ails✔️❌✔️❌bard.google.comg4f.provider.Bard❌❌❌✔️bing.comg4f.provider.Bing❌✔️❌❌chatgpt.aig4f.provider.ChatgptAi❌✔️❌❌chatgptlogin.acg4f.provider.ChatgptLogin✔️❌❌❌deepai.orgg4f.provider.DeepAi✔️❌✔️❌chat.dfehub.comg4f.provider.DfeHub✔️❌✔️❌free.easychat.workg4f.provider.EasyChat✔️❌✔️❌forefront.comg4f.provider.Forefront✔️❌✔️❌chat.getgpt.worldg4f.provider.GetGpt✔️❌✔️❌gpt-gm.h2o.aig4f.provider.H2o❌❌✔️❌liaobots.comg4f.provider.Liaobots✔️✔️✔️✔️supertest.lockchat.appg4f.provider.Lockchat✔️✔️✔️❌opchatgpts.netg4f.provider.Opchatgpts✔️❌❌❌backend.raycast.comg4f.provider.Raycast✔️✔️✔️✔️theb.aig4f.provider.Theb✔️❌✔️❌play.vercel.aig4f.provider.Vercel✔️❌❌❌wewordle.orgg4f.provider.Wewordle✔️❌❌❌you.comg4f.provider.You✔️❌❌❌chat9.yqcloud.topg4f.provider.Yqcloud✔️❌❌❌Other ModelsModelBase ProviderProviderWebsitepalmGoogleg4f.provider.Bardbard.google.comh2ogpt-gm-oasst1-en-2048-falcon-7b-v3Huggingfaceg4f.provider.H2owww.h2o.aih2ogpt-gm-oasst1-en-2048-falcon-40b-v1Huggingfaceg4f.provider.H2owww.h2o.aih2ogpt-gm-oasst1-en-2048-open-llama-13bHuggingfaceg4f.provider.H2owww.h2o.aiclaude-instant-v1Anthropicg4f.provider.Vercelsdk.vercel.aiclaude-v1Anthropicg4f.provider.Vercelsdk.vercel.aiclaude-v2Anthropicg4f.provider.Vercelsdk.vercel.aicommand-light-nightlyCohereg4f.provider.Vercelsdk.vercel.aicommand-nightlyCohereg4f.provider.Vercelsdk.vercel.aigpt-neox-20bHuggingfaceg4f.provider.Vercelsdk.vercel.aioasst-sft-1-pythia-12bHuggingfaceg4f.provider.Vercelsdk.vercel.aioasst-sft-4-pythia-12b-epoch-3.5Huggingfaceg4f.provider.Vercelsdk.vercel.aisantacoderHuggingfaceg4f.provider.Vercelsdk.vercel.aibloomHuggingfaceg4f.provider.Vercelsdk.vercel.aiflan-t5-xxlHuggingfaceg4f.provider.Vercelsdk.vercel.aicode-davinci-002OpenAIg4f.provider.Vercelsdk.vercel.aigpt-3.5-turbo-16kOpenAIg4f.provider.Vercelsdk.vercel.aigpt-3.5-turbo-16k-0613OpenAIg4f.provider.Vercelsdk.vercel.aigpt-4-0613OpenAIg4f.provider.Vercelsdk.vercel.aitext-ada-001OpenAIg4f.provider.Vercelsdk.vercel.aitext-babbage-001OpenAIg4f.provider.Vercelsdk.vercel.aitext-curie-001OpenAIg4f.provider.Vercelsdk.vercel.aitext-davinci-002OpenAIg4f.provider.Vercelsdk.vercel.aitext-davinci-003OpenAIg4f.provider.Vercelsdk.vercel.aillama13b-v2-chatReplicateg4f.provider.Vercelsdk.vercel.aillama7b-v2-chatReplicateg4f.provider.Vercelsdk.vercel.aiRelated gpt4free projects            🎁 Projects      ⭐ Stars      📚 Forks      🛎 Issues      📬 Pull requests                  gpt4free                                      gpt4free-ts                                      ChatGPT-Clone                                      ChatGpt Discord Bot                                      LangChain gpt4free                                      ChatGpt Telegram Bot                              Contributeto add another provider, its very simple:create a new file in g4f/provider with the name of the ProviderImplement a class that extends BaseProvider.from .base_provider import BaseProviderfrom ..typing import CreateResult, Anyclass HogeService(BaseProvider):    url = \""http://hoge.com\""    working = True    supports_gpt_35_turbo = True    @staticmethod    def create_completion(        model: str,        messages: list[dict[str, str]],        stream: bool,        **kwargs: Any,    ) -> CreateResult:        passHere, you can adjust the settings, for example if the website does support streaming, set working to True...Write code to request the provider in create_completion and yield the response, even if its a one-time response, do not hesitate to look at other providers for inspirationAdd the Provider Name in g4f/provider/init.pyfrom .base_provider import BaseProviderfrom .HogeService import HogeService__all__ = [  HogeService,]You are done !, test the provider by calling it:import g4fresponse = g4f.ChatCompletion.create(model='gpt-3.5-turbo', provider=g4f.provider.PROVIDERNAME,                                    messages=[{\""role\"": \""user\"", \""content\"": \""test\""}], stream=g4f.provider.PROVIDERNAME.supports_stream)for message in response:    print(message, flush=True, end='')ChatGPT cloneCurrently implementing new features and trying to scale it, please be patient it may be unstablehttps://chat.g4f.ai/chatThis site was developed by me and includes gpt-4/3.5, internet access and gpt-jailbreak's like DANRun locally here: https://github.com/xtekky/chatgpt-cloneCopyright:This program is licensed under the GNU GPL v3Copyright Notice:xtekky/gpt4free: Copyright (C) 2023 xtekkyThis program is free software: you can redistribute it and/or modifyit under the terms of the GNU General Public License as published bythe Free Software Foundation, either version 3 of the License, or(at your option) any later version.This program is distributed in the hope that it will be useful,but WITHOUT ANY WARRANTY; without even the implied warranty ofMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See theGNU General Public License for more details.You should have received a copy of the GNU General Public Licensealong with this program.  If not, see <https://www.gnu.org/licenses/>.Star History         "
9,open-mmlab/mmdetection,https://github.com/open-mmlab/mmdetection/blob/main/README.md,Python,"           OpenMMLab website                  HOT                      OpenMMLab platform                  TRY IT OUT               📘Documentation |🛠️Installation |👀Model Zoo |🆕Update News |🚀Ongoing Projects |🤔Reporting IssuesEnglish | 简体中文                                              IntroductionMMDetection is an open source object detection toolbox based on PyTorch. It isa part of the OpenMMLab project.The main branch works with PyTorch 1.8+.Major featuresModular DesignWe decompose the detection framework into different components and one can easily construct a customized object detection framework by combining different modules.Support of multiple tasks out of boxThe toolbox directly supports multiple detection tasks such as object detection, instance segmentation, panoptic segmentation, and semi-supervised object detection.High efficiencyAll basic bbox and mask operations run on GPUs. The training speed is faster than or comparable to other codebases, including Detectron2, maskrcnn-benchmark and SimpleDet.State of the artThe toolbox stems from the codebase developed by the MMDet team, who won COCO Detection Challenge in 2018, and we keep pushing it forward.The newly released RTMDet also obtains new state-of-the-art results on real-time instance segmentation and rotated object detection tasks and the best parameter-accuracy trade-off on object detection.Apart from MMDetection, we also released MMEngine for model training and MMCV for computer vision research, which are heavily depended on by this toolbox.What's NewHighlightWe are excited to announce our latest work on real-time object recognition tasks, RTMDet, a family of fully convolutional single-stage detectors. RTMDet not only achieves the best parameter-accuracy trade-off on object detection from tiny to extra-large model sizes but also obtains new state-of-the-art performance on instance segmentation and rotated object detection tasks. Details can be found in the technical report. Pre-trained models are here.TaskDatasetAPFPS(TRT FP16 BS1 3090)Object DetectionCOCO52.8322Instance SegmentationCOCO44.6188Rotated Object DetectionDOTA78.9(single-scale)/81.3(multi-scale)121v3.1.0 was released in 30/6/2023:Supports tracking algorithms including multi-object tracking (MOT) algorithms SORT, DeepSORT, StrongSORT, OCSORT, ByteTrack, QDTrack, and video instance segmentation (VIS) algorithm MaskTrackRCNN, Mask2Former-VIS.Support ViTDetSupports inference and evaluation of multimodal algorithms GLIP and XDecoder, and also supports datasets such as COCO semantic segmentation, COCO Caption, ADE20k general segmentation, and RefCOCO. GLIP fine-tuning will be supported in the future.Provides a gradio demo for image type tasks of MMDetection, making it easy for users to experience.InstallationPlease refer to Installation for installation instructions.Getting StartedPlease see Overview for the general introduction of MMDetection.For detailed user guides and advanced guides, please refer to our documentation:User GuidesTrain & TestLearn about ConfigsInference with existing modelsDataset PrepareTest existing models on standard datasetsTrain predefined models on standard datasetsTrain with customized datasetsTrain with customized models and standard datasetsFinetuning ModelsTest Results SubmissionWeight initializationUse a single stage detector as RPNSemi-supervised Object DetectionUseful ToolsAdvanced GuidesBasic ConceptsComponent CustomizationHow toWe also provide object detection colab tutorial  and instance segmentation colab tutorial .To migrate from MMDetection 2.x, please refer to migration.Overview of Benchmark and Model ZooResults and models are available in the model zoo.  Architectures                    Object Detection                    Instance Segmentation                    Panoptic Segmentation                    Other                                        Fast R-CNN (ICCV'2015)            Faster R-CNN (NeurIPS'2015)            RPN (NeurIPS'2015)            SSD (ECCV'2016)            RetinaNet (ICCV'2017)            Cascade R-CNN (CVPR'2018)            YOLOv3 (ArXiv'2018)            CornerNet (ECCV'2018)            Grid R-CNN (CVPR'2019)            Guided Anchoring (CVPR'2019)            FSAF (CVPR'2019)            CenterNet (CVPR'2019)            Libra R-CNN (CVPR'2019)            TridentNet (ICCV'2019)            FCOS (ICCV'2019)            RepPoints (ICCV'2019)            FreeAnchor (NeurIPS'2019)            CascadeRPN (NeurIPS'2019)            Foveabox (TIP'2020)            Double-Head R-CNN (CVPR'2020)            ATSS (CVPR'2020)            NAS-FCOS (CVPR'2020)            CentripetalNet (CVPR'2020)            AutoAssign (ArXiv'2020)            Side-Aware Boundary Localization (ECCV'2020)            Dynamic R-CNN (ECCV'2020)            DETR (ECCV'2020)            PAA (ECCV'2020)            VarifocalNet (CVPR'2021)            Sparse R-CNN (CVPR'2021)            YOLOF (CVPR'2021)            YOLOX (CVPR'2021)            Deformable DETR (ICLR'2021)            TOOD (ICCV'2021)            DDOD (ACM MM'2021)            RTMDet (ArXiv'2022)            Conditional DETR (ICCV'2021)            DAB-DETR (ICLR'2022)            DINO (ICLR'2023)            GLIP (CVPR'2022)            DiffusionDet (ArXiv'2023)            EfficientDet (CVPR'2020)            Detic (ECCV'2022)                                    Mask R-CNN (ICCV'2017)          Cascade Mask R-CNN (CVPR'2018)          Mask Scoring R-CNN (CVPR'2019)          Hybrid Task Cascade (CVPR'2019)          YOLACT (ICCV'2019)          InstaBoost (ICCV'2019)          SOLO (ECCV'2020)          PointRend (CVPR'2020)          DetectoRS (ArXiv'2020)          SOLOv2 (NeurIPS'2020)          SCNet (AAAI'2021)          QueryInst (ICCV'2021)          Mask2Former (ArXiv'2021)          CondInst (ECCV'2020)          SparseInst (CVPR'2022)          RTMDet (ArXiv'2022)          BoxInst (CVPR'2021)                                      Panoptic FPN (CVPR'2019)          MaskFormer (NeurIPS'2021)          Mask2Former (ArXiv'2021)                                      Contrastive Learning                          SwAV (NeurIPS'2020)          MoCo (CVPR'2020)          MoCov2 (ArXiv'2020)                                  Distillation                          Localization Distillation (CVPR'2022)          Label Assignment Distillation (WACV'2022)                          Semi-Supervised Object Detection                          Soft Teacher (ICCV'2021)                                        Components                    Backbones                    Necks                    Loss                    Common                                  VGG (ICLR'2015)        ResNet (CVPR'2016)        ResNeXt (CVPR'2017)        MobileNetV2 (CVPR'2018)        HRNet (CVPR'2019)        Generalized Attention (ICCV'2019)        GCNet (ICCVW'2019)        Res2Net (TPAMI'2020)        RegNet (CVPR'2020)        ResNeSt (ArXiv'2020)        PVT (ICCV'2021)        Swin (CVPR'2021)        PVTv2 (ArXiv'2021)        ResNet strikes back (ArXiv'2021)        EfficientNet (ArXiv'2021)        ConvNeXt (CVPR'2022)        ConvNeXtv2 (ArXiv'2023)                                PAFPN (CVPR'2018)        NAS-FPN (CVPR'2019)        CARAFE (ICCV'2019)        FPG (ArXiv'2020)        GRoIE (ICPR'2020)        DyHead (CVPR'2021)                                    GHM (AAAI'2019)          Generalized Focal Loss (NeurIPS'2020)          Seasaw Loss (CVPR'2021)                                      OHEM (CVPR'2016)          Group Normalization (ECCV'2018)          DCN (ICCV'2017)          DCNv2 (CVPR'2019)          Weight Standardization (ArXiv'2019)          Prime Sample Attention (CVPR'2020)          Strong Baselines (CVPR'2021)          Resnet strikes back (ArXiv'2021)                        Some other methods are also supported in projects using MMDetection.FAQPlease refer to FAQ for frequently asked questions.ContributingWe appreciate all contributions to improve MMDetection. Ongoing projects can be found in out GitHub Projects. Welcome community users to participate in these projects. Please refer to CONTRIBUTING.md for the contributing guideline.AcknowledgementMMDetection is an open source project that is contributed by researchers and engineers from various colleges and companies. We appreciate all the contributors who implement their methods or add new features, as well as users who give valuable feedbacks.We wish that the toolbox and benchmark could serve the growing research community by providing a flexible toolkit to reimplement existing methods and develop their own new detectors.CitationIf you use this toolbox or benchmark in your research, please cite this project.@article{mmdetection,  title   = {{MMDetection}: Open MMLab Detection Toolbox and Benchmark},  author  = {Chen, Kai and Wang, Jiaqi and Pang, Jiangmiao and Cao, Yuhang and             Xiong, Yu and Li, Xiaoxiao and Sun, Shuyang and Feng, Wansen and             Liu, Ziwei and Xu, Jiarui and Zhang, Zheng and Cheng, Dazhi and             Zhu, Chenchen and Cheng, Tianheng and Zhao, Qijie and Li, Buyu and             Lu, Xin and Zhu, Rui and Wu, Yue and Dai, Jifeng and Wang, Jingdong             and Shi, Jianping and Ouyang, Wanli and Loy, Chen Change and Lin, Dahua},  journal= {arXiv preprint arXiv:1906.07155},  year={2019}}LicenseThis project is released under the Apache 2.0 license.Projects in OpenMMLabMMEngine: OpenMMLab foundational library for training deep learning models.MMCV: OpenMMLab foundational library for computer vision.MMPreTrain: OpenMMLab pre-training toolbox and benchmark.MMagic: OpenMMLab Advanced, Generative and Intelligent Creation toolbox.MMDetection: OpenMMLab detection toolbox and benchmark.MMDetection3D: OpenMMLab's next-generation platform for general 3D object detection.MMRotate: OpenMMLab rotated object detection toolbox and benchmark.MMYOLO: OpenMMLab YOLO series toolbox and benchmark.MMSegmentation: OpenMMLab semantic segmentation toolbox and benchmark.MMOCR: OpenMMLab text detection, recognition, and understanding toolbox.MMPose: OpenMMLab pose estimation toolbox and benchmark.MMHuman3D: OpenMMLab 3D human parametric model toolbox and benchmark.MMSelfSup: OpenMMLab self-supervised learning toolbox and benchmark.MMRazor: OpenMMLab model compression toolbox and benchmark.MMFewShot: OpenMMLab fewshot learning toolbox and benchmark.MMAction2: OpenMMLab's next-generation action understanding toolbox and benchmark.MMTracking: OpenMMLab video perception toolbox and benchmark.MMFlow: OpenMMLab optical flow toolbox and benchmark.MMEditing: OpenMMLab image and video editing toolbox.MMGeneration: OpenMMLab image and video generative models toolbox.MMDeploy: OpenMMLab model deployment framework.MIM: MIM installs OpenMMLab packages.MMEval: A unified evaluation library for multiple machine learning libraries.Playground: A central hub for gathering and showcasing amazing projects built upon OpenMMLab."
10,pallets/flask,https://github.com/pallets/flask/blob/main/README.rst,Python,"FlaskFlask is a lightweight WSGI web application framework. It is designedto make getting started quick and easy, with the ability to scale up tocomplex applications. It began as a simple wrapper around Werkzeugand Jinja and has become one of the most popular Python webapplication frameworks.Flask offers suggestions, but doesn't enforce any dependencies orproject layout. It is up to the developer to choose the tools andlibraries they want to use. There are many extensions provided by thecommunity that make adding new functionality easy.InstallingInstall and update using pip:$ pip install -U FlaskA Simple Example# save this as app.pyfrom flask import Flaskapp = Flask(__name__)@app.route(\""/\"")def hello():    return \""Hello, World!\""$ flask run  * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)ContributingFor guidance on setting up a development environment and how to make acontribution to Flask, see the contributing guidelines.DonateThe Pallets organization develops and supports Flask and the librariesit uses. In order to grow the community of contributors and users, andallow the maintainers to devote more time to the projects, pleasedonate today.LinksDocumentation: https://flask.palletsprojects.com/Changes: https://flask.palletsprojects.com/changes/PyPI Releases: https://pypi.org/project/Flask/Source Code: https://github.com/pallets/flask/Issue Tracker: https://github.com/pallets/flask/issues/Chat: https://discord.gg/pallets"
11,apachecn/ailearning,https://github.com/apachecn/ailearning/blob/master/README.md,Python,"                                AI learning协议：CC BY-NC-SA 4.0一种新技术一旦开始流行，你要么坐上压路机，要么成为铺路石。——Stewart Brand在线阅读在线阅读（v1）QuantLearningApacheCN 中文翻译组 713436582ApacheCN 学习资源注: 广告位合作(物美价廉)，请联系 apachecn@163.com路线图入门只看: 步骤 1 => 2 => 3，你可以当大牛！中级补充 - 资料库: https://github.com/apachecn/ai-roadmap补充算法刷题: https://www.ixigua.com/pseries/6822642486343631363/面试求职: https://www.ixigua.com/pseries/6822563009391493636/机器学习实战: https://www.ixigua.com/pseries/6822816341615968772/NLP教学视频: https://www.ixigua.com/pseries/6828241431295951373/AI常用函数说明: https://github.com/apachecn/AiLearning/tree/master/AI常用函数说明.md1.机器学习 - 基础支持版本VersionSupported3.6.x❌2.7.x✅注意事项:机器学习实战: 仅仅只是学习，请使用 python 2.7.x 版本 （3.6.x 只是修改了部分）基本介绍资料来源: Machine Learning in Action(机器学习实战-个人笔记)统一数据地址: https://github.com/apachecn/data百度云打包地址: apachecn/data#3书籍下载地址: https://github.com/apachecn/data/tree/master/book机器学习下载地址: https://github.com/apachecn/data/tree/master/机器学习深度学习数据地址: https://github.com/apachecn/data/tree/master/深度学习推荐系统数据地址: https://github.com/apachecn/data/tree/master/推荐系统视频网站: 优酷 ／bilibili / Acfun / 网易云课堂，可直接在线播放。（最下方有相应链接）-- 推荐 红色石头: 台湾大学林轩田机器学习笔记-- 推荐 机器学习笔记: https://feisky.xyz/machine-learning学习文档模块章节类型负责人(GitHub)QQ机器学习实战第 1 章: 机器学习基础介绍@毛红动1306014226机器学习实战第 2 章: KNN 近邻算法分类@尤永江279393323机器学习实战第 3 章: 决策树分类@景涛844300439机器学习实战第 4 章: 朴素贝叶斯分类@wnma3mz@分析1003324213244970749机器学习实战第 5 章: Logistic回归分类@微光同尘529925688机器学习实战第 6 章: SVM 支持向量机分类@王德红934969547网上组合内容第 7 章: 集成方法（随机森林和 AdaBoost）分类@片刻529815144机器学习实战第 8 章: 回归回归@微光同尘529925688机器学习实战第 9 章: 树回归回归@微光同尘529925688机器学习实战第 10 章: K-Means 聚类聚类@徐昭清827106588机器学习实战第 11 章: 利用 Apriori 算法进行关联分析频繁项集@刘海飞1049498972机器学习实战第 12 章: FP-growth 高效发现频繁项集频繁项集@程威842725815机器学习实战第 13 章: 利用 PCA 来简化数据工具@廖立娟835670618机器学习实战第 14 章: 利用 SVD 来简化数据工具@张俊皓714974242机器学习实战第 15 章: 大数据与 MapReduce工具@wnma3mz1003324213Ml项目实战第 16 章: 推荐系统（已迁移）项目推荐系统（迁移后地址）第一期的总结2017-04-08: 第一期的总结总结总结529815144网站视频知乎问答-爆炸啦-机器学习该怎么入门？当然我知道，第一句就会被吐槽，因为科班出身的人，不屑的吐了一口唾沫，说傻X，还评论 Andrew Ng 的视频。。我还知道还有一部分人，看 Andrew Ng 的视频就是看不懂，那神秘的数学推导，那迷之微笑的英文版的教学，我何尝又不是这样走过来的？？ 我的心可能比你们都痛，因为我在网上收藏过上10部《机器学习》相关视频，外加国内本土风格的教程: 7月+小象 等等，我都很难去听懂，直到有一天，被一个百度的高级算法分析师推荐说: 《机器学习实战》还不错，通俗易懂，你去试试？？我试了试，还好我的Python基础和调试能力还不错，基本上代码都调试过一遍，很多高大上的 \""理论+推导\""，在我眼中变成了几个 \""加减乘除+循环\""，我想这不就是像我这样的程序员想要的入门教程么？很多程序员说机器学习 TM 太难学了，是的，真 TM 难学，我想最难的是: 没有一本像《机器学习实战》那样的作者愿意以程序员 Coding 角度去给大家讲解！！最近几天，GitHub 涨了 300颗 star，加群的200人， 现在还在不断的增加++，我想大家可能都是感同身受吧！很多想入门新手就是被忽悠着收藏收藏再收藏，但是最后还是什么都没有学到，也就是\""资源收藏家\""，也许新手要的就是 MachineLearning(机器学习) 学习路线图。没错，我可以给你们的一份，因为我们还通过视频记录下来我们的学习过程。水平当然也有限，不过对于新手入门，绝对没问题，如果你还不会，那算我输！！视频怎么看？理论科班出身-建议去学习 Andrew Ng 的视频（Ng 的视频绝对是权威，这个毋庸置疑）编码能力强 - 建议看我们的《机器学习实战-教学版》编码能力弱 - 建议看我们的《机器学习实战-讨论版》，不过在看理论的时候，看 教学版-理论部分；讨论版的废话太多，不过在讲解代码的时候是一行一行讲解的；所以，根据自己的需求，自由的组合。【免费】数学教学视频 - 可汗学院 入门篇@于振梓 推荐: 可汗学院-网易公开课概率统计线性代数可汗学院(概率)可汗学院(统计学)可汗学院(线性代数)机器学习视频 - ApacheCN 教学版AcFunB站优酷网易云课堂【免费】机器/深度学习视频 - 吴恩达机器学习深度学习吴恩达机器学习神经网络和深度学习2.深度学习支持版本VersionSupported3.6.x✅2.7.x❌入门基础反向传递: https://www.cnblogs.com/charlotte77/p/5629865.htmlCNN原理: http://www.cnblogs.com/charlotte77/p/7759802.htmlRNN原理: https://blog.csdn.net/qq_39422642/article/details/78676567LSTM原理: https://blog.csdn.net/weixin_42111770/article/details/80900575Pytorch - 教程-- 待更新TensorFlow 2.0 - 教程-- 待更新目录结构:安装指南Keras 快速入门实战项目 1 电影情感分类实战项目 2 汽车燃油效率实战项目 3 优化 过拟合和欠拟合实战项目 4 古诗词自动生成切分（分词）词性标注命名实体识别句法分析WordNet可以被看作是一个同义词词典词干提取（stemming）与词形还原（lemmatization）https://www.biaodianfu.com/nltk.html/ampTensorFlow 2.0学习网址https://github.com/lyhue1991/eat_tensorflow2_in_30_days3.自然语言处理支持版本VersionSupported3.6.x✅2.7.x❌学习过程中-内心复杂的变化！！！自从学习NLP以后，才发现国内与国外的典型区别:1. 对资源的态度是完全相反的:  1) 国内: 就好像为了名气，举办工作装逼的会议，就是没有干货，全部都是象征性的PPT介绍，不是针对在做的各位  2）国外: 就好像是为了推动nlp进步一样，分享者各种干货资料和具体的实现。（特别是: python自然语言处理）2. 论文的实现:   1) 各种高大上的论文实现，却还是没看到一个像样的GitHub项目！（可能我的搜索能力差了点，一直没找到）  2）国外就不举例了，我看不懂！3. 开源的框架  1）国外的开源框架:  tensorflow/pytorch 文档+教程+视频（官方提供）  2) 国内的开源框架: 额额，还真举例不出来！但是牛逼吹得不比国外差！（MXNet虽然有众多国人参与开发，但不能算是国内开源框架。基于MXNet的动手学深度学习(http://zh.d2l.ai & https://discuss.gluon.ai/t/topic/753)中文教程,已经由沐神(李沐)以及阿斯顿·张讲授录制，公开发布(文档+第一季教程+视频）。)每一次深入都要去翻墙，每一次深入都要Google，每一次看着国内的说: 哈工大、讯飞、中科大、百度、阿里多牛逼，但是资料还是得国外去找！有时候真的挺恨的！真的有点瞧不起自己国内的技术环境！当然谢谢国内很多博客大佬，特别是一些入门的Demo和基本概念。【深入的水平有限，没看懂】【入门须知】必须了解: https://github.com/apachecn/AiLearning/tree/master/nlp【入门教程】强烈推荐: PyTorch 自然语言处理: https://github.com/apachecn/NLP-with-PyTorchPython 自然语言处理 第二版: https://usyiyi.github.io/nlp-py-2e-zh推荐一个liuhuanyong大佬整理的nlp全面知识体系: https://liuhuanyong.github.io开源 - 词向量库集合:https://www.cnblogs.com/Darwin2000/p/5786984.htmlhttps://ai.tencent.com/ailab/nlp/embedding.htmlhttps://blog.csdn.net/xiezj007/article/details/85073890https://github.com/Embedding/Chinese-Word-Vectorshttps://github.com/brightmart/nlp_chinese_corpushttps://github.com/codemayq/chinese_chatbot_corpushttps://github.com/candlewill/Dialog_Corpus1.使用场景 （百度公开课）第一部分 入门介绍1.) 自然语言处理入门介绍第二部分 机器翻译2.) 机器翻译第三部分 篇章分析3.1.) 篇章分析-内容概述3.2.) 篇章分析-内容标签3.3.) 篇章分析-情感分析3.4.) 篇章分析-自动摘要第四部分 UNIT-语言理解与交互技术4.) UNIT-语言理解与交互技术应用领域中文分词:构建DAG图动态规划查找，综合正反向（正向加权反向输出）求得DAG最大概率路径使用了SBME语料训练了一套 HMM + Viterbi 模型，解决未登录词问题1.文本分类（Text Classification）文本分类是指标记句子或文档，例如电子邮件垃圾邮件分类和情感分析。下面是一些很好的初学者文本分类数据集。路透社Newswire主题分类（路透社-21578）。1987年路透社出现的一系列新闻文件，按类别编制索引。另见RCV1，RCV2和TRC2。IMDB电影评论情感分类（斯坦福）。来自网站imdb.com的一系列电影评论及其积极或消极的情绪。新闻组电影评论情感分类（康奈尔）。来自网站imdb.com的一系列电影评论及其积极或消极的情绪。有关更多信息，请参阅帖子:单标签文本分类的数据集。情感分析比赛地址: https://www.kaggle.com/c/word2vec-nlp-tutorial方案一(0.86): WordCount + 朴素 Bayes方案二(0.94): LDA + 分类模型（knn/决策树/逻辑回归/svm/xgboost/随机森林）a) 决策树效果不是很好，这种连续特征不太适合的b) 通过参数调整 200 个topic，信息量保存效果较优（计算主题）方案三(0.72): word2vec + CNN说实话: 没有一个好的机器，是调不出来一个好的结果 (: 逃通过AUC 来评估模型的效果2.语言模型（Language Modeling）语言建模涉及开发一种统计模型，用于预测句子中的下一个单词或一个单词中的下一个单词。它是语音识别和机器翻译等任务中的前置任务。它是语音识别和机器翻译等任务中的前置任务。下面是一些很好的初学者语言建模数据集。古腾堡项目，一系列免费书籍，可以用纯文本检索各种语言。还有更多正式的语料库得到了很好的研究; 例如:布朗大学现代美国英语标准语料库。大量英语单词样本。谷歌10亿字语料库。新词发现中文分词新词发现python3利用互信息和左右信息熵的中文分词新词发现https://github.com/zhanzecheng/Chinese_segment_augment句子相似度识别项目地址: https://www.kaggle.com/c/quora-question-pairs解决方案: word2vec + Bi-GRU文本纠错bi-gram + levenshtein3.图像字幕（Image Captioning）mage字幕是为给定图像生成文本描述的任务。下面是一些很好的初学者图像字幕数据集。上下文中的公共对象（COCO）。包含超过12万张带描述的图像的集合Flickr 8K。从flickr.com获取的8千个描述图像的集合。Flickr 30K。从flickr.com获取的3万个描述图像的集合。欲了解更多，请看帖子:探索图像字幕数据集，2016年4.机器翻译（Machine Translation）机器翻译是将文本从一种语言翻译成另一种语言的任务。下面是一些很好的初学者机器翻译数据集。加拿大第36届议会的协调国会议员。成对的英语和法语句子。欧洲议会诉讼平行语料库1996-2011。句子对一套欧洲语言。有大量标准数据集用于年度机器翻译挑战; 看到:统计机器翻译机器翻译Encoder + Decoder(Attention)参考案例: http://pytorch.apachecn.org/cn/tutorials/intermediate/seq2seq_translation_tutorial.html5.问答系统（Question Answering）问答是一项任务，其中提供了一个句子或文本样本，从中提出问题并且必须回答问题。下面是一些很好的初学者问题回答数据集。斯坦福问题回答数据集（SQuAD）。回答有关维基百科文章的问题。Deepmind问题回答语料库。从每日邮报回答有关新闻文章的问题。亚马逊问答数据。回答有关亚马逊产品的问题。有关更多信息，请参阅帖子:数据集: 我如何获得问答网站的语料库，如Quora或Yahoo Answers或Stack Overflow来分析答案质量？6.语音识别（Speech Recognition）语音识别是将口语的音频转换为人类可读文本的任务。下面是一些很好的初学者语音识别数据集。TIMIT声学 - 语音连续语音语料库。不是免费的，但因其广泛使用而上市。口语美国英语和相关的转录。VoxForge。用于构建用于语音识别的开源数据库的项目。LibriSpeech ASR语料库。从LibriVox收集的大量英语有声读物。7.自动文摘（Document Summarization）文档摘要是创建较大文档的简短有意义描述的任务。下面是一些很好的初学者文档摘要数据集。法律案例报告数据集。收集了4000份法律案件及其摘要。TIPSTER文本摘要评估会议语料库。收集了近200份文件及其摘要。英语新闻文本的AQUAINT语料库。不是免费的，而是广泛使用的。新闻文章的语料库。欲了解更多信息:文档理解会议（DUC）任务。在哪里可以找到用于文本摘要的良好数据集？命名实体识别Bi-LSTM CRF参考案例: http://pytorch.apachecn.org/cn/tutorials/beginner/nlp/advanced_tutorial.htmlCRF推荐文档: https://www.jianshu.com/p/55755fc649b1文本摘要抽取式word2vec + textrankword2vec推荐文档: https://www.zhihu.com/question/44832436/answer/266068967textrank推荐文档: https://blog.csdn.net/BaiHuaXiu123/article/details/77847232Graph图计算【慢慢更新】数据集: https://github.com/apachecn/data/tree/master/graph学习资料: spark graphX实战.pdf 【文件太大不方便提供，自己百度】知识图谱知识图谱，我只认 SimmerChan: 【知识图谱-给AI装个大脑】说实话，我是看这博主老哥写的博客长大的，写的真的是深入浅出。我很喜欢，所以就分享给大家，希望你们也喜欢。进一步阅读如果您希望更深入，本节提供了其他数据集列表。维基百科研究中使用的文本数据集数据集: 计算语言学家和自然语言处理研究人员使用的主要文本语料库是什么？斯坦福统计自然语言处理语料库按字母顺序排列的NLP数据集列表该机构NLTK在DL4J上打开深度学习数据NLP数据集国内开放数据集: https://bosonnlp.com/dev/resource参考比赛收集平台pbharrin/machinelearninginactionML Mastery致谢最近无意收到群友推送的链接，发现得到大佬高度的认可，并在热心的推广。在此感谢:量子位人工智能前沿讲习赞助我们"
12,vagabond-systems/jpmc-task-1,https://github.com/vagabond-systems/jpmc-task-1/blob/main/README.md,Python,JPMC Task 1Starter repo for task 1 of the JPMC software engineering program
13,langchain-ai/langchain,https://github.com/langchain-ai/langchain/blob/master/README.md,Python,"🦜️🔗 LangChain⚡ Building applications with LLMs through composability ⚡Looking for the JS/TS version? Check out LangChain.js.Production Support: As you move your LangChains into production, we'd love to offer more hands-on support.Fill out this form to share more about what you're building, and our team will get in touch.🚨Breaking Changes for select chains (SQLDatabase) on 7/28/23In an effort to make langchain leaner and safer, we are moving select chains to langchain_experimental.This migration has already started, but we are remaining backwards compatible until 7/28.On that date, we will remove functionality from langchain.Read more about the motivation and the progress here.Read how to migrate your code here.Quick Installpip install langchainorpip install langsmith && conda install langchain -c conda-forge🤔 What is this?Large language models (LLMs) are emerging as a transformative technology, enabling developers to build applications that they previously could not. However, using these LLMs in isolation is often insufficient for creating a truly powerful app - the real power comes when you can combine them with other sources of computation or knowledge.This library aims to assist in the development of those types of applications. Common examples of these applications include:❓ Question Answering over specific documentsDocumentationEnd-to-end Example: Question Answering over Notion Database💬 ChatbotsDocumentationEnd-to-end Example: Chat-LangChain🤖 AgentsDocumentationEnd-to-end Example: GPT+WolframAlpha📖 DocumentationPlease see here for full documentation on:Getting started (installation, setting up the environment, simple examples)How-To examples (demos, integrations, helper functions)Reference (full API docs)Resources (high-level explanation of core concepts)🚀 What can this help with?There are six main areas that LangChain is designed to help with.These are, in increasing order of complexity:📃 LLMs and Prompts:This includes prompt management, prompt optimization, a generic interface for all LLMs, and common utilities for working with LLMs.🔗 Chains:Chains go beyond a single LLM call and involve sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.📚 Data Augmented Generation:Data Augmented Generation involves specific types of chains that first interact with an external data source to fetch data for use in the generation step. Examples include summarization of long pieces of text and question/answering over specific data sources.🤖 Agents:Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end-to-end agents.🧠 Memory:Memory refers to persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory.🧐 Evaluation:[BETA] Generative models are notoriously hard to evaluate with traditional metrics. One new way of evaluating them is using language models themselves to do the evaluation. LangChain provides some prompts/chains for assisting in this.For more information on these concepts, please see our full documentation.💁 ContributingAs an open-source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infrastructure, or better documentation.For detailed information on how to contribute, see here."
14,pytorch/vision,https://github.com/pytorch/vision/blob/main/README.md,Python,"torchvisionThe torchvision package consists of popular datasets, model architectures, and common image transformations for computervision.InstallationPlease refer to the officialinstructions to install the stableversions of torch and torchvision on your system.To build source, refer to our contributingpage.The following is the corresponding torchvision versions and supported Pythonversions.torchtorchvisionPythonmain / nightlymain / nightly>=3.8, <=3.112.00.15>=3.8, <=3.111.130.14>=3.7.2, <=3.101.120.13>=3.7, <=3.10    older versionstorchtorchvisionPython1.110.12>=3.7, <=3.101.100.11>=3.6, <=3.91.90.10>=3.6, <=3.91.80.9>=3.6, <=3.91.70.8>=3.6, <=3.91.60.7>=3.6, <=3.81.50.6>=3.5, <=3.81.40.5==2.7, >=3.5, <=3.81.30.4.2 / 0.4.3==2.7, >=3.5, <=3.71.20.4.1==2.7, >=3.5, <=3.71.10.3==2.7, >=3.5, <=3.7<=1.00.2==2.7, >=3.5, <=3.7Image BackendsTorchvision currently supports the following image backends:torch tensorsPIL images:PillowPillow-SIMD - a much faster drop-in replacement for Pillow with SIMD.Read more in in our docs.[UNSTABLE] Video BackendTorchvision currently supports the following video backends:pyav (default) - Pythonic binding for ffmpeg libraries.video_reader - This needs ffmpeg to be installed and torchvision to be built from source. There shouldn't be anyconflicting version of ffmpeg installed. Currently, this is only supported on Linux.conda install -c conda-forge ffmpegpython setup.py installUsing the models on C++TorchVision provides an example project for how to use the models on C++ using JIT Script.Installation From source:mkdir buildcd build# Add -DWITH_CUDA=on support for the CUDA if neededcmake ..makemake installOnce installed, the library can be accessed in cmake (after properly configuring CMAKE_PREFIX_PATH) via theTorchVision::TorchVision target:find_package(TorchVision REQUIRED)target_link_libraries(my-target PUBLIC TorchVision::TorchVision)The TorchVision package will also automatically look for the Torch package and add it as a dependency tomy-target, so make sure that it is also available to cmake via the CMAKE_PREFIX_PATH.For an example setup, take a look at examples/cpp/hello_world.Python linking is disabled by default when compiling TorchVision with CMake, this allows you to run models without anyPython dependency. In some special cases where TorchVision's operators are used from Python code, you may need to linkto Python. This can be done by passing -DUSE_PYTHON=on to CMake.TorchVision OperatorsIn order to get the torchvision operators registered with torch (eg. for the JIT), all you need to do is to ensure thatyou #include <torchvision/vision.h> in your project.DocumentationYou can find the API documentation on the pytorch website: https://pytorch.org/vision/stable/index.htmlContributingSee the CONTRIBUTING file for how to help out.Disclaimer on DatasetsThis is a utility library that downloads and prepares public datasets. We do not host or distribute these datasets,vouch for their quality or fairness, or claim that you have license to use the dataset. It is your responsibility todetermine whether you have permission to use the dataset under the dataset's license.If you're a dataset owner and wish to update any part of it (description, citation, etc.), or do not want your datasetto be included in this library, please get in touch through a GitHub issue. Thanks for your contribution to the MLcommunity!Pre-trained Model LicenseThe pre-trained models provided in this library may have their own licenses or terms and conditions derived from thedataset used for training. It is your responsibility to determine whether you have permission to use the models for youruse case.More specifically, SWAG models are released under the CC-BY-NC 4.0 license. SeeSWAG LICENSE for additional details.Citing TorchVisionIf you find TorchVision useful in your work, please consider citing the following BibTeX entry:@software{torchvision2016,    title        = {TorchVision: PyTorch's Computer Vision library},    author       = {TorchVision maintainers and contributors},    year         = 2016,    journal      = {GitHub repository},    publisher    = {GitHub},    howpublished = {\\url{https://github.com/pytorch/vision}}}"
15,unifyai/ivy,https://github.com/unifyai/ivy/blob/main/README.md,Python,"🚀 We are granting pilot access to Ivy's Compiler and Transpilerto some users, join the waitlist if youwant to test them out!                    Status                                                                                                                                Unified AI                                                                                                Ivy is both an ML transpiler and a framework, currently supporting JAX,TensorFlow, PyTorch and Numpy.Ivy unifies all ML frameworks 💥 enabling you not only to write codethat can be used with any of these frameworks as the backend, but alsoto convert 🔄 any function, model or library written in any of them toyour preferred framework!You can check out Ivy as a transpiler and Ivyas a framework to learn more about this, try outIvy straight away going through the Setting up Ivysection, or dive deep into Ivy's Documentation andExamples!If you would like to contribute, you can join our growingCommunity 🌍, check out our Contributingguide, and take a look at the opentasksif you'd like to dive straight in 🧑‍💻Let's unify.ai together 🦾Ivy as a transpilerIvy's transpiler allows you to use code from any other framework (orfrom any other version of the same framework!) in your own code, by justadding one line of code. Under the hood, Ivy traces a computationalgraph and leverages the frontends and backends to link one framework toanother.This way, Ivy makes all ML-related projects available for you,independently of the framework you want to use to research, develop, ordeploy systems. Feel free to head over to the docs for the full APIreference, but the functions you'd most likely want to use are:# Compiles a function into an efficient fully-functional graph, removing all wrapping and redundant codeivy.compile()# Converts framework-specific code to a different frameworkivy.transpile()# Converts framework-specific code to Ivyivy.unify()These functions can be used eagerly or lazily. If you pass the necessaryarguments for function tracing, the compilation/transpilation step willhappen instantly (eagerly). Otherwise, the compilation/transpilationwill happen only when the returned function is first invoked.import ivyimport jaxivy.set_backend(\""jax\"")# Simple JAX function to transpiledef test_fn(x):    return jax.numpy.sum(x)x1 = ivy.array([1., 2.])# Arguments are available -> transpilation happens eagerlyeager_graph = ivy.transpile(test_fn, source=\""jax\"", to=\""torch\"", args=(x1,))# eager_graph is now torch code and runs efficientlyret = eager_graph(x1)# Arguments are not available -> transpilation happens lazilylazy_graph = ivy.transpile(test_fn, source=\""jax\"", to=\""torch\"")# The transpiled graph is initialized, transpilation will happen hereret = lazy_graph(x1)# lazy_graph is now torch code and runs efficientlyret = lazy_graph(x1)If you want to learn more, you can find more information in the Ivy asa transpiler section of thedocs!When should I use Ivy as a transpiler?If you want to use building blocks published in other frameworks (neuralnetworks, layers, array computing libraries, training pipelines...),you want to integrate code developed in various frameworks, or maybestraight up move code from one framework to another, the transpiler isdefinitely the tool 🔧 for the job! As the output of transpilation isnative code in the target framework, you can use the converted code justas if it was code originally developed in that framework, applyingframework-specific optimizations or tools, instantly exposing yourproject to all of the unique perks of a different framework.Ivy as a frameworkThe Ivy framework is built on top of various essential components,mainly the BackendHandler,which manages what framework is being used behind the scenes and theBackend FunctionalAPIs,which provide framework-specific implementations of the Ivy functions.Likewise, classes such as ivy.Container or ivy.Array are alsoavailable, facilitating the use of structured data and array-likeobjects (learn more about themhere!).All of the functionalities in Ivy are exposed through theIvy functional API and the Ivy stateful API. All functions in theFunctionalAPIare Framework Agnostic Functions, which mean that we can use themlike this:import ivyimport jax.numpy as jnpimport tensorflow as tfimport numpy as npimport torchdef mse_loss(y, target):    return ivy.mean((y - target)**2)jax_mse   = mse_loss(jnp.ones((5,)), jnp.ones((5,)))tf_mse    = mse_loss(tf.ones((5,)), tf.ones((5,)))np_mse    = mse_loss(np.ones((5,)), np.ones((5,)))torch_mse = mse_loss(torch.ones((5,)), torch.ones((5,)))In the example above we show how Ivy's functions are compatible withtensors from different frameworks. This is the same for ALL Ivyfunctions. They can accept tensors from any framework and return thecorrect result.The Ivy StatefulAPI,on the other hand, allows you to define trainable modules and layers,which you can use alone or as a part of any other framework code!import ivyclass Regressor(ivy.Module):    def __init__(self, input_dim, output_dim):        self.input_dim = input_dim        self.output_dim = output_dim        super().__init__()    def _build(self, *args, **kwargs):        self.linear0 = ivy.Linear(self.input_dim, 128)        self.linear1 = ivy.Linear(128, self.output_dim)    def _forward(self, x):        x = self.linear0(x)        x = ivy.functional.relu(x)        x = self.linear1(x)        return xIf we put it all together, we'll have something like this. This exampleuses PyTorch as the backend, but this can easily be changed to yourfavorite framework, such as TensorFlow, or JAX.import ivyclass Regressor(ivy.Module):    def __init__(self, input_dim, output_dim):        self.input_dim = input_dim        self.output_dim = output_dim        super().__init__()    def _build(self, *args, **kwargs):        self.linear0 = ivy.Linear(self.input_dim, 128)        self.linear1 = ivy.Linear(128, self.output_dim)    def _forward(self, x):        x = self.linear0(x)        x = ivy.functional.relu(x)        x = self.linear1(x)        return xivy.set_backend('torch')  # set backend to PyTorch (or any other backend!)model = Regressor(input_dim=1, output_dim=1)optimizer = ivy.Adam(0.3)n_training_examples = 2000noise = ivy.random.random_normal(shape=(n_training_examples, 1), mean=0, std=0.1)x = ivy.linspace(-6, 3, n_training_examples).reshape((n_training_examples, 1))y = 0.2 * x ** 2 + 0.5 * x + 0.1 + noisedef loss_fn(v, x, target):    pred = model(x, v=v)    return ivy.mean((pred - target) ** 2)for epoch in range(40):    # forward pass    pred = model(x)    # compute loss and gradients    loss, grads = ivy.execute_with_gradients(lambda params: loss_fn(*params), (model.v, x, y))    # update parameters    model.v = optimizer.step(model.v, grads)    # print current loss    print(f'Epoch: {epoch + 1:2d} --- Loss: {ivy.to_numpy(loss).item():.5f}')print('Finished training!')The model's output can be visualized as follows:   Last but not least, we are also working on specific extension totallywritten in Ivy and therefore usable within any framework, coveringtopics like Mechanics, ComputerVision,Robotics, a Reinforcement LearningGym,Memory and implementation ofvarious Models or Buildertools with trainers, data loadersand more!                                                        As always, you can find more information about Ivy as a framework inthedocs!When should I use Ivy as a framework?As Ivy supports multiple backends, writing code in Ivy breaks you freefrom framework limitations. If you want to publish highly flexible codefor everyone to use, independently of the framework they are using, oryou plan to develop ML-related tools and want them to be interoperablewith not only the already existing frameworks, but also with futureframeworks, then Ivy is for you!Setting up IvyThere are various ways to use Ivy, depending on your preferredenvironment:Installing using pipThe easiest way to set up Ivy is to install it using pip with thefollowing command:pip install ivyor alternatively:python3 -m pip install ivyDockerIf you prefer to use containers, we also have pre-built Docker imageswith all the supported frameworks and some relevant packages alreadyinstalled, which you can pull from:docker pull unifyai/ivy:latestIf you are working on a GPU device, you can pull from:docker pull unifyai/ivy:latest-gpuInstalling from sourceYou can also install Ivy from source if you want to take advantage ofthe latest changes, but we can't ensure everything will work asexpected. 😅git clone https://github.com/unifyai/ivy.gitcd ivy pip install --user -e .or alternatively, for the last step:python3 -m pip install --user -e .If you want to set up testing and various frameworks it's probably bestto check out the Contributing - SettingUppage, where OS-specific and IDE-specific instructions and videotutorials to do so are available!Using IvyYou can find quite a lot more examples in the corresponding sectionbelow, but using Ivy is as simple as:Multi-backend Supportimport ivyimport torchimport jaxivy.set_backend(\""jax\"")x = jax.numpy.array([1, 2, 3])y = jax.numpy.array([3, 2, 1])z = ivy.add(x, y)ivy.set_backend('torch')x = torch.tensor([1, 2, 3])y = torch.tensor([3, 2, 1])z = ivy.add(x, y)Transpilation APIimport ivyimport torchimport jaxdef jax_fn(x):    a = jax.numpy.dot(x, x)    b = jax.numpy.mean(x)    return x * a + bjax_x = jax.numpy.array([1, 2, 3])torch_x = torch.tensor([1, 2, 3])torch_fn = ivy.transpile(jax_fn, source=\""jax\"", to=\""torch\"", args=(jax_x,))ret = torch_fn(torch_x)DocumentationThe Ivy Docs page holds all the relevantinformation about Ivy and its framework API reference.There, you will find theDesign page, which isa user-focused guide about the architecture and the building blocks ofIvy. Likewise, you can take a look at the Deepdive, which isoriented towards potential contributors of the code base and explainsthe nuances of Ivy in full detail 🔎Another important sections of the docs isBackground, whichcontextualises the problem Ivy is trying to solve and the current MLExplosion,explaining both (1) why is important to solve thisproblemand (2) how we are adhering to existingstandardsto make this happen.Lastly, you can also find there the RelatedWork section,which paints a clear picture of the role Ivy plays in the ML stack,comparing it to other existing solutions in terms of functionalities andlevel.ExamplesThe Examples page features a wide range ofdemos and tutorials showcasing the functionalities of Ivy along withmultiple use cases, but feel free to check out some shorterframework-specific examples here ⬇️I'm using PyTorch    You can use Ivy to get PyTorch code from:               Any model                                    From TensorFlowimport ivyimport torchimport tensorflow as tf# Get a pretrained keras modeleff_encoder = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(    include_top=False, weights=\""imagenet\"", input_shape=(224, 224, 3))# Transpile it into a torch.nn.Module with the corresponding parametersnoise = tf.random.normal(shape=(1, 224, 224, 3))torch_eff_encoder = ivy.transpile(eff_encoder, to=\""torch\"", args=(noise,))# Build a classifier using the transpiled encoderclass Classifier(torch.nn.Module):    def __init__(self, num_classes=20):        super(Classifier, self).__init__()        self.encoder = torch_eff_encoder        self.fc = torch.nn.Linear(1280, num_classes)    def forward(self, x):        x = self.encoder(x)        return self.fc(x)# Initialize a trainable, customizable, torch.nn.Moduleclassifier = Classifier()ret = classifier(torch.rand((1, 244, 244, 3)))   From JAXimport ivyimport jaximport torch# Get a pretrained haiku model# https://unify.ai/demos/scripts/deepmind_perceiver_io.pyfrom deepmind_perceiver_io import key, perceiver_backbone# Transpile it into a torch.nn.Module with the corresponding parametersdummy_input = jax.random.uniform(key, shape=(1, 3, 224, 224))params = perceiver_backbone.init(rng=key, images=dummy_input)backbone = ivy.transpile(    perceiver_backbone, to=\""torch\"", params_v=params, kwargs={\""images\"": dummy_input})# Build a classifier using the transpiled backboneclass PerceiverIOClassifier(torch.nn.Module):    def __init__(self, num_classes=20):        super(PerceiverIOClassifier, self).__init__()        self.backbone = backbone        self.max_pool = torch.nn.MaxPool2d((512, 1))        self.flatten = torch.nn.Flatten()        self.fc = torch.nn.Linear(1024, num_classes)    def forward(self, x):        x = self.backbone(images=x)        x = self.flatten(self.max_pool(x))        return self.fc(x)# Initialize a trainable, customizable, torch.nn.Moduleclassifier = PerceiverIOClassifier()ret = classifier(torch.rand((1, 3, 224, 224)))Any library   From Tensorflowimport ivyimport torchimport osos.environ[\""SM_FRAMEWORK\""] = \""tf.keras\""import segmentation_models as sm# transpile sm from tensorflow to torchtorch_sm = ivy.transpile(sm, source=\""tensorflow\"", to=\""torch\"")# get some image-like arraysoutput = torch.rand((1, 3, 512, 512))target = torch.rand((1, 3, 512, 512))# and use the transpiled version of any function from the library!out = torch_sm.metrics.iou_score(output, target)   From JAXimport ivyimport raximport torch# transpile rax from jax to torchtorch_rax = ivy.transpile(rax, source=\""jax\"", to=\""torch\"")# get some arraysscores = torch.tensor([2.2, 1.3, 5.4])labels = torch.tensor([1.0, 0.0, 0.0])# and use the transpiled version of any function from the library!out = torch_rax.poly1_softmax_loss(scores, labels)   From NumPyimport ivyimport torchimport madmom# transpile madmon from numpy to torchtorch_madmom = ivy.transpile(madmom, source=\""numpy\"", to=\""torch\"")# get some arraysfreqs = torch.arange(20) * 10# and use the transpiled version of any function from the library!out = torch_madmom.audio.filters.hz2midi(freqs)Any function   From Tensorflowimport ivyimport tensorflow as tfimport torchdef loss(predictions, targets):    return tf.sqrt(tf.reduce_mean(tf.square(predictions - targets)))# transpile any function from tf to torchtorch_loss = ivy.transpile(loss, source=\""tensorflow\"", to=\""torch\"")# get some arraysp = torch.tensor([3.0, 2.0, 1.0])t = torch.tensor([0.0, 0.0, 0.0])# and use the transpiled version!out = torch_loss(p, t)   From JAXimport ivyimport jax.numpy as jnpimport torchdef loss(predictions, targets):    return jnp.sqrt(jnp.mean((predictions - targets) ** 2))# transpile any function from jax to torchtorch_loss = ivy.transpile(loss, source=\""jax\"", to=\""torch\"")# get some arraysp = torch.tensor([3.0, 2.0, 1.0])t = torch.tensor([0.0, 0.0, 0.0])# and use the transpiled version!out = torch_loss(p, t)   From NumPyimport ivyimport numpy as npimport torchdef loss(predictions, targets):    return np.sqrt(np.mean((predictions - targets) ** 2))# transpile any function from numpy to torchtorch_loss = ivy.transpile(loss, source=\""numpy\"", to=\""torch\"")# get some arraysp = torch.tensor([3.0, 2.0, 1.0])t = torch.tensor([0.0, 0.0, 0.0])# and use the transpiled version!out = torch_loss(p, t)I'm using TensorFlow You can use Ivy to get TensorFlow code from:Any model   From PyTorchimport ivyimport torchimport timmimport tensorflow as tf# Get a pretrained pytorch modelmlp_encoder = timm.create_model(\""mixer_b16_224\"", pretrained=True, num_classes=0)# Transpile it into a keras.Model with the corresponding parametersnoise = torch.randn(1, 3, 224, 224)mlp_encoder = ivy.transpile(mlp_encoder, to=\""tensorflow\"", args=(noise,))# Build a classifier using the transpiled encoderclass Classifier(tf.keras.Model):    def __init__(self):        super(Classifier, self).__init__()        self.encoder = mlp_encoder        self.output_dense = tf.keras.layers.Dense(units=1000, activation=\""softmax\"")    def call(self, x):        x = self.encoder(x)        return self.output_dense(x)# Transform the classifier and use it as a standard keras.Modelx = tf.random.normal(shape=(1, 3, 224, 224))model = Classifier()ret = model(x)   From JAXimport ivyimport jaximport tensorflow as tf# Get a pretrained haiku model# https://unify.ai/demos/scripts/deepmind_perceiver_io.pyfrom deepmind_perceiver_io import key, perceiver_backbone# Transpile it into a tf.keras.Model with the corresponding parametersdummy_input = jax.random.uniform(key, shape=(1, 3, 224, 224))params = perceiver_backbone.init(rng=key, images=dummy_input)backbone = ivy.transpile(    perceiver_backbone, to=\""tensorflow\"", params_v=params, args=(dummy_input,))# Build a classifier using the transpiled backboneclass PerceiverIOClassifier(tf.keras.Model):    def __init__(self, num_classes=20):        super(PerceiverIOClassifier, self).__init__()        self.backbone = backbone        self.max_pool = tf.keras.layers.MaxPooling1D(pool_size=512)        self.flatten = tf.keras.layers.Flatten()        self.fc = tf.keras.layers.Dense(num_classes)    def call(self, x):        x = self.backbone(x)        x = self.flatten(self.max_pool(x))        return self.fc(x)# Initialize a trainable, customizable, tf.keras.Modelx = tf.random.normal(shape=(1, 3, 224, 224))classifier = PerceiverIOClassifier()ret = classifier(x)Any library   From PyTorchimport ivyimport korniaimport requestsimport numpy as npimport tensorflow as tffrom PIL import Image# transpile kornia from torch to tensorflowtf_kornia = ivy.transpile(kornia, source=\""torch\"", to=\""tensorflow\"")# get an imageurl = \""http://images.cocodataset.org/train2017/000000000034.jpg\""raw_img = Image.open(requests.get(url, stream=True).raw)# convert it to the format expected by korniaimg = np.array(raw_img)img = tf.transpose(tf.constant(img), (2, 0, 1))img = tf.expand_dims(img, 0) / 255# and use the transpiled version of any function from the library!out = tf_kornia.enhance.sharpness(img, 5)   From JAXimport ivyimport raximport tensorflow as tf# transpile rax from jax to tensorflowtf_rax = ivy.transpile(rax, source=\""jax\"", to=\""tensorflow\"")# get some arraysscores = tf.constant([2.2, 1.3, 5.4])labels = tf.constant([1.0, 0.0, 0.0])# and use the transpiled version of any function from the library!out = tf_rax.poly1_softmax_loss(scores, labels)   From NumPyimport ivyimport madmomimport tensorflow as tf# transpile madmom from numpy to tensorflowtf_madmom = ivy.transpile(madmom, source=\""numpy\"", to=\""tensorflow\"")# get some arraysfreqs = tf.range(20) * 10# and use the transpiled version of any function from the library!out = tf_madmom.audio.filters.hz2midi(freqs)Any function   From PyTorchimport ivyimport torchimport tensorflow as tfdef loss(predictions, targets):    return torch.sqrt(torch.mean((predictions - targets) ** 2))# transpile any function from torch to tensorflowtf_loss = ivy.transpile(loss, source=\""torch\"", to=\""tensorflow\"")# get some arraysp = tf.constant([3.0, 2.0, 1.0])t = tf.constant([0.0, 0.0, 0.0])# and use the transpiled version!out = tf_loss(p, t)   From JAXimport ivyimport jax.numpy as jnpimport tensorflow as tfdef loss(predictions, targets):    return jnp.sqrt(jnp.mean((predictions - targets) ** 2))# transpile any function from jax to tensorflowtf_loss = ivy.transpile(loss, source=\""jax\"", to=\""tensorflow\"")# get some arraysp = tf.constant([3.0, 2.0, 1.0])t = tf.constant([0.0, 0.0, 0.0])# and use the transpiled version!out = tf_loss(p, t)   From NumPyimport ivyimport numpy as npimport tensorflow as tfdef loss(predictions, targets):    return np.sqrt(np.mean((predictions - targets) ** 2))# transpile any function from numpy to tensorflowtf_loss = ivy.transpile(loss, source=\""numpy\"", to=\""tensorflow\"")# get some arraysp = tf.constant([3.0, 2.0, 1.0])t = tf.constant([0.0, 0.0, 0.0])# and use the transpiled version!out = tf_loss(p, t)I'm using Jax You can use Ivy to get JAX code from:Any model   From PyTorchimport ivyimport timmimport torchimport jaximport haiku as hk# Get a pretrained pytorch modelmlp_encoder = timm.create_model(\""mixer_b16_224\"", pretrained=True, num_classes=0)# Transpile it into a hk.Module with the corresponding parametersnoise = torch.randn(1, 3, 224, 224)mlp_encoder = ivy.transpile(mlp_encoder, to=\""jax\"", args=(noise,))# Build a classifier using the transpiled encoderclass Classifier(hk.Module):    def __init__(self, num_classes=1000):        super(Classifier, self).__init__()        self.encoder = mlp_encoder()        self.fc = hk.Linear(output_size=num_classes, with_bias=True)    def __call__(self, x):        x = self.encoder(x)        x = self.fc(x)        return xdef _forward_classifier(x):    module = Classifier()    return module(x)# Transform the classifier and use it as a standard hk.Modulerng_key = jax.random.PRNGKey(42)x = jax.random.uniform(key=rng_key, shape=(1, 3, 224, 224), dtype=jax.numpy.float32)forward_classifier = hk.transform(_forward_classifier)params = forward_classifier.init(rng=rng_key, x=x)ret = forward_classifier.apply(params, None, x)   From TensorFlowimport ivyimport jaximport haiku as hkimport tensorflow as tf# Get a pretrained keras modeleff_encoder = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(    include_top=False, weights=\""imagenet\"", input_shape=(224, 224, 3))# Transpile it into a hk.Module with the corresponding parametersnoise = tf.random.normal(shape=(1, 224, 224, 3))hk_eff_encoder = ivy.transpile(eff_encoder, to=\""jax\"", args=(noise,))# Build a classifier using the transpiled encoderclass Classifier(hk.Module):    def __init__(self, num_classes=1000):        super(Classifier, self).__init__()        self.encoder = hk_eff_encoder()        self.fc = hk.Linear(output_size=num_classes, with_bias=True)    def __call__(self, x):        x = self.encoder(x)        x = self.fc(x)        return xdef _forward_classifier(x):    module = Classifier()    return module(x)# Transform the classifier and use it as a standard hk.Modulerng_key = jax.random.PRNGKey(42)dummy_x = jax.random.uniform(key=rng_key, shape=(1, 224, 224, 3))forward_classifier = hk.transform(_forward_classifier)params = forward_classifier.init(rng=rng_key, x=dummy_x)ret = forward_classifier.apply(params, None, dummy_x)Any library   From PyTorchimport ivyimport korniaimport requestsimport jax.numpy as jnpfrom PIL import Image# transpile kornia from torch to jaxjax_kornia = ivy.transpile(kornia, source=\""torch\"", to=\""jax\"")# get an imageurl = \""http://images.cocodataset.org/train2017/000000000034.jpg\""raw_img = Image.open(requests.get(url, stream=True).raw)# convert it to the format expected by korniaimg = jnp.transpose(jnp.array(raw_img), (2, 0, 1))img = jnp.expand_dims(img, 0) / 255# and use the transpiled version of any function from the library!out = jax_kornia.enhance.sharpness(img, 5)   From TensorFlowimport ivyimport jaximport osos.environ[\""SM_FRAMEWORK\""] = \""tf.keras\""import segmentation_models as sm# transpile sm from tensorflow to jaxjax_sm = ivy.transpile(sm, source=\""tensorflow\"", to=\""jax\"")# get some image-like arrayskey = jax.random.PRNGKey(23)key1, key2 = jax.random.split(key)output = jax.random.uniform(key1, (1, 3, 512, 512))target = jax.random.uniform(key2, (1, 3, 512, 512))# and use the transpiled version of any function from the library!out = jax_sm.metrics.iou_score(output, target)   From NumPyimport ivyimport madmomimport jax.numpy as jnp# transpile madmon from numpy to jaxjax_madmom = ivy.transpile(madmom, source=\""numpy\"", to=\""jax\"")# get some arraysfreqs = jnp.arange(20) * 10# and use the transpiled version of any function from the library!out = jax_madmom.audio.filters.hz2midi(freqs)Any function   From PyTorchimport ivyimport torchimport jax.numpy as jnpdef loss(predictions, targets):    return torch.sqrt(torch.mean((predictions - targets) ** 2))# transpile any function from torch to jaxjax_loss = ivy.transpile(loss, source=\""torch\"", to=\""jax\"")# get some arraysp = jnp.array([3.0, 2.0, 1.0])t = jnp.array([0.0, 0.0, 0.0])# and use the transpiled version!out = jax_loss(p, t)   From TensorFlowimport ivyimport tensorflow as tfimport jax.numpy as jnpdef loss(predictions, targets):    return tf.sqrt(tf.reduce_mean(tf.square(predictions - targets)))# transpile any function from tf to jaxjax_loss = ivy.transpile(loss, source=\""tensorflow\"", to=\""jax\"")# get some arraysp = jnp.array([3.0, 2.0, 1.0])t = jnp.array([0.0, 0.0, 0.0])# and use the transpiled version!out = jax_loss(p, t)   From NumPyimport ivyimport numpy as npimport jaximport jax.numpy as jnpjax.config.update('jax_enable_x64', True)def loss(predictions, targets):    return np.sqrt(np.mean((predictions - targets) ** 2))# transpile any function from numpy to jaxjax_loss = ivy.transpile(loss, source=\""numpy\"", to=\""jax\"")# get some arraysp = jnp.array([3.0, 2.0, 1.0])t = jnp.array([0.0, 0.0, 0.0])# and use the transpiled version!out = jax_loss(p, t)I'm using NumPy You can use Ivy to get NumPy code from:Any library   From PyTorchimport ivyimport korniaimport requestsimport numpy as npfrom PIL import Image# transpile kornia from torch to npnp_kornia = ivy.transpile(kornia, source=\""torch\"", to=\""numpy\"")# get an imageurl = \""http://images.cocodataset.org/train2017/000000000034.jpg\""raw_img = Image.open(requests.get(url, stream=True).raw)# convert it to the format expected by korniaimg = np.transpose(np.array(raw_img), (2, 0, 1))img = np.expand_dims(img, 0) / 255# and use the transpiled version of any function from the library!out = np_kornia.enhance.sharpness(img, 5)   From TensorFlowimport ivyimport numpy as npimport osos.environ[\""SM_FRAMEWORK\""] = \""tf.keras\""import segmentation_models as sm# transpile sm from tensorflow to numpynp_sm = ivy.transpile(sm, source=\""tensorflow\"", to=\""numpy\"")# get some image-like arraysoutput = np.random.rand(1, 3, 512, 512).astype(dtype=np.float32)target = np.random.rand(1, 3, 512, 512).astype(dtype=np.float32)# and use the transpiled version of any function from the library!out = np_sm.metrics.iou_score(output, target)   From Jaximport ivyimport raximport numpy as np# transpile rax from jax to numpynp_rax = ivy.transpile(rax, source=\""jax\"", to=\""numpy\"")# get some arraysscores = np.array([2.2, 1.3, 5.4])labels = np.array([1.0, 0.0, 0.0])# and use the transpiled version of any function from the library!out = np_rax.poly1_softmax_loss(scores, labels)Any function   From PyTorchimport ivyimport torchimport numpy as npdef loss(predictions, targets):    return torch.sqrt(torch.mean((predictions - targets) ** 2))# transpile any function from torch to numpynp_loss = ivy.transpile(loss, source=\""torch\"", to=\""numpy\"")# get some arraysp = np.array([3.0, 2.0, 1.0])t = np.array([0.0, 0.0, 0.0])# and use the transpiled version!out = np_loss(p, t)   From TensorFlowimport ivyimport tensorflow as tfimport numpy as npdef loss(predictions, targets):    return tf.sqrt(tf.reduce_mean(tf.square(predictions - targets)))# transpile any function from tf to numpynp_loss = ivy.transpile(loss, source=\""tensorflow\"", to=\""numpy\"")# get some arraysp = np.array([3.0, 2.0, 1.0])t = np.array([0.0, 0.0, 0.0])# and use the transpiled version!out = np_loss(p, t)   From JAXimport ivyimport jax.numpy as jnpimport numpy as npdef loss(predictions, targets):    return jnp.sqrt(jnp.mean((predictions - targets) ** 2))# transpile any function from jax to numpynp_loss = ivy.transpile(loss, source=\""jax\"", to=\""numpy\"")# get some arraysp = np.array([3.0, 2.0, 1.0])t = np.array([0.0, 0.0, 0.0])# and use the transpiled version!out = np_loss(p, t)I'm using Ivy Or you can use Ivy as a framework, breaking yourself (and your code)free from deciding which community to support, allowing anyone to runyour code in their framework of choice!import ivy# a simple image classification modelclass IvyNet(ivy.Module):    def __init__(        self,        h_w=(32, 32),        input_channels=3,        output_channels=512,        num_classes=2,        data_format=\""NCHW\"",        device=\""cpu\"",    ):        self.h_w = h_w        self.input_channels = input_channels        self.output_channels = output_channels        self.num_classes = num_classes        self.data_format = data_format        self.device = device        super().__init__()    def _build(self, *args, **kwargs):        self.extractor = ivy.Sequential(            ivy.Conv2D(self.input_channels, 6, [5, 5], 1, \""SAME\"", data_format=self.data_format),            ivy.GELU(),            ivy.Conv2D(6, 16, [5, 5], 1, \""SAME\"", data_format=self.data_format),            ivy.GELU(),            ivy.Conv2D(16, self.output_channels, [5, 5], 1, \""SAME\"", data_format=self.data_format),            ivy.GELU(),        )        self.classifier = ivy.Sequential(            # since padding is \""SAME\"", this would be image_height x image_width x output_channels            ivy.Linear(self.h_w[0] * self.h_w[1] * self.output_channels, 512),            ivy.GELU(),            ivy.Linear(512, self.num_classes),        )    def _forward(self, x):        x = self.extractor(x)        # flatten all dims except batch dim        x = ivy.flatten(x, start_dim=1, end_dim=-1)        logits = self.classifier(x)        probs = ivy.softmax(logits)        return logits, probsAfter building your model in Ivy, you can set your favourite frameworkas the backend to use its operations under the hood!ivy.set_backend(\""torch\"")model = IvyNet()x = torch.randn(1, 3, 32, 32)logits, probs = model(x)ivy.set_backend(\""tensorflow\"")model = IvyNet()x = tf.random.uniform(shape=(1, 3, 32, 32))logits, probs = model(x)ivy.set_backend(\""jax\"")model = IvyNet()x = jax.random.uniform(key, shape=(1, 3, 32, 32))logits, probs = model(x)ivy.set_backend(\""numpy\"")model = IvyNet()x = np.random.uniform(size=(1, 3, 32, 32))logits, probs = model(x)Last but not least, we can also build the training pipeline in pure ivy⬇️Let's define some helper functions first# helper function for loading the dataset in batchesdef generate_batches(images, classes, dataset_size, batch_size=32):    targets = {k: v for v, k in enumerate(np.unique(classes))}    y_train = [targets[classes[i]] for i in range(len(classes))]    if batch_size > dataset_size:        raise ivy.utils.exceptions.IvyError(\""Use a smaller batch size\"")    for idx in range(0, dataset_size, batch_size):        yield ivy.stack(images[idx : min(idx + batch_size, dataset_size)]), ivy.array(            y_train[idx : min(idx + batch_size, dataset_size)]        )# helper function to get the number of current predictionsdef num_correct(preds, labels):    return (preds.argmax() == labels).sum().to_numpy().item()# define a loss functiondef loss_fn(params):    v, model, x, y = params    y_pred, probs = model(x)    return ivy.cross_entropy(y, probs), probsAnd train this model!# train the model on gpu if it's availabledevice = \""cuda:0\"" if ivy.gpu_is_available() else \""cpu\""# training hyperparamsoptimizer= ivy.Adam(1e-4)batch_size = 64 num_epochs = 20num_classes = 10model = IvyNet(    h_w=(28, 28),    input_channels=1,    output_channels=120,    num_classes=num_classes,    device=device,)model_name = type(model).__name__.lower()# training loopdef train(images, classes, epochs, model, device, num_classes=10, batch_size=32):    # training metrics    epoch_loss = 0.0    running_loss = 0.0    fields = [\""epoch\"", \""epoch_loss\"", \""training_accuracy\""]    metrics = []    dataset_size = len(images)    for epoch in range(epochs):        train_loss, train_correct = 0, 0        train_loop = tqdm(            generate_batches(images, classes, len(images), batch_size=batch_size),            total=dataset_size // batch_size,            position=0,            leave=True,        )        for xbatch, ybatch in train_loop:            if device != \""cpu\"":                xbatch, ybatch = xbatch.to_device(\""gpu:0\""), ybatch.to_device(\""gpu:0\"")            # since the cross entropy function expects the target classes to be in one-hot encoded format            ybatch_encoded = ivy.one_hot(ybatch, num_classes)            # update model params            loss_probs, grads = ivy.execute_with_gradients(                loss_fn,                (model.v, model, xbatch, ybatch_encoded),            )            model.v = optimizer.step(model.v, grads[\""0\""])            batch_loss = ivy.to_numpy(loss_probs[0]).mean().item()  # batch mean loss            epoch_loss += batch_loss * xbatch.shape[0]            train_correct += num_correct(loss_probs[1], ybatch)            train_loop.set_description(f\""Epoch [{epoch + 1:2d}/{epochs}]\"")            train_loop.set_postfix(                running_loss=batch_loss,                accuracy_percentage=(train_correct / dataset_size) * 100,            )        epoch_loss = epoch_loss / dataset_size        training_accuracy = train_correct / dataset_size        metrics.append([epoch, epoch_loss, training_accuracy])        train_loop.write(            f\""\Average training loss: {epoch_loss:.6f}, Train Correct: {train_correct}\"",            end=\""\\"",        )    # write metrics for plotting    with open(f\""/{model_name}_train_summary.csv\"", \""w\"") as f:        f = csv.writer(f)        f.writerow(fields)        f.writerows(metrics)# assuming the dataset(images and classes) are already prepared in a folder      train(images, classes, num_epochs, model, device, num_classes = num_classes, batch_size = batch_size)ContributingWe believe that everyone can contribute and make a difference. Whetherit's writing code 💻, fixing bugs 🐛, or simply sharing feedback 💬,your contributions are definitely welcome and appreciated 🙌Check out all of our open tasks, and find out more info in ourContributingguide in thedocs!Join our amazing community as a code contributor, and help accelerateour journey to unify all ML frameworks!  CommunityIn order to achieve the ambitious goal of unifying AI we definitely needas many hands as possible on it! Whether you are a seasoned developer orjust starting out, you'll find a place here! Join the Ivy community inour Discord 👾 server, which is theperfect place to ask questions, share ideas, and get help from bothfellow developers and the Ivy Team directly!Also! Feel free to follow us onTwitter 🐦 as well, we use it toshare updates, sneak peeks, and all sorts of relevant news, certainly agreat way to stay in the loop 😄Can't wait to see you there!CitationIf you use Ivy for your work, please don't forget to give proper creditby including the accompanying paper📄 in your references. It's a small way to show appreciation and helpto continue to support this and other open source projects 🙌@article{lenton2021ivy,  title={Ivy: Templated deep learning for inter-framework portability},  author={Lenton, Daniel and Pardo, Fabio and Falck, Fabian and James, Stephen and Clark, Ronald},  journal={arXiv preprint arXiv:2102.02886},  year={2021}}"
16,quantopian/zipline,https://github.com/quantopian/zipline/blob/master/README.rst,Python,"Zipline is a Pythonic algorithmic trading library. It is an event-drivensystem for backtesting. Zipline is currently used in production as the backtesting and live-tradingengine powering Quantopian -- a free,community-centered, hosted platform for building and executing tradingstrategies. Quantopian also offers a fully managed service for professionalsthat includes Zipline, Alphalens, Pyfolio, FactSet data, and more.Join our Community!DocumentationWant to Contribute? See our Development GuidelinesFeaturesEase of Use: Zipline tries to get out of your way so that you canfocus on algorithm development. See below for a code example.\""Batteries Included\"": many common statistics likemoving average and linear regression can be readily accessed fromwithin a user-written algorithm.PyData Integration: Input of historical data and output of performance statistics arebased on Pandas DataFrames to integrate nicely into the existingPyData ecosystem.Statistics and Machine Learning Libraries: You can use libraries like matplotlib, scipy,statsmodels, and sklearn to support development, analysis, andvisualization of state-of-the-art trading systems.InstallationZipline currently supports Python 2.7, 3.5, and 3.6, and may be installed viaeither pip or conda.Note: Installing Zipline is slightly more involved than the average Pythonpackage. See the full Zipline Install Documentation for detailedinstructions.For a development installation (used to develop Zipline itself), create andactivate a virtualenv, then run the etc/dev-install script.QuickstartSee our getting started tutorial.The following code implements a simple dual moving average algorithm.from zipline.api import order_target, record, symboldef initialize(context):    context.i = 0    context.asset = symbol('AAPL')def handle_data(context, data):    # Skip first 300 days to get full windows    context.i += 1    if context.i < 300:        return    # Compute averages    # data.history() has to be called with the same params    # from above and returns a pandas dataframe.    short_mavg = data.history(context.asset, 'price', bar_count=100, frequency=\""1d\"").mean()    long_mavg = data.history(context.asset, 'price', bar_count=300, frequency=\""1d\"").mean()    # Trading logic    if short_mavg > long_mavg:        # order_target orders as many shares as needed to        # achieve the desired number of shares.        order_target(context.asset, 100)    elif short_mavg < long_mavg:        order_target(context.asset, 0)    # Save values for later inspection    record(AAPL=data.current(context.asset, 'price'),           short_mavg=short_mavg,           long_mavg=long_mavg)You can then run this algorithm using the Zipline CLI.First, you must download some sample pricing and asset data:$ zipline ingest$ zipline run -f dual_moving_average.py --start 2014-1-1 --end 2018-1-1 -o dma.pickle --no-benchmarkThis will download asset pricing data data sourced from Quandl, and stream it through the algorithm over the specified time range.Then, the resulting performance DataFrame is saved in dma.pickle, which you can load and analyze from within Python.You can find other examples in the zipline/examples directory.Questions?If you find a bug, feel free to open an issue and fill out the issue template.ContributingAll contributions, bug reports, bug fixes, documentation improvements, enhancements, and ideas are welcome. Details on how to set up a development environment can be found in our development guidelines.If you are looking to start working with the Zipline codebase, navigate to the GitHub issues tab and start looking through interesting issues. Sometimes there are issues labeled as Beginner Friendly or Help Wanted.Feel free to ask questions on the mailing list or on Gitter.NotePlease note that Zipline is not a community-led project. Zipline ismaintained by the Quantopian engineering team, and we are quite small andoften busy.Because of this, we want to warn you that we may not attend to your pullrequest, issue, or direct mention in months, or even years. We hope youunderstand, and we hope that this note might help reduce any frustration orwasted time."
17,s0md3v/roop,https://github.com/s0md3v/roop/blob/main/README.md,Python,"RoopTake a video and replace the face in it with a face of your choice. You only need one image of the desired face. No dataset, no training.InstallationBe aware, the installation needs technical skills and is not for beginners. Please do not open platform and installation related issues on GitHub. We have a very helpful Discord community that will guide you to install roop.Basic - It is more likely to work on your computer, but will be quite slowAcceleration - Unleash the full potential of your CPU and GPUUsageStart the program with arguments:python run.py [options]-h, --help                                                                 show this help message and exit-s SOURCE_PATH, --source SOURCE_PATH                                       select an source image-t TARGET_PATH, --target TARGET_PATH                                       select an target image or video-o OUTPUT_PATH, --output OUTPUT_PATH                                       select output file or directory--frame-processor FRAME_PROCESSOR [FRAME_PROCESSOR ...]                    frame processors (choices: face_swapper, face_enhancer, ...)--keep-fps                                                                 keep target fps--keep-frames                                                              keep temporary frames--skip-audio                                                               skip target audio--many-faces                                                               process every face--reference-face-position REFERENCE_FACE_POSITION                          position of the reference face--reference-frame-number REFERENCE_FRAME_NUMBER                            number of the reference frame--similar-face-distance SIMILAR_FACE_DISTANCE                              face distance used for recognition--temp-frame-format {jpg,png}                                              image format used for frame extraction--temp-frame-quality [0-100]                                               image quality used for frame extraction--output-video-encoder {libx264,libx265,libvpx-vp9,h264_nvenc,hevc_nvenc}  encoder used for the output video--output-video-quality [0-100]                                             quality used for the output video--max-memory MAX_MEMORY                                                    maximum amount of RAM in GB--execution-provider {cpu} [{cpu} ...]                                     available execution provider (choices: cpu, ...)--execution-threads EXECUTION_THREADS                                      number of execution threads-v, --version                                                              show program's version number and exitHeadlessUsing the -s/--source, -t/--target and -o/--output argument will run the program in headless mode.DisclaimerThis software is designed to contribute positively to the AI-generated media industry, assisting artists with tasks like character animation and models for clothing.We are aware of the potential ethical issues and have implemented measures to prevent the software from being used for inappropriate content, such as nudity.Users are expected to follow local laws and use the software responsibly. If using real faces, get consent and clearly label deepfakes when sharing. The developers aren't liable for user actions.LicensesOur software uses a lot of third party libraries as well pre-trained models. The users should keep in mind that these third party components have their own license and terms, therefore our license is not being applied.Creditsdeepinsight for their insightface project which provided a well-made library and models.all developers behind the libraries used in this projectDocumentationRead the documentation for a deep dive."
18,owid/covid-19-data,https://github.com/owid/covid-19-data/blob/master/README.md,Python,"COVID-19 Dataset by Our World in Data📢 Find our data on COVID-19 and its documentation in public/data!Project structureThe project contains two independent directories:public/data: Contains the final datasets. This is for people interested in consuming the data andunderstanding all the caveats about it and its metrics.scripts: Contains all the code and intermediate files to produce the final dataset. This is for people interested incontributing to the project or better understanding our internal technical processes.DocumentationIf you are interested in the final dataset file, refer to this document. If you want tolearn more about our processes, refer to our technical documentation.ContributeThanks for considering contributing to this project! A good place to start is our contributionguideline."
19,wbond/package_control_channel,https://github.com/owid/covid-19-data/blob/master/README.md,Python,"COVID-19 Dataset by Our World in Data📢 Find our data on COVID-19 and its documentation in public/data!Project structureThe project contains two independent directories:public/data: Contains the final datasets. This is for people interested in consuming the data andunderstanding all the caveats about it and its metrics.scripts: Contains all the code and intermediate files to produce the final dataset. This is for people interested incontributing to the project or better understanding our internal technical processes.DocumentationIf you are interested in the final dataset file, refer to this document. If you want tolearn more about our processes, refer to our technical documentation.ContributeThanks for considering contributing to this project! A good place to start is our contributionguideline."
20,google/it-cert-automation-practice,https://github.com/google/it-cert-automation-practice/blob/master/README.md,Python,Google IT Automation with Python Professional Certificate - Practice filesThis repository contains the practice files used throughout the courses that arepart of the Google IT Automation with Python Professional CertificateThere's a separate folder for each course.
21,keras-team/keras,https://github.com/keras-team/keras/blob/master/README.md,Python,"Keras: Deep Learning for humansThis repository hosts the development of the Keras library.Read the documentation at keras.io.About KerasKeras is a deep learning API written in Python,running on top of the machine learning platform TensorFlow.It was developed with a focus on enabling fast experimentation andproviding a delightful developer experience.The purpose of Keras is to give an unfair advantage to any developer looking to ship ML-powered apps.Keras is:Simple -- but not simplistic. Keras reduces developer cognitive loadto free you to focus on the parts of the problem that really matter.Keras focuses on ease of use, debugging speed, code elegance & conciseness,maintainability, and deployability (via TFServing, TFLite, TF.js).Flexible -- Keras adopts the principle of progressive disclosure ofcomplexity: simple workflows should be quick and easy, while arbitrarilyadvanced workflows should be possible via a clear path that builds uponwhat you've already learned.Powerful -- Keras provides industry-strength performance andscalability: it is used by organizations and companies including NASA,YouTube, and Waymo. That's right -- your YouTube recommendations arepowered by Keras, and so is the world's most advanced driverless vehicle.Keras & TensorFlow 2TensorFlow 2 is an end-to-end, open-source machine learning platform.You can think of it as an infrastructure layer fordifferentiable programming.It combines four key abilities:Efficiently executing low-level tensor operations on CPU, GPU, or TPU.Computing the gradient of arbitrary differentiable expressions.Scaling computation to many devices, such as clusters of hundreds of GPUs.Exporting programs (\""graphs\"") to external runtimes such as servers, browsers, mobile and embedded devices.Keras is the high-level API of TensorFlow 2: an approachable, highly-productive interfacefor solving machine learning problems,with a focus on modern deep learning. It provides essential abstractions and building blocks for developingand shipping machine learning solutions with high iteration velocity.Keras empowers engineers and researchers to take full advantage of the scalabilityand cross-platform capabilities of TensorFlow 2: you can run Keras on TPU or on large clusters of GPUs,and you can export your Keras models to run in the browser or on a mobile device.First contact with KerasThe core data structures of Keras are layers and models.The simplest type of model is the Sequential model, a linear stack of layers.For more complex architectures, you should use the Keras functional API,which allows you to build arbitrary graphs of layers or write models entirely from scratch via subclassing.Here is the Sequential model:from tensorflow.keras.models import Sequentialmodel = Sequential()Stacking layers is as easy as .add():from tensorflow.keras.layers import Densemodel.add(Dense(units=64, activation='relu'))model.add(Dense(units=10, activation='softmax'))Once your model looks good, configure its learning process with .compile():model.compile(loss='categorical_crossentropy',              optimizer='sgd',              metrics=['accuracy'])If you need to, you can further configure your optimizer. The Keras philosophy is to keep simple things simple,while allowing the user to be fully in control when they need to be (the ultimate control being the easy extensibility of the source code via subclassing).model.compile(loss=tf.keras.losses.categorical_crossentropy,              optimizer=tf.keras.optimizers.SGD(                  learning_rate=0.01, momentum=0.9, nesterov=True))You can now iterate on your training data in batches:# x_train and y_train are Numpy arrays.model.fit(x_train, y_train, epochs=5, batch_size=32)Evaluate your test loss and metrics in one line:loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)Or generate predictions on new data:classes = model.predict(x_test, batch_size=128)What you just saw is the most elementary way to use Keras.However, Keras is also a highly-flexible framework suitable to iterate on state-of-the-art research ideas.Keras follows the principle of progressive disclosure of complexity: it makes it easy to get started,yet it makes it possible to handle arbitrarily advanced use cases,only requiring incremental learning at each step.In pretty much the same way that you were able to train & evaluate a simple neural network above in a few lines,you can use Keras to quickly develop new training procedures or exotic model architectures.Here's a low-level training loop example, combining Keras functionality with the TensorFlow GradientTape:import tensorflow as tf# Prepare an optimizer.optimizer = tf.keras.optimizers.Adam()# Prepare a loss function.loss_fn = tf.keras.losses.kl_divergence# Iterate over the batches of a dataset.for inputs, targets in dataset:    # Open a GradientTape.    with tf.GradientTape() as tape:        # Forward pass.        predictions = model(inputs)        # Compute the loss value for this batch.        loss_value = loss_fn(targets, predictions)    # Get gradients of loss wrt the weights.    gradients = tape.gradient(loss_value, model.trainable_weights)    # Update the weights of the model.    optimizer.apply_gradients(zip(gradients, model.trainable_weights))For more in-depth tutorials about Keras, you can check out:Introduction to Keras for engineersIntroduction to Keras for researchersDeveloper guidesOther learning resourcesInstallationKeras comes packaged with TensorFlow 2 as tensorflow.keras.To start using Keras, simply install TensorFlow 2.You can then import Keras as follows:from tensorflow import kerasRelease and compatibilityKeras has nightly releases (keras-nightly on PyPI)and stable releases (keras on PyPI).The nightly Keras releases are usually compatible with the corresponding versionof the tf-nightly releases(e.g. keras-nightly==2.7.0.dev2021100607 should beused with tf-nightly==2.7.0.dev2021100607).We don't maintain backward compatibility for nightly releases.For stable releases, each Kerasversion maps to a specific stable version of TensorFlow.The table below shows the compatibility version mappingbetween TensorFlow versions and Keras versions.All the release branches can be found on GitHub.All the release binaries can be found on Pypi.SupportYou can ask questions and join the development discussion:In the TensorFlow forum.On the Keras mailing list.Opening an issueYou can also post bug reports and feature requests (only)in GitHub issues.Opening a PRWe welcome contributions! Before opening a PR, please readour contributor guide,and the API design guideline."
22,open-mmlab/mmdetection,https://github.com/open-mmlab/mmdetection/blob/main/README.md,Python,"           OpenMMLab website                  HOT                      OpenMMLab platform                  TRY IT OUT               📘Documentation |🛠️Installation |👀Model Zoo |🆕Update News |🚀Ongoing Projects |🤔Reporting IssuesEnglish | 简体中文                                              IntroductionMMDetection is an open source object detection toolbox based on PyTorch. It isa part of the OpenMMLab project.The main branch works with PyTorch 1.8+.Major featuresModular DesignWe decompose the detection framework into different components and one can easily construct a customized object detection framework by combining different modules.Support of multiple tasks out of boxThe toolbox directly supports multiple detection tasks such as object detection, instance segmentation, panoptic segmentation, and semi-supervised object detection.High efficiencyAll basic bbox and mask operations run on GPUs. The training speed is faster than or comparable to other codebases, including Detectron2, maskrcnn-benchmark and SimpleDet.State of the artThe toolbox stems from the codebase developed by the MMDet team, who won COCO Detection Challenge in 2018, and we keep pushing it forward.The newly released RTMDet also obtains new state-of-the-art results on real-time instance segmentation and rotated object detection tasks and the best parameter-accuracy trade-off on object detection.Apart from MMDetection, we also released MMEngine for model training and MMCV for computer vision research, which are heavily depended on by this toolbox.What's NewHighlightWe are excited to announce our latest work on real-time object recognition tasks, RTMDet, a family of fully convolutional single-stage detectors. RTMDet not only achieves the best parameter-accuracy trade-off on object detection from tiny to extra-large model sizes but also obtains new state-of-the-art performance on instance segmentation and rotated object detection tasks. Details can be found in the technical report. Pre-trained models are here.TaskDatasetAPFPS(TRT FP16 BS1 3090)Object DetectionCOCO52.8322Instance SegmentationCOCO44.6188Rotated Object DetectionDOTA78.9(single-scale)/81.3(multi-scale)121v3.1.0 was released in 30/6/2023:Supports tracking algorithms including multi-object tracking (MOT) algorithms SORT, DeepSORT, StrongSORT, OCSORT, ByteTrack, QDTrack, and video instance segmentation (VIS) algorithm MaskTrackRCNN, Mask2Former-VIS.Support ViTDetSupports inference and evaluation of multimodal algorithms GLIP and XDecoder, and also supports datasets such as COCO semantic segmentation, COCO Caption, ADE20k general segmentation, and RefCOCO. GLIP fine-tuning will be supported in the future.Provides a gradio demo for image type tasks of MMDetection, making it easy for users to experience.InstallationPlease refer to Installation for installation instructions.Getting StartedPlease see Overview for the general introduction of MMDetection.For detailed user guides and advanced guides, please refer to our documentation:User GuidesTrain & TestLearn about ConfigsInference with existing modelsDataset PrepareTest existing models on standard datasetsTrain predefined models on standard datasetsTrain with customized datasetsTrain with customized models and standard datasetsFinetuning ModelsTest Results SubmissionWeight initializationUse a single stage detector as RPNSemi-supervised Object DetectionUseful ToolsAdvanced GuidesBasic ConceptsComponent CustomizationHow toWe also provide object detection colab tutorial  and instance segmentation colab tutorial .To migrate from MMDetection 2.x, please refer to migration.Overview of Benchmark and Model ZooResults and models are available in the model zoo.  Architectures                    Object Detection                    Instance Segmentation                    Panoptic Segmentation                    Other                                        Fast R-CNN (ICCV'2015)            Faster R-CNN (NeurIPS'2015)            RPN (NeurIPS'2015)            SSD (ECCV'2016)            RetinaNet (ICCV'2017)            Cascade R-CNN (CVPR'2018)            YOLOv3 (ArXiv'2018)            CornerNet (ECCV'2018)            Grid R-CNN (CVPR'2019)            Guided Anchoring (CVPR'2019)            FSAF (CVPR'2019)            CenterNet (CVPR'2019)            Libra R-CNN (CVPR'2019)            TridentNet (ICCV'2019)            FCOS (ICCV'2019)            RepPoints (ICCV'2019)            FreeAnchor (NeurIPS'2019)            CascadeRPN (NeurIPS'2019)            Foveabox (TIP'2020)            Double-Head R-CNN (CVPR'2020)            ATSS (CVPR'2020)            NAS-FCOS (CVPR'2020)            CentripetalNet (CVPR'2020)            AutoAssign (ArXiv'2020)            Side-Aware Boundary Localization (ECCV'2020)            Dynamic R-CNN (ECCV'2020)            DETR (ECCV'2020)            PAA (ECCV'2020)            VarifocalNet (CVPR'2021)            Sparse R-CNN (CVPR'2021)            YOLOF (CVPR'2021)            YOLOX (CVPR'2021)            Deformable DETR (ICLR'2021)            TOOD (ICCV'2021)            DDOD (ACM MM'2021)            RTMDet (ArXiv'2022)            Conditional DETR (ICCV'2021)            DAB-DETR (ICLR'2022)            DINO (ICLR'2023)            GLIP (CVPR'2022)            DiffusionDet (ArXiv'2023)            EfficientDet (CVPR'2020)            Detic (ECCV'2022)                                    Mask R-CNN (ICCV'2017)          Cascade Mask R-CNN (CVPR'2018)          Mask Scoring R-CNN (CVPR'2019)          Hybrid Task Cascade (CVPR'2019)          YOLACT (ICCV'2019)          InstaBoost (ICCV'2019)          SOLO (ECCV'2020)          PointRend (CVPR'2020)          DetectoRS (ArXiv'2020)          SOLOv2 (NeurIPS'2020)          SCNet (AAAI'2021)          QueryInst (ICCV'2021)          Mask2Former (ArXiv'2021)          CondInst (ECCV'2020)          SparseInst (CVPR'2022)          RTMDet (ArXiv'2022)          BoxInst (CVPR'2021)                                      Panoptic FPN (CVPR'2019)          MaskFormer (NeurIPS'2021)          Mask2Former (ArXiv'2021)                                      Contrastive Learning                          SwAV (NeurIPS'2020)          MoCo (CVPR'2020)          MoCov2 (ArXiv'2020)                                  Distillation                          Localization Distillation (CVPR'2022)          Label Assignment Distillation (WACV'2022)                          Semi-Supervised Object Detection                          Soft Teacher (ICCV'2021)                                        Components                    Backbones                    Necks                    Loss                    Common                                  VGG (ICLR'2015)        ResNet (CVPR'2016)        ResNeXt (CVPR'2017)        MobileNetV2 (CVPR'2018)        HRNet (CVPR'2019)        Generalized Attention (ICCV'2019)        GCNet (ICCVW'2019)        Res2Net (TPAMI'2020)        RegNet (CVPR'2020)        ResNeSt (ArXiv'2020)        PVT (ICCV'2021)        Swin (CVPR'2021)        PVTv2 (ArXiv'2021)        ResNet strikes back (ArXiv'2021)        EfficientNet (ArXiv'2021)        ConvNeXt (CVPR'2022)        ConvNeXtv2 (ArXiv'2023)                                PAFPN (CVPR'2018)        NAS-FPN (CVPR'2019)        CARAFE (ICCV'2019)        FPG (ArXiv'2020)        GRoIE (ICPR'2020)        DyHead (CVPR'2021)                                    GHM (AAAI'2019)          Generalized Focal Loss (NeurIPS'2020)          Seasaw Loss (CVPR'2021)                                      OHEM (CVPR'2016)          Group Normalization (ECCV'2018)          DCN (ICCV'2017)          DCNv2 (CVPR'2019)          Weight Standardization (ArXiv'2019)          Prime Sample Attention (CVPR'2020)          Strong Baselines (CVPR'2021)          Resnet strikes back (ArXiv'2021)                        Some other methods are also supported in projects using MMDetection.FAQPlease refer to FAQ for frequently asked questions.ContributingWe appreciate all contributions to improve MMDetection. Ongoing projects can be found in out GitHub Projects. Welcome community users to participate in these projects. Please refer to CONTRIBUTING.md for the contributing guideline.AcknowledgementMMDetection is an open source project that is contributed by researchers and engineers from various colleges and companies. We appreciate all the contributors who implement their methods or add new features, as well as users who give valuable feedbacks.We wish that the toolbox and benchmark could serve the growing research community by providing a flexible toolkit to reimplement existing methods and develop their own new detectors.CitationIf you use this toolbox or benchmark in your research, please cite this project.@article{mmdetection,  title   = {{MMDetection}: Open MMLab Detection Toolbox and Benchmark},  author  = {Chen, Kai and Wang, Jiaqi and Pang, Jiangmiao and Cao, Yuhang and             Xiong, Yu and Li, Xiaoxiao and Sun, Shuyang and Feng, Wansen and             Liu, Ziwei and Xu, Jiarui and Zhang, Zheng and Cheng, Dazhi and             Zhu, Chenchen and Cheng, Tianheng and Zhao, Qijie and Li, Buyu and             Lu, Xin and Zhu, Rui and Wu, Yue and Dai, Jifeng and Wang, Jingdong             and Shi, Jianping and Ouyang, Wanli and Loy, Chen Change and Lin, Dahua},  journal= {arXiv preprint arXiv:1906.07155},  year={2019}}LicenseThis project is released under the Apache 2.0 license.Projects in OpenMMLabMMEngine: OpenMMLab foundational library for training deep learning models.MMCV: OpenMMLab foundational library for computer vision.MMPreTrain: OpenMMLab pre-training toolbox and benchmark.MMagic: OpenMMLab Advanced, Generative and Intelligent Creation toolbox.MMDetection: OpenMMLab detection toolbox and benchmark.MMDetection3D: OpenMMLab's next-generation platform for general 3D object detection.MMRotate: OpenMMLab rotated object detection toolbox and benchmark.MMYOLO: OpenMMLab YOLO series toolbox and benchmark.MMSegmentation: OpenMMLab semantic segmentation toolbox and benchmark.MMOCR: OpenMMLab text detection, recognition, and understanding toolbox.MMPose: OpenMMLab pose estimation toolbox and benchmark.MMHuman3D: OpenMMLab 3D human parametric model toolbox and benchmark.MMSelfSup: OpenMMLab self-supervised learning toolbox and benchmark.MMRazor: OpenMMLab model compression toolbox and benchmark.MMFewShot: OpenMMLab fewshot learning toolbox and benchmark.MMAction2: OpenMMLab's next-generation action understanding toolbox and benchmark.MMTracking: OpenMMLab video perception toolbox and benchmark.MMFlow: OpenMMLab optical flow toolbox and benchmark.MMEditing: OpenMMLab image and video editing toolbox.MMGeneration: OpenMMLab image and video generative models toolbox.MMDeploy: OpenMMLab model deployment framework.MIM: MIM installs OpenMMLab packages.MMEval: A unified evaluation library for multiple machine learning libraries.Playground: A central hub for gathering and showcasing amazing projects built upon OpenMMLab."
23,gto76/python-cheatsheet,https://github.com/gto76/python-cheatsheet/blob/main/README.md,Python,"Comprehensive Python CheatsheetDownload text file, Buy PDF, Fork me on GitHub or Check out FAQ.Contents    1. Collections:   List, Dictionary, Set, Tuple, Range, Enumerate, Iterator, Generator.    2. Types:            Type, String, Regular_Exp, Format, Numbers, Combinatorics, Datetime.    3. Syntax:           Args, Inline, Import, Decorator, Class, Duck_Types, Enum, Exception.    4. System:          Exit, Print, Input, Command_Line_Arguments, Open, Path, OS_Commands.    5. Data:               JSON, Pickle, CSV, SQLite, Bytes, Struct, Array, Memory_View, Deque.    6. Advanced:     Threading, Operator, Introspection, Metaprograming, Eval, Coroutines.    7. Libraries:        Progress_Bar, Plot, Table, Curses, Logging, Scraping, Web, Profile,                                  NumPy, Image, Audio, Games, Data.Mainif __name__ == '__main__':      # Runs main() if file wasn't imported.    main()List<list> = <list>[<slice>]        # Or: <list>[from_inclusive : to_exclusive : ±step]<list>.append(<el>)             # Or: <list> += [<el>]<list>.extend(<collection>)     # Or: <list> += <collection><list>.sort()                   # Sorts in ascending order.<list>.reverse()                # Reverses the list in-place.<list> = sorted(<collection>)   # Returns a new sorted list.<iter> = reversed(<list>)       # Returns reversed iterator.sum_of_elements  = sum(<collection>)elementwise_sum  = [sum(pair) for pair in zip(list_a, list_b)]sorted_by_second = sorted(<collection>, key=lambda el: el[1])sorted_by_both   = sorted(<collection>, key=lambda el: (el[1], el[0]))flatter_list     = list(itertools.chain.from_iterable(<list>))product_of_elems = functools.reduce(lambda out, el: out * el, <collection>)list_of_chars    = list(<str>)For details about sorted(), min() and max() see sortable.Module operator provides functions itemgetter() and mul() that offer the same functionality as lambda expressions above.<list>.insert(<int>, <el>)      # Inserts item at index and moves the rest to the right.<el>  = <list>.pop([<int>])     # Removes and returns item at index or from the end.<int> = <list>.count(<el>)      # Returns number of occurrences. Also works on strings.<int> = <list>.index(<el>)      # Returns index of the first occurrence or raises ValueError.<list>.remove(<el>)             # Removes first occurrence of the item or raises ValueError.<list>.clear()                  # Removes all items. Also works on dictionary and set.Dictionary<view> = <dict>.keys()                          # Coll. of keys that reflects changes.<view> = <dict>.values()                        # Coll. of values that reflects changes.<view> = <dict>.items()                         # Coll. of key-value tuples that reflects chgs.value  = <dict>.get(key, default=None)          # Returns default if key is missing.value  = <dict>.setdefault(key, default=None)   # Returns and writes default if key is missing.<dict> = collections.defaultdict(<type>)        # Returns a dict with default value of type.<dict> = collections.defaultdict(lambda: 1)     # Returns a dict with default value 1.<dict> = dict(<collection>)                     # Creates a dict from coll. of key-value pairs.<dict> = dict(zip(keys, values))                # Creates a dict from two collections.<dict> = dict.fromkeys(keys [, value])          # Creates a dict from collection of keys.<dict>.update(<dict>)                           # Adds items. Replaces ones with matching keys.value = <dict>.pop(key)                         # Removes item or raises KeyError.{k for k, v in <dict>.items() if v == value}    # Returns set of keys that point to the value.{k: v for k, v in <dict>.items() if k in keys}  # Returns a dictionary, filtered by keys.Counter>>> from collections import Counter>>> colors = ['blue', 'blue', 'blue', 'red', 'red']>>> counter = Counter(colors)>>> counter['yellow'] += 1Counter({'blue': 3, 'red': 2, 'yellow': 1})>>> counter.most_common()[0]('blue', 3)Set<set> = set()                                   # `{}` returns a dictionary.<set>.add(<el>)                                 # Or: <set> |= {<el>}<set>.update(<collection> [, ...])              # Or: <set> |= <set><set>  = <set>.union(<coll.>)                   # Or: <set> | <set><set>  = <set>.intersection(<coll.>)            # Or: <set> & <set><set>  = <set>.difference(<coll.>)              # Or: <set> - <set><set>  = <set>.symmetric_difference(<coll.>)    # Or: <set> ^ <set><bool> = <set>.issubset(<coll.>)                # Or: <set> <= <set><bool> = <set>.issuperset(<coll.>)              # Or: <set> >= <set><el> = <set>.pop()                              # Raises KeyError if empty.<set>.remove(<el>)                              # Raises KeyError if missing.<set>.discard(<el>)                             # Doesn't raise an error.Frozen SetIs immutable and hashable.That means it can be used as a key in a dictionary or as an element in a set.<frozenset> = frozenset(<collection>)TupleTuple is an immutable and hashable list.<tuple> = ()                               # Empty tuple.<tuple> = (<el>,)                          # Or: <el>,<tuple> = (<el_1>, <el_2> [, ...])         # Or: <el_1>, <el_2> [, ...]Named TupleTuple's subclass with named elements.>>> from collections import namedtuple>>> Point = namedtuple('Point', 'x y')>>> p = Point(1, y=2)Point(x=1, y=2)>>> p[0]1>>> p.x1>>> getattr(p, 'y')2RangeImmutable and hashable sequence of integers.<range> = range(stop)                      # range(to_exclusive)<range> = range(start, stop)               # range(from_inclusive, to_exclusive)<range> = range(start, stop, ±step)        # range(from_inclusive, to_exclusive, ±step_size)>>> [i for i in range(3)][0, 1, 2]Enumeratefor i, el in enumerate(<collection> [, i_start]):    ...Iterator<iter> = iter(<collection>)                # `iter(<iter>)` returns unmodified iterator.<iter> = iter(<function>, to_exclusive)    # A sequence of return values until 'to_exclusive'.<el>   = next(<iter> [, default])          # Raises StopIteration or returns 'default' on end.<list> = list(<iter>)                      # Returns a list of iterator's remaining elements.Itertoolsimport itertools as it<iter> = it.count(start=0, step=1)         # Returns updated value endlessly. Accepts floats.<iter> = it.repeat(<el> [, times])         # Returns element endlessly or 'times' times.<iter> = it.cycle(<collection>)            # Repeats the sequence endlessly.<iter> = it.chain(<coll>, <coll> [, ...])  # Empties collections in order (figuratively).<iter> = it.chain.from_iterable(<coll>)    # Empties collections inside a collection in order.<iter> = it.islice(<coll>, to_exclusive)   # Only returns first 'to_exclusive' elements.<iter> = it.islice(<coll>, from_inc, …)    # `to_exclusive, +step_size`. Indices can be None.GeneratorAny function that contains a yield statement returns a generator.Generators and iterators are interchangeable.def count(start, step):    while True:        yield start        start += step>>> counter = count(10, 2)>>> next(counter), next(counter), next(counter)(10, 12, 14)TypeEverything is an object.Every object has a type.Type and class are synonymous.<type> = type(<el>)                          # Or: <el>.__class__<bool> = isinstance(<el>, <type>)            # Or: issubclass(type(<el>), <type>)>>> type('a'), 'a'.__class__, str(<class 'str'>, <class 'str'>, <class 'str'>)Some types do not have built-in names, so they must be imported:from types import FunctionType, MethodType, LambdaType, GeneratorType, ModuleTypeAbstract Base ClassesEach abstract base class specifies a set of virtual subclasses. These classes are then recognized by isinstance() and issubclass() as subclasses of the ABC, although they are really not. ABC can also manually decide whether or not a specific class is its virtual subclass, usually based on which methods the class has implemented. For instance, Iterable ABC looks for method iter(), while Collection ABC looks for iter(), contains() and len().>>> from collections.abc import Iterable, Collection, Sequence>>> isinstance([1, 2, 3], Iterable)True+------------------+------------+------------+------------+|                  |  Iterable  | Collection |  Sequence  |+------------------+------------+------------+------------+| list, range, str |    yes     |    yes     |    yes     || dict, set        |    yes     |    yes     |            || iter             |    yes     |            |            |+------------------+------------+------------+------------+>>> from numbers import Number, Complex, Real, Rational, Integral>>> isinstance(123, Number)True+--------------------+----------+----------+----------+----------+----------+|                    |  Number  |  Complex |   Real   | Rational | Integral |+--------------------+----------+----------+----------+----------+----------+| int                |   yes    |   yes    |   yes    |   yes    |   yes    || fractions.Fraction |   yes    |   yes    |   yes    |   yes    |          || float              |   yes    |   yes    |   yes    |          |          || complex            |   yes    |   yes    |          |          |          || decimal.Decimal    |   yes    |          |          |          |          |+--------------------+----------+----------+----------+----------+----------+String<str>  = <str>.strip()                       # Strips all whitespace characters from both ends.<str>  = <str>.strip('<chars>')              # Strips all passed characters from both ends.<list> = <str>.split()                       # Splits on one or more whitespace characters.<list> = <str>.split(sep=None, maxsplit=-1)  # Splits on 'sep' str at most 'maxsplit' times.<list> = <str>.splitlines(keepends=False)    # On [\\\r\\f\\v\\x1c-\\x1e\\x85\\u2028\\u2029] and \\r\.<str>  = <str>.join(<coll_of_strings>)       # Joins elements using string as a separator.<bool> = <sub_str> in <str>                  # Checks if string contains the substring.<bool> = <str>.startswith(<sub_str>)         # Pass tuple of strings for multiple options.<bool> = <str>.endswith(<sub_str>)           # Pass tuple of strings for multiple options.<int>  = <str>.find(<sub_str>)               # Returns start index of the first match or -1.<int>  = <str>.index(<sub_str>)              # Same, but raises ValueError if missing.<str>  = <str>.replace(old, new [, count])   # Replaces 'old' with 'new' at most 'count' times.<str>  = <str>.translate(<table>)            # Use `str.maketrans(<dict>)` to generate table.<str>  = chr(<int>)                          # Converts int to Unicode character.<int>  = ord(<str>)                          # Converts Unicode character to int.Also: 'lstrip()', 'rstrip()' and 'rsplit()'.Also: 'lower()', 'upper()', 'capitalize()' and 'title()'.Property Methods+---------------+----------+----------+----------+----------+----------+|               | [ !#$%…] | [a-zA-Z] |  [¼½¾]   |  [²³¹]   |  [0-9]   |+---------------+----------+----------+----------+----------+----------+| isprintable() |   yes    |   yes    |   yes    |   yes    |   yes    || isalnum()     |          |   yes    |   yes    |   yes    |   yes    || isnumeric()   |          |          |   yes    |   yes    |   yes    || isdigit()     |          |          |          |   yes    |   yes    || isdecimal()   |          |          |          |          |   yes    |+---------------+----------+----------+----------+----------+----------+'isspace()' checks for whitespaces: '[ \\t\\\r\\f\\v\\x1c-\\x1f\\x85\\xa0\\u1680…]'.Regeximport re<str>   = re.sub(<regex>, new, text, count=0)  # Substitutes all occurrences with 'new'.<list>  = re.findall(<regex>, text)            # Returns all occurrences as strings.<list>  = re.split(<regex>, text, maxsplit=0)  # Use brackets in regex to include the matches.<Match> = re.search(<regex>, text)             # Searches for first occurrence of the pattern.<Match> = re.match(<regex>, text)              # Searches only at the beginning of the text.<iter>  = re.finditer(<regex>, text)           # Returns all occurrences as Match objects.Argument 'new' can be a function that accepts a Match object and returns a string.Search() and match() return None if they can't find a match.Argument 'flags=re.IGNORECASE' can be used with all functions.Argument 'flags=re.MULTILINE' makes '^' and '$' match the start/end of each line.Argument 'flags=re.DOTALL' makes '.' also accept the '\'.Use r'\\1' or '\\\\1' for backreference ('\\1' returns a character with octal code 1).Add '?' after '*' and '+' to make them non-greedy.Match Object<str>   = <Match>.group()                      # Returns the whole match. Also group(0).<str>   = <Match>.group(1)                     # Returns part in the first bracket.<tuple> = <Match>.groups()                     # Returns all bracketed parts.<int>   = <Match>.start()                      # Returns start index of the match.<int>   = <Match>.end()                        # Returns exclusive end index of the match.Special Sequences'\\d' == '[0-9]'                                # Matches decimal characters.'\\w' == '[a-zA-Z0-9_]'                         # Matches alphanumerics and underscore.'\\s' == '[ \\t\\\r\\f\\v]'                        # Matches whitespaces.By default, decimal characters, alphanumerics and whitespaces from all alphabets are matched unless 'flags=re.ASCII' argument is used.As shown above, it restricts all special sequence matches to the first 128 characters and prevents '\\s' from accepting '[\\x1c-\\x1f]' (the so-called separator characters).Use a capital letter for negation (all non-ASCII characters will be matched when used in combination with ASCII flag).Format<str> = f'{<el_1>}, {<el_2>}'            # Curly brackets can also contain expressions.<str> = '{}, {}'.format(<el_1>, <el_2>)  # Or: '{0}, {a}'.format(<el_1>, a=<el_2>)<str> = '%s, %s' % (<el_1>, <el_2>)      # Redundant and inferior C-style formatting.Example>>> Person = collections.namedtuple('Person', 'name height')>>> person = Person('Jean-Luc', 187)>>> f'{person.name} is {person.height / 100} meters tall.''Jean-Luc is 1.87 meters tall.'General Options{<el>:<10}                               # '<el>      '{<el>:^10}                               # '   <el>   '{<el>:>10}                               # '      <el>'{<el>:.<10}                              # '<el>......'{<el>:0}                                 # '<el>'Options can be generated dynamically: f'{<el>:{<str/int>}[…]}'.Adding '=' to the expression prepends it to the output: f'{1+1=}' returns '1+1=2'.Adding '!r' to the expression converts object to string by calling its repr() method.Strings{'abcde':10}                             # 'abcde     '{'abcde':10.3}                           # 'abc       '{'abcde':.3}                             # 'abc'{'abcde'!r:10}                           # \""'abcde'   \""Numbers{123456:10}                              # '    123456'{123456:10,}                             # '   123,456'{123456:10_}                             # '   123_456'{123456:+10}                             # '   +123456'{123456:=+10}                            # '+   123456'{123456: }                               # ' 123456'{-123456: }                              # '-123456'Floats{1.23456:10.3}                           # '      1.23'{1.23456:10.3f}                          # '     1.235'{1.23456:10.3e}                          # ' 1.235e+00'{1.23456:10.3%}                          # '  123.456%'Comparison of presentation types:+--------------+----------------+----------------+----------------+----------------+|              |    {<float>}   |   {<float>:f}  |   {<float>:e}  |   {<float>:%}  |+--------------+----------------+----------------+----------------+----------------+|  0.000056789 |   '5.6789e-05' |    '0.000057'  | '5.678900e-05' |    '0.005679%' ||  0.00056789  |   '0.00056789' |    '0.000568'  | '5.678900e-04' |    '0.056789%' ||  0.0056789   |   '0.0056789'  |    '0.005679'  | '5.678900e-03' |    '0.567890%' ||  0.056789    |   '0.056789'   |    '0.056789'  | '5.678900e-02' |    '5.678900%' ||  0.56789     |   '0.56789'    |    '0.567890'  | '5.678900e-01' |   '56.789000%' ||  5.6789      |   '5.6789'     |    '5.678900'  | '5.678900e+00' |  '567.890000%' || 56.789       |  '56.789'      |   '56.789000'  | '5.678900e+01' | '5678.900000%' |+--------------+----------------+----------------+----------------+----------------++--------------+----------------+----------------+----------------+----------------+|              |  {<float>:.2}  |  {<float>:.2f} |  {<float>:.2e} |  {<float>:.2%} |+--------------+----------------+----------------+----------------+----------------+|  0.000056789 |    '5.7e-05'   |      '0.00'    |   '5.68e-05'   |      '0.01%'   ||  0.00056789  |    '0.00057'   |      '0.00'    |   '5.68e-04'   |      '0.06%'   ||  0.0056789   |    '0.0057'    |      '0.01'    |   '5.68e-03'   |      '0.57%'   ||  0.056789    |    '0.057'     |      '0.06'    |   '5.68e-02'   |      '5.68%'   ||  0.56789     |    '0.57'      |      '0.57'    |   '5.68e-01'   |     '56.79%'   ||  5.6789      |    '5.7'       |      '5.68'    |   '5.68e+00'   |    '567.89%'   || 56.789       |    '5.7e+01'   |     '56.79'    |   '5.68e+01'   |   '5678.90%'   |+--------------+----------------+----------------+----------------+----------------+'{<float>:g}' is '{<float>:.6}' with stripped zeros, exponent starting at 7 figures.When both rounding up and rounding down are possible, the one that returns result with even last digit is chosen. That makes '{6.5:.0f}' a '6' and '{7.5:.0f}' an '8'.This rule only effects numbers that can be represented exactly by a float (.5, .25, …).Ints{90:c}                                   # 'Z'{90:b}                                   # '1011010'{90:X}                                   # '5A'Numbers<int>      = int(<float/str/bool>)                # Or: math.floor(<float>)<float>    = float(<int/str/bool>)                # Or: <int/float>e±<int><complex>  = complex(real=0, imag=0)              # Or: <int/float/Fraction> ± <int/float>j<Fraction> = fractions.Fraction(0, 1)             # Or: Fraction(numerator=0, denominator=1)<Decimal>  = decimal.Decimal(<str/int>)           # Or: Decimal((sign, digits, exponent))'int(<str>)' and 'float(<str>)' raise ValueError on malformed strings.Decimal numbers are stored exactly, unlike most floats where '1.1 + 2.2 != 3.3'.Floats can be compared with: 'math.isclose(<float>, <float>)'.Precision of decimal operations is set with: 'decimal.getcontext().prec = <int>'.Basic Functions<num> = pow(<num>, <num>)                         # Or: <num> ** <num><num> = abs(<num>)                                # <float> = abs(<complex>)<num> = round(<num> [, ±ndigits])                 # `round(126, -1) == 130`Mathfrom math import e, pi, inf, nan, isinf, isnan    # `<el> == nan` is always False.from math import sin, cos, tan, asin, acos, atan  # Also: degrees, radians.from math import log, log10, log2                 # Log can accept base as second arg.Statisticsfrom statistics import mean, median, variance     # Also: stdev, quantiles, groupby.Randomfrom random import random, randint, choice        # Also: shuffle, gauss, triangular, seed.<float> = random()                                # A float inside [0, 1).<int>   = randint(from_inc, to_inc)               # An int inside [from_inc, to_inc].<el>    = choice(<sequence>)                      # Keeps the sequence intact.Bin, Hex<int> = ±0b<bin>                                  # Or: ±0x<hex><int> = int('±<bin>', 2)                          # Or: int('±<hex>', 16)<int> = int('±0b<bin>', 0)                        # Or: int('±0x<hex>', 0)<str> = bin(<int>)                                # Returns '[-]0b<bin>'.Bitwise Operators<int> = <int> & <int>                             # And (0b1100 & 0b1010 == 0b1000).<int> = <int> | <int>                             # Or  (0b1100 | 0b1010 == 0b1110).<int> = <int> ^ <int>                             # Xor (0b1100 ^ 0b1010 == 0b0110).<int> = <int> << n_bits                           # Left shift. Use >> for right.<int> = ~<int>                                    # Not. Also -<int> - 1.Combinatoricsimport itertools as it>>> list(it.product([0, 1], repeat=3))[(0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1), (1, 0, 0), (1, 0, 1), (1, 1, 0), (1, 1, 1)]>>> list(it.product('abc', 'abc'))                    #   a  b  c[('a', 'a'), ('a', 'b'), ('a', 'c'),                  # a x  x  x ('b', 'a'), ('b', 'b'), ('b', 'c'),                  # b x  x  x ('c', 'a'), ('c', 'b'), ('c', 'c')]                  # c x  x  x>>> list(it.combinations('abc', 2))                   #   a  b  c[('a', 'b'), ('a', 'c'),                              # a .  x  x ('b', 'c')]                                          # b .  .  x>>> list(it.combinations_with_replacement('abc', 2))  #   a  b  c[('a', 'a'), ('a', 'b'), ('a', 'c'),                  # a x  x  x ('b', 'b'), ('b', 'c'),                              # b .  x  x ('c', 'c')]                                          # c .  .  x>>> list(it.permutations('abc', 2))                   #   a  b  c[('a', 'b'), ('a', 'c'),                              # a .  x  x ('b', 'a'), ('b', 'c'),                              # b x  .  x ('c', 'a'), ('c', 'b')]                              # c x  x  .DatetimeProvides 'date', 'time', 'datetime' and 'timedelta' classes. All are immutable and hashable.# pip3 install python-dateutilfrom datetime import date, time, datetime, timedelta, timezonefrom dateutil.tz import tzlocal, gettz, datetime_exists, resolve_imaginary<D>  = date(year, month, day)               # Only accepts valid dates from 1 to 9999 AD.<T>  = time(hour=0, minute=0, second=0)     # Also: `microsecond=0, tzinfo=None, fold=0`.<DT> = datetime(year, month, day, hour=0)   # Also: `minute=0, second=0, microsecond=0, …`.<TD> = timedelta(weeks=0, days=0, hours=0)  # Also: `minutes=0, seconds=0, microseconds=0`.Aware <a> time and datetime objects have defined timezone, while naive <n> don't.If object is naive, it is presumed to be in the system's timezone.'fold=1' means the second pass in case of time jumping back for one hour.Timedelta normalizes arguments to ±days, seconds (< 86 400) and microseconds (< 1M).Use '<D/DT>.weekday()' to get the day of the week as an int, with Monday being 0.'<DTa> = resolve_imaginary(<DTa>)' fixes DTs that fall into the missing hour.Now<D/DTn>  = D/DT.today()                     # Current local date or naive datetime.<DTn>    = DT.utcnow()                      # Naive datetime from current UTC time.<DTa>    = DT.now(<tzinfo>)                 # Aware datetime from current tz time.To extract time use '<DTn>.time()', '<DTa>.time()' or '<DTa>.timetz()'.Timezone<tzinfo> = timezone.utc                     # London without daylight saving time.<tzinfo> = timezone(<timedelta>)            # Timezone with fixed offset from UTC.<tzinfo> = tzlocal()                        # Local timezone. Also gettz().<tzinfo> = gettz('<Continent>/<City>')      # 'Continent/City_Name' timezone or None.<DTa>    = <DT>.astimezone([<tzinfo>])      # Converts DT to the passed or local timezone.<Ta/DTa> = <T/DT>.replace(tzinfo=<tzinfo>)  # Changes object's timezone without conversion.Standard library's zoneinfo.ZoneInfo() can be used instead of gettz() on Python 3.9 and later. It requires 'tzdata' package on Windows.Encode<D/T/DT> = D/T/DT.fromisoformat('<iso>')    # Object from ISO string. Raises ValueError.<DT>     = DT.strptime(<str>, '<format>')   # Datetime from str, according to format.<D/DTn>  = D/DT.fromordinal(<int>)          # D/DTn from days since the Gregorian NYE 1.<DTn>    = DT.fromtimestamp(<real>)         # Local time DTn from seconds since the Epoch.<DTa>    = DT.fromtimestamp(<real>, <tz.>)  # Aware datetime from seconds since the Epoch.ISO strings come in following forms: 'YYYY-MM-DD', 'HH:MM:SS.mmmuuu[±HH:MM]', or both separated by an arbitrary character. All parts following the hours are optional.Python uses the Unix Epoch: '1970-01-01 00:00 UTC', '1970-01-01 01:00 CET', ...Decode<str>    = <D/T/DT>.isoformat(sep='T')      # Also `timespec='auto/hours/minutes/seconds/…'`.<str>    = <D/T/DT>.strftime('<format>')    # Custom string representation.<int>    = <D/DT>.toordinal()               # Days since Gregorian NYE 1, ignoring time and tz.<float>  = <DTn>.timestamp()                # Seconds since the Epoch, from DTn in local tz.<float>  = <DTa>.timestamp()                # Seconds since the Epoch, from aware datetime.Format>>> dt = datetime.strptime('2015-05-14 23:39:00.00 +0200', '%Y-%m-%d %H:%M:%S.%f %z')>>> dt.strftime(\""%A, %dth of %B '%y, %I:%M%p %Z\"")\""Thursday, 14th of May '15, 11:39PM UTC+02:00\""'%z' accepts '±HH[:]MM' and returns '±HHMM' or empty string if datetime is naive.'%Z' accepts 'UTC/GMT' and local timezone's code and returns timezone's name, 'UTC[±HH:MM]' if timezone is nameless, or an empty string if datetime is naive.For abbreviated weekday and month use '%a' and '%b'.Arithmetics<D/DT>   = <D/DT>  ± <TD>                   # Returned datetime can fall into missing hour.<TD>     = <D/DTn> - <D/DTn>                # Returns the difference. Ignores time jumps.<TD>     = <DTa>   - <DTa>                  # Ignores time jumps if they share tzinfo object.<TD>     = <TD>    * <int/float>            # Also: <TD> = abs(<TD>) and <TD> = <TD> ±% <TD>.<float>  = <TD>    / <TD>                   # How many weeks/years there are in TD. Also //.ArgumentsInside Function Callfunc(<positional_args>)                           # func(0, 0)func(<keyword_args>)                              # func(x=0, y=0)func(<positional_args>, <keyword_args>)           # func(0, y=0)Inside Function Definitiondef func(<nondefault_args>): ...                  # def func(x, y): ...def func(<default_args>): ...                     # def func(x=0, y=0): ...def func(<nondefault_args>, <default_args>): ...  # def func(x, y=0): ...Default values are evaluated when function is first encountered in the scope.Any mutation of a mutable default value will persist between invocations!Splat OperatorInside Function CallSplat expands a collection into positional arguments, while splatty-splat expands a dictionary into keyword arguments.args   = (1, 2)kwargs = {'x': 3, 'y': 4, 'z': 5}func(*args, **kwargs)Is the same as:func(1, 2, x=3, y=4, z=5)Inside Function DefinitionSplat combines zero or more positional arguments into a tuple, while splatty-splat combines zero or more keyword arguments into a dictionary.def add(*a):    return sum(a)>>> add(1, 2, 3)6Legal argument combinations:def f(*args): ...               # f(1, 2, 3)def f(x, *args): ...            # f(1, 2, 3)def f(*args, z): ...            # f(1, 2, z=3)def f(**kwargs): ...            # f(x=1, y=2, z=3)def f(x, **kwargs): ...         # f(x=1, y=2, z=3) | f(1, y=2, z=3)def f(*args, **kwargs): ...     # f(x=1, y=2, z=3) | f(1, y=2, z=3) | f(1, 2, z=3) | f(1, 2, 3)def f(x, *args, **kwargs): ...  # f(x=1, y=2, z=3) | f(1, y=2, z=3) | f(1, 2, z=3) | f(1, 2, 3)def f(*args, y, **kwargs): ...  # f(x=1, y=2, z=3) | f(1, y=2, z=3)def f(*, x, y, z): ...          # f(x=1, y=2, z=3)def f(x, *, y, z): ...          # f(x=1, y=2, z=3) | f(1, y=2, z=3)def f(x, y, *, z): ...          # f(x=1, y=2, z=3) | f(1, y=2, z=3) | f(1, 2, z=3)Other Uses<list>  = [*<coll.> [, ...]]    # Or: list(<collection>) [+ ...]<tuple> = (*<coll.>, [...])     # Or: tuple(<collection>) [+ ...]<set>   = {*<coll.> [, ...]}    # Or: set(<collection>) [| ...]<dict>  = {**<dict> [, ...]}    # Or: dict(**<dict> [, ...])head, *body, tail = <coll.>     # Head or tail can be omitted.InlineLambda<func> = lambda: <return_value>                     # A single statement function.<func> = lambda <arg_1>, <arg_2>: <return_value>    # Also accepts default arguments.Comprehensions<list> = [i+1 for i in range(10)]                   # Or: [1, 2, ..., 10]<iter> = (i for i in range(10) if i > 5)            # Or: iter([6, 7, 8, 9])<set>  = {i+5 for i in range(10)}                   # Or: {5, 6, ..., 14}<dict> = {i: i*2 for i in range(10)}                # Or: {0: 0, 1: 2, ..., 9: 18}>>> [l+r for l in 'abc' for r in 'abc']['aa', 'ab', 'ac', ..., 'cc']Map, Filter, Reducefrom functools import reduce<iter> = map(lambda x: x + 1, range(10))            # Or: iter([1, 2, ..., 10])<iter> = filter(lambda x: x > 5, range(10))         # Or: iter([6, 7, 8, 9])<obj>  = reduce(lambda out, x: out + x, range(10))  # Or: 45Any, All<bool> = any(<collection>)                          # Is `bool(<el>)` True for any element.<bool> = all(<collection>)                          # Is True for all elements or empty.Conditional Expression<obj> = <exp> if <condition> else <exp>             # Only one expression gets evaluated.>>> [a if a else 'zero' for a in (0, 1, 2, 3)]      # `any([0, '', [], None]) == False`['zero', 1, 2, 3]Named Tuple, Enum, Dataclassfrom collections import namedtuplePoint = namedtuple('Point', 'x y')                  # Creates a tuple's subclass.point = Point(0, 0)                                 # Returns its instance.from enum import EnumDirection = Enum('Direction', 'N E S W')            # Creates an enum.direction = Direction.N                             # Returns its member.from dataclasses import make_dataclassPlayer = make_dataclass('Player', ['loc', 'dir'])   # Creates a class.player = Player(point, direction)                   # Returns its instance.Importsimport <module>            # Imports a built-in or '<module>.py'.import <package>           # Imports a built-in or '<package>/__init__.py'.import <package>.<module>  # Imports a built-in or '<package>/<module>.py'.Package is a collection of modules, but it can also define its own objects.On a filesystem this corresponds to a directory of Python files with an optional init script.Running 'import <package>' does not automatically provide access to the package's modules unless they are explicitly imported in its init script.ClosureWe have/get a closure in Python when:A nested function references a value of its enclosing function and thenthe enclosing function returns the nested function.def get_multiplier(a):    def out(b):        return a * b    return out>>> multiply_by_3 = get_multiplier(3)>>> multiply_by_3(10)30If multiple nested functions within enclosing function reference the same value, that value gets shared.To dynamically access function's first free variable use '<function>.__closure__[0].cell_contents'.Partialfrom functools import partial<function> = partial(<function> [, <arg_1>, <arg_2>, ...])>>> def multiply(a, b):...     return a * b>>> multiply_by_3 = partial(multiply, 3)>>> multiply_by_3(10)30Partial is also useful in cases when function needs to be passed as an argument because it enables us to set its arguments beforehand.A few examples being: 'defaultdict(<function>)', 'iter(<function>, to_exclusive)' and dataclass's 'field(default_factory=<function>)'.Non-LocalIf variable is being assigned to anywhere in the scope, it is regarded as a local variable, unless it is declared as a 'global' or a 'nonlocal'.def get_counter():    i = 0    def out():        nonlocal i        i += 1        return i    return out>>> counter = get_counter()>>> counter(), counter(), counter()(1, 2, 3)DecoratorA decorator takes a function, adds some functionality and returns it.It can be any callable, but is usually implemented as a function that returns a closure.@decorator_namedef function_that_gets_passed_to_decorator():    ...Debugger ExampleDecorator that prints function's name every time the function is called.from functools import wrapsdef debug(func):    @wraps(func)    def out(*args, **kwargs):        print(func.__name__)        return func(*args, **kwargs)    return out@debugdef add(x, y):    return x + yWraps is a helper decorator that copies the metadata of the passed function (func) to the function it is wrapping (out).Without it 'add.__name__' would return 'out'.LRU CacheDecorator that caches function's return values. All function's arguments must be hashable.from functools import lru_cache@lru_cache(maxsize=None)def fib(n):    return n if n < 2 else fib(n-2) + fib(n-1)Default size of the cache is 128 values. Passing 'maxsize=None' makes it unbounded.CPython interpreter limits recursion depth to 1000 by default. To increase it use 'sys.setrecursionlimit(<depth>)'.Parametrized DecoratorA decorator that accepts arguments and returns a normal decorator that accepts a function.from functools import wrapsdef debug(print_result=False):    def decorator(func):        @wraps(func)        def out(*args, **kwargs):            result = func(*args, **kwargs)            print(func.__name__, result if print_result else '')            return result        return out    return decorator@debug(print_result=True)def add(x, y):    return x + yUsing only '@debug' to decorate the add() function would not work here, because debug would then receive the add() function as a 'print_result' argument. Decorators can however manually check if the argument they received is a function and act accordingly.Classclass <name>:    def __init__(self, a):        self.a = a    def __repr__(self):        class_name = self.__class__.__name__        return f'{class_name}({self.a!r})'    def __str__(self):        return str(self.a)    @classmethod    def get_class_name(cls):        return cls.__name__Return value of repr() should be unambiguous and of str() readable.If only repr() is defined, it will also be used for str().Methods decorated with '@staticmethod' do not receive 'self' nor 'cls' as their first arg.Expressions that call the str() method:print(<el>)f'{<el>}'logging.warning(<el>)csv.writer(<file>).writerow([<el>])raise Exception(<el>)Expressions that call the repr() method:print/str/repr([<el>])print/str/repr({<el>: <el>})f'{<el>!r}'Z = dataclasses.make_dataclass('Z', ['a']); print/str/repr(Z(<el>))>>> <el>Constructor Overloadingclass <name>:    def __init__(self, a=None):        self.a = aInheritanceclass Person:    def __init__(self, name, age):        self.name = name        self.age  = ageclass Employee(Person):    def __init__(self, name, age, staff_num):        super().__init__(name, age)        self.staff_num = staff_numMultiple Inheritanceclass A: passclass B: passclass C(A, B): passMRO determines the order in which parent classes are traversed when searching for a method or an attribute:>>> C.mro()[<class 'C'>, <class 'A'>, <class 'B'>, <class 'object'>]PropertyPythonic way of implementing getters and setters.class Person:    @property    def name(self):        return ' '.join(self._name)    @name.setter    def name(self, value):        self._name = value.split()>>> person = Person()>>> person.name = '\\t Guido  van Rossum \'>>> person.name'Guido van Rossum'DataclassDecorator that automatically generates init(), repr() and eq() special methods.from dataclasses import dataclass, field@dataclass(order=False, frozen=False)class <class_name>:    <attr_name>: <type>    <attr_name>: <type> = <default_value>    <attr_name>: list/dict/set = field(default_factory=list/dict/set)Objects can be made sortable with 'order=True' and immutable with 'frozen=True'.For object to be hashable, all attributes must be hashable and 'frozen' must be True.Function field() is needed because '<attr_name>: list = []' would make a list that is shared among all instances. Its 'default_factory' argument can be any callable.For attributes of arbitrary type use 'typing.Any'.Inline:from dataclasses import make_dataclass<class> = make_dataclass('<class_name>', <coll_of_attribute_names>)<class> = make_dataclass('<class_name>', <coll_of_tuples>)<tuple> = ('<attr_name>', <type> [, <default_value>])Rest of type annotations (CPython interpreter ignores them all):import collections.abc as abc, typing as tp<var_name>: list/set/abc.Iterable/abc.Sequence/tp.Optional[<type>] [= <obj>]<var_name>: dict/tuple/tp.Union[<type>, ...] [= <obj>]def func(<arg_name>: <type> [= <obj>]) -> <type>: ...SlotsMechanism that restricts objects to attributes listed in 'slots' and significantly reduces their memory footprint.class MyClassWithSlots:    __slots__ = ['a']    def __init__(self):        self.a = 1Copyfrom copy import copy, deepcopy<object> = copy(<object>)<object> = deepcopy(<object>)Duck TypesA duck type is an implicit type that prescribes a set of special methods. Any object that has those methods defined is considered a member of that duck type.ComparableIf eq() method is not overridden, it returns 'id(self) == id(other)', which is the same as 'self is other'.That means all objects compare not equal by default.Only the left side object has eq() method called, unless it returns NotImplemented, in which case the right object is consulted. False is returned if both return NotImplemented.Ne() automatically works on any object that has eq() defined.class MyComparable:    def __init__(self, a):        self.a = a    def __eq__(self, other):        if isinstance(other, type(self)):            return self.a == other.a        return NotImplementedHashableHashable object needs both hash() and eq() methods and its hash value should never change.Hashable objects that compare equal must have the same hash value, meaning default hash() that returns 'id(self)' will not do.That is why Python automatically makes classes unhashable if you only implement eq().class MyHashable:    def __init__(self, a):        self._a = a    @property    def a(self):        return self._a    def __eq__(self, other):        if isinstance(other, type(self)):            return self.a == other.a        return NotImplemented    def __hash__(self):        return hash(self.a)SortableWith 'total_ordering' decorator, you only need to provide eq() and one of lt(), gt(), le() or ge() special methods and the rest will be automatically generated.Functions sorted() and min() only require lt() method, while max() only requires gt(). However, it is best to define them all so that confusion doesn't arise in other contexts.When two lists, strings or dataclasses are compared, their values get compared in order until a pair of unequal values is found. The comparison of this two values is then returned. The shorter sequence is considered smaller in case of all values being equal.For proper alphabetical order pass 'key=locale.strxfrm' to sorted() after running 'locale.setlocale(locale.LC_COLLATE, \""en_US.UTF-8\"")'.from functools import total_ordering@total_orderingclass MySortable:    def __init__(self, a):        self.a = a    def __eq__(self, other):        if isinstance(other, type(self)):            return self.a == other.a        return NotImplemented    def __lt__(self, other):        if isinstance(other, type(self)):            return self.a < other.a        return NotImplementedIteratorAny object that has methods next() and iter() is an iterator.Next() should return next item or raise StopIteration.Iter() should return 'self'.class Counter:    def __init__(self):        self.i = 0    def __next__(self):        self.i += 1        return self.i    def __iter__(self):        return self>>> counter = Counter()>>> next(counter), next(counter), next(counter)(1, 2, 3)Python has many different iterator objects:Sequence iterators returned by the iter() function, such as list_iterator and set_iterator.Objects returned by the itertools module, such as count, repeat and cycle.Generators returned by the generator functions and generator expressions.File objects returned by the open() function, etc.CallableAll functions and classes have a call() method, hence are callable.When this cheatsheet uses '<function>' as an argument, it actually means '<callable>'.class Counter:    def __init__(self):        self.i = 0    def __call__(self):        self.i += 1        return self.i>>> counter = Counter()>>> counter(), counter(), counter()(1, 2, 3)Context ManagerWith statements only work with objects that have enter() and exit() special methods.Enter() should lock the resources and optionally return an object.Exit() should release the resources.Any exception that happens inside the with block is passed to the exit() method.The exit() method can suppress the exception by returning a true value.class MyOpen:    def __init__(self, filename):        self.filename = filename    def __enter__(self):        self.file = open(self.filename)        return self.file    def __exit__(self, exc_type, exception, traceback):        self.file.close()>>> with open('test.txt', 'w') as file:...     file.write('Hello World!')>>> with MyOpen('test.txt') as file:...     print(file.read())Hello World!Iterable Duck TypesIterableOnly required method is iter(). It should return an iterator of object's items.Contains() automatically works on any object that has iter() defined.class MyIterable:    def __init__(self, a):        self.a = a    def __iter__(self):        return iter(self.a)    def __contains__(self, el):        return el in self.a>>> obj = MyIterable([1, 2, 3])>>> [el for el in obj][1, 2, 3]>>> 1 in objTrueCollectionOnly required methods are iter() and len(). Len() should return the number of items.This cheatsheet actually means '<iterable>' when it uses '<collection>'.I chose not to use the name 'iterable' because it sounds scarier and more vague than 'collection'. The only drawback of this decision is that the reader could think a certain function doesn't accept iterators when it does, since iterators are the only built-in objects that are iterable but are not collections.class MyCollection:    def __init__(self, a):        self.a = a    def __iter__(self):        return iter(self.a)    def __contains__(self, el):        return el in self.a    def __len__(self):        return len(self.a)SequenceOnly required methods are getitem() and len().Getitem() should return an item at the passed index or raise IndexError.Iter() and contains() automatically work on any object that has getitem() defined.Reversed() automatically works on any object that has getitem() and len() defined.class MySequence:    def __init__(self, a):        self.a = a    def __iter__(self):        return iter(self.a)    def __contains__(self, el):        return el in self.a    def __len__(self):        return len(self.a)    def __getitem__(self, i):        return self.a[i]    def __reversed__(self):        return reversed(self.a)Discrepancies between glossary definitions and abstract base classes:Glossary defines iterable as any object with iter() or getitem() and sequence as any object with getitem() and len(). It does not define collection.Passing ABC Iterable to isinstance() or issubclass() checks whether object/class has method iter(), while ABC Collection checks for iter(), contains() and len().ABC SequenceIt's a richer interface than the basic sequence.Extending it generates iter(), contains(), reversed(), index() and count().Unlike 'abc.Iterable' and 'abc.Collection', it is not a duck type. That is why 'issubclass(MySequence, abc.Sequence)' would return False even if MySequence had all the methods defined. It however recognizes list, tuple, range, str, bytes, bytearray, array, memoryview and deque, because they are registered as its virtual subclasses.from collections import abcclass MyAbcSequence(abc.Sequence):    def __init__(self, a):        self.a = a    def __len__(self):        return len(self.a)    def __getitem__(self, i):        return self.a[i]Table of required and automatically available special methods:+------------+------------+------------+------------+--------------+|            |  Iterable  | Collection |  Sequence  | abc.Sequence |+------------+------------+------------+------------+--------------+| iter()     |    REQ     |    REQ     |    Yes     |     Yes      || contains() |    Yes     |    Yes     |    Yes     |     Yes      || len()      |            |    REQ     |    REQ     |     REQ      || getitem()  |            |            |    REQ     |     REQ      || reversed() |            |            |    Yes     |     Yes      || index()    |            |            |            |     Yes      || count()    |            |            |            |     Yes      |+------------+------------+------------+------------+--------------+Other ABCs that generate missing methods are: MutableSequence, Set, MutableSet, Mapping and MutableMapping.Names of their required methods are stored in '<abc>.__abstractmethods__'.Enumfrom enum import Enum, autoclass <enum_name>(Enum):    <member_name> = auto()    <member_name> = <value>    <member_name> = <value>, <value>Function auto() returns an increment of the last numeric value or 1.Accessing a member named after a reserved keyword causes SyntaxError.Methods receive the member they were called on as the 'self' argument.<member> = <enum>.<member_name>           # Returns a member.<member> = <enum>['<member_name>']        # Returns a member. Raises KeyError.<member> = <enum>(<value>)                # Returns a member. Raises ValueError.<str>    = <member>.name                  # Returns member's name.<obj>    = <member>.value                 # Returns member's value.<list>   = list(<enum>)                   # Returns enum's members.<list>   = [a.name for a in <enum>]       # Returns enum's member names.<list>   = [a.value for a in <enum>]      # Returns enum's member values.<member> = random.choice(list(<enum>))    # Returns a random member.def get_next_member(member):    members = list(type(member))    index = members.index(member) + 1    return members[index % len(members)]InlineCutlery = Enum('Cutlery', 'FORK KNIFE SPOON')Cutlery = Enum('Cutlery', ['FORK', 'KNIFE', 'SPOON'])Cutlery = Enum('Cutlery', {'FORK': 1, 'KNIFE': 2, 'SPOON': 3})User-defined functions cannot be values, so they must be wrapped:from functools import partialLogicOp = Enum('LogicOp', {'AND': partial(lambda l, r: l and r),                           'OR':  partial(lambda l, r: l or r)})Exceptionstry:    <code>except <exception>:    <code>Complex Exampletry:    <code_1>except <exception_a>:    <code_2_a>except <exception_b>:    <code_2_b>else:    <code_2_c>finally:    <code_3>Code inside the 'else' block will only be executed if 'try' block had no exceptions.Code inside the 'finally' block will always be executed (unless a signal is received).All variables that are initialized in executed blocks are also visible in all subsequent blocks, as well as outside the try/except clause (only function blocks delimit scope).To catch signals use 'signal.signal(signal_number, <func>)'.Catching Exceptionsexcept <exception>: ...except <exception> as <name>: ...except (<exception>, [...]): ...except (<exception>, [...]) as <name>: ...Also catches subclasses of the exception.Use 'traceback.print_exc()' to print the error message to stderr.Use 'print(<name>)' to print just the cause of the exception (its arguments).Use 'logging.exception(<message>)' to log the passed message, followed by the full error message of the caught exception.Raising Exceptionsraise <exception>raise <exception>()raise <exception>(<el> [, ...])Re-raising caught exception:except <exception> [as <name>]:    ...    raiseException Objectarguments = <name>.argsexc_type  = <name>.__class__filename  = <name>.__traceback__.tb_frame.f_code.co_filenamefunc_name = <name>.__traceback__.tb_frame.f_code.co_nameline      = linecache.getline(filename, <name>.__traceback__.tb_lineno)trace_str = ''.join(traceback.format_tb(<name>.__traceback__))error_msg = ''.join(traceback.format_exception(type(<name>), <name>, <name>.__traceback__))Built-in ExceptionsBaseException +-- SystemExit                   # Raised by the sys.exit() function. +-- KeyboardInterrupt            # Raised when the user hits the interrupt key (ctrl-c). +-- Exception                    # User-defined exceptions should be derived from this class.      +-- ArithmeticError         # Base class for arithmetic errors such as ZeroDivisionError.      +-- AssertionError          # Raised by `assert <exp>` if expression returns false value.      +-- AttributeError          # Raised when object doesn't have requested attribute/method.      +-- EOFError                # Raised by input() when it hits an end-of-file condition.      +-- LookupError             # Base class for errors when a collection can't find an item.      |    +-- IndexError         # Raised when a sequence index is out of range.      |    +-- KeyError           # Raised when a dictionary key or set element is missing.      +-- MemoryError             # Out of memory. Could be too late to start deleting vars.      +-- NameError               # Raised when nonexistent name (variable/func/class) is used.      |    +-- UnboundLocalError  # Raised when local name is used before it's being defined.      +-- OSError                 # Errors such as FileExistsError/PermissionError (see #Open).      |    +-- ConnectionError    # Errors such as BrokenPipeError/ConnectionAbortedError.      +-- RuntimeError            # Raised by errors that don't fall into other categories.      |    +-- NotImplementedErr  # Can be raised by abstract methods or by unfinished code.      |    +-- RecursionError     # Raised when the maximum recursion depth is exceeded.      +-- StopIteration           # Raised by next() when run on an empty iterator.      +-- TypeError               # Raised when an argument is of the wrong type.      +-- ValueError              # When argument has the right type but inappropriate value.Collections and their exceptions:+-----------+------------+------------+------------+|           |    List    |    Set     |    Dict    |+-----------+------------+------------+------------+| getitem() | IndexError |            |  KeyError  || pop()     | IndexError |  KeyError  |  KeyError  || remove()  | ValueError |  KeyError  |            || index()   | ValueError |            |            |+-----------+------------+------------+------------+Useful built-in exceptions:raise TypeError('Argument is of the wrong type!')raise ValueError('Argument has the right type but an inappropriate value!')raise RuntimeError('None of above!')User-defined Exceptionsclass MyError(Exception): passclass MyInputError(MyError): passExitExits the interpreter by raising SystemExit exception.import syssys.exit()                        # Exits with exit code 0 (success).sys.exit(<el>)                    # Prints to stderr and exits with 1.sys.exit(<int>)                   # Exits with passed exit code.Printprint(<el_1>, ..., sep=' ', end='\', file=sys.stdout, flush=False)Use 'file=sys.stderr' for messages about errors.Use 'flush=True' to forcibly flush the stream.Pretty Printfrom pprint import pprintpprint(<collection>, width=80, depth=None, compact=False, sort_dicts=True)Levels deeper than 'depth' get replaced by '...'.InputReads a line from the user input or pipe if present.<str> = input(prompt=None)Trailing newline gets stripped.Prompt string is printed to the standard output before reading input.Raises EOFError when user hits EOF (ctrl-d/ctrl-z⏎) or input stream gets exhausted.Command Line Argumentsimport sysscripts_path = sys.argv[0]arguments    = sys.argv[1:]Argument Parserfrom argparse import ArgumentParser, FileTypep = ArgumentParser(description=<str>)p.add_argument('-<short_name>', '--<name>', action='store_true')  # Flag.p.add_argument('-<short_name>', '--<name>', type=<type>)          # Option.p.add_argument('<name>', type=<type>, nargs=1)                    # First argument.p.add_argument('<name>', type=<type>, nargs='+')                  # Remaining arguments.p.add_argument('<name>', type=<type>, nargs='*')                  # Optional arguments.args  = p.parse_args()                                            # Exits on error.value = args.<name>Use 'help=<str>' to set argument description that will be displayed in help message.Use 'default=<el>' to set the default value.Use 'type=FileType(<mode>)' for files. Accepts 'encoding', but 'newline' is None.OpenOpens the file and returns a corresponding file object.<file> = open(<path>, mode='r', encoding=None, newline=None)'encoding=None' means that the default encoding is used, which is platform dependent. Best practice is to use 'encoding=\""utf-8\""' whenever possible.'newline=None' means all different end of line combinations are converted to '\' on read, while on write all '\' characters are converted to system's default line separator.'newline=\""\""' means no conversions take place, but input is still broken into chunks by readline() and readlines() on every '\', '\\r' and '\\r\'.Modes'r'  - Read (default).'w'  - Write (truncate).'x'  - Write or fail if the file already exists.'a'  - Append.'w+' - Read and write (truncate).'r+' - Read and write from the start.'a+' - Read and write from the end.'t'  - Text mode (default).'b'  - Binary mode ('br', 'bw', 'bx', …).Exceptions'FileNotFoundError' can be raised when reading with 'r' or 'r+'.'FileExistsError' can be raised when writing with 'x'.'IsADirectoryError' and 'PermissionError' can be raised by any.'OSError' is the parent class of all listed exceptions.File Object<file>.seek(0)                      # Moves to the start of the file.<file>.seek(offset)                 # Moves 'offset' chars/bytes from the start.<file>.seek(0, 2)                   # Moves to the end of the file.<bin_file>.seek(±offset, <anchor>)  # Anchor: 0 start, 1 current position, 2 end.<str/bytes> = <file>.read(size=-1)  # Reads 'size' chars/bytes or until EOF.<str/bytes> = <file>.readline()     # Returns a line or empty string/bytes on EOF.<list>      = <file>.readlines()    # Returns a list of remaining lines.<str/bytes> = next(<file>)          # Returns a line using buffer. Do not mix.<file>.write(<str/bytes>)           # Writes a string or bytes object.<file>.writelines(<collection>)     # Writes a coll. of strings or bytes objects.<file>.flush()                      # Flushes write buffer. Runs every 4096/8192 B.Methods do not add or strip trailing newlines, not even writelines().Read Text from Filedef read_file(filename):    with open(filename, encoding='utf-8') as file:        return file.readlines()Write Text to Filedef write_to_file(filename, text):    with open(filename, 'w', encoding='utf-8') as file:        file.write(text)Pathsimport os, globfrom pathlib import Path<str>  = os.getcwd()                # Returns the current working directory.<str>  = os.path.join(<path>, ...)  # Joins two or more pathname components.<str>  = os.path.realpath(<path>)   # Resolves symlinks and calls path.abspath().<str>  = os.path.basename(<path>)   # Returns final component of the path.<str>  = os.path.dirname(<path>)    # Returns path without the final component.<tup.> = os.path.splitext(<path>)   # Splits on last period of the final component.<list> = os.listdir(path='.')       # Returns filenames located at the path.<list> = glob.glob('<pattern>')     # Returns paths matching the wildcard pattern.<bool> = os.path.exists(<path>)     # Or: <Path>.exists()<bool> = os.path.isfile(<path>)     # Or: <DirEntry/Path>.is_file()<bool> = os.path.isdir(<path>)      # Or: <DirEntry/Path>.is_dir()<stat> = os.stat(<path>)            # Or: <DirEntry/Path>.stat()<real> = <stat>.st_mtime/st_size/…  # Modification time, size in bytes, ...DirEntryUnlike listdir(), scandir() returns DirEntry objects that cache isfile, isdir and on Windows also stat information, thus significantly increasing the performance of code that requires it.<iter> = os.scandir(path='.')       # Returns DirEntry objects located at the path.<str>  = <DirEntry>.path            # Returns the whole path as a string.<str>  = <DirEntry>.name            # Returns final component as a string.<file> = open(<DirEntry>)           # Opens the file and returns a file object.Path Object<Path> = Path(<path> [, ...])       # Accepts strings, Paths and DirEntry objects.<Path> = <path> / <path> [/ ...]    # First or second path must be a Path object.<Path> = <Path>.resolve()           # Returns absolute path with resolved symlinks.<Path> = Path()                     # Returns relative cwd. Also Path('.').<Path> = Path.cwd()                 # Returns absolute cwd. Also Path().resolve().<Path> = Path.home()                # Returns user's home directory (absolute).<Path> = Path(__file__).resolve()   # Returns script's path if cwd wasn't changed.<Path> = <Path>.parent              # Returns Path without the final component.<str>  = <Path>.name                # Returns final component as a string.<str>  = <Path>.stem                # Returns final component without extension.<str>  = <Path>.suffix              # Returns final component's extension.<tup.> = <Path>.parts               # Returns all components as strings.<iter> = <Path>.iterdir()           # Returns directory contents as Path objects.<iter> = <Path>.glob('<pattern>')   # Returns Paths matching the wildcard pattern.<str>  = str(<Path>)                # Returns path as a string.<file> = open(<Path>)               # Also <Path>.read/write_text/bytes().OS Commandsimport os, shutil, subprocessos.chdir(<path>)                    # Changes the current working directory.os.mkdir(<path>, mode=0o777)        # Creates a directory. Permissions are in octal.os.makedirs(<path>, mode=0o777)     # Creates all path's dirs. Also `exist_ok=False`.shutil.copy(from, to)               # Copies the file. 'to' can exist or be a dir.shutil.copy2(from, to)              # Also copies creation and modification time.shutil.copytree(from, to)           # Copies the directory. 'to' must not exist.os.rename(from, to)                 # Renames/moves the file or directory.os.replace(from, to)                # Same, but overwrites file 'to' even on Windows.shutil.move(from, to)               # Rename() that moves into 'to' if it's a dir.os.remove(<path>)                   # Deletes the file.os.rmdir(<path>)                    # Deletes the empty directory.shutil.rmtree(<path>)               # Deletes the directory.Paths can be either strings, Paths or DirEntry objects.Functions report OS related errors by raising either OSError or one of its subclasses.Shell Commands<pipe> = os.popen('<command>')      # Executes command in sh/cmd. Returns its stdout pipe.<str>  = <pipe>.read(size=-1)       # Reads 'size' chars or until EOF. Also readline/s().<int>  = <pipe>.close()             # Closes the pipe. Returns None on success (returncode 0).Sends '1 + 1' to the basic calculator and captures its output:>>> subprocess.run('bc', input='1 + 1\', capture_output=True, text=True)CompletedProcess(args='bc', returncode=0, stdout='2\', stderr='')Sends test.in to the basic calculator running in standard mode and saves its output to test.out:>>> from shlex import split>>> os.popen('echo 1 + 1 > test.in')>>> subprocess.run(split('bc -s'), stdin=open('test.in'), stdout=open('test.out', 'w'))CompletedProcess(args=['bc', '-s'], returncode=0)>>> open('test.out').read()'2\'JSONText file format for storing collections of strings and numbers.import json<str>    = json.dumps(<object>)     # Converts object to JSON string.<object> = json.loads(<str>)        # Converts JSON string to object.Read Object from JSON Filedef read_json_file(filename):    with open(filename, encoding='utf-8') as file:        return json.load(file)Write Object to JSON Filedef write_to_json_file(filename, an_object):    with open(filename, 'w', encoding='utf-8') as file:        json.dump(an_object, file, ensure_ascii=False, indent=2)PickleBinary file format for storing Python objects.import pickle<bytes>  = pickle.dumps(<object>)   # Converts object to bytes object.<object> = pickle.loads(<bytes>)    # Converts bytes object to object.Read Object from Filedef read_pickle_file(filename):    with open(filename, 'rb') as file:        return pickle.load(file)Write Object to Filedef write_to_pickle_file(filename, an_object):    with open(filename, 'wb') as file:        pickle.dump(an_object, file)CSVText file format for storing spreadsheets.import csvRead<reader> = csv.reader(<file>)       # Also: `dialect='excel', delimiter=','`.<list>   = next(<reader>)           # Returns next row as a list of strings.<list>   = list(<reader>)           # Returns a list of remaining rows.File must be opened with a 'newline=\""\""' argument, or newlines embedded inside quoted fields will not be interpreted correctly!To print the spreadsheet to the console use Tabulate library.For XML and binary Excel files (xlsx, xlsm and xlsb) use Pandas library.Reader accepts any iterator of strings, not just files.Write<writer> = csv.writer(<file>)       # Also: `dialect='excel', delimiter=','`.<writer>.writerow(<collection>)     # Encodes objects using `str(<el>)`.<writer>.writerows(<coll_of_coll>)  # Appends multiple rows.File must be opened with a 'newline=\""\""' argument, or '\\r' will be added in front of every '\' on platforms that use '\\r\' line endings!Parameters'dialect' - Master parameter that sets the default values. String or a 'csv.Dialect' object.'delimiter' - A one-character string used to separate fields.'quotechar' - Character for quoting fields that contain special characters.'doublequote' - Whether quotechars inside fields are/get doubled or escaped.'skipinitialspace' - Is space character at the start of the field stripped by the reader.'lineterminator' - How writer terminates rows. Reader is hardcoded to '\', '\\r', '\\r\'.'quoting' - 0: As necessary, 1: All, 2: All but numbers which are read as floats, 3: None.'escapechar' - Character for escaping quotechars if 'doublequote' is False.Dialects+------------------+--------------+--------------+--------------+|                  |     excel    |   excel-tab  |     unix     |+------------------+--------------+--------------+--------------+| delimiter        |       ','    |      '\\t'    |       ','    || quotechar        |       '\""'    |       '\""'    |       '\""'    || doublequote      |      True    |      True    |      True    || skipinitialspace |     False    |     False    |     False    || lineterminator   |    '\\r\'    |    '\\r\'    |      '\'    || quoting          |         0    |         0    |         1    || escapechar       |      None    |      None    |      None    |+------------------+--------------+--------------+--------------+Read Rows from CSV Filedef read_csv_file(filename, dialect='excel', **params):    with open(filename, encoding='utf-8', newline='') as file:        return list(csv.reader(file, dialect, **params))Write Rows to CSV Filedef write_to_csv_file(filename, rows, dialect='excel', **params):    with open(filename, 'w', encoding='utf-8', newline='') as file:        writer = csv.writer(file, dialect, **params)        writer.writerows(rows)SQLiteA server-less database engine that stores each database into a separate file.import sqlite3<conn> = sqlite3.connect(<path>)                # Opens existing or new file. Also ':memory:'.<conn>.close()                                  # Closes the connection.Read<cursor> = <conn>.execute('<query>')            # Can raise a subclass of sqlite3.Error.<tuple>  = <cursor>.fetchone()                  # Returns next row. Also next(<cursor>).<list>   = <cursor>.fetchall()                  # Returns remaining rows. Also list(<cursor>).Write<conn>.execute('<query>')                       # Can raise a subclass of sqlite3.Error.<conn>.commit()                                 # Saves all changes since the last commit.<conn>.rollback()                               # Discards all changes since the last commit.Or:with <conn>:                                    # Exits the block with commit() or rollback(),    <conn>.execute('<query>')                   # depending on whether any exception occurred.Placeholders<conn>.execute('<query>', <list/tuple>)         # Replaces '?'s in query with values.<conn>.execute('<query>', <dict/namedtuple>)    # Replaces ':<key>'s with values.<conn>.executemany('<query>', <coll_of_above>)  # Runs execute() multiple times.Passed values can be of type str, int, float, bytes, None, bool, datetime.date or datetime.datetime.Bools will be stored and returned as ints and dates as ISO formatted strings.ExampleValues are not actually saved in this example because 'conn.commit()' is omitted!>>> conn = sqlite3.connect('test.db')>>> conn.execute('CREATE TABLE person (person_id INTEGER PRIMARY KEY, name, height)')>>> conn.execute('INSERT INTO person VALUES (NULL, ?, ?)', ('Jean-Luc', 187)).lastrowid1>>> conn.execute('SELECT * FROM person').fetchall()[(1, 'Jean-Luc', 187)]SqlAlchemy# $ pip3 install sqlalchemyfrom sqlalchemy import create_engine, text<engine> = create_engine('<url>')               # Url: 'dialect://user:password@host/dbname'.<conn>   = <engine>.connect()                   # Creates a connection. Also <conn>.close().<cursor> = <conn>.execute(text('<query>'), …)   # Replaces ':<key>'s with keyword arguments.with <conn>.begin(): ...                        # Exits the block with commit or rollback.+------------+--------------+----------+----------------------------------+| Dialect    | pip3 install | import   |           Dependencies           |+------------+--------------+----------+----------------------------------+| mysql      | mysqlclient  | MySQLdb  | www.pypi.org/project/mysqlclient || postgresql | psycopg2     | psycopg2 | www.pypi.org/project/psycopg2    || mssql      | pyodbc       | pyodbc   | www.pypi.org/project/pyodbc      || oracle     | oracledb     | oracledb | www.pypi.org/project/oracledb    |+------------+--------------+----------+----------------------------------+BytesBytes object is an immutable sequence of single bytes. Mutable version is called bytearray.<bytes> = b'<str>'                          # Only accepts ASCII characters and \\x00-\\xff.<int>   = <bytes>[<index>]                  # Returns an int in range from 0 to 255.<bytes> = <bytes>[<slice>]                  # Returns bytes even if it has only one element.<bytes> = <bytes>.join(<coll_of_bytes>)     # Joins elements using bytes as a separator.Encode<bytes> = bytes(<coll_of_ints>)             # Ints must be in range from 0 to 255.<bytes> = bytes(<str>, 'utf-8')             # Or: <str>.encode('utf-8')<bytes> = <int>.to_bytes(n_bytes, …)        # `byteorder='big/little', signed=False`.<bytes> = bytes.fromhex('<hex>')            # Hex pairs can be separated by whitespaces.Decode<list>  = list(<bytes>)                     # Returns ints in range from 0 to 255.<str>   = str(<bytes>, 'utf-8')             # Or: <bytes>.decode('utf-8')<int>   = int.from_bytes(<bytes>, …)        # `byteorder='big/little', signed=False`.'<hex>' = <bytes>.hex()                     # Returns hex pairs. Accepts `sep=<str>`.Read Bytes from Filedef read_bytes(filename):    with open(filename, 'rb') as file:        return file.read()Write Bytes to Filedef write_bytes(filename, bytes_obj):    with open(filename, 'wb') as file:        file.write(bytes_obj)StructModule that performs conversions between a sequence of numbers and a bytes object.System’s type sizes, byte order, and alignment rules are used by default.from struct import pack, unpack<bytes> = pack('<format>', <el_1> [, ...])  # Packages arguments or raises struct.error.<tuple> = unpack('<format>', <bytes>)       # Use iter_unpack() for iterator of tuples.>>> pack('>hhl', 1, 2, 3)b'\\x00\\x01\\x00\\x02\\x00\\x00\\x00\\x03'>>> unpack('>hhl', b'\\x00\\x01\\x00\\x02\\x00\\x00\\x00\\x03')(1, 2, 3)FormatFor standard type sizes and manual alignment (padding) start format string with:'=' - System's byte order (usually little-endian).'<' - Little-endian.'>' - Big-endian (also '!').Besides numbers, pack() and unpack() also support bytes objects as part of the sequence:'c' - A bytes object with a single element. For pad byte use 'x'.'<n>s' - A bytes object with n elements.Integer types. Use a capital letter for unsigned type. Minimum and standard sizes are in brackets:'b' - char (1/1)'h' - short (2/2)'i' - int (2/4)'l' - long (4/4)'q' - long long (8/8)Floating point types (struct always uses standard sizes):'f' - float (4/4)'d' - double (8/8)ArrayList that can only hold numbers of a predefined type. Available types and their minimum sizes in bytes are listed above. Sizes and byte order are always determined by the system, however bytes of each element can be swapped with byteswap() method.from array import array<array> = array('<typecode>', <collection>)    # Array from collection of numbers.<array> = array('<typecode>', <bytes>)         # Array from bytes object.<array> = array('<typecode>', <array>)         # Treats array as a sequence of numbers.<bytes> = bytes(<array>)                       # Or: <array>.tobytes()<file>.write(<array>)                          # Writes array to the binary file.Memory ViewA sequence object that points to the memory of another bytes-like object.Each element can reference a single or multiple consecutive bytes, depending on format.Order and number of elements can be changed with slicing.Casting only works between char and other types and uses system's sizes.Byte order is always determined by the system.<mview> = memoryview(<bytes/bytearray/array>)  # Immutable if bytes, else mutable.<real>  = <mview>[<index>]                     # Returns an int or a float.<mview> = <mview>[<slice>]                     # Mview with rearranged elements.<mview> = <mview>.cast('<typecode>')           # Casts memoryview to the new format.<mview>.release()                              # Releases the object's memory buffer.<bytes> = bytes(<mview>)                       # Returns a new bytes object.<bytes> = <bytes>.join(<coll_of_mviews>)       # Joins mviews using bytes object as sep.<array> = array('<typecode>', <mview>)         # Treats mview as a sequence of numbers.<file>.write(<mview>)                          # Writes mview to the binary file.<list>  = list(<mview>)                        # Returns a list of ints or floats.<str>   = str(<mview>, 'utf-8')                # Treats mview as a bytes object.<int>   = int.from_bytes(<mview>, …)           # `byteorder='big/little', signed=False`.'<hex>' = <mview>.hex()                        # Treats mview as a bytes object.DequeA thread-safe list with efficient appends and pops from either side. Pronounced \""deck\"".from collections import deque<deque> = deque(<collection>)                  # Also `maxlen=None`.<deque>.appendleft(<el>)                       # Opposite element is dropped if full.<deque>.extendleft(<collection>)               # Collection gets reversed.<el> = <deque>.popleft()                       # Raises IndexError if empty.<deque>.rotate(n=1)                            # Rotates elements to the right.ThreadingCPython interpreter can only run a single thread at a time.That is why using multiple threads won't result in a faster execution, unless at least one of the threads contains an I/O operation.from threading import Thread, RLock, Semaphore, Event, Barrierfrom concurrent.futures import ThreadPoolExecutor, as_completedThread<Thread> = Thread(target=<function>)           # Use `args=<collection>` to set the arguments.<Thread>.start()                               # Starts the thread.<bool> = <Thread>.is_alive()                   # Checks if the thread has finished executing.<Thread>.join()                                # Waits for the thread to finish.Use 'kwargs=<dict>' to pass keyword arguments to the function.Use 'daemon=True', or the program will not be able to exit while the thread is alive.Lock<lock> = RLock()                               # Lock that can only be released by acquirer.<lock>.acquire()                               # Waits for the lock to be available.<lock>.release()                               # Makes the lock available again.Or:with <lock>:                                   # Enters the block by calling acquire() and    ...                                        # exits it with release(), even on error.Semaphore, Event, Barrier<Semaphore> = Semaphore(value=1)               # Lock that can be acquired by 'value' threads.<Event>     = Event()                          # Method wait() blocks until set() is called.<Barrier>   = Barrier(n_times)                 # Wait() blocks until it's called n_times.Queue<Queue> = queue.Queue(maxsize=0)               # A thread-safe FIFO queue. Also LifoQueue.<Queue>.put(<el>)                              # Blocks until queue stops being full.<Queue>.put_nowait(<el>)                       # Raises queue.Full exception if full.<el> = <Queue>.get()                           # Blocks until queue stops being empty.<el> = <Queue>.get_nowait()                    # Raises queue.Empty exception if empty.Thread Pool Executor<Exec> = ThreadPoolExecutor(max_workers=None)  # Or: `with ThreadPoolExecutor() as <name>: …`<iter> = <Exec>.map(<func>, <args_1>, ...)     # Multithreaded and non-lazy map(). Keeps order.<Futr> = <Exec>.submit(<func>, <arg_1>, ...)   # Creates a thread and returns its Future obj.<Exec>.shutdown(wait=True)                     # Blocks until all threads finish executing.<bool> = <Future>.done()                       # Checks if the thread has finished executing.<obj>  = <Future>.result(timeout=None)         # Waits for thread to finish and returns result.<bool> = <Future>.cancel()                     # Returns False if thread is already running.<iter> = as_completed(<coll_of_Futures>)       # Each Future is yielded as it completes.Map() and as_completed() also accept 'timeout' argument that causes TimeoutError if result isn't available in 'timeout' seconds after next() is called.Exceptions that happen inside threads are raised when next() is called on map's iterator or when result() is called on a Future. Its exception() method returns exception or None.ProcessPoolExecutor provides true parallelism, but everything sent to/from workers must be pickable. Queues must be sent using executor's 'initargs' and 'initializer' parameters.OperatorModule of functions that provide the functionality of operators. Functions are ordered by operator precedence, starting with least binding.import operator as op<bool> = op.not_(<obj>)                                        # not (or/and are not provided)<bool> = op.eq/ne/lt/le/gt/ge/contains(<obj>, <obj>)           # ==, !=, <, <=, >, >=, in<obj>  = op.or_/xor/and_(<int/set>, <int/set>)                 # |, ^, &<obj>  = op.add/sub/mul/truediv/floordiv/mod(<obj>, <obj>)     # +, -, *, /, //, %<num>  = op.neg/invert(<num>)                                  # -, ~<num>  = op.pow(<num>, <num>)                                  # **<func> = op.itemgetter/attrgetter/methodcaller(<obj> [, ...])  # [index/key], .name, .name()elementwise_sum  = map(op.add, list_a, list_b)sorted_by_second = sorted(<collection>, key=op.itemgetter(1))sorted_by_both   = sorted(<collection>, key=op.itemgetter(1, 0))product_of_elems = functools.reduce(op.mul, <collection>)union_of_sets    = functools.reduce(op.or_, <coll_of_sets>)first_element    = op.methodcaller('pop', 0)(<list>)Bitwise operators require objects to have and(), or() and xor() special methods, unlike logical operators that work on all types of objects.Also: '<bool> = <bool> &|^ <bool>' and '<int> = <bool> &|^ <int>'.Introspection<list> = dir()                             # Names of local variables, functions, classes, etc.<dict> = vars()                            # Dict of local variables, etc. Also locals().<dict> = globals()                         # Dict of global vars, etc. (incl. '__builtins__').Attributes<list> = dir(<object>)                     # Names of object's attributes (incl. methods).<dict> = vars(<object>)                    # Dict of writable attributes. Also <obj>.__dict__.<bool> = hasattr(<object>, '<attr_name>')  # Checks if getattr() raises an AttributeError.value  = getattr(<object>, '<attr_name>')  # Raises AttributeError if attribute is missing.setattr(<object>, '<attr_name>', value)    # Only works on objects with '__dict__' attribute.delattr(<object>, '<attr_name>')           # Same. Also `del <object>.<attr_name>`.Parameters<Sig>  = inspect.signature(<function>)     # Function's Signature object.<dict> = <Sig>.parameters                  # Dict of Parameter objects.<memb> = <Param>.kind                      # Member of ParameterKind enum.<obj>  = <Param>.default                   # Default value or Parameter.empty.<type> = <Param>.annotation                # Type or Parameter.empty.MetaprogrammingCode that generates code.TypeType is the root class. If only passed an object it returns its type (class). Otherwise it creates a new class.<class> = type('<class_name>', <tuple_of_parents>, <dict_of_class_attributes>)>>> Z = type('Z', (), {'a': 'abcde', 'b': 12345})>>> z = Z()Meta ClassA class that creates classes.def my_meta_class(name, parents, attrs):    attrs['a'] = 'abcde'    return type(name, parents, attrs)Or:class MyMetaClass(type):    def __new__(cls, name, parents, attrs):        attrs['a'] = 'abcde'        return type.__new__(cls, name, parents, attrs)New() is a class method that gets called before init(). If it returns an instance of its class, then that instance gets passed to init() as a 'self' argument.It receives the same arguments as init(), except for the first one that specifies the desired type of the returned instance (MyMetaClass in our case).Like in our case, new() can also be called directly, usually from a new() method of a child class (def __new__(cls): return super().__new__(cls)).The only difference between the examples above is that my_meta_class() returns a class of type type, while MyMetaClass() returns a class of type MyMetaClass.Metaclass AttributeRight before a class is created it checks if it has the 'metaclass' attribute defined. If not, it recursively checks if any of its parents has it defined and eventually comes to type().class MyClass(metaclass=MyMetaClass):    b = 12345>>> MyClass.a, MyClass.b('abcde', 12345)Type Diagramtype(MyClass) == MyMetaClass         # MyClass is an instance of MyMetaClass.type(MyMetaClass) == type            # MyMetaClass is an instance of type.+-------------+-------------+|   Classes   | Metaclasses |+-------------+-------------||   MyClass <-- MyMetaClass ||             |     ^       ||    object <----- type <+  ||             |     | +--+  ||     str <---------+       |+-------------+-------------+Inheritance DiagramMyClass.__base__ == object           # MyClass is a subclass of object.MyMetaClass.__base__ == type         # MyMetaClass is a subclass of type.+-------------+-------------+|   Classes   | Metaclasses |+-------------+-------------||   MyClass   | MyMetaClass ||      ^      |     ^       ||    object -----> type     ||      v      |             ||     str     |             |+-------------+-------------+Eval>>> from ast import literal_eval>>> literal_eval('[1, 2, 3]')[1, 2, 3]>>> literal_eval('1 + 2')ValueError: malformed node or stringCoroutinesCoroutines have a lot in common with threads, but unlike threads, they only give up control when they call another coroutine and they don’t use as much memory.Coroutine definition starts with 'async' and its call with 'await'.'asyncio.run(<coroutine>)' is the main entry point for asynchronous programs.Functions wait(), gather() and as_completed() start multiple coroutines at the same time.Asyncio module also provides its own Queue, Event, Lock and Semaphore classes.Runs a terminal game where you control an asterisk that must avoid numbers:import asyncio, collections, curses, curses.textpad, enum, randomP = collections.namedtuple('P', 'x y')         # PositionD = enum.Enum('D', 'n e s w')                  # DirectionW, H = 15, 7                                   # Width, Heightdef main(screen):    curses.curs_set(0)                         # Makes cursor invisible.    screen.nodelay(True)                       # Makes getch() non-blocking.    asyncio.run(main_coroutine(screen))        # Starts running asyncio code.async def main_coroutine(screen):    moves = asyncio.Queue()    state = {'*': P(0, 0), **{id_: P(W//2, H//2) for id_ in range(10)}}    ai    = [random_controller(id_, moves) for id_ in range(10)]    mvc   = [human_controller(screen, moves), model(moves, state), view(state, screen)]    tasks = [asyncio.create_task(cor) for cor in ai + mvc]    await asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED)async def random_controller(id_, moves):    while True:        d = random.choice(list(D))        moves.put_nowait((id_, d))        await asyncio.sleep(random.triangular(0.01, 0.65))async def human_controller(screen, moves):    while True:        key_mappings = {258: D.s, 259: D.n, 260: D.w, 261: D.e}        if d := key_mappings.get(screen.getch()):            moves.put_nowait(('*', d))        await asyncio.sleep(0.005)async def model(moves, state):    while state['*'] not in (state[id_] for id_ in range(10)):        id_, d = await moves.get()        x, y   = state[id_]        deltas = {D.n: P(0, -1), D.e: P(1, 0), D.s: P(0, 1), D.w: P(-1, 0)}        state[id_] = P((x + deltas[d].x) % W, (y + deltas[d].y) % H)async def view(state, screen):    offset = P(curses.COLS//2 - W//2, curses.LINES//2 - H//2)    while True:        screen.erase()        curses.textpad.rectangle(screen, offset.y-1, offset.x-1, offset.y+H, offset.x+W)        for id_, p in state.items():            screen.addstr(offset.y + (p.y - state['*'].y + H//2) % H,                          offset.x + (p.x - state['*'].x + W//2) % W, str(id_))        screen.refresh()        await asyncio.sleep(0.005)if __name__ == '__main__':    curses.wrapper(main)LibrariesProgress Bar# $ pip3 install tqdm>>> from tqdm import tqdm>>> from time import sleep>>> for el in tqdm([1, 2, 3], desc='Processing'):...     sleep(1)Processing: 100%|████████████████████| 3/3 [00:03<00:00,  1.00s/it]Plot# $ pip3 install matplotlibimport matplotlib.pyplot as pltplt.plot/bar/scatter(x_data, y_data [, label=<str>])  # Or: plt.plot(y_data)plt.legend()                                          # Adds a legend.plt.savefig(<path>)                                   # Saves the figure.plt.show()                                            # Displays the figure.plt.clf()                                             # Clears the figure.TablePrints a CSV file as an ASCII table:# $ pip3 install tabulateimport csv, tabulatewith open('test.csv', encoding='utf-8', newline='') as file:    rows   = csv.reader(file)    header = next(rows)    table  = tabulate.tabulate(rows, header)print(table)CursesRuns a basic file explorer in the terminal:import curses, osfrom curses import A_REVERSE, KEY_DOWN, KEY_UP, KEY_LEFT, KEY_RIGHT, KEY_ENTERdef main(screen):    ch, first, selected, paths = 0, 0, 0, os.listdir()    while ch != ord('q'):        height, width = screen.getmaxyx()        screen.erase()        for y, filename in enumerate(paths[first : first+height]):            color = A_REVERSE if filename == paths[selected] else 0            screen.addnstr(y, 0, filename, width-1, color)        ch = screen.getch()        selected += (ch == KEY_DOWN) - (ch == KEY_UP)        selected = max(0, min(len(paths)-1, selected))        first += (selected >= first + height) - (selected < first)        if ch in [KEY_LEFT, KEY_RIGHT, KEY_ENTER, ord('\'), ord('\\r')]:            new_dir = '..' if ch == KEY_LEFT else paths[selected]            if os.path.isdir(new_dir):                os.chdir(new_dir)                first, selected, paths = 0, 0, os.listdir()if __name__ == '__main__':    curses.wrapper(main)Loggingimport logginglogging.basicConfig(filename=<path>)              # Configures the root logger.logging.debug/info/warning/error/critical(<str>)  # Logs to the root logger.<Logger> = logging.getLogger(__name__)            # Logger named after the module.<Logger>.<level>(<str>)                           # Messages propagate to the root logger.<Logger>.exception(<str>)                         # Calls error() with caught exception.Setuplogging.basicConfig(    filename=None,                                # Logs to console by default.    format='%(levelname)s:%(name)s:%(message)s',  # Add `%(asctime)s` for datetime.    level=logging.WARNING,                        # Drops messages with lower priority.    handlers=[logging.StreamHandler()]            # Uses FileHandler if filename is set.)<Formatter> = logging.Formatter('<format>')       # Creates a Formatter.<Handler> = logging.FileHandler(<path>)           # Creates a Handler.<Handler>.setFormatter(<Formatter>)               # Adds Formatter to the Handler.<Handler>.setLevel(<int/str>)                     # Processes all messages by default.<Logger>.addHandler(<Handler>)                    # Adds Handler to the Logger.<Logger>.setLevel(<int/str>)                      # What is sent to handlers and parent.Parent logger can be specified by naming the child logger '<parent>.<name>'.Formatter also supports: pathname, filename, funcName, lineno, thread and process.A 'handlers.RotatingFileHandler' creates and deletes log files based on 'maxBytes' and 'backupCount' arguments.Creates a logger that writes all messages to a file and sends them to the root logger that prints to stdout:>>> logging.basicConfig(level='WARNING')>>> logger = logging.getLogger('my_module')>>> handler = logging.FileHandler('test.log')>>> formatter = logging.Formatter('%(asctime)s %(levelname)s:%(name)s:%(message)s')>>> handler.setFormatter(formatter)>>> logger.addHandler(handler)>>> logger.critical('Running out of disk space.')CRITICAL:my_module:Running out of disk space.>>> print(open('test.log').read())2023-02-07 23:21:01,430 CRITICAL:my_module:Running out of disk space.ScrapingScrapes Python's URL, version number and logo from its Wikipedia page:# $ pip3 install requests beautifulsoup4import requests, bs4, os, sysWIKI_URL = 'https://en.wikipedia.org/wiki/Python_(programming_language)'try:    html       = requests.get(WIKI_URL).text    document   = bs4.BeautifulSoup(html, 'html.parser')    table      = document.find('table', class_='infobox vevent')    python_url = table.find('th', text='Website').next_sibling.a['href']    version    = table.find('th', text='Stable release').next_sibling.strings.__next__()    logo_url   = table.find('img')['src']    logo       = requests.get(f'https:{logo_url}').content    filename   = os.path.basename(logo_url)    with open(filename, 'wb') as file:        file.write(logo)    print(f'{python_url}, {version}, file://{os.path.abspath(filename)}')except requests.exceptions.ConnectionError:    print(\""You've got problems with connection.\"", file=sys.stderr)WebFlask is a micro web framework/server. If you just want to open a html file in a web browser use 'webbrowser.open(<path>)' instead.# $ pip3 install flaskfrom flask import Flask, send_from_directory, render_template_string, requestapp = Flask(__name__)app.run(host=None, port=None, debug=None)Starts the app at 'http://localhost:5000'. Use 'host=\""0.0.0.0\""' to run externally.Install a WSGI server like Waitress and a HTTP server such as Nginx for better security.Debug mode restarts the app whenever script changes and displays errors in the browser.Static Request@app.route('/img/<path:filename>')def serve_file(filename):    return send_from_directory('dirname/', filename)Dynamic Request@app.route('/<sport>')def serve_html(sport):    return render_template_string('<h1>{{title}}</h1>', title=sport)To return an error code use 'abort(<int>)' and to redirect use 'redirect(<url>)'.'request.args[<str>]' returns parameter from the query string (URL part after '?').Use 'session[key] = value' to store session data like username, etc.REST Request@app.post('/<sport>/odds')def serve_json(sport):    team = request.form['team']    return {'team': team, 'odds': [2.09, 3.74, 3.68]}Starts the app in its own thread and queries it with a post request:# $ pip3 install requests>>> import threading, requests>>> threading.Thread(target=app.run, daemon=True).start()>>> url = 'http://localhost:5000/football/odds'>>> request_data = {'team': 'arsenal f.c.'}>>> response = requests.post(url, data=request_data)>>> response.json(){'team': 'arsenal f.c.', 'odds': [2.09, 3.74, 3.68]}Profilingfrom time import perf_counterstart_time = perf_counter()...duration_in_seconds = perf_counter() - start_timeTiming a Snippet>>> from timeit import timeit>>> timeit('list(range(10000))', number=1000, globals=globals(), setup='pass')0.19373Profiling by Line$ pip3 install line_profiler$ echo '@profiledef main():    a = list(range(10000))    b = set(range(10000))main()' > test.py$ kernprof -lv test.pyLine #   Hits     Time  Per Hit   % Time  Line Contents=======================================================     1                                    @profile     2                                    def main():     3      1    219.0    219.0     31.1      a = list(range(10000))     4      1    487.0    487.0     68.9      b = set(range(10000))Call and Flame Graphs$ pip3 install gprof2dot snakeviz; apt/brew install graphviz$ tail -n 4 test.py > test.py$ python3 -m cProfile -o test.prof test.py$ gprof2dot -f pstats test.prof | dot -Tpng -o test.png; xdg-open/open test.png$ snakeviz test.profSampling and Memory Profilers+--------------+-------------------------------+------------+----------+------+| pip3 install |          How to run           |   Target   |   Type   | Live |+--------------+-------------------------------+------------+----------+------+| py-spy       | py-spy top -- python3 test.py |    CPU     | Sampling | Yes  || pyinstrument | pyinstrument test.py          |    CPU     | Sampling | No   || scalene      | scalene test.py               | CPU+Memory | Sampling | No   || memray       | memray run --live test.py     |   Memory   | Tracing  | Yes  || filprofiler  | fil-profile run test.py       |   Memory   | Tracing  | No   |+--------------+-------------------------------+------------+----------+------+NumPyArray manipulation mini-language. It can run up to one hundred times faster than the equivalent Python code. An even faster alternative that runs on a GPU is called CuPy.# $ pip3 install numpyimport numpy as np<array> = np.array(<list/list_of_lists/…>)              # Returns a 1d/2d/… NumPy array.<array> = np.zeros/ones/empty(<shape>)                  # Also np.full(<shape>, <el>).<array> = np.arange(from_inc, to_exc, ±step)            # Also np.linspace(start, stop, len).<array> = np.random.randint(from_inc, to_exc, <shape>)  # Also np.random.random(<shape>).<view>  = <array>.reshape(<shape>)                      # Also `<array>.shape = <shape>`.<array> = <array>.flatten()                             # Also `<view> = <array>.ravel()`.<view>  = <array>.transpose()                           # Or: <array>.T<array> = np.copy/abs/sqrt/log/int64(<array>)           # Returns new array of the same shape.<array> = <array>.sum/max/mean/argmax/all(axis)         # Passed dimension gets aggregated.<array> = np.apply_along_axis(<func>, axis, <array>)    # Func can return a scalar or array.<array> = np.concatenate(<list_of_arrays>, axis=0)      # Links arrays along first axis (rows).<array> = np.row_stack/column_stack(<list_of_arrays>)   # Treats 1d arrays as rows or columns.<array> = np.tile/repeat(<array>, <int/list>)           # Tiles array or repeats its elements.Shape is a tuple of dimension sizes. A 100x50 RGB image has shape (50, 100, 3).Axis is an index of the dimension that gets aggregated. Leftmost dimension has index 0. Summing the RGB image along axis 2 will return a greyscale image with shape (50, 100).Indexing<el>       = <2d_array>[row_index, column_index]        # <3d_a>[table_i, row_i, column_i]<1d_view>  = <2d_array>[row_index]                      # <3d_a>[table_i, row_i]<1d_view>  = <2d_array>[:, column_index]                # <3d_a>[table_i, :, column_i]<2d_view>  = <2d_array>[rows_slice, columns_slice]      # <3d_a>[table_i, rows_s, columns_s]<2d_array> = <2d_array>[row_indexes]                    # <3d_a>[table_i/is, row_is]<2d_array> = <2d_array>[:, column_indexes]              # <3d_a>[table_i/is, :, column_is]<1d_array> = <2d_array>[row_indexes, column_indexes]    # <3d_a>[table_i/is, row_is, column_is]<1d_array> = <2d_array>[row_indexes, column_index]      # <3d_a>[table_i/is, row_is, column_i]<2d_bools> = <2d_array> ><== <el/1d/2d_array>           # 1d_array must have size of a row.<1d/2d_a>  = <2d_array>[<2d/1d_bools>]                  # 1d_bools must have size of a column.Indexes should not be tuples because Python converts 'obj[i, j]'  to 'obj[(i, j)]'!Any value that is broadcastable to the indexed shape can be assigned to the selection.BroadcastingSet of rules by which NumPy functions operate on arrays of different sizes and/or dimensions.left  = [[0.1], [0.6], [0.8]]                           # Shape: (3, 1)right = [ 0.1 ,  0.6 ,  0.8 ]                           # Shape: (3,)1. If array shapes differ in length, left-pad the shorter shape with ones:left  = [[0.1], [0.6], [0.8]]                           # Shape: (3, 1)right = [[0.1 ,  0.6 ,  0.8]]                           # Shape: (1, 3) <- !2. If any dimensions differ in size, expand the ones that have size 1 by duplicating their elements:left  = [[0.1,  0.1,  0.1],                             # Shape: (3, 3) <- !         [0.6,  0.6,  0.6],         [0.8,  0.8,  0.8]]right = [[0.1,  0.6,  0.8],                             # Shape: (3, 3) <- !         [0.1,  0.6,  0.8],         [0.1,  0.6,  0.8]]ExampleFor each point returns index of its nearest point ([0.1, 0.6, 0.8] => [1, 2, 1]):>>> points = np.array([0.1, 0.6, 0.8]) [ 0.1,  0.6,  0.8]>>> wrapped_points = points.reshape(3, 1)[[ 0.1], [ 0.6], [ 0.8]]>>> distances = wrapped_points - points[[ 0. , -0.5, -0.7], [ 0.5,  0. , -0.2], [ 0.7,  0.2,  0. ]]>>> distances = np.abs(distances)[[ 0. ,  0.5,  0.7], [ 0.5,  0. ,  0.2], [ 0.7,  0.2,  0. ]]>>> i = np.arange(3)[0, 1, 2]>>> distances[i, i] = np.inf[[ inf,  0.5,  0.7], [ 0.5,  inf,  0.2], [ 0.7,  0.2,  inf]]>>> distances.argmin(1)[1, 2, 1]Image# $ pip3 install pillowfrom PIL import Image, ImageDraw<Image> = Image.new('<mode>', (width, height))  # Also `color=<int/tuple/str>`.<Image> = Image.open(<path>)                    # Identifies format based on file contents.<Image> = <Image>.convert('<mode>')             # Converts image to the new mode.<Image>.save(<path>)                            # Selects format based on the path extension.<Image>.show()                                  # Opens image in the default preview app.<int/tuple> = <Image>.getpixel((x, y))          # Returns a pixel.<Image>.putpixel((x, y), <int/tuple>)           # Writes a pixel to the image.<ImagingCore> = <Image>.getdata()               # Returns a flattened view of the pixels.<Image>.putdata(<list/ImagingCore>)             # Writes a flattened sequence of pixels.<Image>.paste(<Image>, (x, y))                  # Writes passed image to the image.<Image> = <Image>.filter(<Filter>)              # `<Filter> = ImageFilter.<name>([<args>])`<Image> = <Enhance>.enhance(<float>)            # `<Enhance> = ImageEnhance.<name>(<Image>)`<array> = np.array(<Image>)                     # Creates NumPy array from the image.<Image> = Image.fromarray(np.uint8(<array>))    # Use <array>.clip(0, 255) to clip the values.Modes'1' - 1-bit pixels, black and white, stored with one pixel per byte.'L' - 8-bit pixels, greyscale.'RGB' - 3x8-bit pixels, true color.'RGBA' - 4x8-bit pixels, true color with transparency mask.'HSV' - 3x8-bit pixels, Hue, Saturation, Value color space.ExamplesCreates a PNG image of a rainbow gradient:WIDTH, HEIGHT = 100, 100n_pixels = WIDTH * HEIGHThues = (255 * i/n_pixels for i in range(n_pixels))img = Image.new('HSV', (WIDTH, HEIGHT))img.putdata([(int(h), 255, 255) for h in hues])img.convert('RGB').save('test.png')Adds noise to a PNG image and displays it:from random import randintadd_noise = lambda value: max(0, min(255, value + randint(-20, 20)))img = Image.open('test.png').convert('HSV')img.putdata([(add_noise(h), s, v) for h, s, v in img.getdata()])img.show()Image Draw<ImageDraw> = ImageDraw.Draw(<Image>)           # Object for adding 2D graphics to the image.<ImageDraw>.point((x, y))                       # Draws a point. Truncates floats into ints.<ImageDraw>.line((x1, y1, x2, y2 [, ...]))      # To get anti-aliasing use Image's resize().<ImageDraw>.arc((x1, y1, x2, y2), deg1, deg2)   # Always draws in clockwise direction.<ImageDraw>.rectangle((x1, y1, x2, y2))         # To rotate use Image's rotate() and paste().<ImageDraw>.polygon((x1, y1, x2, y2, ...))      # Last point gets connected to the first.<ImageDraw>.ellipse((x1, y1, x2, y2))           # To rotate use Image's rotate() and paste().<ImageDraw>.text((x, y), text, font=<Font>)     # `<Font> = ImageFont.truetype(<path>, size)`Use 'fill=<color>' to set the primary color.Use 'width=<int>' to set the width of lines or contours.Use 'outline=<color>' to set the color of the contours.Color can be an int, tuple, '#rrggbb[aa]' string or a color name.AnimationCreates a GIF of a bouncing ball:# $ pip3 install imageiofrom PIL import Image, ImageDrawimport imageioWIDTH, HEIGHT, R = 126, 126, 10frames = []for velocity in range(1, 16):    y = sum(range(velocity))    frame = Image.new('L', (WIDTH, HEIGHT))    draw  = ImageDraw.Draw(frame)    draw.ellipse((WIDTH/2-R, y, WIDTH/2+R, y+R*2), fill='white')    frames.append(frame)frames += reversed(frames[1:-1])imageio.mimsave('test.gif', frames, duration=0.03)Audioimport wave<Wave_read>  = wave.open('<path>', 'rb')        # Opens the WAV file.framerate    = <Wave_read>.getframerate()       # Number of frames per second.nchannels    = <Wave_read>.getnchannels()       # Number of samples per frame.sampwidth    = <Wave_read>.getsampwidth()       # Sample size in bytes.nframes      = <Wave_read>.getnframes()         # Number of frames.<params>     = <Wave_read>.getparams()          # Immutable collection of above.<bytes>      = <Wave_read>.readframes(nframes)  # Returns next 'nframes' frames.<Wave_write> = wave.open('<path>', 'wb')        # Truncates existing file.<Wave_write>.setframerate(<int>)                # 44100 for CD, 48000 for video.<Wave_write>.setnchannels(<int>)                # 1 for mono, 2 for stereo.<Wave_write>.setsampwidth(<int>)                # 2 for CD quality sound.<Wave_write>.setparams(<params>)                # Sets all parameters.<Wave_write>.writeframes(<bytes>)               # Appends frames to the file.Bytes object contains a sequence of frames, each consisting of one or more samples.In a stereo signal, the first sample of a frame belongs to the left channel.Each sample consists of one or more bytes that, when converted to an integer, indicate the displacement of a speaker membrane at a given moment.If sample width is one byte, then the integer should be encoded unsigned.For all other sizes, the integer should be encoded signed with little-endian byte order.Sample Values+-----------+-----------+------+-----------+| sampwidth |    min    | zero |    max    |+-----------+-----------+------+-----------+|     1     |         0 |  128 |       255 ||     2     |    -32768 |    0 |     32767 ||     3     |  -8388608 |    0 |   8388607 |+-----------+-----------+------+-----------+Read Float Samples from WAV Filedef read_wav_file(filename):    def get_int(bytes_obj):        an_int = int.from_bytes(bytes_obj, 'little', signed=(sampwidth != 1))        return an_int - 128 * (sampwidth == 1)    with wave.open(filename, 'rb') as file:        sampwidth = file.getsampwidth()        frames = file.readframes(-1)    bytes_samples = (frames[i : i+sampwidth] for i in range(0, len(frames), sampwidth))    return [get_int(b) / pow(2, sampwidth * 8 - 1) for b in bytes_samples]Write Float Samples to WAV Filedef write_to_wav_file(filename, float_samples, nchannels=1, sampwidth=2, framerate=44100):    def get_bytes(a_float):        a_float = max(-1, min(1 - 2e-16, a_float))        a_float += sampwidth == 1        a_float *= pow(2, sampwidth * 8 - 1)        return int(a_float).to_bytes(sampwidth, 'little', signed=(sampwidth != 1))    with wave.open(filename, 'wb') as file:        file.setnchannels(nchannels)        file.setsampwidth(sampwidth)        file.setframerate(framerate)        file.writeframes(b''.join(get_bytes(f) for f in float_samples))ExamplesSaves a 440 Hz sine wave to a mono WAV file:from math import pi, sinsamples_f = (sin(i * 2 * pi * 440 / 44100) for i in range(100_000))write_to_wav_file('test.wav', samples_f)Adds noise to a mono WAV file:from random import randomadd_noise = lambda value: value + (random() - 0.5) * 0.03samples_f = (add_noise(f) for f in read_wav_file('test.wav'))write_to_wav_file('test.wav', samples_f)Plays a WAV file:# $ pip3 install simpleaudiofrom simpleaudio import play_bufferwith wave.open('test.wav', 'rb') as file:    p = file.getparams()    frames = file.readframes(-1)    play_buffer(frames, p.nchannels, p.sampwidth, p.framerate)Text to Speech# $ pip3 install pyttsx3import pyttsx3engine = pyttsx3.init()engine.say('Sally sells seashells by the seashore.')engine.runAndWait()SynthesizerPlays Popcorn by Gershon Kingsley:# $ pip3 install simpleaudioimport array, itertools as it, math, simpleaudioF  = 44100P1 = '71♩,69♪,,71♩,66♪,,62♩,66♪,,59♩,,'P2 = '71♩,73♪,,74♩,73♪,,74♪,,71♪,,73♩,71♪,,73♪,,69♪,,71♩,69♪,,71♪,,67♪,,71♩,,'get_pause   = lambda seconds: it.repeat(0, int(seconds * F))sin_f       = lambda i, hz: math.sin(i * 2 * math.pi * hz / F)get_wave    = lambda hz, seconds: (sin_f(i, hz) for i in range(int(seconds * F)))get_hz      = lambda key: 8.176 * 2 ** (int(key) / 12)parse_note  = lambda note: (get_hz(note[:2]), 1/4 if '♩' in note else 1/8)get_samples = lambda note: get_wave(*parse_note(note)) if note else get_pause(1/8)samples_f   = it.chain.from_iterable(get_samples(n) for n in f'{P1},{P1},{P2}'.split(','))samples_i   = array.array('h', (int(f * 30000) for f in samples_f))simpleaudio.play_buffer(samples_i, 1, 2, F)Pygame# $ pip3 install pygameimport pygame as pgpg.init()screen = pg.display.set_mode((500, 500))rect = pg.Rect(240, 240, 20, 20)while not pg.event.get(pg.QUIT):    deltas = {pg.K_UP: (0, -20), pg.K_RIGHT: (20, 0), pg.K_DOWN: (0, 20), pg.K_LEFT: (-20, 0)}    for event in pg.event.get(pg.KEYDOWN):        dx, dy = deltas.get(event.key, (0, 0))        rect = rect.move((dx, dy))    screen.fill((0, 0, 0))    pg.draw.rect(screen, (255, 255, 255), rect)    pg.display.flip()RectangleObject for storing rectangular coordinates.<Rect> = pg.Rect(x, y, width, height)           # Floats get truncated into ints.<int>  = <Rect>.x/y/centerx/centery/…           # Top, right, bottom, left. Allows assignments.<tup.> = <Rect>.topleft/center/…                # Topright, bottomright, bottomleft. Same.<Rect> = <Rect>.move((delta_x, delta_y))        # Use move_ip() to move in-place.<bool> = <Rect>.collidepoint((x, y))            # Checks if rectangle contains the point.<bool> = <Rect>.colliderect(<Rect>)             # Checks if two rectangles overlap.<int>  = <Rect>.collidelist(<list_of_Rect>)     # Returns index of first colliding Rect or -1.<list> = <Rect>.collidelistall(<list_of_Rect>)  # Returns indexes of all colliding rectangles.SurfaceObject for representing images.<Surf> = pg.display.set_mode((width, height))   # Opens new window and returns its surface.<Surf> = pg.Surface((width, height))            # New RGB surface. RGBA if `flags=pg.SRCALPHA`.<Surf> = pg.image.load(<path/file>)             # Loads the image. Format depends on source.<Surf> = pg.surfarray.make_surface(<np_array>)  # Also `<np_arr> = surfarray.pixels3d(<Surf>)`.<Surf> = <Surf>.subsurface(<Rect>)              # Creates a new surface from the cutout.<Surf>.fill(color)                              # Tuple, Color('#rrggbb[aa]') or Color(<name>).<Surf>.set_at((x, y), color)                    # Updates pixel. Also <Surf>.get_at((x, y)).<Surf>.blit(<Surf>, (x, y))                     # Draws passed surface to the surface.from pygame.transform import scale, ...<Surf> = scale(<Surf>, (width, height))         # Returns scaled surface.<Surf> = rotate(<Surf>, anticlock_degrees)      # Returns rotated and scaled surface.<Surf> = flip(<Surf>, x_bool, y_bool)           # Returns flipped surface.from pygame.draw import line, ...line(<Surf>, color, (x1, y1), (x2, y2), width)  # Draws a line to the surface.arc(<Surf>, color, <Rect>, from_rad, to_rad)    # Also ellipse(<Surf>, color, <Rect>, width=0).rect(<Surf>, color, <Rect>, width=0)            # Also polygon(<Surf>, color, points, width=0).Font<Font> = pg.font.Font(<path/file>, size)        # Loads TTF file. Pass None for default font.<Surf> = <Font>.render(text, antialias, color)  # Background color can be specified at the end.Sound<Sound> = pg.mixer.Sound(<path/file/bytes>)     # Loads WAV file or array of signed shorts.<Sound>.play/stop()                             # Also <Sound>.set_volume(<float>).Basic Mario Brothers Exampleimport collections, dataclasses, enum, io, itertools as it, pygame as pg, urllib.requestfrom random import randintP = collections.namedtuple('P', 'x y')          # PositionD = enum.Enum('D', 'n e s w')                   # DirectionW, H, MAX_S = 50, 50, P(5, 10)                  # Width, Height, Max speeddef main():    def get_screen():        pg.init()        return pg.display.set_mode((W*16, H*16))    def get_images():        url = 'https://gto76.github.io/python-cheatsheet/web/mario_bros.png'        img = pg.image.load(io.BytesIO(urllib.request.urlopen(url).read()))        return [img.subsurface(get_rect(x, 0)) for x in range(img.get_width() // 16)]    def get_mario():        Mario = dataclasses.make_dataclass('Mario', 'rect spd facing_left frame_cycle'.split())        return Mario(get_rect(1, 1), P(0, 0), False, it.cycle(range(3)))    def get_tiles():        border = [(x, y) for x in range(W) for y in range(H) if x in [0, W-1] or y in [0, H-1]]        platforms = [(randint(1, W-2), randint(2, H-2)) for _ in range(W*H // 10)]        return [get_rect(x, y) for x, y in border + platforms]    def get_rect(x, y):        return pg.Rect(x*16, y*16, 16, 16)    run(get_screen(), get_images(), get_mario(), get_tiles())def run(screen, images, mario, tiles):    clock = pg.time.Clock()    pressed = set()    while not pg.event.get(pg.QUIT) and clock.tick(28):        keys = {pg.K_UP: D.n, pg.K_RIGHT: D.e, pg.K_DOWN: D.s, pg.K_LEFT: D.w}        pressed |= {keys.get(e.key) for e in pg.event.get(pg.KEYDOWN)}        pressed -= {keys.get(e.key) for e in pg.event.get(pg.KEYUP)}        update_speed(mario, tiles, pressed)        update_position(mario, tiles)        draw(screen, images, mario, tiles, pressed)def update_speed(mario, tiles, pressed):    x, y = mario.spd    x += 2 * ((D.e in pressed) - (D.w in pressed))    x += (x < 0) - (x > 0)    y += 1 if D.s not in get_boundaries(mario.rect, tiles) else (D.n in pressed) * -10    mario.spd = P(x=max(-MAX_S.x, min(MAX_S.x, x)), y=max(-MAX_S.y, min(MAX_S.y, y)))def update_position(mario, tiles):    x, y = mario.rect.topleft    n_steps = max(abs(s) for s in mario.spd)    for _ in range(n_steps):        mario.spd = stop_on_collision(mario.spd, get_boundaries(mario.rect, tiles))        mario.rect.topleft = x, y = x + (mario.spd.x / n_steps), y + (mario.spd.y / n_steps)def get_boundaries(rect, tiles):    deltas = {D.n: P(0, -1), D.e: P(1, 0), D.s: P(0, 1), D.w: P(-1, 0)}    return {d for d, delta in deltas.items() if rect.move(delta).collidelist(tiles) != -1}def stop_on_collision(spd, bounds):    return P(x=0 if (D.w in bounds and spd.x < 0) or (D.e in bounds and spd.x > 0) else spd.x,             y=0 if (D.n in bounds and spd.y < 0) or (D.s in bounds and spd.y > 0) else spd.y)def draw(screen, images, mario, tiles, pressed):    def get_marios_image_index():        if D.s not in get_boundaries(mario.rect, tiles):            return 4        return next(mario.frame_cycle) if {D.w, D.e} & pressed else 6    screen.fill((85, 168, 255))    mario.facing_left = (D.w in pressed) if {D.w, D.e} & pressed else mario.facing_left    screen.blit(images[get_marios_image_index() + mario.facing_left * 9], mario.rect)    for t in tiles:        screen.blit(images[18 if t.x in [0, (W-1)*16] or t.y in [0, (H-1)*16] else 19], t)    pg.display.flip()if __name__ == '__main__':    main()Pandas# $ pip3 install pandas matplotlibimport pandas as pd, matplotlib.pyplot as pltSeriesOrdered dictionary with a name.>>> pd.Series([1, 2], index=['x', 'y'], name='a')x    1y    2Name: a, dtype: int64<Sr> = pd.Series(<list>)                       # Assigns RangeIndex starting at 0.<Sr> = pd.Series(<dict>)                       # Takes dictionary's keys for index.<Sr> = pd.Series(<dict/Series>, index=<list>)  # Only keeps items with keys specified in index.<el> = <Sr>.loc[key]                           # Or: <Sr>.iloc[index]<Sr> = <Sr>.loc[keys]                          # Or: <Sr>.iloc[indexes]<Sr> = <Sr>.loc[from_key : to_key_inclusive]   # Or: <Sr>.iloc[from_i : to_i_exclusive]<el> = <Sr>[key/index]                         # Or: <Sr>.key<Sr> = <Sr>[keys/indexes]                      # Or: <Sr>[<keys_slice/slice>]<Sr> = <Sr>[bools]                             # Or: <Sr>.loc/iloc[bools]<Sr> = <Sr> ><== <el/Sr>                       # Returns a Series of bools.<Sr> = <Sr> +-*/ <el/Sr>                       # Items with non-matching keys get value NaN.<Sr> = pd.concat(<coll_of_Sr>)                 # Concats multiple Series into one long Series.<Sr> = <Sr>.combine_first(<Sr>)                # Adds items that are not yet present.<Sr>.update(<Sr>)                              # Updates items that are already present.<Sr>.plot.line/area/bar/pie/hist()             # Generates a Matplotlib plot.plt.show()                                     # Displays the plot. Also plt.savefig(<path>).Series — Aggregate, Transform, Map:<el> = <Sr>.sum/max/mean/idxmax/all()          # Or: <Sr>.agg(lambda <Sr>: <el>)<Sr> = <Sr>.rank/diff/cumsum/ffill/interpl()   # Or: <Sr>.agg/transform(lambda <Sr>: <Sr>)<Sr> = <Sr>.fillna(<el>)                       # Or: <Sr>.agg/transform/map(lambda <el>: <el>)>>> sr = pd.Series([1, 2], index=['x', 'y'])x    1y    2+---------------+-------------+-------------+---------------+|               |    'sum'    |   ['sum']   | {'s': 'sum'}  |+---------------+-------------+-------------+---------------+| sr.apply(…)   |      3      |    sum  3   |     s  3      || sr.agg(…)     |             |             |               |+---------------+-------------+-------------+---------------++---------------+-------------+-------------+---------------+|               |    'rank'   |   ['rank']  | {'r': 'rank'} |+---------------+-------------+-------------+---------------+| sr.apply(…)   |             |      rank   |               || sr.agg(…)     |     x  1    |   x     1   |    r  x  1    ||               |     y  2    |   y     2   |       y  2    |+---------------+-------------+-------------+---------------+Keys/indexes/bools can't be tuples because 'obj[x, y]' is converted to 'obj[(x, y)]'!Methods ffill(), interpolate(), fillna() and dropna() accept 'inplace=True'.Last result has a hierarchical index. Use '<Sr>[key_1, key_2]' to get its values.DataFrameTable with labeled rows and columns.>>> pd.DataFrame([[1, 2], [3, 4]], index=['a', 'b'], columns=['x', 'y'])   x  ya  1  2b  3  4<DF>    = pd.DataFrame(<list_of_rows>)         # Rows can be either lists, dicts or series.<DF>    = pd.DataFrame(<dict_of_columns>)      # Columns can be either lists, dicts or series.<el>    = <DF>.loc[row_key, column_key]        # Or: <DF>.iloc[row_index, column_index]<Sr/DF> = <DF>.loc[row_key/s]                  # Or: <DF>.iloc[row_index/es]<Sr/DF> = <DF>.loc[:, column_key/s]            # Or: <DF>.iloc[:, column_index/es]<DF>    = <DF>.loc[row_bools, column_bools]    # Or: <DF>.iloc[row_bools, column_bools]<Sr/DF> = <DF>[column_key/s]                   # Or: <DF>.column_key<DF>    = <DF>[row_bools]                      # Keeps rows as specified by bools.<DF>    = <DF>[<DF_of_bools>]                  # Assigns NaN to False values.<DF>    = <DF> ><== <el/Sr/DF>                 # Returns DF of bools. Sr is treated as a row.<DF>    = <DF> +-*/ <el/Sr/DF>                 # Items with non-matching keys get value NaN.<DF>    = <DF>.set_index(column_key)           # Replaces row keys with values from a column.<DF>    = <DF>.reset_index(drop=False)         # Drops or moves row keys to column named index.<DF>    = <DF>.sort_index(ascending=True)      # Sorts rows by row keys. Use `axis=1` for cols.<DF>    = <DF>.sort_values(column_key/s)       # Sorts rows by the passed column/s. Same.DataFrame — Merge, Join, Concat:>>> l = pd.DataFrame([[1, 2], [3, 4]], index=['a', 'b'], columns=['x', 'y'])   x  ya  1  2b  3  4>>> r = pd.DataFrame([[4, 5], [6, 7]], index=['b', 'c'], columns=['y', 'z'])   y  zb  4  5c  6  7+------------------------+---------------+------------+------------+--------------------------+|                        |    'outer'    |   'inner'  |   'left'   |       Description        |+------------------------+---------------+------------+------------+--------------------------+| l.merge(r, on='y',     |    x   y   z  | x   y   z  | x   y   z  | Merges on column if 'on' ||            how=…)      | 0  1   2   .  | 3   4   5  | 1   2   .  | or 'left/right_on' are   ||                        | 1  3   4   5  |            | 3   4   5  | set, else on shared cols.||                        | 2  .   6   7  |            |            | Uses 'inner' by default. |+------------------------+---------------+------------+------------+--------------------------+| l.join(r, lsuffix='l', |    x yl yr  z |            | x yl yr  z | Merges on row keys.      ||           rsuffix='r', | a  1  2  .  . | x yl yr  z | 1  2  .  . | Uses 'left' by default.  ||           how=…)       | b  3  4  4  5 | 3  4  4  5 | 3  4  4  5 | If r is a Series, it is  ||                        | c  .  .  6  7 |            |            | treated as a column.     |+------------------------+---------------+------------+------------+--------------------------+| pd.concat([l, r],      |    x   y   z  |     y      |            | Adds rows at the bottom. ||           axis=0,      | a  1   2   .  |     2      |            | Uses 'outer' by default. ||           join=…)      | b  3   4   .  |     4      |            | A Series is treated as a ||                        | b  .   4   5  |     4      |            | column. To add a row use ||                        | c  .   6   7  |     6      |            | pd.concat([l, DF([sr])]).|+------------------------+---------------+------------+------------+--------------------------+| pd.concat([l, r],      |    x  y  y  z |            |            | Adds columns at the      ||           axis=1,      | a  1  2  .  . | x  y  y  z |            | right end. Uses 'outer'  ||           join=…)      | b  3  4  4  5 | 3  4  4  5 |            | by default. A Series is  ||                        | c  .  .  6  7 |            |            | treated as a column.     |+------------------------+---------------+------------+------------+--------------------------+| l.combine_first(r)     |    x   y   z  |            |            | Adds missing rows and    ||                        | a  1   2   .  |            |            | columns. Also updates    ||                        | b  3   4   5  |            |            | items that contain NaN.  ||                        | c  .   6   7  |            |            | R must be a DataFrame.   |+------------------------+---------------+------------+------------+--------------------------+DataFrame — Aggregate, Transform, Map:<Sr> = <DF>.sum/max/mean/idxmax/all()          # Or: <DF>.apply/agg(lambda <Sr>: <el>)<DF> = <DF>.rank/diff/cumsum/ffill/interpl()   # Or: <DF>.apply/agg/transfrm(lambda <Sr>: <Sr>)<DF> = <DF>.fillna(<el>)                       # Or: <DF>.applymap(lambda <el>: <el>)All operations operate on columns by default. Pass 'axis=1' to process the rows instead.>>> df = pd.DataFrame([[1, 2], [3, 4]], index=['a', 'b'], columns=['x', 'y'])   x  ya  1  2b  3  4+-----------------+-------------+-------------+---------------+|                 |    'sum'    |   ['sum']   | {'x': 'sum'}  |+-----------------+-------------+-------------+---------------+| df.apply(…)     |             |       x  y  |               || df.agg(…)       |     x  4    |  sum  4  6  |     x  4      ||                 |     y  6    |             |               |+-----------------+-------------+-------------+---------------++-----------------+-------------+-------------+---------------+|                 |    'rank'   |   ['rank']  | {'x': 'rank'} |+-----------------+-------------+-------------+---------------+| df.apply(…)     |      x  y   |      x    y |        x      || df.agg(…)       |   a  1  1   |   rank rank |     a  1      || df.transform(…) |   b  2  2   | a    1    1 |     b  2      ||                 |             | b    2    2 |               |+-----------------+-------------+-------------+---------------+Use '<DF>[col_key_1, col_key_2][row_key]' to get the fifth result's values.DataFrame — Plot, Encode, Decode:<DF>.plot.line/area/bar/hist/scatter/box()     # Also: `x=column_key, y=column_key/s`.plt.show()                                     # Displays the plot. Also plt.savefig(<path>).<DF> = pd.read_json/html('<str/path/url>')     # Run `$ pip3 install beautifulsoup4 lxml`.<DF> = pd.read_csv/pickle/excel('<path/url>')  # Use `sheet_name=None` to get all Excel sheets.<DF> = pd.read_sql('<table/query>', <conn.>)   # Accepts SQLite3 or SQLAlchemy connection.<DF> = pd.read_clipboard()                     # Reads a copied table from the clipboard.<dict> = <DF>.to_dict(['d/l/s/…'])             # Returns columns as dicts, lists or series.<str>  = <DF>.to_json/html/csv([<path>])       # Also to_markdown/latex([<path>]).<DF>.to_pickle/excel(<path>)                   # Run `$ pip3 install \""pandas[excel]\"" odfpy`.<DF>.to_sql('<table_name>', <connection>)      # Accepts SQLite3 or SQLAlchemy connection.GroupByObject that groups together rows of a dataframe based on the value of the passed column.>>> df = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 6]], list('abc'), list('xyz'))>>> df.groupby('z').get_group(6)   x  y  zb  4  5  6c  7  8  6<GB> = <DF>.groupby(column_key/s)              # Splits DF into groups based on passed column.<DF> = <GB>.apply(<func>)                      # Maps each group. Func can return DF, Sr or el.<GB> = <GB>[column_key]                        # Single column GB. All operations return a Sr.GroupBy — Aggregate, Transform, Map:<DF> = <GB>.sum/max/mean/idxmax/all()          # Or: <GB>.agg(lambda <Sr>: <el>)<DF> = <GB>.rank/diff/cumsum/ffill()           # Or: <GB>.transform(lambda <Sr>: <Sr>)<DF> = <GB>.fillna(<el>)                       # Or: <GB>.transform(lambda <Sr>: <Sr>)>>> gb = df.groupby('z')      x  y  z3: a  1  2  36: b  4  5  6   c  7  8  6+-----------------+-------------+-------------+-------------+---------------+|                 |    'sum'    |    'rank'   |   ['rank']  | {'x': 'rank'} |+-----------------+-------------+-------------+-------------+---------------+| gb.agg(…)       |      x   y  |      x  y   |      x    y |        x      ||                 |  z          |   a  1  1   |   rank rank |     a  1      ||                 |  3   1   2  |   b  1  1   | a    1    1 |     b  1      ||                 |  6  11  13  |   c  2  2   | b    1    1 |     c  2      ||                 |             |             | c    2    2 |               |+-----------------+-------------+-------------+-------------+---------------+| gb.transform(…) |      x   y  |      x  y   |             |               ||                 |  a   1   2  |   a  1  1   |             |               ||                 |  b  11  13  |   b  1  1   |             |               ||                 |  c  11  13  |   c  2  2   |             |               |+-----------------+-------------+-------------+-------------+---------------+RollingObject for rolling window calculations.<RSr/RDF/RGB> = <Sr/DF/GB>.rolling(win_size)   # Also: `min_periods=None, center=False`.<RSr/RDF/RGB> = <RDF/RGB>[column_key/s]        # Or: <RDF/RGB>.column_key<Sr/DF>       = <R>.mean/sum/max()             # Or: <R>.apply/agg(<agg_func/str>)Plotly# $ pip3 install plotly kaleidofrom plotly.express import line<Figure> = line(<DF>, x=<col_name>, y=<col_name>)           # Or: line(x=<list>, y=<list>)<Figure>.update_layout(margin=dict(t=0, r=0, b=0, l=0), …)  # `paper_bgcolor='rgb(0, 0, 0)'`.<Figure>.write_html/json/image('<path>')                    # Also <Figure>.show().Displays a line chart of total coronavirus deaths per million grouped by continent:covid = pd.read_csv('https://covid.ourworldindata.org/data/owid-covid-data.csv',                    usecols=['iso_code', 'date', 'total_deaths', 'population'])continents = pd.read_csv('https://gist.githubusercontent.com/stevewithington/20a69c0b6d2ff'                         '846ea5d35e5fc47f26c/raw/country-and-continent-codes-list-csv.csv',                         usecols=['Three_Letter_Country_Code', 'Continent_Name'])df = pd.merge(covid, continents, left_on='iso_code', right_on='Three_Letter_Country_Code')df = df.groupby(['Continent_Name', 'date']).sum().reset_index()df['Total Deaths per Million'] = df.total_deaths * 1e6 / df.populationdf = df[df.date > '2020-03-14']df = df.rename({'date': 'Date', 'Continent_Name': 'Continent'}, axis='columns')line(df, x='Date', y='Total Deaths per Million', color='Continent').show()Displays a multi-axis line chart of total coronavirus cases and changes in prices of Bitcoin, Dow Jones and gold:import pandas as pd, plotly.graph_objects as godef main():    display_data(wrangle_data(*scrape_data()))def scrape_data():    def scrape_covid():        url = 'https://covid.ourworldindata.org/data/owid-covid-data.csv'        df = pd.read_csv(url, usecols=['location', 'date', 'total_cases'])        return df[df.location == 'World'].set_index('date').total_cases    def scrape_yahoo(slug):        url = (f'https://query1.finance.yahoo.com/v7/finance/download/{slug}?'               'period1=1579651200&period2=9999999999&interval=1d&events=history')        df = pd.read_csv(url, usecols=['Date', 'Close'])        return df.set_index('Date').Close    out = scrape_covid(), scrape_yahoo('BTC-USD'), scrape_yahoo('GC=F'), scrape_yahoo('^DJI')    return map(pd.Series.rename, out, ['Total Cases', 'Bitcoin', 'Gold', 'Dow Jones'])def wrangle_data(covid, bitcoin, gold, dow):    df = pd.concat([bitcoin, gold, dow], axis=1)  # Joins columns on dates.    df = df.sort_index().interpolate()            # Sorts by date and interpolates NaN-s.    df = df.loc['2020-02-23':]                    # Discards rows before '2020-02-23'.    df = (df / df.iloc[0]) * 100                  # Calculates percentages relative to day 1.    df = df.join(covid)                           # Adds column with covid cases.    return df.sort_values(df.index[-1], axis=1)   # Sorts columns by last day's value.def display_data(df):    figure = go.Figure()    for col_name in reversed(df.columns):        yaxis = 'y1' if col_name == 'Total Cases' else 'y2'        trace = go.Scatter(x=df.index, y=df[col_name], name=col_name, yaxis=yaxis)        figure.add_trace(trace)    figure.update_layout(        yaxis1=dict(title='Total Cases', rangemode='tozero'),        yaxis2=dict(title='%', rangemode='tozero', overlaying='y', side='right'),        legend=dict(x=1.1),        height=450    )    figure.show()if __name__ == '__main__':    main()PySimpleGUI# $ pip3 install PySimpleGUIimport PySimpleGUI as sglayout = [[sg.Text(\""What's your name?\"")], [sg.Input()], [sg.Button('Ok')]]window = sg.Window('Window Title', layout)event, values = window.read()print(f'Hello {values[0]}!' if event == 'Ok' else '')AppendixCythonLibrary that compiles Python code into C.# $ pip3 install cythonimport pyximport; pyximport.install()import <cython_script><cython_script>.main()Definitions:All 'cdef' definitions are optional, but they contribute to the speed-up.Script needs to be saved with a 'pyx' extension.cdef <ctype> <var_name> = <el>cdef <ctype>[n_elements] <var_name> = [<el>, <el>, ...]cdef <ctype/void> <func_name>(<ctype> <arg_name>): ...cdef class <class_name>:    cdef public <ctype> <attr_name>    def __init__(self, <ctype> <arg_name>):        self.<attr_name> = <arg_name>cdef enum <enum_name>: <member_name>, <member_name>, ...Virtual EnvironmentsSystem for installing libraries directly into project's directory.$ python3 -m venv <name>      # Creates virtual environment in current directory.$ source <name>/bin/activate  # Activates venv. On Windows run `<name>\\Scripts\\activate`.$ pip3 install <library>      # Installs the library into active environment.$ python3 <path>              # Runs the script in active environment. Also `./<path>`.$ deactivate                  # Deactivates virtual environment.Basic Script Template#!/usr/bin/env python3## Usage: .py#from sys import argv, exitfrom collections import defaultdict, namedtuplefrom dataclasses import make_dataclassfrom enum import Enumimport functools as ft, itertools as it, operator as op, redef main():    pass#####  UTIL#def read_file(filename):    with open(filename, encoding='utf-8') as file:        return file.readlines()if __name__ == '__main__':    main()IndexOnly available in the PDF.Ctrl+F / ⌘F is usually sufficient.Searching '#<title>' on the webpage will limit the search to the titles."
24,facebookresearch/fairseq,https://github.com/facebookresearch/fairseq/blob/main/README.md,Python,"                  Fairseq(-py) is a sequence modeling toolkit that allows researchers anddevelopers to train custom models for translation, summarization, languagemodeling and other text generation tasks.We provide reference implementations of various sequence modeling papers:List of implemented papersConvolutional Neural Networks (CNN)Language Modeling with Gated Convolutional Networks (Dauphin et al., 2017)Convolutional Sequence to Sequence Learning (Gehring et al., 2017)Classical Structured Prediction Losses for Sequence to Sequence Learning (Edunov et al., 2018)Hierarchical Neural Story Generation (Fan et al., 2018)wav2vec: Unsupervised Pre-training for Speech Recognition (Schneider et al., 2019)LightConv and DynamicConv modelsPay Less Attention with Lightweight and Dynamic Convolutions (Wu et al., 2019)Long Short-Term Memory (LSTM) networksEffective Approaches to Attention-based Neural Machine Translation (Luong et al., 2015)Transformer (self-attention) networksAttention Is All You Need (Vaswani et al., 2017)Scaling Neural Machine Translation (Ott et al., 2018)Understanding Back-Translation at Scale (Edunov et al., 2018)Adaptive Input Representations for Neural Language Modeling (Baevski and Auli, 2018)Lexically constrained decoding with dynamic beam allocation (Post & Vilar, 2018)Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context (Dai et al., 2019)Adaptive Attention Span in Transformers (Sukhbaatar et al., 2019)Mixture Models for Diverse Machine Translation: Tricks of the Trade (Shen et al., 2019)RoBERTa: A Robustly Optimized BERT Pretraining Approach (Liu et al., 2019)Facebook FAIR's WMT19 News Translation Task Submission (Ng et al., 2019)Jointly Learning to Align and Translate with Transformer Models (Garg et al., 2019)Multilingual Denoising Pre-training for Neural Machine Translation (Liu et at., 2020)Neural Machine Translation with Byte-Level Subwords (Wang et al., 2020)Unsupervised Quality Estimation for Neural Machine Translation (Fomicheva et al., 2020)wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations (Baevski et al., 2020)Generating Medical Reports from Patient-Doctor Conversations Using Sequence-to-Sequence Models (Enarvi et al., 2020)Linformer: Self-Attention with Linear Complexity (Wang et al., 2020)Cross-lingual Retrieval for Iterative Self-Supervised Training (Tran et al., 2020)Deep Transformers with Latent Depth (Li et al., 2020)Unsupervised Cross-lingual Representation Learning for Speech Recognition (Conneau et al., 2020)Self-training and Pre-training are Complementary for Speech Recognition (Xu et al., 2020)Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training (Hsu, et al., 2021)Unsupervised Speech Recognition (Baevski, et al., 2021)Simple and Effective Zero-shot Cross-lingual Phoneme Recognition (Xu et al., 2021)VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding (Xu et. al., 2021)VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding (Xu et. al., 2021)NormFormer: Improved Transformer Pretraining with Extra Normalization (Shleifer et. al, 2021)Non-autoregressive TransformersNon-Autoregressive Neural Machine Translation (Gu et al., 2017)Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement (Lee et al. 2018)Insertion Transformer: Flexible Sequence Generation via Insertion Operations (Stern et al. 2019)Mask-Predict: Parallel Decoding of Conditional Masked Language Models (Ghazvininejad et al., 2019)Levenshtein Transformer (Gu et al., 2019)FinetuningBetter Fine-Tuning by Reducing Representational Collapse (Aghajanyan et al. 2020)What's New:May 2023 Released models for Scaling Speech Technology to 1,000+ Languages  (Pratap, et al., 2023)June 2022 Released code for wav2vec-U 2.0 from Towards End-to-end Unsupervised Speech Recognition (Liu, et al., 2022)May 2022 Integration with xFormersDecember 2021 Released Direct speech-to-speech translation codeOctober 2021 Released VideoCLIP and VLM modelsOctober 2021 Released multilingual finetuned XLSR-53 modelSeptember 2021 master branch renamed to main.July 2021 Released DrNMT codeJuly 2021 Released Robust wav2vec 2.0 modelJune 2021 Released XLMR-XL and XLMR-XXL modelsMay 2021 Released Unsupervised Speech Recognition codeMarch 2021 Added full parameter and optimizer state sharding + CPU offloadingFebruary 2021 Added LASER training codeDecember 2020: Added Adaptive Attention Span codeDecember 2020: GottBERT model and code releasedNovember 2020: Adopted the Hydra configuration frameworksee documentation explaining how to use it for new and existing projectsNovember 2020: fairseq 0.10.0 releasedOctober 2020: Added R3F/R4F (Better Fine-Tuning) codeOctober 2020: Deep Transformer with Latent Depth code releasedOctober 2020: Added CRISS models and codePrevious updatesSeptember 2020: Added Linformer codeSeptember 2020: Added pointer-generator networksAugust 2020: Added lexically constrained decodingAugust 2020: wav2vec2 models and code releasedJuly 2020: Unsupervised Quality Estimation code releasedMay 2020: Follow fairseq on TwitterApril 2020: Monotonic Multihead Attention code releasedApril 2020: Quant-Noise code releasedApril 2020: Initial model parallel support and 11B parameters unidirectional LM releasedMarch 2020: Byte-level BPE code releasedFebruary 2020: mBART model and code releasedFebruary 2020: Added tutorial for back-translationDecember 2019: fairseq 0.9.0 releasedNovember 2019: VizSeq released (a visual analysis toolkit for evaluating fairseq models)November 2019: CamemBERT model and code releasedNovember 2019: BART model and code releasedNovember 2019: XLM-R models and code releasedSeptember 2019: Nonautoregressive translation code releasedAugust 2019: WMT'19 models releasedJuly 2019: fairseq relicensed under MIT licenseJuly 2019: RoBERTa models and code releasedJune 2019: wav2vec models and code releasedFeatures:multi-GPU training on one machine or across multiple machines (data and model parallel)fast generation on both CPU and GPU with multiple search algorithms implemented:beam searchDiverse Beam Search (Vijayakumar et al., 2016)sampling (unconstrained, top-k and top-p/nucleus)lexically constrained decoding (Post & Vilar, 2018)gradient accumulation enables training with large mini-batches even on a single GPUmixed precision training (trains faster with less GPU memory on NVIDIA tensor cores)extensible: easily register new models, criterions, tasks, optimizers and learning rate schedulersflexible configuration based on Hydra allowing a combination of code, command-line and file based configurationfull parameter and optimizer state shardingoffloading parameters to CPUWe also provide pre-trained models for translation and language modelingwith a convenient torch.hub interface:en2de = torch.hub.load('pytorch/fairseq', 'transformer.wmt19.en-de.single_model')en2de.translate('Hello world', beam=5)# 'Hallo Welt'See the PyTorch Hub tutorials for translationand RoBERTa for more examples.Requirements and InstallationPyTorch version >= 1.10.0Python version >= 3.8For training new models, you'll also need an NVIDIA GPU and NCCLTo install fairseq and develop locally:git clone https://github.com/pytorch/fairseqcd fairseqpip install --editable ./# on MacOS:# CFLAGS=\""-stdlib=libc++\"" pip install --editable ./# to install the latest stable release (0.10.x)# pip install fairseqFor faster training install NVIDIA's apex library:git clone https://github.com/NVIDIA/apexcd apexpip install -v --no-cache-dir --global-option=\""--cpp_ext\"" --global-option=\""--cuda_ext\"" \\  --global-option=\""--deprecated_fused_adam\"" --global-option=\""--xentropy\"" \\  --global-option=\""--fast_multihead_attn\"" ./For large datasets install PyArrow: pip install pyarrowIf you use Docker make sure to increase the shared memory size either with --ipc=host or --shm-sizeas command line options to nvidia-docker run .Getting StartedThe full documentation contains instructionsfor getting started, training new models and extending fairseq with new modeltypes and tasks.Pre-trained models and examplesWe provide pre-trained models and pre-processed, binarized test sets for several tasks listed below,as well as example training and evaluation commands.Translation: convolutional and transformer models are availableLanguage Modeling: convolutional and transformer models are availableWe also have more detailed READMEs to reproduce results from specific papers:XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale (Babu et al., 2021)Cross-lingual Retrieval for Iterative Self-Supervised Training (Tran et al., 2020)wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations (Baevski et al., 2020)Unsupervised Quality Estimation for Neural Machine Translation (Fomicheva et al., 2020)Training with Quantization Noise for Extreme Model Compression ({Fan*, Stock*} et al., 2020)Neural Machine Translation with Byte-Level Subwords (Wang et al., 2020)Multilingual Denoising Pre-training for Neural Machine Translation (Liu et at., 2020)Reducing Transformer Depth on Demand with Structured Dropout (Fan et al., 2019)Jointly Learning to Align and Translate with Transformer Models (Garg et al., 2019)Levenshtein Transformer (Gu et al., 2019)Facebook FAIR's WMT19 News Translation Task Submission (Ng et al., 2019)RoBERTa: A Robustly Optimized BERT Pretraining Approach (Liu et al., 2019)wav2vec: Unsupervised Pre-training for Speech Recognition (Schneider et al., 2019)Mixture Models for Diverse Machine Translation: Tricks of the Trade (Shen et al., 2019)Pay Less Attention with Lightweight and Dynamic Convolutions (Wu et al., 2019)Understanding Back-Translation at Scale (Edunov et al., 2018)Classical Structured Prediction Losses for Sequence to Sequence Learning (Edunov et al., 2018)Hierarchical Neural Story Generation (Fan et al., 2018)Scaling Neural Machine Translation (Ott et al., 2018)Convolutional Sequence to Sequence Learning (Gehring et al., 2017)Language Modeling with Gated Convolutional Networks (Dauphin et al., 2017)Join the fairseq communityTwitter: https://twitter.com/fairseqFacebook page: https://www.facebook.com/groups/fairseq.usersGoogle group: https://groups.google.com/forum/#!forum/fairseq-usersLicensefairseq(-py) is MIT-licensed.The license applies to the pre-trained models as well.CitationPlease cite as:@inproceedings{ott2019fairseq,  title = {fairseq: A Fast, Extensible Toolkit for Sequence Modeling},  author = {Myle Ott and Sergey Edunov and Alexei Baevski and Angela Fan and Sam Gross and Nathan Ng and David Grangier and Michael Auli},  booktitle = {Proceedings of NAACL-HLT 2019: Demonstrations},  year = {2019},}"
25,Jack-Cherish/python-spider,https://github.com/Jack-Cherish/python-spider/blob/master/README.md,Python,注：2020年最新连载教程请移步：Python Spider 2020Python Spider原创文章每周最少两篇，后续最新文章会在【公众号】首发，视频【B站】首发，大家可以加我【微信】进交流群，技术交流或提意见都可以，欢迎Star！              声明代码、教程仅限于学习交流，请勿用于任何商业用途！目录爬虫小工具文件下载小助手爬虫实战笔趣看小说下载百度文库免费文章下载助手_rev1百度文库免费文章下载助手_rev2《帅啊》网帅哥图片下载构建代理IP池《火影忍者》漫画下载财务报表下载小助手一小时入门网络爬虫抖音App视频下载GEETEST验证码识别12306抢票小助手百万英雄答题辅助系统网易云音乐免费音乐批量下载B站免费视频和弹幕批量下载京东商品晒单图下载正方教务管理系统个人信息查询其它爬虫小工具downloader.py:文件下载小助手一个可以用于下载图片、视频、文件的小工具，有下载进度显示功能。稍加修改即可添加到自己的爬虫中。动态示意图：爬虫实战biqukan.py:《笔趣看》盗版小说网站，爬取小说工具第三方依赖库安装： pip3 install beautifulsoup4使用方法： python biqukan.pybaiduwenku.py: 百度文库word文章爬取原理说明：http://blog.csdn.net/c406495762/article/details/72331737代码不完善，没有进行打包，不具通用性，纯属娱乐。shuaia.py: 爬取《帅啊》网，帅哥图片《帅啊》网URL：http://www.shuaia.net/index.html原理说明：http://blog.csdn.net/c406495762/article/details/72597755第三方依赖库安装： pip3 install requests beautifulsoup4daili.py: 构建代理IP池原理说明：http://blog.csdn.net/c406495762/article/details/72793480carton: 使用Scrapy爬取《火影忍者》漫画代码可以爬取整个《火影忍者》漫画所有章节的内容，保存到本地。更改地址，可以爬取其他漫画。保存地址可以在settings.py中修改。动漫网站：http://comic.kukudm.com/原理说明：http://blog.csdn.net/c406495762/article/details/72858983hero.py: 《王者荣耀》推荐出装查询小助手网页爬取已经会了，想过爬取手机APP里的内容吗？原理说明：http://blog.csdn.net/c406495762/article/details/76850843financical.py: 财务报表下载小助手爬取的数据存入数据库会吗？《跟股神巴菲特学习炒股之财务报表入库(MySQL)》也许能给你一些思路。原理说明：http://blog.csdn.net/c406495762/article/details/77801899动态示意图：one_hour_spider:一小时入门Python3网络爬虫。原理说明:知乎：https://zhuanlan.zhihu.com/p/29809609CSDN：http://blog.csdn.net/c406495762/article/details/78123502本次实战内容有：网络小说下载(静态网站)-biqukan优美壁纸下载(动态网站)-unsplash视频下载douyin.py:抖音App视频下载抖音App的视频下载，就是普通的App爬取。原理说明:个人网站：http://cuijiahua.com/blog/2018/03/spider-5.htmldouyin_pro:抖音App视频下载（升级版）抖音App的视频下载，添加视频解析网站，支持无水印视频下载，使用第三方平台解析。原理说明:个人网站：http://cuijiahua.com/blog/2018/03/spider-5.htmldouyin:抖音App视频下载（升级版2）抖音App的视频下载，添加视频解析网站，支持无水印视频下载，通过url解析，无需第三方平台。原理说明:个人网站：http://cuijiahua.com/blog/2018/03/spider-5.html动态示意图：geetest.py:GEETEST验证码识别原理说明:无12306.py:用Python抢火车票简单代码可以自己慢慢丰富，蛮简单，有爬虫基础很好操作，没有原理说明。baiwan:百万英雄辅助答题效果图：原理说明：个人网站：http://cuijiahua.com/blog/2018/01/spider_3.html功能介绍：服务器端，使用Python（baiwan.py）通过抓包获得的接口获取答题数据，解析之后通过百度知道搜索接口匹配答案，将最终匹配的结果写入文件（file.txt)。手机抓包不会的朋友，可以看下我的早期手机APP抓包教程。Node.js（app.js）每隔1s读取一次file.txt文件，并将读取结果通过socket.io推送给客户端（index.html）。亲测答题延时在3s左右。声明：没做过后端和前端，花了一天时间，现学现卖弄好的，javascript也是现看现用，百度的程序，调试调试而已。可能有很多用法比较low的地方，用法不对，请勿见怪，有大牛感兴趣，可以自行完善。Netease:根据歌单下载网易云音乐效果图：原理说明：暂无功能介绍：根据music_list.txt文件里的歌单的信息下载网易云音乐，将自己喜欢的音乐进行批量下载。bilibili：B站视频和弹幕批量下载原理说明：暂无使用说明： python bilibili.py -d 猫 -k 猫 -p 10 三个参数： -d\t保存视频的文件夹名 -k\tB站搜索的关键字 -p\t下载搜索结果前多少页jingdong：京东商品晒单图下载效果图：原理说明：暂无使用说明： python jd.py -k 芒果  三个参数： -d\t保存图片的路径，默认为fd.py文件所在文件夹 -k\t搜索关键词 -n  \t下载商品的晒单图个数，即n个商店的晒单图zhengfang_system_spider：对正方教务管理系统个人课表，个人学生成绩，绩点等简单爬取效果图：原理说明：暂无使用说明： cd zhengfang_system_spider pip install -r requirements.txt python spider.py其它欢迎 Pull requests，感谢贡献。更多精彩，敬请期待！ 
26,Yorko/mlcourse.ai,https://github.com/Yorko/mlcourse.ai/blob/main/README.md,Python,"mlcourse.ai – Open Machine Learning Coursemlcourse.ai is an open Machine Learning course by OpenDataScience (ods.ai), led by Yury Kashnitsky (yorko). Having both a Ph.D. degree in applied math and a Kaggle Competitions Master tier, Yury aimed at designing an ML course with a perfect balance between theory and practice. Thus, the course meets you with math formulae in lectures, and a lot of practice in a form of assignments and  Kaggle Inclass competitions. Currently, the course is in a self-paced mode. Here we guide you through the self-paced mlcourse.ai.Bonus:Additionally, you can purchase a Bonus Assignments pack with the best non-demo versions of mlcourse.ai assignments. Select the \""Bonus Assignments\"" tier. Refer to the details of the deal on the main page mlcourse.ai.Mirrors (🇬🇧-only): mlcourse.ai (main site), Kaggle Dataset (same notebooks as Kaggle Notebooks)Self-paced passingYou are guided through 10 weeks of mlcourse.ai. For each week, from Pandas to Gradient Boosting, instructions are given on which articles to read, lectures to watch, what assignments to accomplish.ArticlesThis is the list of published articles on medium.com 🇬🇧, habr.com 🇷🇺. Also notebooks in Chinese are mentioned 🇨🇳 and links to Kaggle Notebooks (in English) are given. Icons are clickable.Exploratory Data Analysis with Pandas 🇬🇧 🇷🇺 🇨🇳, Kaggle NotebookVisual Data Analysis with Python 🇬🇧 🇷🇺 🇨🇳, Kaggle Notebooks: part1, part2Classification, Decision Trees and k Nearest Neighbors 🇬🇧 🇷🇺 🇨🇳, Kaggle NotebookLinear Classification and Regression 🇬🇧 🇷🇺 🇨🇳, Kaggle Notebooks: part1, part2, part3, part4, part5Bagging and Random Forest 🇬🇧 🇷🇺 🇨🇳, Kaggle Notebooks: part1, part2, part3Feature Engineering and Feature Selection 🇬🇧 🇷🇺 🇨🇳, Kaggle NotebookUnsupervised Learning: Principal Component Analysis and Clustering 🇬🇧 🇷🇺 🇨🇳, Kaggle NotebookVowpal Wabbit: Learning with Gigabytes of Data 🇬🇧 🇷🇺 🇨🇳, Kaggle NotebookTime Series Analysis with Python, part 1 🇬🇧 🇷🇺 🇨🇳. Predicting future with Facebook Prophet, part 2 🇬🇧, 🇨🇳 Kaggle Notebooks: part1, part2Gradient Boosting 🇬🇧 🇷🇺, 🇨🇳, Kaggle NotebookLecturesVideolectures are uploaded to this YouTube playlist.Introduction, video, slidesExploratory data analysis with Pandas, videoVisualization, main plots for EDA, videoDecision trees: theory and practical partLogistic regression: theoretical foundations, practical part (baselines in the \""Alice\"" competition)Ensembles and Random Forest – part 1. Classification metrics – part 2. Example of a business task, predicting a customer payment – part 3Linear regression and regularization - theory, LASSO & Ridge, LTV prediction - practiceUnsupervised learning - Principal Component Analysis and ClusteringStochastic Gradient Descent for classification and regression - part 1, part 2 TBATime series analysis with Python (ARIMA, Prophet) - videoGradient boosting: basic ideas - part 1, key ideas behind Xgboost, LightGBM, and CatBoost + practice - part 2AssignmentsThe following are demo-assignments. Additionally, within the \""Bonus Assignments\"" tier you can get access to non-demo assignments.Exploratory data analysis with Pandas, nbviewer, Kaggle Notebook, solutionAnalyzing cardiovascular disease data, nbviewer, Kaggle Notebook, solutionDecision trees with a toy task and the UCI Adult dataset, nbviewer, Kaggle Notebook, solutionSarcasm detection, Kaggle Notebook, solution. Linear Regression as an optimization problem, nbviewer, Kaggle NotebookLogistic Regression and Random Forest in the credit scoring problem, nbviewer, Kaggle Notebook, solutionExploring OLS, Lasso and Random Forest in a regression task, nbviewer, Kaggle Notebook, solutionUnsupervised learning, nbviewer, Kaggle Notebook, solutionImplementing online regressor, nbviewer, Kaggle Notebook, solutionTime series analysis, nbviewer, Kaggle Notebook, solutionBeating baseline in a competition, Kaggle NotebookBonus assignmentsAdditionally, you can purchase a Bonus Assignments pack with the best non-demo versions of mlcourse.ai assignments. Select the \""Bonus Assignments\"" tier on Patreon or a similar tier on Boosty (rus).                                          Details of the dealmlcourse.ai is still in self-paced mode but we offer you Bonus Assignments with solutions for a contribution of $17/month. The idea is that you pay for ~1-5 months while studying the course materials, but a single contribution is still fine and opens your access to the bonus pack.Note: the first payment is charged at the moment of joining the Tier Patreon, and the next payment is charged on the 1st day of the next month, thus it's better to purchase the pack in the 1st half of the month.mlcourse.ai is never supposed to go fully monetized (it's created in the wonderful open ODS.ai community and will remain open and free) but it'd help to cover some operational costs, and Yury also put in quite some effort into assembling all the best assignments into one pack. Please note that unlike the rest of the course content, Bonus Assignments are copyrighted. Informally, Yury's fine if you share the pack with 2-3 friends but public sharing of the Bonus Assignments pack is prohibited.  The bonus pack contains 10 assignments, in some of them you are challenged to beat a baseline in a Kaggle competition under thorough guidance (\""Alice\"" and \""Medium\"") or implement an algorithm from scratch -- efficient stochastic gradient descent classifier and gradient boosting.Kaggle competitionsCatch Me If You Can: Intruder Detection through Webpage Session Tracking. Kaggle InclassPredicting popularity of a Medium article. Kaggle InclassDotA 2 winner prediction. Kaggle InclassCiting mlcourse.aiIf you happen to cite mlcourse.ai in your work, you can use this BibTeX record:@misc{mlcourse_ai,    author = {Kashnitsky, Yury},    title = {mlcourse.ai – Open Machine Learning Course},    year = {2020},    publisher = {GitHub},    journal = {GitHub repository},    howpublished = {\\url{https://github.com/Yorko/mlcourse.ai}},}CommunityYou can join the Singularis.ai Slack community to ask questions on the course materials. The community is mostly Russian-speaking but questions in English are still welcome."
27,jenkins-docs/simple-python-pyinstaller-app,https://github.com/jenkins-docs/simple-python-pyinstaller-app/blob/master/README.md,Python,"simple-python-pyinstaller-appThis repository is for theBuild a Python app with PyInstallertutorial in the Jenkins User Documentation.The repository contains a simple Python application which is a command line tool \""add2vals\"" that outputs the addition of two values. If at least one of thevalues is a string, \""add2vals\"" treats both values as a string and insteadconcatenates the values. The \""add2\"" function in the \""calc\"" library (which\""add2vals\"" imports) is accompanied by a set of unit tests. These are tested with pytest to check that this function works as expected and the results are savedto a JUnit XML report.The delivery of the \""add2vals\"" tool through PyInstaller converts this tool intoa standalone executable file for Linux, which you can download through Jenkinsand execute at the command line on Linux machines without Python.The jenkins directory contains an example of the Jenkinsfile (i.e. Pipeline)you'll be creating yourself during the tutorial."
28,davidsandberg/facenet,https://github.com/davidsandberg/facenet/blob/master/README.md,Python,"Face Recognition using Tensorflow This is a TensorFlow implementation of the face recognizer described in the paper\""FaceNet: A Unified Embedding for Face Recognition and Clustering\"". The project also uses ideas from the paper \""Deep Face Recognition\"" from the Visual Geometry Group at Oxford.CompatibilityThe code is tested using Tensorflow r1.7 under Ubuntu 14.04 with Python 2.7 and Python 3.5. The test cases can be found here and the results can be found here.NewsDateUpdate2018-04-10Added new models trained on Casia-WebFace and VGGFace2 (see below). Note that the models uses fixed image standardization (see wiki).2018-03-31Added a new, more flexible input pipeline as well as a bunch of minor updates.2017-05-13Removed a bunch of older non-slim models. Moved the last bottleneck layer into the respective models. Corrected normalization of Center Loss.2017-05-06Added code to train a classifier on your own images. Renamed facenet_train.py to train_tripletloss.py and facenet_train_classifier.py to train_softmax.py.2017-03-02Added pretrained models that generate 128-dimensional embeddings.2017-02-22Updated to Tensorflow r1.0. Added Continuous Integration using Travis-CI.2017-02-03Added models where only trainable variables has been stored in the checkpoint. These are therefore significantly smaller.2017-01-27Added a model trained on a subset of the MS-Celeb-1M dataset. The LFW accuracy of this model is around 0.994.2017‑01‑02Updated to run with Tensorflow r0.12. Not sure if it runs with older versions of Tensorflow though.Pre-trained modelsModel nameLFW accuracyTraining datasetArchitecture20180408-1029000.9905CASIA-WebFaceInception ResNet v120180402-1147590.9965VGGFace2Inception ResNet v1NOTE: If you use any of the models, please do not forget to give proper credit to those providing the training dataset as well.InspirationThe code is heavily inspired by the OpenFace implementation.Training dataThe CASIA-WebFace dataset has been used for training. This training set consists of total of 453 453 images over 10 575 identities after face detection. Some performance improvement has been seen if the dataset has been filtered before training. Some more information about how this was done will come later.The best performing model has been trained on the VGGFace2 dataset consisting of ~3.3M faces and ~9000 classes.Pre-processingFace alignment using MTCNNOne problem with the above approach seems to be that the Dlib face detector misses some of the hard examples (partial occlusion, silhouettes, etc). This makes the training set too \""easy\"" which causes the model to perform worse on other benchmarks.To solve this, other face landmark detectors has been tested. One face landmark detector that has proven to work very well in this setting is theMulti-task CNN. A Matlab/Caffe implementation can be found here and this has been used for face alignment with very good results. A Python/Tensorflow implementation of MTCNN can be found here. This implementation does not give identical results to the Matlab/Caffe implementation but the performance is very similar.Running trainingCurrently, the best results are achieved by training the model using softmax loss. Details on how to train a model using softmax loss on the CASIA-WebFace dataset can be found on the page Classifier training of Inception-ResNet-v1 and .Pre-trained modelsInception-ResNet-v1 modelA couple of pretrained models are provided. They are trained using softmax loss with the Inception-Resnet-v1 model. The datasets has been aligned using MTCNN.PerformanceThe accuracy on LFW for the model 20180402-114759 is 0.99650+-0.00252. A description of how to run the test can be found on the page Validate on LFW. Note that the input images to the model need to be standardized using fixed image standardization (use the option --use_fixed_image_standardization when running e.g. validate_on_lfw.py)."
29,yandex-praktikum/calc_and_win,https://github.com/yandex-praktikum/calc_and_win/blob/master/README.md,Python,"calc_and_winРепозиторий игры \""Рассчитай и победи!\"""
30,google/it-cert-automation-practice,https://github.com/google/it-cert-automation-practice/blob/master/README.md,Python,Google IT Automation with Python Professional Certificate - Practice filesThis repository contains the practice files used throughout the courses that arepart of the Google IT Automation with Python Professional CertificateThere's a separate folder for each course.
31,keras-team/keras,https://github.com/keras-team/keras/blob/master/README.md,Python,"Keras: Deep Learning for humansThis repository hosts the development of the Keras library.Read the documentation at keras.io.About KerasKeras is a deep learning API written in Python,running on top of the machine learning platform TensorFlow.It was developed with a focus on enabling fast experimentation andproviding a delightful developer experience.The purpose of Keras is to give an unfair advantage to any developer looking to ship ML-powered apps.Keras is:Simple -- but not simplistic. Keras reduces developer cognitive loadto free you to focus on the parts of the problem that really matter.Keras focuses on ease of use, debugging speed, code elegance & conciseness,maintainability, and deployability (via TFServing, TFLite, TF.js).Flexible -- Keras adopts the principle of progressive disclosure ofcomplexity: simple workflows should be quick and easy, while arbitrarilyadvanced workflows should be possible via a clear path that builds uponwhat you've already learned.Powerful -- Keras provides industry-strength performance andscalability: it is used by organizations and companies including NASA,YouTube, and Waymo. That's right -- your YouTube recommendations arepowered by Keras, and so is the world's most advanced driverless vehicle.Keras & TensorFlow 2TensorFlow 2 is an end-to-end, open-source machine learning platform.You can think of it as an infrastructure layer fordifferentiable programming.It combines four key abilities:Efficiently executing low-level tensor operations on CPU, GPU, or TPU.Computing the gradient of arbitrary differentiable expressions.Scaling computation to many devices, such as clusters of hundreds of GPUs.Exporting programs (\""graphs\"") to external runtimes such as servers, browsers, mobile and embedded devices.Keras is the high-level API of TensorFlow 2: an approachable, highly-productive interfacefor solving machine learning problems,with a focus on modern deep learning. It provides essential abstractions and building blocks for developingand shipping machine learning solutions with high iteration velocity.Keras empowers engineers and researchers to take full advantage of the scalabilityand cross-platform capabilities of TensorFlow 2: you can run Keras on TPU or on large clusters of GPUs,and you can export your Keras models to run in the browser or on a mobile device.First contact with KerasThe core data structures of Keras are layers and models.The simplest type of model is the Sequential model, a linear stack of layers.For more complex architectures, you should use the Keras functional API,which allows you to build arbitrary graphs of layers or write models entirely from scratch via subclassing.Here is the Sequential model:from tensorflow.keras.models import Sequentialmodel = Sequential()Stacking layers is as easy as .add():from tensorflow.keras.layers import Densemodel.add(Dense(units=64, activation='relu'))model.add(Dense(units=10, activation='softmax'))Once your model looks good, configure its learning process with .compile():model.compile(loss='categorical_crossentropy',              optimizer='sgd',              metrics=['accuracy'])If you need to, you can further configure your optimizer. The Keras philosophy is to keep simple things simple,while allowing the user to be fully in control when they need to be (the ultimate control being the easy extensibility of the source code via subclassing).model.compile(loss=tf.keras.losses.categorical_crossentropy,              optimizer=tf.keras.optimizers.SGD(                  learning_rate=0.01, momentum=0.9, nesterov=True))You can now iterate on your training data in batches:# x_train and y_train are Numpy arrays.model.fit(x_train, y_train, epochs=5, batch_size=32)Evaluate your test loss and metrics in one line:loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)Or generate predictions on new data:classes = model.predict(x_test, batch_size=128)What you just saw is the most elementary way to use Keras.However, Keras is also a highly-flexible framework suitable to iterate on state-of-the-art research ideas.Keras follows the principle of progressive disclosure of complexity: it makes it easy to get started,yet it makes it possible to handle arbitrarily advanced use cases,only requiring incremental learning at each step.In pretty much the same way that you were able to train & evaluate a simple neural network above in a few lines,you can use Keras to quickly develop new training procedures or exotic model architectures.Here's a low-level training loop example, combining Keras functionality with the TensorFlow GradientTape:import tensorflow as tf# Prepare an optimizer.optimizer = tf.keras.optimizers.Adam()# Prepare a loss function.loss_fn = tf.keras.losses.kl_divergence# Iterate over the batches of a dataset.for inputs, targets in dataset:    # Open a GradientTape.    with tf.GradientTape() as tape:        # Forward pass.        predictions = model(inputs)        # Compute the loss value for this batch.        loss_value = loss_fn(targets, predictions)    # Get gradients of loss wrt the weights.    gradients = tape.gradient(loss_value, model.trainable_weights)    # Update the weights of the model.    optimizer.apply_gradients(zip(gradients, model.trainable_weights))For more in-depth tutorials about Keras, you can check out:Introduction to Keras for engineersIntroduction to Keras for researchersDeveloper guidesOther learning resourcesInstallationKeras comes packaged with TensorFlow 2 as tensorflow.keras.To start using Keras, simply install TensorFlow 2.You can then import Keras as follows:from tensorflow import kerasRelease and compatibilityKeras has nightly releases (keras-nightly on PyPI)and stable releases (keras on PyPI).The nightly Keras releases are usually compatible with the corresponding versionof the tf-nightly releases(e.g. keras-nightly==2.7.0.dev2021100607 should beused with tf-nightly==2.7.0.dev2021100607).We don't maintain backward compatibility for nightly releases.For stable releases, each Kerasversion maps to a specific stable version of TensorFlow.The table below shows the compatibility version mappingbetween TensorFlow versions and Keras versions.All the release branches can be found on GitHub.All the release binaries can be found on Pypi.SupportYou can ask questions and join the development discussion:In the TensorFlow forum.On the Keras mailing list.Opening an issueYou can also post bug reports and feature requests (only)in GitHub issues.Opening a PRWe welcome contributions! Before opening a PR, please readour contributor guide,and the API design guideline."
32,open-mmlab/mmdetection,https://github.com/open-mmlab/mmdetection/blob/main/README.md,Python,"           OpenMMLab website                  HOT                      OpenMMLab platform                  TRY IT OUT               📘Documentation |🛠️Installation |👀Model Zoo |🆕Update News |🚀Ongoing Projects |🤔Reporting IssuesEnglish | 简体中文                                              IntroductionMMDetection is an open source object detection toolbox based on PyTorch. It isa part of the OpenMMLab project.The main branch works with PyTorch 1.8+.Major featuresModular DesignWe decompose the detection framework into different components and one can easily construct a customized object detection framework by combining different modules.Support of multiple tasks out of boxThe toolbox directly supports multiple detection tasks such as object detection, instance segmentation, panoptic segmentation, and semi-supervised object detection.High efficiencyAll basic bbox and mask operations run on GPUs. The training speed is faster than or comparable to other codebases, including Detectron2, maskrcnn-benchmark and SimpleDet.State of the artThe toolbox stems from the codebase developed by the MMDet team, who won COCO Detection Challenge in 2018, and we keep pushing it forward.The newly released RTMDet also obtains new state-of-the-art results on real-time instance segmentation and rotated object detection tasks and the best parameter-accuracy trade-off on object detection.Apart from MMDetection, we also released MMEngine for model training and MMCV for computer vision research, which are heavily depended on by this toolbox.What's NewHighlightWe are excited to announce our latest work on real-time object recognition tasks, RTMDet, a family of fully convolutional single-stage detectors. RTMDet not only achieves the best parameter-accuracy trade-off on object detection from tiny to extra-large model sizes but also obtains new state-of-the-art performance on instance segmentation and rotated object detection tasks. Details can be found in the technical report. Pre-trained models are here.TaskDatasetAPFPS(TRT FP16 BS1 3090)Object DetectionCOCO52.8322Instance SegmentationCOCO44.6188Rotated Object DetectionDOTA78.9(single-scale)/81.3(multi-scale)121v3.1.0 was released in 30/6/2023:Supports tracking algorithms including multi-object tracking (MOT) algorithms SORT, DeepSORT, StrongSORT, OCSORT, ByteTrack, QDTrack, and video instance segmentation (VIS) algorithm MaskTrackRCNN, Mask2Former-VIS.Support ViTDetSupports inference and evaluation of multimodal algorithms GLIP and XDecoder, and also supports datasets such as COCO semantic segmentation, COCO Caption, ADE20k general segmentation, and RefCOCO. GLIP fine-tuning will be supported in the future.Provides a gradio demo for image type tasks of MMDetection, making it easy for users to experience.InstallationPlease refer to Installation for installation instructions.Getting StartedPlease see Overview for the general introduction of MMDetection.For detailed user guides and advanced guides, please refer to our documentation:User GuidesTrain & TestLearn about ConfigsInference with existing modelsDataset PrepareTest existing models on standard datasetsTrain predefined models on standard datasetsTrain with customized datasetsTrain with customized models and standard datasetsFinetuning ModelsTest Results SubmissionWeight initializationUse a single stage detector as RPNSemi-supervised Object DetectionUseful ToolsAdvanced GuidesBasic ConceptsComponent CustomizationHow toWe also provide object detection colab tutorial  and instance segmentation colab tutorial .To migrate from MMDetection 2.x, please refer to migration.Overview of Benchmark and Model ZooResults and models are available in the model zoo.  Architectures                    Object Detection                    Instance Segmentation                    Panoptic Segmentation                    Other                                        Fast R-CNN (ICCV'2015)            Faster R-CNN (NeurIPS'2015)            RPN (NeurIPS'2015)            SSD (ECCV'2016)            RetinaNet (ICCV'2017)            Cascade R-CNN (CVPR'2018)            YOLOv3 (ArXiv'2018)            CornerNet (ECCV'2018)            Grid R-CNN (CVPR'2019)            Guided Anchoring (CVPR'2019)            FSAF (CVPR'2019)            CenterNet (CVPR'2019)            Libra R-CNN (CVPR'2019)            TridentNet (ICCV'2019)            FCOS (ICCV'2019)            RepPoints (ICCV'2019)            FreeAnchor (NeurIPS'2019)            CascadeRPN (NeurIPS'2019)            Foveabox (TIP'2020)            Double-Head R-CNN (CVPR'2020)            ATSS (CVPR'2020)            NAS-FCOS (CVPR'2020)            CentripetalNet (CVPR'2020)            AutoAssign (ArXiv'2020)            Side-Aware Boundary Localization (ECCV'2020)            Dynamic R-CNN (ECCV'2020)            DETR (ECCV'2020)            PAA (ECCV'2020)            VarifocalNet (CVPR'2021)            Sparse R-CNN (CVPR'2021)            YOLOF (CVPR'2021)            YOLOX (CVPR'2021)            Deformable DETR (ICLR'2021)            TOOD (ICCV'2021)            DDOD (ACM MM'2021)            RTMDet (ArXiv'2022)            Conditional DETR (ICCV'2021)            DAB-DETR (ICLR'2022)            DINO (ICLR'2023)            GLIP (CVPR'2022)            DiffusionDet (ArXiv'2023)            EfficientDet (CVPR'2020)            Detic (ECCV'2022)                                    Mask R-CNN (ICCV'2017)          Cascade Mask R-CNN (CVPR'2018)          Mask Scoring R-CNN (CVPR'2019)          Hybrid Task Cascade (CVPR'2019)          YOLACT (ICCV'2019)          InstaBoost (ICCV'2019)          SOLO (ECCV'2020)          PointRend (CVPR'2020)          DetectoRS (ArXiv'2020)          SOLOv2 (NeurIPS'2020)          SCNet (AAAI'2021)          QueryInst (ICCV'2021)          Mask2Former (ArXiv'2021)          CondInst (ECCV'2020)          SparseInst (CVPR'2022)          RTMDet (ArXiv'2022)          BoxInst (CVPR'2021)                                      Panoptic FPN (CVPR'2019)          MaskFormer (NeurIPS'2021)          Mask2Former (ArXiv'2021)                                      Contrastive Learning                          SwAV (NeurIPS'2020)          MoCo (CVPR'2020)          MoCov2 (ArXiv'2020)                                  Distillation                          Localization Distillation (CVPR'2022)          Label Assignment Distillation (WACV'2022)                          Semi-Supervised Object Detection                          Soft Teacher (ICCV'2021)                                        Components                    Backbones                    Necks                    Loss                    Common                                  VGG (ICLR'2015)        ResNet (CVPR'2016)        ResNeXt (CVPR'2017)        MobileNetV2 (CVPR'2018)        HRNet (CVPR'2019)        Generalized Attention (ICCV'2019)        GCNet (ICCVW'2019)        Res2Net (TPAMI'2020)        RegNet (CVPR'2020)        ResNeSt (ArXiv'2020)        PVT (ICCV'2021)        Swin (CVPR'2021)        PVTv2 (ArXiv'2021)        ResNet strikes back (ArXiv'2021)        EfficientNet (ArXiv'2021)        ConvNeXt (CVPR'2022)        ConvNeXtv2 (ArXiv'2023)                                PAFPN (CVPR'2018)        NAS-FPN (CVPR'2019)        CARAFE (ICCV'2019)        FPG (ArXiv'2020)        GRoIE (ICPR'2020)        DyHead (CVPR'2021)                                    GHM (AAAI'2019)          Generalized Focal Loss (NeurIPS'2020)          Seasaw Loss (CVPR'2021)                                      OHEM (CVPR'2016)          Group Normalization (ECCV'2018)          DCN (ICCV'2017)          DCNv2 (CVPR'2019)          Weight Standardization (ArXiv'2019)          Prime Sample Attention (CVPR'2020)          Strong Baselines (CVPR'2021)          Resnet strikes back (ArXiv'2021)                        Some other methods are also supported in projects using MMDetection.FAQPlease refer to FAQ for frequently asked questions.ContributingWe appreciate all contributions to improve MMDetection. Ongoing projects can be found in out GitHub Projects. Welcome community users to participate in these projects. Please refer to CONTRIBUTING.md for the contributing guideline.AcknowledgementMMDetection is an open source project that is contributed by researchers and engineers from various colleges and companies. We appreciate all the contributors who implement their methods or add new features, as well as users who give valuable feedbacks.We wish that the toolbox and benchmark could serve the growing research community by providing a flexible toolkit to reimplement existing methods and develop their own new detectors.CitationIf you use this toolbox or benchmark in your research, please cite this project.@article{mmdetection,  title   = {{MMDetection}: Open MMLab Detection Toolbox and Benchmark},  author  = {Chen, Kai and Wang, Jiaqi and Pang, Jiangmiao and Cao, Yuhang and             Xiong, Yu and Li, Xiaoxiao and Sun, Shuyang and Feng, Wansen and             Liu, Ziwei and Xu, Jiarui and Zhang, Zheng and Cheng, Dazhi and             Zhu, Chenchen and Cheng, Tianheng and Zhao, Qijie and Li, Buyu and             Lu, Xin and Zhu, Rui and Wu, Yue and Dai, Jifeng and Wang, Jingdong             and Shi, Jianping and Ouyang, Wanli and Loy, Chen Change and Lin, Dahua},  journal= {arXiv preprint arXiv:1906.07155},  year={2019}}LicenseThis project is released under the Apache 2.0 license.Projects in OpenMMLabMMEngine: OpenMMLab foundational library for training deep learning models.MMCV: OpenMMLab foundational library for computer vision.MMPreTrain: OpenMMLab pre-training toolbox and benchmark.MMagic: OpenMMLab Advanced, Generative and Intelligent Creation toolbox.MMDetection: OpenMMLab detection toolbox and benchmark.MMDetection3D: OpenMMLab's next-generation platform for general 3D object detection.MMRotate: OpenMMLab rotated object detection toolbox and benchmark.MMYOLO: OpenMMLab YOLO series toolbox and benchmark.MMSegmentation: OpenMMLab semantic segmentation toolbox and benchmark.MMOCR: OpenMMLab text detection, recognition, and understanding toolbox.MMPose: OpenMMLab pose estimation toolbox and benchmark.MMHuman3D: OpenMMLab 3D human parametric model toolbox and benchmark.MMSelfSup: OpenMMLab self-supervised learning toolbox and benchmark.MMRazor: OpenMMLab model compression toolbox and benchmark.MMFewShot: OpenMMLab fewshot learning toolbox and benchmark.MMAction2: OpenMMLab's next-generation action understanding toolbox and benchmark.MMTracking: OpenMMLab video perception toolbox and benchmark.MMFlow: OpenMMLab optical flow toolbox and benchmark.MMEditing: OpenMMLab image and video editing toolbox.MMGeneration: OpenMMLab image and video generative models toolbox.MMDeploy: OpenMMLab model deployment framework.MIM: MIM installs OpenMMLab packages.MMEval: A unified evaluation library for multiple machine learning libraries.Playground: A central hub for gathering and showcasing amazing projects built upon OpenMMLab."
33,gto76/python-cheatsheet,https://github.com/gto76/python-cheatsheet/blob/main/README.md,Python,"Comprehensive Python CheatsheetDownload text file, Buy PDF, Fork me on GitHub or Check out FAQ.Contents    1. Collections:   List, Dictionary, Set, Tuple, Range, Enumerate, Iterator, Generator.    2. Types:            Type, String, Regular_Exp, Format, Numbers, Combinatorics, Datetime.    3. Syntax:           Args, Inline, Import, Decorator, Class, Duck_Types, Enum, Exception.    4. System:          Exit, Print, Input, Command_Line_Arguments, Open, Path, OS_Commands.    5. Data:               JSON, Pickle, CSV, SQLite, Bytes, Struct, Array, Memory_View, Deque.    6. Advanced:     Threading, Operator, Introspection, Metaprograming, Eval, Coroutines.    7. Libraries:        Progress_Bar, Plot, Table, Curses, Logging, Scraping, Web, Profile,                                  NumPy, Image, Audio, Games, Data.Mainif __name__ == '__main__':      # Runs main() if file wasn't imported.    main()List<list> = <list>[<slice>]        # Or: <list>[from_inclusive : to_exclusive : ±step]<list>.append(<el>)             # Or: <list> += [<el>]<list>.extend(<collection>)     # Or: <list> += <collection><list>.sort()                   # Sorts in ascending order.<list>.reverse()                # Reverses the list in-place.<list> = sorted(<collection>)   # Returns a new sorted list.<iter> = reversed(<list>)       # Returns reversed iterator.sum_of_elements  = sum(<collection>)elementwise_sum  = [sum(pair) for pair in zip(list_a, list_b)]sorted_by_second = sorted(<collection>, key=lambda el: el[1])sorted_by_both   = sorted(<collection>, key=lambda el: (el[1], el[0]))flatter_list     = list(itertools.chain.from_iterable(<list>))product_of_elems = functools.reduce(lambda out, el: out * el, <collection>)list_of_chars    = list(<str>)For details about sorted(), min() and max() see sortable.Module operator provides functions itemgetter() and mul() that offer the same functionality as lambda expressions above.<list>.insert(<int>, <el>)      # Inserts item at index and moves the rest to the right.<el>  = <list>.pop([<int>])     # Removes and returns item at index or from the end.<int> = <list>.count(<el>)      # Returns number of occurrences. Also works on strings.<int> = <list>.index(<el>)      # Returns index of the first occurrence or raises ValueError.<list>.remove(<el>)             # Removes first occurrence of the item or raises ValueError.<list>.clear()                  # Removes all items. Also works on dictionary and set.Dictionary<view> = <dict>.keys()                          # Coll. of keys that reflects changes.<view> = <dict>.values()                        # Coll. of values that reflects changes.<view> = <dict>.items()                         # Coll. of key-value tuples that reflects chgs.value  = <dict>.get(key, default=None)          # Returns default if key is missing.value  = <dict>.setdefault(key, default=None)   # Returns and writes default if key is missing.<dict> = collections.defaultdict(<type>)        # Returns a dict with default value of type.<dict> = collections.defaultdict(lambda: 1)     # Returns a dict with default value 1.<dict> = dict(<collection>)                     # Creates a dict from coll. of key-value pairs.<dict> = dict(zip(keys, values))                # Creates a dict from two collections.<dict> = dict.fromkeys(keys [, value])          # Creates a dict from collection of keys.<dict>.update(<dict>)                           # Adds items. Replaces ones with matching keys.value = <dict>.pop(key)                         # Removes item or raises KeyError.{k for k, v in <dict>.items() if v == value}    # Returns set of keys that point to the value.{k: v for k, v in <dict>.items() if k in keys}  # Returns a dictionary, filtered by keys.Counter>>> from collections import Counter>>> colors = ['blue', 'blue', 'blue', 'red', 'red']>>> counter = Counter(colors)>>> counter['yellow'] += 1Counter({'blue': 3, 'red': 2, 'yellow': 1})>>> counter.most_common()[0]('blue', 3)Set<set> = set()                                   # `{}` returns a dictionary.<set>.add(<el>)                                 # Or: <set> |= {<el>}<set>.update(<collection> [, ...])              # Or: <set> |= <set><set>  = <set>.union(<coll.>)                   # Or: <set> | <set><set>  = <set>.intersection(<coll.>)            # Or: <set> & <set><set>  = <set>.difference(<coll.>)              # Or: <set> - <set><set>  = <set>.symmetric_difference(<coll.>)    # Or: <set> ^ <set><bool> = <set>.issubset(<coll.>)                # Or: <set> <= <set><bool> = <set>.issuperset(<coll.>)              # Or: <set> >= <set><el> = <set>.pop()                              # Raises KeyError if empty.<set>.remove(<el>)                              # Raises KeyError if missing.<set>.discard(<el>)                             # Doesn't raise an error.Frozen SetIs immutable and hashable.That means it can be used as a key in a dictionary or as an element in a set.<frozenset> = frozenset(<collection>)TupleTuple is an immutable and hashable list.<tuple> = ()                               # Empty tuple.<tuple> = (<el>,)                          # Or: <el>,<tuple> = (<el_1>, <el_2> [, ...])         # Or: <el_1>, <el_2> [, ...]Named TupleTuple's subclass with named elements.>>> from collections import namedtuple>>> Point = namedtuple('Point', 'x y')>>> p = Point(1, y=2)Point(x=1, y=2)>>> p[0]1>>> p.x1>>> getattr(p, 'y')2RangeImmutable and hashable sequence of integers.<range> = range(stop)                      # range(to_exclusive)<range> = range(start, stop)               # range(from_inclusive, to_exclusive)<range> = range(start, stop, ±step)        # range(from_inclusive, to_exclusive, ±step_size)>>> [i for i in range(3)][0, 1, 2]Enumeratefor i, el in enumerate(<collection> [, i_start]):    ...Iterator<iter> = iter(<collection>)                # `iter(<iter>)` returns unmodified iterator.<iter> = iter(<function>, to_exclusive)    # A sequence of return values until 'to_exclusive'.<el>   = next(<iter> [, default])          # Raises StopIteration or returns 'default' on end.<list> = list(<iter>)                      # Returns a list of iterator's remaining elements.Itertoolsimport itertools as it<iter> = it.count(start=0, step=1)         # Returns updated value endlessly. Accepts floats.<iter> = it.repeat(<el> [, times])         # Returns element endlessly or 'times' times.<iter> = it.cycle(<collection>)            # Repeats the sequence endlessly.<iter> = it.chain(<coll>, <coll> [, ...])  # Empties collections in order (figuratively).<iter> = it.chain.from_iterable(<coll>)    # Empties collections inside a collection in order.<iter> = it.islice(<coll>, to_exclusive)   # Only returns first 'to_exclusive' elements.<iter> = it.islice(<coll>, from_inc, …)    # `to_exclusive, +step_size`. Indices can be None.GeneratorAny function that contains a yield statement returns a generator.Generators and iterators are interchangeable.def count(start, step):    while True:        yield start        start += step>>> counter = count(10, 2)>>> next(counter), next(counter), next(counter)(10, 12, 14)TypeEverything is an object.Every object has a type.Type and class are synonymous.<type> = type(<el>)                          # Or: <el>.__class__<bool> = isinstance(<el>, <type>)            # Or: issubclass(type(<el>), <type>)>>> type('a'), 'a'.__class__, str(<class 'str'>, <class 'str'>, <class 'str'>)Some types do not have built-in names, so they must be imported:from types import FunctionType, MethodType, LambdaType, GeneratorType, ModuleTypeAbstract Base ClassesEach abstract base class specifies a set of virtual subclasses. These classes are then recognized by isinstance() and issubclass() as subclasses of the ABC, although they are really not. ABC can also manually decide whether or not a specific class is its virtual subclass, usually based on which methods the class has implemented. For instance, Iterable ABC looks for method iter(), while Collection ABC looks for iter(), contains() and len().>>> from collections.abc import Iterable, Collection, Sequence>>> isinstance([1, 2, 3], Iterable)True+------------------+------------+------------+------------+|                  |  Iterable  | Collection |  Sequence  |+------------------+------------+------------+------------+| list, range, str |    yes     |    yes     |    yes     || dict, set        |    yes     |    yes     |            || iter             |    yes     |            |            |+------------------+------------+------------+------------+>>> from numbers import Number, Complex, Real, Rational, Integral>>> isinstance(123, Number)True+--------------------+----------+----------+----------+----------+----------+|                    |  Number  |  Complex |   Real   | Rational | Integral |+--------------------+----------+----------+----------+----------+----------+| int                |   yes    |   yes    |   yes    |   yes    |   yes    || fractions.Fraction |   yes    |   yes    |   yes    |   yes    |          || float              |   yes    |   yes    |   yes    |          |          || complex            |   yes    |   yes    |          |          |          || decimal.Decimal    |   yes    |          |          |          |          |+--------------------+----------+----------+----------+----------+----------+String<str>  = <str>.strip()                       # Strips all whitespace characters from both ends.<str>  = <str>.strip('<chars>')              # Strips all passed characters from both ends.<list> = <str>.split()                       # Splits on one or more whitespace characters.<list> = <str>.split(sep=None, maxsplit=-1)  # Splits on 'sep' str at most 'maxsplit' times.<list> = <str>.splitlines(keepends=False)    # On [\\\r\\f\\v\\x1c-\\x1e\\x85\\u2028\\u2029] and \\r\.<str>  = <str>.join(<coll_of_strings>)       # Joins elements using string as a separator.<bool> = <sub_str> in <str>                  # Checks if string contains the substring.<bool> = <str>.startswith(<sub_str>)         # Pass tuple of strings for multiple options.<bool> = <str>.endswith(<sub_str>)           # Pass tuple of strings for multiple options.<int>  = <str>.find(<sub_str>)               # Returns start index of the first match or -1.<int>  = <str>.index(<sub_str>)              # Same, but raises ValueError if missing.<str>  = <str>.replace(old, new [, count])   # Replaces 'old' with 'new' at most 'count' times.<str>  = <str>.translate(<table>)            # Use `str.maketrans(<dict>)` to generate table.<str>  = chr(<int>)                          # Converts int to Unicode character.<int>  = ord(<str>)                          # Converts Unicode character to int.Also: 'lstrip()', 'rstrip()' and 'rsplit()'.Also: 'lower()', 'upper()', 'capitalize()' and 'title()'.Property Methods+---------------+----------+----------+----------+----------+----------+|               | [ !#$%…] | [a-zA-Z] |  [¼½¾]   |  [²³¹]   |  [0-9]   |+---------------+----------+----------+----------+----------+----------+| isprintable() |   yes    |   yes    |   yes    |   yes    |   yes    || isalnum()     |          |   yes    |   yes    |   yes    |   yes    || isnumeric()   |          |          |   yes    |   yes    |   yes    || isdigit()     |          |          |          |   yes    |   yes    || isdecimal()   |          |          |          |          |   yes    |+---------------+----------+----------+----------+----------+----------+'isspace()' checks for whitespaces: '[ \\t\\\r\\f\\v\\x1c-\\x1f\\x85\\xa0\\u1680…]'.Regeximport re<str>   = re.sub(<regex>, new, text, count=0)  # Substitutes all occurrences with 'new'.<list>  = re.findall(<regex>, text)            # Returns all occurrences as strings.<list>  = re.split(<regex>, text, maxsplit=0)  # Use brackets in regex to include the matches.<Match> = re.search(<regex>, text)             # Searches for first occurrence of the pattern.<Match> = re.match(<regex>, text)              # Searches only at the beginning of the text.<iter>  = re.finditer(<regex>, text)           # Returns all occurrences as Match objects.Argument 'new' can be a function that accepts a Match object and returns a string.Search() and match() return None if they can't find a match.Argument 'flags=re.IGNORECASE' can be used with all functions.Argument 'flags=re.MULTILINE' makes '^' and '$' match the start/end of each line.Argument 'flags=re.DOTALL' makes '.' also accept the '\'.Use r'\\1' or '\\\\1' for backreference ('\\1' returns a character with octal code 1).Add '?' after '*' and '+' to make them non-greedy.Match Object<str>   = <Match>.group()                      # Returns the whole match. Also group(0).<str>   = <Match>.group(1)                     # Returns part in the first bracket.<tuple> = <Match>.groups()                     # Returns all bracketed parts.<int>   = <Match>.start()                      # Returns start index of the match.<int>   = <Match>.end()                        # Returns exclusive end index of the match.Special Sequences'\\d' == '[0-9]'                                # Matches decimal characters.'\\w' == '[a-zA-Z0-9_]'                         # Matches alphanumerics and underscore.'\\s' == '[ \\t\\\r\\f\\v]'                        # Matches whitespaces.By default, decimal characters, alphanumerics and whitespaces from all alphabets are matched unless 'flags=re.ASCII' argument is used.As shown above, it restricts all special sequence matches to the first 128 characters and prevents '\\s' from accepting '[\\x1c-\\x1f]' (the so-called separator characters).Use a capital letter for negation (all non-ASCII characters will be matched when used in combination with ASCII flag).Format<str> = f'{<el_1>}, {<el_2>}'            # Curly brackets can also contain expressions.<str> = '{}, {}'.format(<el_1>, <el_2>)  # Or: '{0}, {a}'.format(<el_1>, a=<el_2>)<str> = '%s, %s' % (<el_1>, <el_2>)      # Redundant and inferior C-style formatting.Example>>> Person = collections.namedtuple('Person', 'name height')>>> person = Person('Jean-Luc', 187)>>> f'{person.name} is {person.height / 100} meters tall.''Jean-Luc is 1.87 meters tall.'General Options{<el>:<10}                               # '<el>      '{<el>:^10}                               # '   <el>   '{<el>:>10}                               # '      <el>'{<el>:.<10}                              # '<el>......'{<el>:0}                                 # '<el>'Options can be generated dynamically: f'{<el>:{<str/int>}[…]}'.Adding '=' to the expression prepends it to the output: f'{1+1=}' returns '1+1=2'.Adding '!r' to the expression converts object to string by calling its repr() method.Strings{'abcde':10}                             # 'abcde     '{'abcde':10.3}                           # 'abc       '{'abcde':.3}                             # 'abc'{'abcde'!r:10}                           # \""'abcde'   \""Numbers{123456:10}                              # '    123456'{123456:10,}                             # '   123,456'{123456:10_}                             # '   123_456'{123456:+10}                             # '   +123456'{123456:=+10}                            # '+   123456'{123456: }                               # ' 123456'{-123456: }                              # '-123456'Floats{1.23456:10.3}                           # '      1.23'{1.23456:10.3f}                          # '     1.235'{1.23456:10.3e}                          # ' 1.235e+00'{1.23456:10.3%}                          # '  123.456%'Comparison of presentation types:+--------------+----------------+----------------+----------------+----------------+|              |    {<float>}   |   {<float>:f}  |   {<float>:e}  |   {<float>:%}  |+--------------+----------------+----------------+----------------+----------------+|  0.000056789 |   '5.6789e-05' |    '0.000057'  | '5.678900e-05' |    '0.005679%' ||  0.00056789  |   '0.00056789' |    '0.000568'  | '5.678900e-04' |    '0.056789%' ||  0.0056789   |   '0.0056789'  |    '0.005679'  | '5.678900e-03' |    '0.567890%' ||  0.056789    |   '0.056789'   |    '0.056789'  | '5.678900e-02' |    '5.678900%' ||  0.56789     |   '0.56789'    |    '0.567890'  | '5.678900e-01' |   '56.789000%' ||  5.6789      |   '5.6789'     |    '5.678900'  | '5.678900e+00' |  '567.890000%' || 56.789       |  '56.789'      |   '56.789000'  | '5.678900e+01' | '5678.900000%' |+--------------+----------------+----------------+----------------+----------------++--------------+----------------+----------------+----------------+----------------+|              |  {<float>:.2}  |  {<float>:.2f} |  {<float>:.2e} |  {<float>:.2%} |+--------------+----------------+----------------+----------------+----------------+|  0.000056789 |    '5.7e-05'   |      '0.00'    |   '5.68e-05'   |      '0.01%'   ||  0.00056789  |    '0.00057'   |      '0.00'    |   '5.68e-04'   |      '0.06%'   ||  0.0056789   |    '0.0057'    |      '0.01'    |   '5.68e-03'   |      '0.57%'   ||  0.056789    |    '0.057'     |      '0.06'    |   '5.68e-02'   |      '5.68%'   ||  0.56789     |    '0.57'      |      '0.57'    |   '5.68e-01'   |     '56.79%'   ||  5.6789      |    '5.7'       |      '5.68'    |   '5.68e+00'   |    '567.89%'   || 56.789       |    '5.7e+01'   |     '56.79'    |   '5.68e+01'   |   '5678.90%'   |+--------------+----------------+----------------+----------------+----------------+'{<float>:g}' is '{<float>:.6}' with stripped zeros, exponent starting at 7 figures.When both rounding up and rounding down are possible, the one that returns result with even last digit is chosen. That makes '{6.5:.0f}' a '6' and '{7.5:.0f}' an '8'.This rule only effects numbers that can be represented exactly by a float (.5, .25, …).Ints{90:c}                                   # 'Z'{90:b}                                   # '1011010'{90:X}                                   # '5A'Numbers<int>      = int(<float/str/bool>)                # Or: math.floor(<float>)<float>    = float(<int/str/bool>)                # Or: <int/float>e±<int><complex>  = complex(real=0, imag=0)              # Or: <int/float/Fraction> ± <int/float>j<Fraction> = fractions.Fraction(0, 1)             # Or: Fraction(numerator=0, denominator=1)<Decimal>  = decimal.Decimal(<str/int>)           # Or: Decimal((sign, digits, exponent))'int(<str>)' and 'float(<str>)' raise ValueError on malformed strings.Decimal numbers are stored exactly, unlike most floats where '1.1 + 2.2 != 3.3'.Floats can be compared with: 'math.isclose(<float>, <float>)'.Precision of decimal operations is set with: 'decimal.getcontext().prec = <int>'.Basic Functions<num> = pow(<num>, <num>)                         # Or: <num> ** <num><num> = abs(<num>)                                # <float> = abs(<complex>)<num> = round(<num> [, ±ndigits])                 # `round(126, -1) == 130`Mathfrom math import e, pi, inf, nan, isinf, isnan    # `<el> == nan` is always False.from math import sin, cos, tan, asin, acos, atan  # Also: degrees, radians.from math import log, log10, log2                 # Log can accept base as second arg.Statisticsfrom statistics import mean, median, variance     # Also: stdev, quantiles, groupby.Randomfrom random import random, randint, choice        # Also: shuffle, gauss, triangular, seed.<float> = random()                                # A float inside [0, 1).<int>   = randint(from_inc, to_inc)               # An int inside [from_inc, to_inc].<el>    = choice(<sequence>)                      # Keeps the sequence intact.Bin, Hex<int> = ±0b<bin>                                  # Or: ±0x<hex><int> = int('±<bin>', 2)                          # Or: int('±<hex>', 16)<int> = int('±0b<bin>', 0)                        # Or: int('±0x<hex>', 0)<str> = bin(<int>)                                # Returns '[-]0b<bin>'.Bitwise Operators<int> = <int> & <int>                             # And (0b1100 & 0b1010 == 0b1000).<int> = <int> | <int>                             # Or  (0b1100 | 0b1010 == 0b1110).<int> = <int> ^ <int>                             # Xor (0b1100 ^ 0b1010 == 0b0110).<int> = <int> << n_bits                           # Left shift. Use >> for right.<int> = ~<int>                                    # Not. Also -<int> - 1.Combinatoricsimport itertools as it>>> list(it.product([0, 1], repeat=3))[(0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1), (1, 0, 0), (1, 0, 1), (1, 1, 0), (1, 1, 1)]>>> list(it.product('abc', 'abc'))                    #   a  b  c[('a', 'a'), ('a', 'b'), ('a', 'c'),                  # a x  x  x ('b', 'a'), ('b', 'b'), ('b', 'c'),                  # b x  x  x ('c', 'a'), ('c', 'b'), ('c', 'c')]                  # c x  x  x>>> list(it.combinations('abc', 2))                   #   a  b  c[('a', 'b'), ('a', 'c'),                              # a .  x  x ('b', 'c')]                                          # b .  .  x>>> list(it.combinations_with_replacement('abc', 2))  #   a  b  c[('a', 'a'), ('a', 'b'), ('a', 'c'),                  # a x  x  x ('b', 'b'), ('b', 'c'),                              # b .  x  x ('c', 'c')]                                          # c .  .  x>>> list(it.permutations('abc', 2))                   #   a  b  c[('a', 'b'), ('a', 'c'),                              # a .  x  x ('b', 'a'), ('b', 'c'),                              # b x  .  x ('c', 'a'), ('c', 'b')]                              # c x  x  .DatetimeProvides 'date', 'time', 'datetime' and 'timedelta' classes. All are immutable and hashable.# pip3 install python-dateutilfrom datetime import date, time, datetime, timedelta, timezonefrom dateutil.tz import tzlocal, gettz, datetime_exists, resolve_imaginary<D>  = date(year, month, day)               # Only accepts valid dates from 1 to 9999 AD.<T>  = time(hour=0, minute=0, second=0)     # Also: `microsecond=0, tzinfo=None, fold=0`.<DT> = datetime(year, month, day, hour=0)   # Also: `minute=0, second=0, microsecond=0, …`.<TD> = timedelta(weeks=0, days=0, hours=0)  # Also: `minutes=0, seconds=0, microseconds=0`.Aware <a> time and datetime objects have defined timezone, while naive <n> don't.If object is naive, it is presumed to be in the system's timezone.'fold=1' means the second pass in case of time jumping back for one hour.Timedelta normalizes arguments to ±days, seconds (< 86 400) and microseconds (< 1M).Use '<D/DT>.weekday()' to get the day of the week as an int, with Monday being 0.'<DTa> = resolve_imaginary(<DTa>)' fixes DTs that fall into the missing hour.Now<D/DTn>  = D/DT.today()                     # Current local date or naive datetime.<DTn>    = DT.utcnow()                      # Naive datetime from current UTC time.<DTa>    = DT.now(<tzinfo>)                 # Aware datetime from current tz time.To extract time use '<DTn>.time()', '<DTa>.time()' or '<DTa>.timetz()'.Timezone<tzinfo> = timezone.utc                     # London without daylight saving time.<tzinfo> = timezone(<timedelta>)            # Timezone with fixed offset from UTC.<tzinfo> = tzlocal()                        # Local timezone. Also gettz().<tzinfo> = gettz('<Continent>/<City>')      # 'Continent/City_Name' timezone or None.<DTa>    = <DT>.astimezone([<tzinfo>])      # Converts DT to the passed or local timezone.<Ta/DTa> = <T/DT>.replace(tzinfo=<tzinfo>)  # Changes object's timezone without conversion.Standard library's zoneinfo.ZoneInfo() can be used instead of gettz() on Python 3.9 and later. It requires 'tzdata' package on Windows.Encode<D/T/DT> = D/T/DT.fromisoformat('<iso>')    # Object from ISO string. Raises ValueError.<DT>     = DT.strptime(<str>, '<format>')   # Datetime from str, according to format.<D/DTn>  = D/DT.fromordinal(<int>)          # D/DTn from days since the Gregorian NYE 1.<DTn>    = DT.fromtimestamp(<real>)         # Local time DTn from seconds since the Epoch.<DTa>    = DT.fromtimestamp(<real>, <tz.>)  # Aware datetime from seconds since the Epoch.ISO strings come in following forms: 'YYYY-MM-DD', 'HH:MM:SS.mmmuuu[±HH:MM]', or both separated by an arbitrary character. All parts following the hours are optional.Python uses the Unix Epoch: '1970-01-01 00:00 UTC', '1970-01-01 01:00 CET', ...Decode<str>    = <D/T/DT>.isoformat(sep='T')      # Also `timespec='auto/hours/minutes/seconds/…'`.<str>    = <D/T/DT>.strftime('<format>')    # Custom string representation.<int>    = <D/DT>.toordinal()               # Days since Gregorian NYE 1, ignoring time and tz.<float>  = <DTn>.timestamp()                # Seconds since the Epoch, from DTn in local tz.<float>  = <DTa>.timestamp()                # Seconds since the Epoch, from aware datetime.Format>>> dt = datetime.strptime('2015-05-14 23:39:00.00 +0200', '%Y-%m-%d %H:%M:%S.%f %z')>>> dt.strftime(\""%A, %dth of %B '%y, %I:%M%p %Z\"")\""Thursday, 14th of May '15, 11:39PM UTC+02:00\""'%z' accepts '±HH[:]MM' and returns '±HHMM' or empty string if datetime is naive.'%Z' accepts 'UTC/GMT' and local timezone's code and returns timezone's name, 'UTC[±HH:MM]' if timezone is nameless, or an empty string if datetime is naive.For abbreviated weekday and month use '%a' and '%b'.Arithmetics<D/DT>   = <D/DT>  ± <TD>                   # Returned datetime can fall into missing hour.<TD>     = <D/DTn> - <D/DTn>                # Returns the difference. Ignores time jumps.<TD>     = <DTa>   - <DTa>                  # Ignores time jumps if they share tzinfo object.<TD>     = <TD>    * <int/float>            # Also: <TD> = abs(<TD>) and <TD> = <TD> ±% <TD>.<float>  = <TD>    / <TD>                   # How many weeks/years there are in TD. Also //.ArgumentsInside Function Callfunc(<positional_args>)                           # func(0, 0)func(<keyword_args>)                              # func(x=0, y=0)func(<positional_args>, <keyword_args>)           # func(0, y=0)Inside Function Definitiondef func(<nondefault_args>): ...                  # def func(x, y): ...def func(<default_args>): ...                     # def func(x=0, y=0): ...def func(<nondefault_args>, <default_args>): ...  # def func(x, y=0): ...Default values are evaluated when function is first encountered in the scope.Any mutation of a mutable default value will persist between invocations!Splat OperatorInside Function CallSplat expands a collection into positional arguments, while splatty-splat expands a dictionary into keyword arguments.args   = (1, 2)kwargs = {'x': 3, 'y': 4, 'z': 5}func(*args, **kwargs)Is the same as:func(1, 2, x=3, y=4, z=5)Inside Function DefinitionSplat combines zero or more positional arguments into a tuple, while splatty-splat combines zero or more keyword arguments into a dictionary.def add(*a):    return sum(a)>>> add(1, 2, 3)6Legal argument combinations:def f(*args): ...               # f(1, 2, 3)def f(x, *args): ...            # f(1, 2, 3)def f(*args, z): ...            # f(1, 2, z=3)def f(**kwargs): ...            # f(x=1, y=2, z=3)def f(x, **kwargs): ...         # f(x=1, y=2, z=3) | f(1, y=2, z=3)def f(*args, **kwargs): ...     # f(x=1, y=2, z=3) | f(1, y=2, z=3) | f(1, 2, z=3) | f(1, 2, 3)def f(x, *args, **kwargs): ...  # f(x=1, y=2, z=3) | f(1, y=2, z=3) | f(1, 2, z=3) | f(1, 2, 3)def f(*args, y, **kwargs): ...  # f(x=1, y=2, z=3) | f(1, y=2, z=3)def f(*, x, y, z): ...          # f(x=1, y=2, z=3)def f(x, *, y, z): ...          # f(x=1, y=2, z=3) | f(1, y=2, z=3)def f(x, y, *, z): ...          # f(x=1, y=2, z=3) | f(1, y=2, z=3) | f(1, 2, z=3)Other Uses<list>  = [*<coll.> [, ...]]    # Or: list(<collection>) [+ ...]<tuple> = (*<coll.>, [...])     # Or: tuple(<collection>) [+ ...]<set>   = {*<coll.> [, ...]}    # Or: set(<collection>) [| ...]<dict>  = {**<dict> [, ...]}    # Or: dict(**<dict> [, ...])head, *body, tail = <coll.>     # Head or tail can be omitted.InlineLambda<func> = lambda: <return_value>                     # A single statement function.<func> = lambda <arg_1>, <arg_2>: <return_value>    # Also accepts default arguments.Comprehensions<list> = [i+1 for i in range(10)]                   # Or: [1, 2, ..., 10]<iter> = (i for i in range(10) if i > 5)            # Or: iter([6, 7, 8, 9])<set>  = {i+5 for i in range(10)}                   # Or: {5, 6, ..., 14}<dict> = {i: i*2 for i in range(10)}                # Or: {0: 0, 1: 2, ..., 9: 18}>>> [l+r for l in 'abc' for r in 'abc']['aa', 'ab', 'ac', ..., 'cc']Map, Filter, Reducefrom functools import reduce<iter> = map(lambda x: x + 1, range(10))            # Or: iter([1, 2, ..., 10])<iter> = filter(lambda x: x > 5, range(10))         # Or: iter([6, 7, 8, 9])<obj>  = reduce(lambda out, x: out + x, range(10))  # Or: 45Any, All<bool> = any(<collection>)                          # Is `bool(<el>)` True for any element.<bool> = all(<collection>)                          # Is True for all elements or empty.Conditional Expression<obj> = <exp> if <condition> else <exp>             # Only one expression gets evaluated.>>> [a if a else 'zero' for a in (0, 1, 2, 3)]      # `any([0, '', [], None]) == False`['zero', 1, 2, 3]Named Tuple, Enum, Dataclassfrom collections import namedtuplePoint = namedtuple('Point', 'x y')                  # Creates a tuple's subclass.point = Point(0, 0)                                 # Returns its instance.from enum import EnumDirection = Enum('Direction', 'N E S W')            # Creates an enum.direction = Direction.N                             # Returns its member.from dataclasses import make_dataclassPlayer = make_dataclass('Player', ['loc', 'dir'])   # Creates a class.player = Player(point, direction)                   # Returns its instance.Importsimport <module>            # Imports a built-in or '<module>.py'.import <package>           # Imports a built-in or '<package>/__init__.py'.import <package>.<module>  # Imports a built-in or '<package>/<module>.py'.Package is a collection of modules, but it can also define its own objects.On a filesystem this corresponds to a directory of Python files with an optional init script.Running 'import <package>' does not automatically provide access to the package's modules unless they are explicitly imported in its init script.ClosureWe have/get a closure in Python when:A nested function references a value of its enclosing function and thenthe enclosing function returns the nested function.def get_multiplier(a):    def out(b):        return a * b    return out>>> multiply_by_3 = get_multiplier(3)>>> multiply_by_3(10)30If multiple nested functions within enclosing function reference the same value, that value gets shared.To dynamically access function's first free variable use '<function>.__closure__[0].cell_contents'.Partialfrom functools import partial<function> = partial(<function> [, <arg_1>, <arg_2>, ...])>>> def multiply(a, b):...     return a * b>>> multiply_by_3 = partial(multiply, 3)>>> multiply_by_3(10)30Partial is also useful in cases when function needs to be passed as an argument because it enables us to set its arguments beforehand.A few examples being: 'defaultdict(<function>)', 'iter(<function>, to_exclusive)' and dataclass's 'field(default_factory=<function>)'.Non-LocalIf variable is being assigned to anywhere in the scope, it is regarded as a local variable, unless it is declared as a 'global' or a 'nonlocal'.def get_counter():    i = 0    def out():        nonlocal i        i += 1        return i    return out>>> counter = get_counter()>>> counter(), counter(), counter()(1, 2, 3)DecoratorA decorator takes a function, adds some functionality and returns it.It can be any callable, but is usually implemented as a function that returns a closure.@decorator_namedef function_that_gets_passed_to_decorator():    ...Debugger ExampleDecorator that prints function's name every time the function is called.from functools import wrapsdef debug(func):    @wraps(func)    def out(*args, **kwargs):        print(func.__name__)        return func(*args, **kwargs)    return out@debugdef add(x, y):    return x + yWraps is a helper decorator that copies the metadata of the passed function (func) to the function it is wrapping (out).Without it 'add.__name__' would return 'out'.LRU CacheDecorator that caches function's return values. All function's arguments must be hashable.from functools import lru_cache@lru_cache(maxsize=None)def fib(n):    return n if n < 2 else fib(n-2) + fib(n-1)Default size of the cache is 128 values. Passing 'maxsize=None' makes it unbounded.CPython interpreter limits recursion depth to 1000 by default. To increase it use 'sys.setrecursionlimit(<depth>)'.Parametrized DecoratorA decorator that accepts arguments and returns a normal decorator that accepts a function.from functools import wrapsdef debug(print_result=False):    def decorator(func):        @wraps(func)        def out(*args, **kwargs):            result = func(*args, **kwargs)            print(func.__name__, result if print_result else '')            return result        return out    return decorator@debug(print_result=True)def add(x, y):    return x + yUsing only '@debug' to decorate the add() function would not work here, because debug would then receive the add() function as a 'print_result' argument. Decorators can however manually check if the argument they received is a function and act accordingly.Classclass <name>:    def __init__(self, a):        self.a = a    def __repr__(self):        class_name = self.__class__.__name__        return f'{class_name}({self.a!r})'    def __str__(self):        return str(self.a)    @classmethod    def get_class_name(cls):        return cls.__name__Return value of repr() should be unambiguous and of str() readable.If only repr() is defined, it will also be used for str().Methods decorated with '@staticmethod' do not receive 'self' nor 'cls' as their first arg.Expressions that call the str() method:print(<el>)f'{<el>}'logging.warning(<el>)csv.writer(<file>).writerow([<el>])raise Exception(<el>)Expressions that call the repr() method:print/str/repr([<el>])print/str/repr({<el>: <el>})f'{<el>!r}'Z = dataclasses.make_dataclass('Z', ['a']); print/str/repr(Z(<el>))>>> <el>Constructor Overloadingclass <name>:    def __init__(self, a=None):        self.a = aInheritanceclass Person:    def __init__(self, name, age):        self.name = name        self.age  = ageclass Employee(Person):    def __init__(self, name, age, staff_num):        super().__init__(name, age)        self.staff_num = staff_numMultiple Inheritanceclass A: passclass B: passclass C(A, B): passMRO determines the order in which parent classes are traversed when searching for a method or an attribute:>>> C.mro()[<class 'C'>, <class 'A'>, <class 'B'>, <class 'object'>]PropertyPythonic way of implementing getters and setters.class Person:    @property    def name(self):        return ' '.join(self._name)    @name.setter    def name(self, value):        self._name = value.split()>>> person = Person()>>> person.name = '\\t Guido  van Rossum \'>>> person.name'Guido van Rossum'DataclassDecorator that automatically generates init(), repr() and eq() special methods.from dataclasses import dataclass, field@dataclass(order=False, frozen=False)class <class_name>:    <attr_name>: <type>    <attr_name>: <type> = <default_value>    <attr_name>: list/dict/set = field(default_factory=list/dict/set)Objects can be made sortable with 'order=True' and immutable with 'frozen=True'.For object to be hashable, all attributes must be hashable and 'frozen' must be True.Function field() is needed because '<attr_name>: list = []' would make a list that is shared among all instances. Its 'default_factory' argument can be any callable.For attributes of arbitrary type use 'typing.Any'.Inline:from dataclasses import make_dataclass<class> = make_dataclass('<class_name>', <coll_of_attribute_names>)<class> = make_dataclass('<class_name>', <coll_of_tuples>)<tuple> = ('<attr_name>', <type> [, <default_value>])Rest of type annotations (CPython interpreter ignores them all):import collections.abc as abc, typing as tp<var_name>: list/set/abc.Iterable/abc.Sequence/tp.Optional[<type>] [= <obj>]<var_name>: dict/tuple/tp.Union[<type>, ...] [= <obj>]def func(<arg_name>: <type> [= <obj>]) -> <type>: ...SlotsMechanism that restricts objects to attributes listed in 'slots' and significantly reduces their memory footprint.class MyClassWithSlots:    __slots__ = ['a']    def __init__(self):        self.a = 1Copyfrom copy import copy, deepcopy<object> = copy(<object>)<object> = deepcopy(<object>)Duck TypesA duck type is an implicit type that prescribes a set of special methods. Any object that has those methods defined is considered a member of that duck type.ComparableIf eq() method is not overridden, it returns 'id(self) == id(other)', which is the same as 'self is other'.That means all objects compare not equal by default.Only the left side object has eq() method called, unless it returns NotImplemented, in which case the right object is consulted. False is returned if both return NotImplemented.Ne() automatically works on any object that has eq() defined.class MyComparable:    def __init__(self, a):        self.a = a    def __eq__(self, other):        if isinstance(other, type(self)):            return self.a == other.a        return NotImplementedHashableHashable object needs both hash() and eq() methods and its hash value should never change.Hashable objects that compare equal must have the same hash value, meaning default hash() that returns 'id(self)' will not do.That is why Python automatically makes classes unhashable if you only implement eq().class MyHashable:    def __init__(self, a):        self._a = a    @property    def a(self):        return self._a    def __eq__(self, other):        if isinstance(other, type(self)):            return self.a == other.a        return NotImplemented    def __hash__(self):        return hash(self.a)SortableWith 'total_ordering' decorator, you only need to provide eq() and one of lt(), gt(), le() or ge() special methods and the rest will be automatically generated.Functions sorted() and min() only require lt() method, while max() only requires gt(). However, it is best to define them all so that confusion doesn't arise in other contexts.When two lists, strings or dataclasses are compared, their values get compared in order until a pair of unequal values is found. The comparison of this two values is then returned. The shorter sequence is considered smaller in case of all values being equal.For proper alphabetical order pass 'key=locale.strxfrm' to sorted() after running 'locale.setlocale(locale.LC_COLLATE, \""en_US.UTF-8\"")'.from functools import total_ordering@total_orderingclass MySortable:    def __init__(self, a):        self.a = a    def __eq__(self, other):        if isinstance(other, type(self)):            return self.a == other.a        return NotImplemented    def __lt__(self, other):        if isinstance(other, type(self)):            return self.a < other.a        return NotImplementedIteratorAny object that has methods next() and iter() is an iterator.Next() should return next item or raise StopIteration.Iter() should return 'self'.class Counter:    def __init__(self):        self.i = 0    def __next__(self):        self.i += 1        return self.i    def __iter__(self):        return self>>> counter = Counter()>>> next(counter), next(counter), next(counter)(1, 2, 3)Python has many different iterator objects:Sequence iterators returned by the iter() function, such as list_iterator and set_iterator.Objects returned by the itertools module, such as count, repeat and cycle.Generators returned by the generator functions and generator expressions.File objects returned by the open() function, etc.CallableAll functions and classes have a call() method, hence are callable.When this cheatsheet uses '<function>' as an argument, it actually means '<callable>'.class Counter:    def __init__(self):        self.i = 0    def __call__(self):        self.i += 1        return self.i>>> counter = Counter()>>> counter(), counter(), counter()(1, 2, 3)Context ManagerWith statements only work with objects that have enter() and exit() special methods.Enter() should lock the resources and optionally return an object.Exit() should release the resources.Any exception that happens inside the with block is passed to the exit() method.The exit() method can suppress the exception by returning a true value.class MyOpen:    def __init__(self, filename):        self.filename = filename    def __enter__(self):        self.file = open(self.filename)        return self.file    def __exit__(self, exc_type, exception, traceback):        self.file.close()>>> with open('test.txt', 'w') as file:...     file.write('Hello World!')>>> with MyOpen('test.txt') as file:...     print(file.read())Hello World!Iterable Duck TypesIterableOnly required method is iter(). It should return an iterator of object's items.Contains() automatically works on any object that has iter() defined.class MyIterable:    def __init__(self, a):        self.a = a    def __iter__(self):        return iter(self.a)    def __contains__(self, el):        return el in self.a>>> obj = MyIterable([1, 2, 3])>>> [el for el in obj][1, 2, 3]>>> 1 in objTrueCollectionOnly required methods are iter() and len(). Len() should return the number of items.This cheatsheet actually means '<iterable>' when it uses '<collection>'.I chose not to use the name 'iterable' because it sounds scarier and more vague than 'collection'. The only drawback of this decision is that the reader could think a certain function doesn't accept iterators when it does, since iterators are the only built-in objects that are iterable but are not collections.class MyCollection:    def __init__(self, a):        self.a = a    def __iter__(self):        return iter(self.a)    def __contains__(self, el):        return el in self.a    def __len__(self):        return len(self.a)SequenceOnly required methods are getitem() and len().Getitem() should return an item at the passed index or raise IndexError.Iter() and contains() automatically work on any object that has getitem() defined.Reversed() automatically works on any object that has getitem() and len() defined.class MySequence:    def __init__(self, a):        self.a = a    def __iter__(self):        return iter(self.a)    def __contains__(self, el):        return el in self.a    def __len__(self):        return len(self.a)    def __getitem__(self, i):        return self.a[i]    def __reversed__(self):        return reversed(self.a)Discrepancies between glossary definitions and abstract base classes:Glossary defines iterable as any object with iter() or getitem() and sequence as any object with getitem() and len(). It does not define collection.Passing ABC Iterable to isinstance() or issubclass() checks whether object/class has method iter(), while ABC Collection checks for iter(), contains() and len().ABC SequenceIt's a richer interface than the basic sequence.Extending it generates iter(), contains(), reversed(), index() and count().Unlike 'abc.Iterable' and 'abc.Collection', it is not a duck type. That is why 'issubclass(MySequence, abc.Sequence)' would return False even if MySequence had all the methods defined. It however recognizes list, tuple, range, str, bytes, bytearray, array, memoryview and deque, because they are registered as its virtual subclasses.from collections import abcclass MyAbcSequence(abc.Sequence):    def __init__(self, a):        self.a = a    def __len__(self):        return len(self.a)    def __getitem__(self, i):        return self.a[i]Table of required and automatically available special methods:+------------+------------+------------+------------+--------------+|            |  Iterable  | Collection |  Sequence  | abc.Sequence |+------------+------------+------------+------------+--------------+| iter()     |    REQ     |    REQ     |    Yes     |     Yes      || contains() |    Yes     |    Yes     |    Yes     |     Yes      || len()      |            |    REQ     |    REQ     |     REQ      || getitem()  |            |            |    REQ     |     REQ      || reversed() |            |            |    Yes     |     Yes      || index()    |            |            |            |     Yes      || count()    |            |            |            |     Yes      |+------------+------------+------------+------------+--------------+Other ABCs that generate missing methods are: MutableSequence, Set, MutableSet, Mapping and MutableMapping.Names of their required methods are stored in '<abc>.__abstractmethods__'.Enumfrom enum import Enum, autoclass <enum_name>(Enum):    <member_name> = auto()    <member_name> = <value>    <member_name> = <value>, <value>Function auto() returns an increment of the last numeric value or 1.Accessing a member named after a reserved keyword causes SyntaxError.Methods receive the member they were called on as the 'self' argument.<member> = <enum>.<member_name>           # Returns a member.<member> = <enum>['<member_name>']        # Returns a member. Raises KeyError.<member> = <enum>(<value>)                # Returns a member. Raises ValueError.<str>    = <member>.name                  # Returns member's name.<obj>    = <member>.value                 # Returns member's value.<list>   = list(<enum>)                   # Returns enum's members.<list>   = [a.name for a in <enum>]       # Returns enum's member names.<list>   = [a.value for a in <enum>]      # Returns enum's member values.<member> = random.choice(list(<enum>))    # Returns a random member.def get_next_member(member):    members = list(type(member))    index = members.index(member) + 1    return members[index % len(members)]InlineCutlery = Enum('Cutlery', 'FORK KNIFE SPOON')Cutlery = Enum('Cutlery', ['FORK', 'KNIFE', 'SPOON'])Cutlery = Enum('Cutlery', {'FORK': 1, 'KNIFE': 2, 'SPOON': 3})User-defined functions cannot be values, so they must be wrapped:from functools import partialLogicOp = Enum('LogicOp', {'AND': partial(lambda l, r: l and r),                           'OR':  partial(lambda l, r: l or r)})Exceptionstry:    <code>except <exception>:    <code>Complex Exampletry:    <code_1>except <exception_a>:    <code_2_a>except <exception_b>:    <code_2_b>else:    <code_2_c>finally:    <code_3>Code inside the 'else' block will only be executed if 'try' block had no exceptions.Code inside the 'finally' block will always be executed (unless a signal is received).All variables that are initialized in executed blocks are also visible in all subsequent blocks, as well as outside the try/except clause (only function blocks delimit scope).To catch signals use 'signal.signal(signal_number, <func>)'.Catching Exceptionsexcept <exception>: ...except <exception> as <name>: ...except (<exception>, [...]): ...except (<exception>, [...]) as <name>: ...Also catches subclasses of the exception.Use 'traceback.print_exc()' to print the error message to stderr.Use 'print(<name>)' to print just the cause of the exception (its arguments).Use 'logging.exception(<message>)' to log the passed message, followed by the full error message of the caught exception.Raising Exceptionsraise <exception>raise <exception>()raise <exception>(<el> [, ...])Re-raising caught exception:except <exception> [as <name>]:    ...    raiseException Objectarguments = <name>.argsexc_type  = <name>.__class__filename  = <name>.__traceback__.tb_frame.f_code.co_filenamefunc_name = <name>.__traceback__.tb_frame.f_code.co_nameline      = linecache.getline(filename, <name>.__traceback__.tb_lineno)trace_str = ''.join(traceback.format_tb(<name>.__traceback__))error_msg = ''.join(traceback.format_exception(type(<name>), <name>, <name>.__traceback__))Built-in ExceptionsBaseException +-- SystemExit                   # Raised by the sys.exit() function. +-- KeyboardInterrupt            # Raised when the user hits the interrupt key (ctrl-c). +-- Exception                    # User-defined exceptions should be derived from this class.      +-- ArithmeticError         # Base class for arithmetic errors such as ZeroDivisionError.      +-- AssertionError          # Raised by `assert <exp>` if expression returns false value.      +-- AttributeError          # Raised when object doesn't have requested attribute/method.      +-- EOFError                # Raised by input() when it hits an end-of-file condition.      +-- LookupError             # Base class for errors when a collection can't find an item.      |    +-- IndexError         # Raised when a sequence index is out of range.      |    +-- KeyError           # Raised when a dictionary key or set element is missing.      +-- MemoryError             # Out of memory. Could be too late to start deleting vars.      +-- NameError               # Raised when nonexistent name (variable/func/class) is used.      |    +-- UnboundLocalError  # Raised when local name is used before it's being defined.      +-- OSError                 # Errors such as FileExistsError/PermissionError (see #Open).      |    +-- ConnectionError    # Errors such as BrokenPipeError/ConnectionAbortedError.      +-- RuntimeError            # Raised by errors that don't fall into other categories.      |    +-- NotImplementedErr  # Can be raised by abstract methods or by unfinished code.      |    +-- RecursionError     # Raised when the maximum recursion depth is exceeded.      +-- StopIteration           # Raised by next() when run on an empty iterator.      +-- TypeError               # Raised when an argument is of the wrong type.      +-- ValueError              # When argument has the right type but inappropriate value.Collections and their exceptions:+-----------+------------+------------+------------+|           |    List    |    Set     |    Dict    |+-----------+------------+------------+------------+| getitem() | IndexError |            |  KeyError  || pop()     | IndexError |  KeyError  |  KeyError  || remove()  | ValueError |  KeyError  |            || index()   | ValueError |            |            |+-----------+------------+------------+------------+Useful built-in exceptions:raise TypeError('Argument is of the wrong type!')raise ValueError('Argument has the right type but an inappropriate value!')raise RuntimeError('None of above!')User-defined Exceptionsclass MyError(Exception): passclass MyInputError(MyError): passExitExits the interpreter by raising SystemExit exception.import syssys.exit()                        # Exits with exit code 0 (success).sys.exit(<el>)                    # Prints to stderr and exits with 1.sys.exit(<int>)                   # Exits with passed exit code.Printprint(<el_1>, ..., sep=' ', end='\', file=sys.stdout, flush=False)Use 'file=sys.stderr' for messages about errors.Use 'flush=True' to forcibly flush the stream.Pretty Printfrom pprint import pprintpprint(<collection>, width=80, depth=None, compact=False, sort_dicts=True)Levels deeper than 'depth' get replaced by '...'.InputReads a line from the user input or pipe if present.<str> = input(prompt=None)Trailing newline gets stripped.Prompt string is printed to the standard output before reading input.Raises EOFError when user hits EOF (ctrl-d/ctrl-z⏎) or input stream gets exhausted.Command Line Argumentsimport sysscripts_path = sys.argv[0]arguments    = sys.argv[1:]Argument Parserfrom argparse import ArgumentParser, FileTypep = ArgumentParser(description=<str>)p.add_argument('-<short_name>', '--<name>', action='store_true')  # Flag.p.add_argument('-<short_name>', '--<name>', type=<type>)          # Option.p.add_argument('<name>', type=<type>, nargs=1)                    # First argument.p.add_argument('<name>', type=<type>, nargs='+')                  # Remaining arguments.p.add_argument('<name>', type=<type>, nargs='*')                  # Optional arguments.args  = p.parse_args()                                            # Exits on error.value = args.<name>Use 'help=<str>' to set argument description that will be displayed in help message.Use 'default=<el>' to set the default value.Use 'type=FileType(<mode>)' for files. Accepts 'encoding', but 'newline' is None.OpenOpens the file and returns a corresponding file object.<file> = open(<path>, mode='r', encoding=None, newline=None)'encoding=None' means that the default encoding is used, which is platform dependent. Best practice is to use 'encoding=\""utf-8\""' whenever possible.'newline=None' means all different end of line combinations are converted to '\' on read, while on write all '\' characters are converted to system's default line separator.'newline=\""\""' means no conversions take place, but input is still broken into chunks by readline() and readlines() on every '\', '\\r' and '\\r\'.Modes'r'  - Read (default).'w'  - Write (truncate).'x'  - Write or fail if the file already exists.'a'  - Append.'w+' - Read and write (truncate).'r+' - Read and write from the start.'a+' - Read and write from the end.'t'  - Text mode (default).'b'  - Binary mode ('br', 'bw', 'bx', …).Exceptions'FileNotFoundError' can be raised when reading with 'r' or 'r+'.'FileExistsError' can be raised when writing with 'x'.'IsADirectoryError' and 'PermissionError' can be raised by any.'OSError' is the parent class of all listed exceptions.File Object<file>.seek(0)                      # Moves to the start of the file.<file>.seek(offset)                 # Moves 'offset' chars/bytes from the start.<file>.seek(0, 2)                   # Moves to the end of the file.<bin_file>.seek(±offset, <anchor>)  # Anchor: 0 start, 1 current position, 2 end.<str/bytes> = <file>.read(size=-1)  # Reads 'size' chars/bytes or until EOF.<str/bytes> = <file>.readline()     # Returns a line or empty string/bytes on EOF.<list>      = <file>.readlines()    # Returns a list of remaining lines.<str/bytes> = next(<file>)          # Returns a line using buffer. Do not mix.<file>.write(<str/bytes>)           # Writes a string or bytes object.<file>.writelines(<collection>)     # Writes a coll. of strings or bytes objects.<file>.flush()                      # Flushes write buffer. Runs every 4096/8192 B.Methods do not add or strip trailing newlines, not even writelines().Read Text from Filedef read_file(filename):    with open(filename, encoding='utf-8') as file:        return file.readlines()Write Text to Filedef write_to_file(filename, text):    with open(filename, 'w', encoding='utf-8') as file:        file.write(text)Pathsimport os, globfrom pathlib import Path<str>  = os.getcwd()                # Returns the current working directory.<str>  = os.path.join(<path>, ...)  # Joins two or more pathname components.<str>  = os.path.realpath(<path>)   # Resolves symlinks and calls path.abspath().<str>  = os.path.basename(<path>)   # Returns final component of the path.<str>  = os.path.dirname(<path>)    # Returns path without the final component.<tup.> = os.path.splitext(<path>)   # Splits on last period of the final component.<list> = os.listdir(path='.')       # Returns filenames located at the path.<list> = glob.glob('<pattern>')     # Returns paths matching the wildcard pattern.<bool> = os.path.exists(<path>)     # Or: <Path>.exists()<bool> = os.path.isfile(<path>)     # Or: <DirEntry/Path>.is_file()<bool> = os.path.isdir(<path>)      # Or: <DirEntry/Path>.is_dir()<stat> = os.stat(<path>)            # Or: <DirEntry/Path>.stat()<real> = <stat>.st_mtime/st_size/…  # Modification time, size in bytes, ...DirEntryUnlike listdir(), scandir() returns DirEntry objects that cache isfile, isdir and on Windows also stat information, thus significantly increasing the performance of code that requires it.<iter> = os.scandir(path='.')       # Returns DirEntry objects located at the path.<str>  = <DirEntry>.path            # Returns the whole path as a string.<str>  = <DirEntry>.name            # Returns final component as a string.<file> = open(<DirEntry>)           # Opens the file and returns a file object.Path Object<Path> = Path(<path> [, ...])       # Accepts strings, Paths and DirEntry objects.<Path> = <path> / <path> [/ ...]    # First or second path must be a Path object.<Path> = <Path>.resolve()           # Returns absolute path with resolved symlinks.<Path> = Path()                     # Returns relative cwd. Also Path('.').<Path> = Path.cwd()                 # Returns absolute cwd. Also Path().resolve().<Path> = Path.home()                # Returns user's home directory (absolute).<Path> = Path(__file__).resolve()   # Returns script's path if cwd wasn't changed.<Path> = <Path>.parent              # Returns Path without the final component.<str>  = <Path>.name                # Returns final component as a string.<str>  = <Path>.stem                # Returns final component without extension.<str>  = <Path>.suffix              # Returns final component's extension.<tup.> = <Path>.parts               # Returns all components as strings.<iter> = <Path>.iterdir()           # Returns directory contents as Path objects.<iter> = <Path>.glob('<pattern>')   # Returns Paths matching the wildcard pattern.<str>  = str(<Path>)                # Returns path as a string.<file> = open(<Path>)               # Also <Path>.read/write_text/bytes().OS Commandsimport os, shutil, subprocessos.chdir(<path>)                    # Changes the current working directory.os.mkdir(<path>, mode=0o777)        # Creates a directory. Permissions are in octal.os.makedirs(<path>, mode=0o777)     # Creates all path's dirs. Also `exist_ok=False`.shutil.copy(from, to)               # Copies the file. 'to' can exist or be a dir.shutil.copy2(from, to)              # Also copies creation and modification time.shutil.copytree(from, to)           # Copies the directory. 'to' must not exist.os.rename(from, to)                 # Renames/moves the file or directory.os.replace(from, to)                # Same, but overwrites file 'to' even on Windows.shutil.move(from, to)               # Rename() that moves into 'to' if it's a dir.os.remove(<path>)                   # Deletes the file.os.rmdir(<path>)                    # Deletes the empty directory.shutil.rmtree(<path>)               # Deletes the directory.Paths can be either strings, Paths or DirEntry objects.Functions report OS related errors by raising either OSError or one of its subclasses.Shell Commands<pipe> = os.popen('<command>')      # Executes command in sh/cmd. Returns its stdout pipe.<str>  = <pipe>.read(size=-1)       # Reads 'size' chars or until EOF. Also readline/s().<int>  = <pipe>.close()             # Closes the pipe. Returns None on success (returncode 0).Sends '1 + 1' to the basic calculator and captures its output:>>> subprocess.run('bc', input='1 + 1\', capture_output=True, text=True)CompletedProcess(args='bc', returncode=0, stdout='2\', stderr='')Sends test.in to the basic calculator running in standard mode and saves its output to test.out:>>> from shlex import split>>> os.popen('echo 1 + 1 > test.in')>>> subprocess.run(split('bc -s'), stdin=open('test.in'), stdout=open('test.out', 'w'))CompletedProcess(args=['bc', '-s'], returncode=0)>>> open('test.out').read()'2\'JSONText file format for storing collections of strings and numbers.import json<str>    = json.dumps(<object>)     # Converts object to JSON string.<object> = json.loads(<str>)        # Converts JSON string to object.Read Object from JSON Filedef read_json_file(filename):    with open(filename, encoding='utf-8') as file:        return json.load(file)Write Object to JSON Filedef write_to_json_file(filename, an_object):    with open(filename, 'w', encoding='utf-8') as file:        json.dump(an_object, file, ensure_ascii=False, indent=2)PickleBinary file format for storing Python objects.import pickle<bytes>  = pickle.dumps(<object>)   # Converts object to bytes object.<object> = pickle.loads(<bytes>)    # Converts bytes object to object.Read Object from Filedef read_pickle_file(filename):    with open(filename, 'rb') as file:        return pickle.load(file)Write Object to Filedef write_to_pickle_file(filename, an_object):    with open(filename, 'wb') as file:        pickle.dump(an_object, file)CSVText file format for storing spreadsheets.import csvRead<reader> = csv.reader(<file>)       # Also: `dialect='excel', delimiter=','`.<list>   = next(<reader>)           # Returns next row as a list of strings.<list>   = list(<reader>)           # Returns a list of remaining rows.File must be opened with a 'newline=\""\""' argument, or newlines embedded inside quoted fields will not be interpreted correctly!To print the spreadsheet to the console use Tabulate library.For XML and binary Excel files (xlsx, xlsm and xlsb) use Pandas library.Reader accepts any iterator of strings, not just files.Write<writer> = csv.writer(<file>)       # Also: `dialect='excel', delimiter=','`.<writer>.writerow(<collection>)     # Encodes objects using `str(<el>)`.<writer>.writerows(<coll_of_coll>)  # Appends multiple rows.File must be opened with a 'newline=\""\""' argument, or '\\r' will be added in front of every '\' on platforms that use '\\r\' line endings!Parameters'dialect' - Master parameter that sets the default values. String or a 'csv.Dialect' object.'delimiter' - A one-character string used to separate fields.'quotechar' - Character for quoting fields that contain special characters.'doublequote' - Whether quotechars inside fields are/get doubled or escaped.'skipinitialspace' - Is space character at the start of the field stripped by the reader.'lineterminator' - How writer terminates rows. Reader is hardcoded to '\', '\\r', '\\r\'.'quoting' - 0: As necessary, 1: All, 2: All but numbers which are read as floats, 3: None.'escapechar' - Character for escaping quotechars if 'doublequote' is False.Dialects+------------------+--------------+--------------+--------------+|                  |     excel    |   excel-tab  |     unix     |+------------------+--------------+--------------+--------------+| delimiter        |       ','    |      '\\t'    |       ','    || quotechar        |       '\""'    |       '\""'    |       '\""'    || doublequote      |      True    |      True    |      True    || skipinitialspace |     False    |     False    |     False    || lineterminator   |    '\\r\'    |    '\\r\'    |      '\'    || quoting          |         0    |         0    |         1    || escapechar       |      None    |      None    |      None    |+------------------+--------------+--------------+--------------+Read Rows from CSV Filedef read_csv_file(filename, dialect='excel', **params):    with open(filename, encoding='utf-8', newline='') as file:        return list(csv.reader(file, dialect, **params))Write Rows to CSV Filedef write_to_csv_file(filename, rows, dialect='excel', **params):    with open(filename, 'w', encoding='utf-8', newline='') as file:        writer = csv.writer(file, dialect, **params)        writer.writerows(rows)SQLiteA server-less database engine that stores each database into a separate file.import sqlite3<conn> = sqlite3.connect(<path>)                # Opens existing or new file. Also ':memory:'.<conn>.close()                                  # Closes the connection.Read<cursor> = <conn>.execute('<query>')            # Can raise a subclass of sqlite3.Error.<tuple>  = <cursor>.fetchone()                  # Returns next row. Also next(<cursor>).<list>   = <cursor>.fetchall()                  # Returns remaining rows. Also list(<cursor>).Write<conn>.execute('<query>')                       # Can raise a subclass of sqlite3.Error.<conn>.commit()                                 # Saves all changes since the last commit.<conn>.rollback()                               # Discards all changes since the last commit.Or:with <conn>:                                    # Exits the block with commit() or rollback(),    <conn>.execute('<query>')                   # depending on whether any exception occurred.Placeholders<conn>.execute('<query>', <list/tuple>)         # Replaces '?'s in query with values.<conn>.execute('<query>', <dict/namedtuple>)    # Replaces ':<key>'s with values.<conn>.executemany('<query>', <coll_of_above>)  # Runs execute() multiple times.Passed values can be of type str, int, float, bytes, None, bool, datetime.date or datetime.datetime.Bools will be stored and returned as ints and dates as ISO formatted strings.ExampleValues are not actually saved in this example because 'conn.commit()' is omitted!>>> conn = sqlite3.connect('test.db')>>> conn.execute('CREATE TABLE person (person_id INTEGER PRIMARY KEY, name, height)')>>> conn.execute('INSERT INTO person VALUES (NULL, ?, ?)', ('Jean-Luc', 187)).lastrowid1>>> conn.execute('SELECT * FROM person').fetchall()[(1, 'Jean-Luc', 187)]SqlAlchemy# $ pip3 install sqlalchemyfrom sqlalchemy import create_engine, text<engine> = create_engine('<url>')               # Url: 'dialect://user:password@host/dbname'.<conn>   = <engine>.connect()                   # Creates a connection. Also <conn>.close().<cursor> = <conn>.execute(text('<query>'), …)   # Replaces ':<key>'s with keyword arguments.with <conn>.begin(): ...                        # Exits the block with commit or rollback.+------------+--------------+----------+----------------------------------+| Dialect    | pip3 install | import   |           Dependencies           |+------------+--------------+----------+----------------------------------+| mysql      | mysqlclient  | MySQLdb  | www.pypi.org/project/mysqlclient || postgresql | psycopg2     | psycopg2 | www.pypi.org/project/psycopg2    || mssql      | pyodbc       | pyodbc   | www.pypi.org/project/pyodbc      || oracle     | oracledb     | oracledb | www.pypi.org/project/oracledb    |+------------+--------------+----------+----------------------------------+BytesBytes object is an immutable sequence of single bytes. Mutable version is called bytearray.<bytes> = b'<str>'                          # Only accepts ASCII characters and \\x00-\\xff.<int>   = <bytes>[<index>]                  # Returns an int in range from 0 to 255.<bytes> = <bytes>[<slice>]                  # Returns bytes even if it has only one element.<bytes> = <bytes>.join(<coll_of_bytes>)     # Joins elements using bytes as a separator.Encode<bytes> = bytes(<coll_of_ints>)             # Ints must be in range from 0 to 255.<bytes> = bytes(<str>, 'utf-8')             # Or: <str>.encode('utf-8')<bytes> = <int>.to_bytes(n_bytes, …)        # `byteorder='big/little', signed=False`.<bytes> = bytes.fromhex('<hex>')            # Hex pairs can be separated by whitespaces.Decode<list>  = list(<bytes>)                     # Returns ints in range from 0 to 255.<str>   = str(<bytes>, 'utf-8')             # Or: <bytes>.decode('utf-8')<int>   = int.from_bytes(<bytes>, …)        # `byteorder='big/little', signed=False`.'<hex>' = <bytes>.hex()                     # Returns hex pairs. Accepts `sep=<str>`.Read Bytes from Filedef read_bytes(filename):    with open(filename, 'rb') as file:        return file.read()Write Bytes to Filedef write_bytes(filename, bytes_obj):    with open(filename, 'wb') as file:        file.write(bytes_obj)StructModule that performs conversions between a sequence of numbers and a bytes object.System’s type sizes, byte order, and alignment rules are used by default.from struct import pack, unpack<bytes> = pack('<format>', <el_1> [, ...])  # Packages arguments or raises struct.error.<tuple> = unpack('<format>', <bytes>)       # Use iter_unpack() for iterator of tuples.>>> pack('>hhl', 1, 2, 3)b'\\x00\\x01\\x00\\x02\\x00\\x00\\x00\\x03'>>> unpack('>hhl', b'\\x00\\x01\\x00\\x02\\x00\\x00\\x00\\x03')(1, 2, 3)FormatFor standard type sizes and manual alignment (padding) start format string with:'=' - System's byte order (usually little-endian).'<' - Little-endian.'>' - Big-endian (also '!').Besides numbers, pack() and unpack() also support bytes objects as part of the sequence:'c' - A bytes object with a single element. For pad byte use 'x'.'<n>s' - A bytes object with n elements.Integer types. Use a capital letter for unsigned type. Minimum and standard sizes are in brackets:'b' - char (1/1)'h' - short (2/2)'i' - int (2/4)'l' - long (4/4)'q' - long long (8/8)Floating point types (struct always uses standard sizes):'f' - float (4/4)'d' - double (8/8)ArrayList that can only hold numbers of a predefined type. Available types and their minimum sizes in bytes are listed above. Sizes and byte order are always determined by the system, however bytes of each element can be swapped with byteswap() method.from array import array<array> = array('<typecode>', <collection>)    # Array from collection of numbers.<array> = array('<typecode>', <bytes>)         # Array from bytes object.<array> = array('<typecode>', <array>)         # Treats array as a sequence of numbers.<bytes> = bytes(<array>)                       # Or: <array>.tobytes()<file>.write(<array>)                          # Writes array to the binary file.Memory ViewA sequence object that points to the memory of another bytes-like object.Each element can reference a single or multiple consecutive bytes, depending on format.Order and number of elements can be changed with slicing.Casting only works between char and other types and uses system's sizes.Byte order is always determined by the system.<mview> = memoryview(<bytes/bytearray/array>)  # Immutable if bytes, else mutable.<real>  = <mview>[<index>]                     # Returns an int or a float.<mview> = <mview>[<slice>]                     # Mview with rearranged elements.<mview> = <mview>.cast('<typecode>')           # Casts memoryview to the new format.<mview>.release()                              # Releases the object's memory buffer.<bytes> = bytes(<mview>)                       # Returns a new bytes object.<bytes> = <bytes>.join(<coll_of_mviews>)       # Joins mviews using bytes object as sep.<array> = array('<typecode>', <mview>)         # Treats mview as a sequence of numbers.<file>.write(<mview>)                          # Writes mview to the binary file.<list>  = list(<mview>)                        # Returns a list of ints or floats.<str>   = str(<mview>, 'utf-8')                # Treats mview as a bytes object.<int>   = int.from_bytes(<mview>, …)           # `byteorder='big/little', signed=False`.'<hex>' = <mview>.hex()                        # Treats mview as a bytes object.DequeA thread-safe list with efficient appends and pops from either side. Pronounced \""deck\"".from collections import deque<deque> = deque(<collection>)                  # Also `maxlen=None`.<deque>.appendleft(<el>)                       # Opposite element is dropped if full.<deque>.extendleft(<collection>)               # Collection gets reversed.<el> = <deque>.popleft()                       # Raises IndexError if empty.<deque>.rotate(n=1)                            # Rotates elements to the right.ThreadingCPython interpreter can only run a single thread at a time.That is why using multiple threads won't result in a faster execution, unless at least one of the threads contains an I/O operation.from threading import Thread, RLock, Semaphore, Event, Barrierfrom concurrent.futures import ThreadPoolExecutor, as_completedThread<Thread> = Thread(target=<function>)           # Use `args=<collection>` to set the arguments.<Thread>.start()                               # Starts the thread.<bool> = <Thread>.is_alive()                   # Checks if the thread has finished executing.<Thread>.join()                                # Waits for the thread to finish.Use 'kwargs=<dict>' to pass keyword arguments to the function.Use 'daemon=True', or the program will not be able to exit while the thread is alive.Lock<lock> = RLock()                               # Lock that can only be released by acquirer.<lock>.acquire()                               # Waits for the lock to be available.<lock>.release()                               # Makes the lock available again.Or:with <lock>:                                   # Enters the block by calling acquire() and    ...                                        # exits it with release(), even on error.Semaphore, Event, Barrier<Semaphore> = Semaphore(value=1)               # Lock that can be acquired by 'value' threads.<Event>     = Event()                          # Method wait() blocks until set() is called.<Barrier>   = Barrier(n_times)                 # Wait() blocks until it's called n_times.Queue<Queue> = queue.Queue(maxsize=0)               # A thread-safe FIFO queue. Also LifoQueue.<Queue>.put(<el>)                              # Blocks until queue stops being full.<Queue>.put_nowait(<el>)                       # Raises queue.Full exception if full.<el> = <Queue>.get()                           # Blocks until queue stops being empty.<el> = <Queue>.get_nowait()                    # Raises queue.Empty exception if empty.Thread Pool Executor<Exec> = ThreadPoolExecutor(max_workers=None)  # Or: `with ThreadPoolExecutor() as <name>: …`<iter> = <Exec>.map(<func>, <args_1>, ...)     # Multithreaded and non-lazy map(). Keeps order.<Futr> = <Exec>.submit(<func>, <arg_1>, ...)   # Creates a thread and returns its Future obj.<Exec>.shutdown(wait=True)                     # Blocks until all threads finish executing.<bool> = <Future>.done()                       # Checks if the thread has finished executing.<obj>  = <Future>.result(timeout=None)         # Waits for thread to finish and returns result.<bool> = <Future>.cancel()                     # Returns False if thread is already running.<iter> = as_completed(<coll_of_Futures>)       # Each Future is yielded as it completes.Map() and as_completed() also accept 'timeout' argument that causes TimeoutError if result isn't available in 'timeout' seconds after next() is called.Exceptions that happen inside threads are raised when next() is called on map's iterator or when result() is called on a Future. Its exception() method returns exception or None.ProcessPoolExecutor provides true parallelism, but everything sent to/from workers must be pickable. Queues must be sent using executor's 'initargs' and 'initializer' parameters.OperatorModule of functions that provide the functionality of operators. Functions are ordered by operator precedence, starting with least binding.import operator as op<bool> = op.not_(<obj>)                                        # not (or/and are not provided)<bool> = op.eq/ne/lt/le/gt/ge/contains(<obj>, <obj>)           # ==, !=, <, <=, >, >=, in<obj>  = op.or_/xor/and_(<int/set>, <int/set>)                 # |, ^, &<obj>  = op.add/sub/mul/truediv/floordiv/mod(<obj>, <obj>)     # +, -, *, /, //, %<num>  = op.neg/invert(<num>)                                  # -, ~<num>  = op.pow(<num>, <num>)                                  # **<func> = op.itemgetter/attrgetter/methodcaller(<obj> [, ...])  # [index/key], .name, .name()elementwise_sum  = map(op.add, list_a, list_b)sorted_by_second = sorted(<collection>, key=op.itemgetter(1))sorted_by_both   = sorted(<collection>, key=op.itemgetter(1, 0))product_of_elems = functools.reduce(op.mul, <collection>)union_of_sets    = functools.reduce(op.or_, <coll_of_sets>)first_element    = op.methodcaller('pop', 0)(<list>)Bitwise operators require objects to have and(), or() and xor() special methods, unlike logical operators that work on all types of objects.Also: '<bool> = <bool> &|^ <bool>' and '<int> = <bool> &|^ <int>'.Introspection<list> = dir()                             # Names of local variables, functions, classes, etc.<dict> = vars()                            # Dict of local variables, etc. Also locals().<dict> = globals()                         # Dict of global vars, etc. (incl. '__builtins__').Attributes<list> = dir(<object>)                     # Names of object's attributes (incl. methods).<dict> = vars(<object>)                    # Dict of writable attributes. Also <obj>.__dict__.<bool> = hasattr(<object>, '<attr_name>')  # Checks if getattr() raises an AttributeError.value  = getattr(<object>, '<attr_name>')  # Raises AttributeError if attribute is missing.setattr(<object>, '<attr_name>', value)    # Only works on objects with '__dict__' attribute.delattr(<object>, '<attr_name>')           # Same. Also `del <object>.<attr_name>`.Parameters<Sig>  = inspect.signature(<function>)     # Function's Signature object.<dict> = <Sig>.parameters                  # Dict of Parameter objects.<memb> = <Param>.kind                      # Member of ParameterKind enum.<obj>  = <Param>.default                   # Default value or Parameter.empty.<type> = <Param>.annotation                # Type or Parameter.empty.MetaprogrammingCode that generates code.TypeType is the root class. If only passed an object it returns its type (class). Otherwise it creates a new class.<class> = type('<class_name>', <tuple_of_parents>, <dict_of_class_attributes>)>>> Z = type('Z', (), {'a': 'abcde', 'b': 12345})>>> z = Z()Meta ClassA class that creates classes.def my_meta_class(name, parents, attrs):    attrs['a'] = 'abcde'    return type(name, parents, attrs)Or:class MyMetaClass(type):    def __new__(cls, name, parents, attrs):        attrs['a'] = 'abcde'        return type.__new__(cls, name, parents, attrs)New() is a class method that gets called before init(). If it returns an instance of its class, then that instance gets passed to init() as a 'self' argument.It receives the same arguments as init(), except for the first one that specifies the desired type of the returned instance (MyMetaClass in our case).Like in our case, new() can also be called directly, usually from a new() method of a child class (def __new__(cls): return super().__new__(cls)).The only difference between the examples above is that my_meta_class() returns a class of type type, while MyMetaClass() returns a class of type MyMetaClass.Metaclass AttributeRight before a class is created it checks if it has the 'metaclass' attribute defined. If not, it recursively checks if any of its parents has it defined and eventually comes to type().class MyClass(metaclass=MyMetaClass):    b = 12345>>> MyClass.a, MyClass.b('abcde', 12345)Type Diagramtype(MyClass) == MyMetaClass         # MyClass is an instance of MyMetaClass.type(MyMetaClass) == type            # MyMetaClass is an instance of type.+-------------+-------------+|   Classes   | Metaclasses |+-------------+-------------||   MyClass <-- MyMetaClass ||             |     ^       ||    object <----- type <+  ||             |     | +--+  ||     str <---------+       |+-------------+-------------+Inheritance DiagramMyClass.__base__ == object           # MyClass is a subclass of object.MyMetaClass.__base__ == type         # MyMetaClass is a subclass of type.+-------------+-------------+|   Classes   | Metaclasses |+-------------+-------------||   MyClass   | MyMetaClass ||      ^      |     ^       ||    object -----> type     ||      v      |             ||     str     |             |+-------------+-------------+Eval>>> from ast import literal_eval>>> literal_eval('[1, 2, 3]')[1, 2, 3]>>> literal_eval('1 + 2')ValueError: malformed node or stringCoroutinesCoroutines have a lot in common with threads, but unlike threads, they only give up control when they call another coroutine and they don’t use as much memory.Coroutine definition starts with 'async' and its call with 'await'.'asyncio.run(<coroutine>)' is the main entry point for asynchronous programs.Functions wait(), gather() and as_completed() start multiple coroutines at the same time.Asyncio module also provides its own Queue, Event, Lock and Semaphore classes.Runs a terminal game where you control an asterisk that must avoid numbers:import asyncio, collections, curses, curses.textpad, enum, randomP = collections.namedtuple('P', 'x y')         # PositionD = enum.Enum('D', 'n e s w')                  # DirectionW, H = 15, 7                                   # Width, Heightdef main(screen):    curses.curs_set(0)                         # Makes cursor invisible.    screen.nodelay(True)                       # Makes getch() non-blocking.    asyncio.run(main_coroutine(screen))        # Starts running asyncio code.async def main_coroutine(screen):    moves = asyncio.Queue()    state = {'*': P(0, 0), **{id_: P(W//2, H//2) for id_ in range(10)}}    ai    = [random_controller(id_, moves) for id_ in range(10)]    mvc   = [human_controller(screen, moves), model(moves, state), view(state, screen)]    tasks = [asyncio.create_task(cor) for cor in ai + mvc]    await asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED)async def random_controller(id_, moves):    while True:        d = random.choice(list(D))        moves.put_nowait((id_, d))        await asyncio.sleep(random.triangular(0.01, 0.65))async def human_controller(screen, moves):    while True:        key_mappings = {258: D.s, 259: D.n, 260: D.w, 261: D.e}        if d := key_mappings.get(screen.getch()):            moves.put_nowait(('*', d))        await asyncio.sleep(0.005)async def model(moves, state):    while state['*'] not in (state[id_] for id_ in range(10)):        id_, d = await moves.get()        x, y   = state[id_]        deltas = {D.n: P(0, -1), D.e: P(1, 0), D.s: P(0, 1), D.w: P(-1, 0)}        state[id_] = P((x + deltas[d].x) % W, (y + deltas[d].y) % H)async def view(state, screen):    offset = P(curses.COLS//2 - W//2, curses.LINES//2 - H//2)    while True:        screen.erase()        curses.textpad.rectangle(screen, offset.y-1, offset.x-1, offset.y+H, offset.x+W)        for id_, p in state.items():            screen.addstr(offset.y + (p.y - state['*'].y + H//2) % H,                          offset.x + (p.x - state['*'].x + W//2) % W, str(id_))        screen.refresh()        await asyncio.sleep(0.005)if __name__ == '__main__':    curses.wrapper(main)LibrariesProgress Bar# $ pip3 install tqdm>>> from tqdm import tqdm>>> from time import sleep>>> for el in tqdm([1, 2, 3], desc='Processing'):...     sleep(1)Processing: 100%|████████████████████| 3/3 [00:03<00:00,  1.00s/it]Plot# $ pip3 install matplotlibimport matplotlib.pyplot as pltplt.plot/bar/scatter(x_data, y_data [, label=<str>])  # Or: plt.plot(y_data)plt.legend()                                          # Adds a legend.plt.savefig(<path>)                                   # Saves the figure.plt.show()                                            # Displays the figure.plt.clf()                                             # Clears the figure.TablePrints a CSV file as an ASCII table:# $ pip3 install tabulateimport csv, tabulatewith open('test.csv', encoding='utf-8', newline='') as file:    rows   = csv.reader(file)    header = next(rows)    table  = tabulate.tabulate(rows, header)print(table)CursesRuns a basic file explorer in the terminal:import curses, osfrom curses import A_REVERSE, KEY_DOWN, KEY_UP, KEY_LEFT, KEY_RIGHT, KEY_ENTERdef main(screen):    ch, first, selected, paths = 0, 0, 0, os.listdir()    while ch != ord('q'):        height, width = screen.getmaxyx()        screen.erase()        for y, filename in enumerate(paths[first : first+height]):            color = A_REVERSE if filename == paths[selected] else 0            screen.addnstr(y, 0, filename, width-1, color)        ch = screen.getch()        selected += (ch == KEY_DOWN) - (ch == KEY_UP)        selected = max(0, min(len(paths)-1, selected))        first += (selected >= first + height) - (selected < first)        if ch in [KEY_LEFT, KEY_RIGHT, KEY_ENTER, ord('\'), ord('\\r')]:            new_dir = '..' if ch == KEY_LEFT else paths[selected]            if os.path.isdir(new_dir):                os.chdir(new_dir)                first, selected, paths = 0, 0, os.listdir()if __name__ == '__main__':    curses.wrapper(main)Loggingimport logginglogging.basicConfig(filename=<path>)              # Configures the root logger.logging.debug/info/warning/error/critical(<str>)  # Logs to the root logger.<Logger> = logging.getLogger(__name__)            # Logger named after the module.<Logger>.<level>(<str>)                           # Messages propagate to the root logger.<Logger>.exception(<str>)                         # Calls error() with caught exception.Setuplogging.basicConfig(    filename=None,                                # Logs to console by default.    format='%(levelname)s:%(name)s:%(message)s',  # Add `%(asctime)s` for datetime.    level=logging.WARNING,                        # Drops messages with lower priority.    handlers=[logging.StreamHandler()]            # Uses FileHandler if filename is set.)<Formatter> = logging.Formatter('<format>')       # Creates a Formatter.<Handler> = logging.FileHandler(<path>)           # Creates a Handler.<Handler>.setFormatter(<Formatter>)               # Adds Formatter to the Handler.<Handler>.setLevel(<int/str>)                     # Processes all messages by default.<Logger>.addHandler(<Handler>)                    # Adds Handler to the Logger.<Logger>.setLevel(<int/str>)                      # What is sent to handlers and parent.Parent logger can be specified by naming the child logger '<parent>.<name>'.Formatter also supports: pathname, filename, funcName, lineno, thread and process.A 'handlers.RotatingFileHandler' creates and deletes log files based on 'maxBytes' and 'backupCount' arguments.Creates a logger that writes all messages to a file and sends them to the root logger that prints to stdout:>>> logging.basicConfig(level='WARNING')>>> logger = logging.getLogger('my_module')>>> handler = logging.FileHandler('test.log')>>> formatter = logging.Formatter('%(asctime)s %(levelname)s:%(name)s:%(message)s')>>> handler.setFormatter(formatter)>>> logger.addHandler(handler)>>> logger.critical('Running out of disk space.')CRITICAL:my_module:Running out of disk space.>>> print(open('test.log').read())2023-02-07 23:21:01,430 CRITICAL:my_module:Running out of disk space.ScrapingScrapes Python's URL, version number and logo from its Wikipedia page:# $ pip3 install requests beautifulsoup4import requests, bs4, os, sysWIKI_URL = 'https://en.wikipedia.org/wiki/Python_(programming_language)'try:    html       = requests.get(WIKI_URL).text    document   = bs4.BeautifulSoup(html, 'html.parser')    table      = document.find('table', class_='infobox vevent')    python_url = table.find('th', text='Website').next_sibling.a['href']    version    = table.find('th', text='Stable release').next_sibling.strings.__next__()    logo_url   = table.find('img')['src']    logo       = requests.get(f'https:{logo_url}').content    filename   = os.path.basename(logo_url)    with open(filename, 'wb') as file:        file.write(logo)    print(f'{python_url}, {version}, file://{os.path.abspath(filename)}')except requests.exceptions.ConnectionError:    print(\""You've got problems with connection.\"", file=sys.stderr)WebFlask is a micro web framework/server. If you just want to open a html file in a web browser use 'webbrowser.open(<path>)' instead.# $ pip3 install flaskfrom flask import Flask, send_from_directory, render_template_string, requestapp = Flask(__name__)app.run(host=None, port=None, debug=None)Starts the app at 'http://localhost:5000'. Use 'host=\""0.0.0.0\""' to run externally.Install a WSGI server like Waitress and a HTTP server such as Nginx for better security.Debug mode restarts the app whenever script changes and displays errors in the browser.Static Request@app.route('/img/<path:filename>')def serve_file(filename):    return send_from_directory('dirname/', filename)Dynamic Request@app.route('/<sport>')def serve_html(sport):    return render_template_string('<h1>{{title}}</h1>', title=sport)To return an error code use 'abort(<int>)' and to redirect use 'redirect(<url>)'.'request.args[<str>]' returns parameter from the query string (URL part after '?').Use 'session[key] = value' to store session data like username, etc.REST Request@app.post('/<sport>/odds')def serve_json(sport):    team = request.form['team']    return {'team': team, 'odds': [2.09, 3.74, 3.68]}Starts the app in its own thread and queries it with a post request:# $ pip3 install requests>>> import threading, requests>>> threading.Thread(target=app.run, daemon=True).start()>>> url = 'http://localhost:5000/football/odds'>>> request_data = {'team': 'arsenal f.c.'}>>> response = requests.post(url, data=request_data)>>> response.json(){'team': 'arsenal f.c.', 'odds': [2.09, 3.74, 3.68]}Profilingfrom time import perf_counterstart_time = perf_counter()...duration_in_seconds = perf_counter() - start_timeTiming a Snippet>>> from timeit import timeit>>> timeit('list(range(10000))', number=1000, globals=globals(), setup='pass')0.19373Profiling by Line$ pip3 install line_profiler$ echo '@profiledef main():    a = list(range(10000))    b = set(range(10000))main()' > test.py$ kernprof -lv test.pyLine #   Hits     Time  Per Hit   % Time  Line Contents=======================================================     1                                    @profile     2                                    def main():     3      1    219.0    219.0     31.1      a = list(range(10000))     4      1    487.0    487.0     68.9      b = set(range(10000))Call and Flame Graphs$ pip3 install gprof2dot snakeviz; apt/brew install graphviz$ tail -n 4 test.py > test.py$ python3 -m cProfile -o test.prof test.py$ gprof2dot -f pstats test.prof | dot -Tpng -o test.png; xdg-open/open test.png$ snakeviz test.profSampling and Memory Profilers+--------------+-------------------------------+------------+----------+------+| pip3 install |          How to run           |   Target   |   Type   | Live |+--------------+-------------------------------+------------+----------+------+| py-spy       | py-spy top -- python3 test.py |    CPU     | Sampling | Yes  || pyinstrument | pyinstrument test.py          |    CPU     | Sampling | No   || scalene      | scalene test.py               | CPU+Memory | Sampling | No   || memray       | memray run --live test.py     |   Memory   | Tracing  | Yes  || filprofiler  | fil-profile run test.py       |   Memory   | Tracing  | No   |+--------------+-------------------------------+------------+----------+------+NumPyArray manipulation mini-language. It can run up to one hundred times faster than the equivalent Python code. An even faster alternative that runs on a GPU is called CuPy.# $ pip3 install numpyimport numpy as np<array> = np.array(<list/list_of_lists/…>)              # Returns a 1d/2d/… NumPy array.<array> = np.zeros/ones/empty(<shape>)                  # Also np.full(<shape>, <el>).<array> = np.arange(from_inc, to_exc, ±step)            # Also np.linspace(start, stop, len).<array> = np.random.randint(from_inc, to_exc, <shape>)  # Also np.random.random(<shape>).<view>  = <array>.reshape(<shape>)                      # Also `<array>.shape = <shape>`.<array> = <array>.flatten()                             # Also `<view> = <array>.ravel()`.<view>  = <array>.transpose()                           # Or: <array>.T<array> = np.copy/abs/sqrt/log/int64(<array>)           # Returns new array of the same shape.<array> = <array>.sum/max/mean/argmax/all(axis)         # Passed dimension gets aggregated.<array> = np.apply_along_axis(<func>, axis, <array>)    # Func can return a scalar or array.<array> = np.concatenate(<list_of_arrays>, axis=0)      # Links arrays along first axis (rows).<array> = np.row_stack/column_stack(<list_of_arrays>)   # Treats 1d arrays as rows or columns.<array> = np.tile/repeat(<array>, <int/list>)           # Tiles array or repeats its elements.Shape is a tuple of dimension sizes. A 100x50 RGB image has shape (50, 100, 3).Axis is an index of the dimension that gets aggregated. Leftmost dimension has index 0. Summing the RGB image along axis 2 will return a greyscale image with shape (50, 100).Indexing<el>       = <2d_array>[row_index, column_index]        # <3d_a>[table_i, row_i, column_i]<1d_view>  = <2d_array>[row_index]                      # <3d_a>[table_i, row_i]<1d_view>  = <2d_array>[:, column_index]                # <3d_a>[table_i, :, column_i]<2d_view>  = <2d_array>[rows_slice, columns_slice]      # <3d_a>[table_i, rows_s, columns_s]<2d_array> = <2d_array>[row_indexes]                    # <3d_a>[table_i/is, row_is]<2d_array> = <2d_array>[:, column_indexes]              # <3d_a>[table_i/is, :, column_is]<1d_array> = <2d_array>[row_indexes, column_indexes]    # <3d_a>[table_i/is, row_is, column_is]<1d_array> = <2d_array>[row_indexes, column_index]      # <3d_a>[table_i/is, row_is, column_i]<2d_bools> = <2d_array> ><== <el/1d/2d_array>           # 1d_array must have size of a row.<1d/2d_a>  = <2d_array>[<2d/1d_bools>]                  # 1d_bools must have size of a column.Indexes should not be tuples because Python converts 'obj[i, j]'  to 'obj[(i, j)]'!Any value that is broadcastable to the indexed shape can be assigned to the selection.BroadcastingSet of rules by which NumPy functions operate on arrays of different sizes and/or dimensions.left  = [[0.1], [0.6], [0.8]]                           # Shape: (3, 1)right = [ 0.1 ,  0.6 ,  0.8 ]                           # Shape: (3,)1. If array shapes differ in length, left-pad the shorter shape with ones:left  = [[0.1], [0.6], [0.8]]                           # Shape: (3, 1)right = [[0.1 ,  0.6 ,  0.8]]                           # Shape: (1, 3) <- !2. If any dimensions differ in size, expand the ones that have size 1 by duplicating their elements:left  = [[0.1,  0.1,  0.1],                             # Shape: (3, 3) <- !         [0.6,  0.6,  0.6],         [0.8,  0.8,  0.8]]right = [[0.1,  0.6,  0.8],                             # Shape: (3, 3) <- !         [0.1,  0.6,  0.8],         [0.1,  0.6,  0.8]]ExampleFor each point returns index of its nearest point ([0.1, 0.6, 0.8] => [1, 2, 1]):>>> points = np.array([0.1, 0.6, 0.8]) [ 0.1,  0.6,  0.8]>>> wrapped_points = points.reshape(3, 1)[[ 0.1], [ 0.6], [ 0.8]]>>> distances = wrapped_points - points[[ 0. , -0.5, -0.7], [ 0.5,  0. , -0.2], [ 0.7,  0.2,  0. ]]>>> distances = np.abs(distances)[[ 0. ,  0.5,  0.7], [ 0.5,  0. ,  0.2], [ 0.7,  0.2,  0. ]]>>> i = np.arange(3)[0, 1, 2]>>> distances[i, i] = np.inf[[ inf,  0.5,  0.7], [ 0.5,  inf,  0.2], [ 0.7,  0.2,  inf]]>>> distances.argmin(1)[1, 2, 1]Image# $ pip3 install pillowfrom PIL import Image, ImageDraw<Image> = Image.new('<mode>', (width, height))  # Also `color=<int/tuple/str>`.<Image> = Image.open(<path>)                    # Identifies format based on file contents.<Image> = <Image>.convert('<mode>')             # Converts image to the new mode.<Image>.save(<path>)                            # Selects format based on the path extension.<Image>.show()                                  # Opens image in the default preview app.<int/tuple> = <Image>.getpixel((x, y))          # Returns a pixel.<Image>.putpixel((x, y), <int/tuple>)           # Writes a pixel to the image.<ImagingCore> = <Image>.getdata()               # Returns a flattened view of the pixels.<Image>.putdata(<list/ImagingCore>)             # Writes a flattened sequence of pixels.<Image>.paste(<Image>, (x, y))                  # Writes passed image to the image.<Image> = <Image>.filter(<Filter>)              # `<Filter> = ImageFilter.<name>([<args>])`<Image> = <Enhance>.enhance(<float>)            # `<Enhance> = ImageEnhance.<name>(<Image>)`<array> = np.array(<Image>)                     # Creates NumPy array from the image.<Image> = Image.fromarray(np.uint8(<array>))    # Use <array>.clip(0, 255) to clip the values.Modes'1' - 1-bit pixels, black and white, stored with one pixel per byte.'L' - 8-bit pixels, greyscale.'RGB' - 3x8-bit pixels, true color.'RGBA' - 4x8-bit pixels, true color with transparency mask.'HSV' - 3x8-bit pixels, Hue, Saturation, Value color space.ExamplesCreates a PNG image of a rainbow gradient:WIDTH, HEIGHT = 100, 100n_pixels = WIDTH * HEIGHThues = (255 * i/n_pixels for i in range(n_pixels))img = Image.new('HSV', (WIDTH, HEIGHT))img.putdata([(int(h), 255, 255) for h in hues])img.convert('RGB').save('test.png')Adds noise to a PNG image and displays it:from random import randintadd_noise = lambda value: max(0, min(255, value + randint(-20, 20)))img = Image.open('test.png').convert('HSV')img.putdata([(add_noise(h), s, v) for h, s, v in img.getdata()])img.show()Image Draw<ImageDraw> = ImageDraw.Draw(<Image>)           # Object for adding 2D graphics to the image.<ImageDraw>.point((x, y))                       # Draws a point. Truncates floats into ints.<ImageDraw>.line((x1, y1, x2, y2 [, ...]))      # To get anti-aliasing use Image's resize().<ImageDraw>.arc((x1, y1, x2, y2), deg1, deg2)   # Always draws in clockwise direction.<ImageDraw>.rectangle((x1, y1, x2, y2))         # To rotate use Image's rotate() and paste().<ImageDraw>.polygon((x1, y1, x2, y2, ...))      # Last point gets connected to the first.<ImageDraw>.ellipse((x1, y1, x2, y2))           # To rotate use Image's rotate() and paste().<ImageDraw>.text((x, y), text, font=<Font>)     # `<Font> = ImageFont.truetype(<path>, size)`Use 'fill=<color>' to set the primary color.Use 'width=<int>' to set the width of lines or contours.Use 'outline=<color>' to set the color of the contours.Color can be an int, tuple, '#rrggbb[aa]' string or a color name.AnimationCreates a GIF of a bouncing ball:# $ pip3 install imageiofrom PIL import Image, ImageDrawimport imageioWIDTH, HEIGHT, R = 126, 126, 10frames = []for velocity in range(1, 16):    y = sum(range(velocity))    frame = Image.new('L', (WIDTH, HEIGHT))    draw  = ImageDraw.Draw(frame)    draw.ellipse((WIDTH/2-R, y, WIDTH/2+R, y+R*2), fill='white')    frames.append(frame)frames += reversed(frames[1:-1])imageio.mimsave('test.gif', frames, duration=0.03)Audioimport wave<Wave_read>  = wave.open('<path>', 'rb')        # Opens the WAV file.framerate    = <Wave_read>.getframerate()       # Number of frames per second.nchannels    = <Wave_read>.getnchannels()       # Number of samples per frame.sampwidth    = <Wave_read>.getsampwidth()       # Sample size in bytes.nframes      = <Wave_read>.getnframes()         # Number of frames.<params>     = <Wave_read>.getparams()          # Immutable collection of above.<bytes>      = <Wave_read>.readframes(nframes)  # Returns next 'nframes' frames.<Wave_write> = wave.open('<path>', 'wb')        # Truncates existing file.<Wave_write>.setframerate(<int>)                # 44100 for CD, 48000 for video.<Wave_write>.setnchannels(<int>)                # 1 for mono, 2 for stereo.<Wave_write>.setsampwidth(<int>)                # 2 for CD quality sound.<Wave_write>.setparams(<params>)                # Sets all parameters.<Wave_write>.writeframes(<bytes>)               # Appends frames to the file.Bytes object contains a sequence of frames, each consisting of one or more samples.In a stereo signal, the first sample of a frame belongs to the left channel.Each sample consists of one or more bytes that, when converted to an integer, indicate the displacement of a speaker membrane at a given moment.If sample width is one byte, then the integer should be encoded unsigned.For all other sizes, the integer should be encoded signed with little-endian byte order.Sample Values+-----------+-----------+------+-----------+| sampwidth |    min    | zero |    max    |+-----------+-----------+------+-----------+|     1     |         0 |  128 |       255 ||     2     |    -32768 |    0 |     32767 ||     3     |  -8388608 |    0 |   8388607 |+-----------+-----------+------+-----------+Read Float Samples from WAV Filedef read_wav_file(filename):    def get_int(bytes_obj):        an_int = int.from_bytes(bytes_obj, 'little', signed=(sampwidth != 1))        return an_int - 128 * (sampwidth == 1)    with wave.open(filename, 'rb') as file:        sampwidth = file.getsampwidth()        frames = file.readframes(-1)    bytes_samples = (frames[i : i+sampwidth] for i in range(0, len(frames), sampwidth))    return [get_int(b) / pow(2, sampwidth * 8 - 1) for b in bytes_samples]Write Float Samples to WAV Filedef write_to_wav_file(filename, float_samples, nchannels=1, sampwidth=2, framerate=44100):    def get_bytes(a_float):        a_float = max(-1, min(1 - 2e-16, a_float))        a_float += sampwidth == 1        a_float *= pow(2, sampwidth * 8 - 1)        return int(a_float).to_bytes(sampwidth, 'little', signed=(sampwidth != 1))    with wave.open(filename, 'wb') as file:        file.setnchannels(nchannels)        file.setsampwidth(sampwidth)        file.setframerate(framerate)        file.writeframes(b''.join(get_bytes(f) for f in float_samples))ExamplesSaves a 440 Hz sine wave to a mono WAV file:from math import pi, sinsamples_f = (sin(i * 2 * pi * 440 / 44100) for i in range(100_000))write_to_wav_file('test.wav', samples_f)Adds noise to a mono WAV file:from random import randomadd_noise = lambda value: value + (random() - 0.5) * 0.03samples_f = (add_noise(f) for f in read_wav_file('test.wav'))write_to_wav_file('test.wav', samples_f)Plays a WAV file:# $ pip3 install simpleaudiofrom simpleaudio import play_bufferwith wave.open('test.wav', 'rb') as file:    p = file.getparams()    frames = file.readframes(-1)    play_buffer(frames, p.nchannels, p.sampwidth, p.framerate)Text to Speech# $ pip3 install pyttsx3import pyttsx3engine = pyttsx3.init()engine.say('Sally sells seashells by the seashore.')engine.runAndWait()SynthesizerPlays Popcorn by Gershon Kingsley:# $ pip3 install simpleaudioimport array, itertools as it, math, simpleaudioF  = 44100P1 = '71♩,69♪,,71♩,66♪,,62♩,66♪,,59♩,,'P2 = '71♩,73♪,,74♩,73♪,,74♪,,71♪,,73♩,71♪,,73♪,,69♪,,71♩,69♪,,71♪,,67♪,,71♩,,'get_pause   = lambda seconds: it.repeat(0, int(seconds * F))sin_f       = lambda i, hz: math.sin(i * 2 * math.pi * hz / F)get_wave    = lambda hz, seconds: (sin_f(i, hz) for i in range(int(seconds * F)))get_hz      = lambda key: 8.176 * 2 ** (int(key) / 12)parse_note  = lambda note: (get_hz(note[:2]), 1/4 if '♩' in note else 1/8)get_samples = lambda note: get_wave(*parse_note(note)) if note else get_pause(1/8)samples_f   = it.chain.from_iterable(get_samples(n) for n in f'{P1},{P1},{P2}'.split(','))samples_i   = array.array('h', (int(f * 30000) for f in samples_f))simpleaudio.play_buffer(samples_i, 1, 2, F)Pygame# $ pip3 install pygameimport pygame as pgpg.init()screen = pg.display.set_mode((500, 500))rect = pg.Rect(240, 240, 20, 20)while not pg.event.get(pg.QUIT):    deltas = {pg.K_UP: (0, -20), pg.K_RIGHT: (20, 0), pg.K_DOWN: (0, 20), pg.K_LEFT: (-20, 0)}    for event in pg.event.get(pg.KEYDOWN):        dx, dy = deltas.get(event.key, (0, 0))        rect = rect.move((dx, dy))    screen.fill((0, 0, 0))    pg.draw.rect(screen, (255, 255, 255), rect)    pg.display.flip()RectangleObject for storing rectangular coordinates.<Rect> = pg.Rect(x, y, width, height)           # Floats get truncated into ints.<int>  = <Rect>.x/y/centerx/centery/…           # Top, right, bottom, left. Allows assignments.<tup.> = <Rect>.topleft/center/…                # Topright, bottomright, bottomleft. Same.<Rect> = <Rect>.move((delta_x, delta_y))        # Use move_ip() to move in-place.<bool> = <Rect>.collidepoint((x, y))            # Checks if rectangle contains the point.<bool> = <Rect>.colliderect(<Rect>)             # Checks if two rectangles overlap.<int>  = <Rect>.collidelist(<list_of_Rect>)     # Returns index of first colliding Rect or -1.<list> = <Rect>.collidelistall(<list_of_Rect>)  # Returns indexes of all colliding rectangles.SurfaceObject for representing images.<Surf> = pg.display.set_mode((width, height))   # Opens new window and returns its surface.<Surf> = pg.Surface((width, height))            # New RGB surface. RGBA if `flags=pg.SRCALPHA`.<Surf> = pg.image.load(<path/file>)             # Loads the image. Format depends on source.<Surf> = pg.surfarray.make_surface(<np_array>)  # Also `<np_arr> = surfarray.pixels3d(<Surf>)`.<Surf> = <Surf>.subsurface(<Rect>)              # Creates a new surface from the cutout.<Surf>.fill(color)                              # Tuple, Color('#rrggbb[aa]') or Color(<name>).<Surf>.set_at((x, y), color)                    # Updates pixel. Also <Surf>.get_at((x, y)).<Surf>.blit(<Surf>, (x, y))                     # Draws passed surface to the surface.from pygame.transform import scale, ...<Surf> = scale(<Surf>, (width, height))         # Returns scaled surface.<Surf> = rotate(<Surf>, anticlock_degrees)      # Returns rotated and scaled surface.<Surf> = flip(<Surf>, x_bool, y_bool)           # Returns flipped surface.from pygame.draw import line, ...line(<Surf>, color, (x1, y1), (x2, y2), width)  # Draws a line to the surface.arc(<Surf>, color, <Rect>, from_rad, to_rad)    # Also ellipse(<Surf>, color, <Rect>, width=0).rect(<Surf>, color, <Rect>, width=0)            # Also polygon(<Surf>, color, points, width=0).Font<Font> = pg.font.Font(<path/file>, size)        # Loads TTF file. Pass None for default font.<Surf> = <Font>.render(text, antialias, color)  # Background color can be specified at the end.Sound<Sound> = pg.mixer.Sound(<path/file/bytes>)     # Loads WAV file or array of signed shorts.<Sound>.play/stop()                             # Also <Sound>.set_volume(<float>).Basic Mario Brothers Exampleimport collections, dataclasses, enum, io, itertools as it, pygame as pg, urllib.requestfrom random import randintP = collections.namedtuple('P', 'x y')          # PositionD = enum.Enum('D', 'n e s w')                   # DirectionW, H, MAX_S = 50, 50, P(5, 10)                  # Width, Height, Max speeddef main():    def get_screen():        pg.init()        return pg.display.set_mode((W*16, H*16))    def get_images():        url = 'https://gto76.github.io/python-cheatsheet/web/mario_bros.png'        img = pg.image.load(io.BytesIO(urllib.request.urlopen(url).read()))        return [img.subsurface(get_rect(x, 0)) for x in range(img.get_width() // 16)]    def get_mario():        Mario = dataclasses.make_dataclass('Mario', 'rect spd facing_left frame_cycle'.split())        return Mario(get_rect(1, 1), P(0, 0), False, it.cycle(range(3)))    def get_tiles():        border = [(x, y) for x in range(W) for y in range(H) if x in [0, W-1] or y in [0, H-1]]        platforms = [(randint(1, W-2), randint(2, H-2)) for _ in range(W*H // 10)]        return [get_rect(x, y) for x, y in border + platforms]    def get_rect(x, y):        return pg.Rect(x*16, y*16, 16, 16)    run(get_screen(), get_images(), get_mario(), get_tiles())def run(screen, images, mario, tiles):    clock = pg.time.Clock()    pressed = set()    while not pg.event.get(pg.QUIT) and clock.tick(28):        keys = {pg.K_UP: D.n, pg.K_RIGHT: D.e, pg.K_DOWN: D.s, pg.K_LEFT: D.w}        pressed |= {keys.get(e.key) for e in pg.event.get(pg.KEYDOWN)}        pressed -= {keys.get(e.key) for e in pg.event.get(pg.KEYUP)}        update_speed(mario, tiles, pressed)        update_position(mario, tiles)        draw(screen, images, mario, tiles, pressed)def update_speed(mario, tiles, pressed):    x, y = mario.spd    x += 2 * ((D.e in pressed) - (D.w in pressed))    x += (x < 0) - (x > 0)    y += 1 if D.s not in get_boundaries(mario.rect, tiles) else (D.n in pressed) * -10    mario.spd = P(x=max(-MAX_S.x, min(MAX_S.x, x)), y=max(-MAX_S.y, min(MAX_S.y, y)))def update_position(mario, tiles):    x, y = mario.rect.topleft    n_steps = max(abs(s) for s in mario.spd)    for _ in range(n_steps):        mario.spd = stop_on_collision(mario.spd, get_boundaries(mario.rect, tiles))        mario.rect.topleft = x, y = x + (mario.spd.x / n_steps), y + (mario.spd.y / n_steps)def get_boundaries(rect, tiles):    deltas = {D.n: P(0, -1), D.e: P(1, 0), D.s: P(0, 1), D.w: P(-1, 0)}    return {d for d, delta in deltas.items() if rect.move(delta).collidelist(tiles) != -1}def stop_on_collision(spd, bounds):    return P(x=0 if (D.w in bounds and spd.x < 0) or (D.e in bounds and spd.x > 0) else spd.x,             y=0 if (D.n in bounds and spd.y < 0) or (D.s in bounds and spd.y > 0) else spd.y)def draw(screen, images, mario, tiles, pressed):    def get_marios_image_index():        if D.s not in get_boundaries(mario.rect, tiles):            return 4        return next(mario.frame_cycle) if {D.w, D.e} & pressed else 6    screen.fill((85, 168, 255))    mario.facing_left = (D.w in pressed) if {D.w, D.e} & pressed else mario.facing_left    screen.blit(images[get_marios_image_index() + mario.facing_left * 9], mario.rect)    for t in tiles:        screen.blit(images[18 if t.x in [0, (W-1)*16] or t.y in [0, (H-1)*16] else 19], t)    pg.display.flip()if __name__ == '__main__':    main()Pandas# $ pip3 install pandas matplotlibimport pandas as pd, matplotlib.pyplot as pltSeriesOrdered dictionary with a name.>>> pd.Series([1, 2], index=['x', 'y'], name='a')x    1y    2Name: a, dtype: int64<Sr> = pd.Series(<list>)                       # Assigns RangeIndex starting at 0.<Sr> = pd.Series(<dict>)                       # Takes dictionary's keys for index.<Sr> = pd.Series(<dict/Series>, index=<list>)  # Only keeps items with keys specified in index.<el> = <Sr>.loc[key]                           # Or: <Sr>.iloc[index]<Sr> = <Sr>.loc[keys]                          # Or: <Sr>.iloc[indexes]<Sr> = <Sr>.loc[from_key : to_key_inclusive]   # Or: <Sr>.iloc[from_i : to_i_exclusive]<el> = <Sr>[key/index]                         # Or: <Sr>.key<Sr> = <Sr>[keys/indexes]                      # Or: <Sr>[<keys_slice/slice>]<Sr> = <Sr>[bools]                             # Or: <Sr>.loc/iloc[bools]<Sr> = <Sr> ><== <el/Sr>                       # Returns a Series of bools.<Sr> = <Sr> +-*/ <el/Sr>                       # Items with non-matching keys get value NaN.<Sr> = pd.concat(<coll_of_Sr>)                 # Concats multiple Series into one long Series.<Sr> = <Sr>.combine_first(<Sr>)                # Adds items that are not yet present.<Sr>.update(<Sr>)                              # Updates items that are already present.<Sr>.plot.line/area/bar/pie/hist()             # Generates a Matplotlib plot.plt.show()                                     # Displays the plot. Also plt.savefig(<path>).Series — Aggregate, Transform, Map:<el> = <Sr>.sum/max/mean/idxmax/all()          # Or: <Sr>.agg(lambda <Sr>: <el>)<Sr> = <Sr>.rank/diff/cumsum/ffill/interpl()   # Or: <Sr>.agg/transform(lambda <Sr>: <Sr>)<Sr> = <Sr>.fillna(<el>)                       # Or: <Sr>.agg/transform/map(lambda <el>: <el>)>>> sr = pd.Series([1, 2], index=['x', 'y'])x    1y    2+---------------+-------------+-------------+---------------+|               |    'sum'    |   ['sum']   | {'s': 'sum'}  |+---------------+-------------+-------------+---------------+| sr.apply(…)   |      3      |    sum  3   |     s  3      || sr.agg(…)     |             |             |               |+---------------+-------------+-------------+---------------++---------------+-------------+-------------+---------------+|               |    'rank'   |   ['rank']  | {'r': 'rank'} |+---------------+-------------+-------------+---------------+| sr.apply(…)   |             |      rank   |               || sr.agg(…)     |     x  1    |   x     1   |    r  x  1    ||               |     y  2    |   y     2   |       y  2    |+---------------+-------------+-------------+---------------+Keys/indexes/bools can't be tuples because 'obj[x, y]' is converted to 'obj[(x, y)]'!Methods ffill(), interpolate(), fillna() and dropna() accept 'inplace=True'.Last result has a hierarchical index. Use '<Sr>[key_1, key_2]' to get its values.DataFrameTable with labeled rows and columns.>>> pd.DataFrame([[1, 2], [3, 4]], index=['a', 'b'], columns=['x', 'y'])   x  ya  1  2b  3  4<DF>    = pd.DataFrame(<list_of_rows>)         # Rows can be either lists, dicts or series.<DF>    = pd.DataFrame(<dict_of_columns>)      # Columns can be either lists, dicts or series.<el>    = <DF>.loc[row_key, column_key]        # Or: <DF>.iloc[row_index, column_index]<Sr/DF> = <DF>.loc[row_key/s]                  # Or: <DF>.iloc[row_index/es]<Sr/DF> = <DF>.loc[:, column_key/s]            # Or: <DF>.iloc[:, column_index/es]<DF>    = <DF>.loc[row_bools, column_bools]    # Or: <DF>.iloc[row_bools, column_bools]<Sr/DF> = <DF>[column_key/s]                   # Or: <DF>.column_key<DF>    = <DF>[row_bools]                      # Keeps rows as specified by bools.<DF>    = <DF>[<DF_of_bools>]                  # Assigns NaN to False values.<DF>    = <DF> ><== <el/Sr/DF>                 # Returns DF of bools. Sr is treated as a row.<DF>    = <DF> +-*/ <el/Sr/DF>                 # Items with non-matching keys get value NaN.<DF>    = <DF>.set_index(column_key)           # Replaces row keys with values from a column.<DF>    = <DF>.reset_index(drop=False)         # Drops or moves row keys to column named index.<DF>    = <DF>.sort_index(ascending=True)      # Sorts rows by row keys. Use `axis=1` for cols.<DF>    = <DF>.sort_values(column_key/s)       # Sorts rows by the passed column/s. Same.DataFrame — Merge, Join, Concat:>>> l = pd.DataFrame([[1, 2], [3, 4]], index=['a', 'b'], columns=['x', 'y'])   x  ya  1  2b  3  4>>> r = pd.DataFrame([[4, 5], [6, 7]], index=['b', 'c'], columns=['y', 'z'])   y  zb  4  5c  6  7+------------------------+---------------+------------+------------+--------------------------+|                        |    'outer'    |   'inner'  |   'left'   |       Description        |+------------------------+---------------+------------+------------+--------------------------+| l.merge(r, on='y',     |    x   y   z  | x   y   z  | x   y   z  | Merges on column if 'on' ||            how=…)      | 0  1   2   .  | 3   4   5  | 1   2   .  | or 'left/right_on' are   ||                        | 1  3   4   5  |            | 3   4   5  | set, else on shared cols.||                        | 2  .   6   7  |            |            | Uses 'inner' by default. |+------------------------+---------------+------------+------------+--------------------------+| l.join(r, lsuffix='l', |    x yl yr  z |            | x yl yr  z | Merges on row keys.      ||           rsuffix='r', | a  1  2  .  . | x yl yr  z | 1  2  .  . | Uses 'left' by default.  ||           how=…)       | b  3  4  4  5 | 3  4  4  5 | 3  4  4  5 | If r is a Series, it is  ||                        | c  .  .  6  7 |            |            | treated as a column.     |+------------------------+---------------+------------+------------+--------------------------+| pd.concat([l, r],      |    x   y   z  |     y      |            | Adds rows at the bottom. ||           axis=0,      | a  1   2   .  |     2      |            | Uses 'outer' by default. ||           join=…)      | b  3   4   .  |     4      |            | A Series is treated as a ||                        | b  .   4   5  |     4      |            | column. To add a row use ||                        | c  .   6   7  |     6      |            | pd.concat([l, DF([sr])]).|+------------------------+---------------+------------+------------+--------------------------+| pd.concat([l, r],      |    x  y  y  z |            |            | Adds columns at the      ||           axis=1,      | a  1  2  .  . | x  y  y  z |            | right end. Uses 'outer'  ||           join=…)      | b  3  4  4  5 | 3  4  4  5 |            | by default. A Series is  ||                        | c  .  .  6  7 |            |            | treated as a column.     |+------------------------+---------------+------------+------------+--------------------------+| l.combine_first(r)     |    x   y   z  |            |            | Adds missing rows and    ||                        | a  1   2   .  |            |            | columns. Also updates    ||                        | b  3   4   5  |            |            | items that contain NaN.  ||                        | c  .   6   7  |            |            | R must be a DataFrame.   |+------------------------+---------------+------------+------------+--------------------------+DataFrame — Aggregate, Transform, Map:<Sr> = <DF>.sum/max/mean/idxmax/all()          # Or: <DF>.apply/agg(lambda <Sr>: <el>)<DF> = <DF>.rank/diff/cumsum/ffill/interpl()   # Or: <DF>.apply/agg/transfrm(lambda <Sr>: <Sr>)<DF> = <DF>.fillna(<el>)                       # Or: <DF>.applymap(lambda <el>: <el>)All operations operate on columns by default. Pass 'axis=1' to process the rows instead.>>> df = pd.DataFrame([[1, 2], [3, 4]], index=['a', 'b'], columns=['x', 'y'])   x  ya  1  2b  3  4+-----------------+-------------+-------------+---------------+|                 |    'sum'    |   ['sum']   | {'x': 'sum'}  |+-----------------+-------------+-------------+---------------+| df.apply(…)     |             |       x  y  |               || df.agg(…)       |     x  4    |  sum  4  6  |     x  4      ||                 |     y  6    |             |               |+-----------------+-------------+-------------+---------------++-----------------+-------------+-------------+---------------+|                 |    'rank'   |   ['rank']  | {'x': 'rank'} |+-----------------+-------------+-------------+---------------+| df.apply(…)     |      x  y   |      x    y |        x      || df.agg(…)       |   a  1  1   |   rank rank |     a  1      || df.transform(…) |   b  2  2   | a    1    1 |     b  2      ||                 |             | b    2    2 |               |+-----------------+-------------+-------------+---------------+Use '<DF>[col_key_1, col_key_2][row_key]' to get the fifth result's values.DataFrame — Plot, Encode, Decode:<DF>.plot.line/area/bar/hist/scatter/box()     # Also: `x=column_key, y=column_key/s`.plt.show()                                     # Displays the plot. Also plt.savefig(<path>).<DF> = pd.read_json/html('<str/path/url>')     # Run `$ pip3 install beautifulsoup4 lxml`.<DF> = pd.read_csv/pickle/excel('<path/url>')  # Use `sheet_name=None` to get all Excel sheets.<DF> = pd.read_sql('<table/query>', <conn.>)   # Accepts SQLite3 or SQLAlchemy connection.<DF> = pd.read_clipboard()                     # Reads a copied table from the clipboard.<dict> = <DF>.to_dict(['d/l/s/…'])             # Returns columns as dicts, lists or series.<str>  = <DF>.to_json/html/csv([<path>])       # Also to_markdown/latex([<path>]).<DF>.to_pickle/excel(<path>)                   # Run `$ pip3 install \""pandas[excel]\"" odfpy`.<DF>.to_sql('<table_name>', <connection>)      # Accepts SQLite3 or SQLAlchemy connection.GroupByObject that groups together rows of a dataframe based on the value of the passed column.>>> df = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 6]], list('abc'), list('xyz'))>>> df.groupby('z').get_group(6)   x  y  zb  4  5  6c  7  8  6<GB> = <DF>.groupby(column_key/s)              # Splits DF into groups based on passed column.<DF> = <GB>.apply(<func>)                      # Maps each group. Func can return DF, Sr or el.<GB> = <GB>[column_key]                        # Single column GB. All operations return a Sr.GroupBy — Aggregate, Transform, Map:<DF> = <GB>.sum/max/mean/idxmax/all()          # Or: <GB>.agg(lambda <Sr>: <el>)<DF> = <GB>.rank/diff/cumsum/ffill()           # Or: <GB>.transform(lambda <Sr>: <Sr>)<DF> = <GB>.fillna(<el>)                       # Or: <GB>.transform(lambda <Sr>: <Sr>)>>> gb = df.groupby('z')      x  y  z3: a  1  2  36: b  4  5  6   c  7  8  6+-----------------+-------------+-------------+-------------+---------------+|                 |    'sum'    |    'rank'   |   ['rank']  | {'x': 'rank'} |+-----------------+-------------+-------------+-------------+---------------+| gb.agg(…)       |      x   y  |      x  y   |      x    y |        x      ||                 |  z          |   a  1  1   |   rank rank |     a  1      ||                 |  3   1   2  |   b  1  1   | a    1    1 |     b  1      ||                 |  6  11  13  |   c  2  2   | b    1    1 |     c  2      ||                 |             |             | c    2    2 |               |+-----------------+-------------+-------------+-------------+---------------+| gb.transform(…) |      x   y  |      x  y   |             |               ||                 |  a   1   2  |   a  1  1   |             |               ||                 |  b  11  13  |   b  1  1   |             |               ||                 |  c  11  13  |   c  2  2   |             |               |+-----------------+-------------+-------------+-------------+---------------+RollingObject for rolling window calculations.<RSr/RDF/RGB> = <Sr/DF/GB>.rolling(win_size)   # Also: `min_periods=None, center=False`.<RSr/RDF/RGB> = <RDF/RGB>[column_key/s]        # Or: <RDF/RGB>.column_key<Sr/DF>       = <R>.mean/sum/max()             # Or: <R>.apply/agg(<agg_func/str>)Plotly# $ pip3 install plotly kaleidofrom plotly.express import line<Figure> = line(<DF>, x=<col_name>, y=<col_name>)           # Or: line(x=<list>, y=<list>)<Figure>.update_layout(margin=dict(t=0, r=0, b=0, l=0), …)  # `paper_bgcolor='rgb(0, 0, 0)'`.<Figure>.write_html/json/image('<path>')                    # Also <Figure>.show().Displays a line chart of total coronavirus deaths per million grouped by continent:covid = pd.read_csv('https://covid.ourworldindata.org/data/owid-covid-data.csv',                    usecols=['iso_code', 'date', 'total_deaths', 'population'])continents = pd.read_csv('https://gist.githubusercontent.com/stevewithington/20a69c0b6d2ff'                         '846ea5d35e5fc47f26c/raw/country-and-continent-codes-list-csv.csv',                         usecols=['Three_Letter_Country_Code', 'Continent_Name'])df = pd.merge(covid, continents, left_on='iso_code', right_on='Three_Letter_Country_Code')df = df.groupby(['Continent_Name', 'date']).sum().reset_index()df['Total Deaths per Million'] = df.total_deaths * 1e6 / df.populationdf = df[df.date > '2020-03-14']df = df.rename({'date': 'Date', 'Continent_Name': 'Continent'}, axis='columns')line(df, x='Date', y='Total Deaths per Million', color='Continent').show()Displays a multi-axis line chart of total coronavirus cases and changes in prices of Bitcoin, Dow Jones and gold:import pandas as pd, plotly.graph_objects as godef main():    display_data(wrangle_data(*scrape_data()))def scrape_data():    def scrape_covid():        url = 'https://covid.ourworldindata.org/data/owid-covid-data.csv'        df = pd.read_csv(url, usecols=['location', 'date', 'total_cases'])        return df[df.location == 'World'].set_index('date').total_cases    def scrape_yahoo(slug):        url = (f'https://query1.finance.yahoo.com/v7/finance/download/{slug}?'               'period1=1579651200&period2=9999999999&interval=1d&events=history')        df = pd.read_csv(url, usecols=['Date', 'Close'])        return df.set_index('Date').Close    out = scrape_covid(), scrape_yahoo('BTC-USD'), scrape_yahoo('GC=F'), scrape_yahoo('^DJI')    return map(pd.Series.rename, out, ['Total Cases', 'Bitcoin', 'Gold', 'Dow Jones'])def wrangle_data(covid, bitcoin, gold, dow):    df = pd.concat([bitcoin, gold, dow], axis=1)  # Joins columns on dates.    df = df.sort_index().interpolate()            # Sorts by date and interpolates NaN-s.    df = df.loc['2020-02-23':]                    # Discards rows before '2020-02-23'.    df = (df / df.iloc[0]) * 100                  # Calculates percentages relative to day 1.    df = df.join(covid)                           # Adds column with covid cases.    return df.sort_values(df.index[-1], axis=1)   # Sorts columns by last day's value.def display_data(df):    figure = go.Figure()    for col_name in reversed(df.columns):        yaxis = 'y1' if col_name == 'Total Cases' else 'y2'        trace = go.Scatter(x=df.index, y=df[col_name], name=col_name, yaxis=yaxis)        figure.add_trace(trace)    figure.update_layout(        yaxis1=dict(title='Total Cases', rangemode='tozero'),        yaxis2=dict(title='%', rangemode='tozero', overlaying='y', side='right'),        legend=dict(x=1.1),        height=450    )    figure.show()if __name__ == '__main__':    main()PySimpleGUI# $ pip3 install PySimpleGUIimport PySimpleGUI as sglayout = [[sg.Text(\""What's your name?\"")], [sg.Input()], [sg.Button('Ok')]]window = sg.Window('Window Title', layout)event, values = window.read()print(f'Hello {values[0]}!' if event == 'Ok' else '')AppendixCythonLibrary that compiles Python code into C.# $ pip3 install cythonimport pyximport; pyximport.install()import <cython_script><cython_script>.main()Definitions:All 'cdef' definitions are optional, but they contribute to the speed-up.Script needs to be saved with a 'pyx' extension.cdef <ctype> <var_name> = <el>cdef <ctype>[n_elements] <var_name> = [<el>, <el>, ...]cdef <ctype/void> <func_name>(<ctype> <arg_name>): ...cdef class <class_name>:    cdef public <ctype> <attr_name>    def __init__(self, <ctype> <arg_name>):        self.<attr_name> = <arg_name>cdef enum <enum_name>: <member_name>, <member_name>, ...Virtual EnvironmentsSystem for installing libraries directly into project's directory.$ python3 -m venv <name>      # Creates virtual environment in current directory.$ source <name>/bin/activate  # Activates venv. On Windows run `<name>\\Scripts\\activate`.$ pip3 install <library>      # Installs the library into active environment.$ python3 <path>              # Runs the script in active environment. Also `./<path>`.$ deactivate                  # Deactivates virtual environment.Basic Script Template#!/usr/bin/env python3## Usage: .py#from sys import argv, exitfrom collections import defaultdict, namedtuplefrom dataclasses import make_dataclassfrom enum import Enumimport functools as ft, itertools as it, operator as op, redef main():    pass#####  UTIL#def read_file(filename):    with open(filename, encoding='utf-8') as file:        return file.readlines()if __name__ == '__main__':    main()IndexOnly available in the PDF.Ctrl+F / ⌘F is usually sufficient.Searching '#<title>' on the webpage will limit the search to the titles."
34,facebookresearch/fairseq,https://github.com/facebookresearch/fairseq/blob/main/README.md,Python,"                  Fairseq(-py) is a sequence modeling toolkit that allows researchers anddevelopers to train custom models for translation, summarization, languagemodeling and other text generation tasks.We provide reference implementations of various sequence modeling papers:List of implemented papersConvolutional Neural Networks (CNN)Language Modeling with Gated Convolutional Networks (Dauphin et al., 2017)Convolutional Sequence to Sequence Learning (Gehring et al., 2017)Classical Structured Prediction Losses for Sequence to Sequence Learning (Edunov et al., 2018)Hierarchical Neural Story Generation (Fan et al., 2018)wav2vec: Unsupervised Pre-training for Speech Recognition (Schneider et al., 2019)LightConv and DynamicConv modelsPay Less Attention with Lightweight and Dynamic Convolutions (Wu et al., 2019)Long Short-Term Memory (LSTM) networksEffective Approaches to Attention-based Neural Machine Translation (Luong et al., 2015)Transformer (self-attention) networksAttention Is All You Need (Vaswani et al., 2017)Scaling Neural Machine Translation (Ott et al., 2018)Understanding Back-Translation at Scale (Edunov et al., 2018)Adaptive Input Representations for Neural Language Modeling (Baevski and Auli, 2018)Lexically constrained decoding with dynamic beam allocation (Post & Vilar, 2018)Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context (Dai et al., 2019)Adaptive Attention Span in Transformers (Sukhbaatar et al., 2019)Mixture Models for Diverse Machine Translation: Tricks of the Trade (Shen et al., 2019)RoBERTa: A Robustly Optimized BERT Pretraining Approach (Liu et al., 2019)Facebook FAIR's WMT19 News Translation Task Submission (Ng et al., 2019)Jointly Learning to Align and Translate with Transformer Models (Garg et al., 2019)Multilingual Denoising Pre-training for Neural Machine Translation (Liu et at., 2020)Neural Machine Translation with Byte-Level Subwords (Wang et al., 2020)Unsupervised Quality Estimation for Neural Machine Translation (Fomicheva et al., 2020)wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations (Baevski et al., 2020)Generating Medical Reports from Patient-Doctor Conversations Using Sequence-to-Sequence Models (Enarvi et al., 2020)Linformer: Self-Attention with Linear Complexity (Wang et al., 2020)Cross-lingual Retrieval for Iterative Self-Supervised Training (Tran et al., 2020)Deep Transformers with Latent Depth (Li et al., 2020)Unsupervised Cross-lingual Representation Learning for Speech Recognition (Conneau et al., 2020)Self-training and Pre-training are Complementary for Speech Recognition (Xu et al., 2020)Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training (Hsu, et al., 2021)Unsupervised Speech Recognition (Baevski, et al., 2021)Simple and Effective Zero-shot Cross-lingual Phoneme Recognition (Xu et al., 2021)VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding (Xu et. al., 2021)VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding (Xu et. al., 2021)NormFormer: Improved Transformer Pretraining with Extra Normalization (Shleifer et. al, 2021)Non-autoregressive TransformersNon-Autoregressive Neural Machine Translation (Gu et al., 2017)Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement (Lee et al. 2018)Insertion Transformer: Flexible Sequence Generation via Insertion Operations (Stern et al. 2019)Mask-Predict: Parallel Decoding of Conditional Masked Language Models (Ghazvininejad et al., 2019)Levenshtein Transformer (Gu et al., 2019)FinetuningBetter Fine-Tuning by Reducing Representational Collapse (Aghajanyan et al. 2020)What's New:May 2023 Released models for Scaling Speech Technology to 1,000+ Languages  (Pratap, et al., 2023)June 2022 Released code for wav2vec-U 2.0 from Towards End-to-end Unsupervised Speech Recognition (Liu, et al., 2022)May 2022 Integration with xFormersDecember 2021 Released Direct speech-to-speech translation codeOctober 2021 Released VideoCLIP and VLM modelsOctober 2021 Released multilingual finetuned XLSR-53 modelSeptember 2021 master branch renamed to main.July 2021 Released DrNMT codeJuly 2021 Released Robust wav2vec 2.0 modelJune 2021 Released XLMR-XL and XLMR-XXL modelsMay 2021 Released Unsupervised Speech Recognition codeMarch 2021 Added full parameter and optimizer state sharding + CPU offloadingFebruary 2021 Added LASER training codeDecember 2020: Added Adaptive Attention Span codeDecember 2020: GottBERT model and code releasedNovember 2020: Adopted the Hydra configuration frameworksee documentation explaining how to use it for new and existing projectsNovember 2020: fairseq 0.10.0 releasedOctober 2020: Added R3F/R4F (Better Fine-Tuning) codeOctober 2020: Deep Transformer with Latent Depth code releasedOctober 2020: Added CRISS models and codePrevious updatesSeptember 2020: Added Linformer codeSeptember 2020: Added pointer-generator networksAugust 2020: Added lexically constrained decodingAugust 2020: wav2vec2 models and code releasedJuly 2020: Unsupervised Quality Estimation code releasedMay 2020: Follow fairseq on TwitterApril 2020: Monotonic Multihead Attention code releasedApril 2020: Quant-Noise code releasedApril 2020: Initial model parallel support and 11B parameters unidirectional LM releasedMarch 2020: Byte-level BPE code releasedFebruary 2020: mBART model and code releasedFebruary 2020: Added tutorial for back-translationDecember 2019: fairseq 0.9.0 releasedNovember 2019: VizSeq released (a visual analysis toolkit for evaluating fairseq models)November 2019: CamemBERT model and code releasedNovember 2019: BART model and code releasedNovember 2019: XLM-R models and code releasedSeptember 2019: Nonautoregressive translation code releasedAugust 2019: WMT'19 models releasedJuly 2019: fairseq relicensed under MIT licenseJuly 2019: RoBERTa models and code releasedJune 2019: wav2vec models and code releasedFeatures:multi-GPU training on one machine or across multiple machines (data and model parallel)fast generation on both CPU and GPU with multiple search algorithms implemented:beam searchDiverse Beam Search (Vijayakumar et al., 2016)sampling (unconstrained, top-k and top-p/nucleus)lexically constrained decoding (Post & Vilar, 2018)gradient accumulation enables training with large mini-batches even on a single GPUmixed precision training (trains faster with less GPU memory on NVIDIA tensor cores)extensible: easily register new models, criterions, tasks, optimizers and learning rate schedulersflexible configuration based on Hydra allowing a combination of code, command-line and file based configurationfull parameter and optimizer state shardingoffloading parameters to CPUWe also provide pre-trained models for translation and language modelingwith a convenient torch.hub interface:en2de = torch.hub.load('pytorch/fairseq', 'transformer.wmt19.en-de.single_model')en2de.translate('Hello world', beam=5)# 'Hallo Welt'See the PyTorch Hub tutorials for translationand RoBERTa for more examples.Requirements and InstallationPyTorch version >= 1.10.0Python version >= 3.8For training new models, you'll also need an NVIDIA GPU and NCCLTo install fairseq and develop locally:git clone https://github.com/pytorch/fairseqcd fairseqpip install --editable ./# on MacOS:# CFLAGS=\""-stdlib=libc++\"" pip install --editable ./# to install the latest stable release (0.10.x)# pip install fairseqFor faster training install NVIDIA's apex library:git clone https://github.com/NVIDIA/apexcd apexpip install -v --no-cache-dir --global-option=\""--cpp_ext\"" --global-option=\""--cuda_ext\"" \\  --global-option=\""--deprecated_fused_adam\"" --global-option=\""--xentropy\"" \\  --global-option=\""--fast_multihead_attn\"" ./For large datasets install PyArrow: pip install pyarrowIf you use Docker make sure to increase the shared memory size either with --ipc=host or --shm-sizeas command line options to nvidia-docker run .Getting StartedThe full documentation contains instructionsfor getting started, training new models and extending fairseq with new modeltypes and tasks.Pre-trained models and examplesWe provide pre-trained models and pre-processed, binarized test sets for several tasks listed below,as well as example training and evaluation commands.Translation: convolutional and transformer models are availableLanguage Modeling: convolutional and transformer models are availableWe also have more detailed READMEs to reproduce results from specific papers:XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale (Babu et al., 2021)Cross-lingual Retrieval for Iterative Self-Supervised Training (Tran et al., 2020)wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations (Baevski et al., 2020)Unsupervised Quality Estimation for Neural Machine Translation (Fomicheva et al., 2020)Training with Quantization Noise for Extreme Model Compression ({Fan*, Stock*} et al., 2020)Neural Machine Translation with Byte-Level Subwords (Wang et al., 2020)Multilingual Denoising Pre-training for Neural Machine Translation (Liu et at., 2020)Reducing Transformer Depth on Demand with Structured Dropout (Fan et al., 2019)Jointly Learning to Align and Translate with Transformer Models (Garg et al., 2019)Levenshtein Transformer (Gu et al., 2019)Facebook FAIR's WMT19 News Translation Task Submission (Ng et al., 2019)RoBERTa: A Robustly Optimized BERT Pretraining Approach (Liu et al., 2019)wav2vec: Unsupervised Pre-training for Speech Recognition (Schneider et al., 2019)Mixture Models for Diverse Machine Translation: Tricks of the Trade (Shen et al., 2019)Pay Less Attention with Lightweight and Dynamic Convolutions (Wu et al., 2019)Understanding Back-Translation at Scale (Edunov et al., 2018)Classical Structured Prediction Losses for Sequence to Sequence Learning (Edunov et al., 2018)Hierarchical Neural Story Generation (Fan et al., 2018)Scaling Neural Machine Translation (Ott et al., 2018)Convolutional Sequence to Sequence Learning (Gehring et al., 2017)Language Modeling with Gated Convolutional Networks (Dauphin et al., 2017)Join the fairseq communityTwitter: https://twitter.com/fairseqFacebook page: https://www.facebook.com/groups/fairseq.usersGoogle group: https://groups.google.com/forum/#!forum/fairseq-usersLicensefairseq(-py) is MIT-licensed.The license applies to the pre-trained models as well.CitationPlease cite as:@inproceedings{ott2019fairseq,  title = {fairseq: A Fast, Extensible Toolkit for Sequence Modeling},  author = {Myle Ott and Sergey Edunov and Alexei Baevski and Angela Fan and Sam Gross and Nathan Ng and David Grangier and Michael Auli},  booktitle = {Proceedings of NAACL-HLT 2019: Demonstrations},  year = {2019},}"
35,Jack-Cherish/python-spider,https://github.com/Jack-Cherish/python-spider/blob/master/README.md,Python,注：2020年最新连载教程请移步：Python Spider 2020Python Spider原创文章每周最少两篇，后续最新文章会在【公众号】首发，视频【B站】首发，大家可以加我【微信】进交流群，技术交流或提意见都可以，欢迎Star！              声明代码、教程仅限于学习交流，请勿用于任何商业用途！目录爬虫小工具文件下载小助手爬虫实战笔趣看小说下载百度文库免费文章下载助手_rev1百度文库免费文章下载助手_rev2《帅啊》网帅哥图片下载构建代理IP池《火影忍者》漫画下载财务报表下载小助手一小时入门网络爬虫抖音App视频下载GEETEST验证码识别12306抢票小助手百万英雄答题辅助系统网易云音乐免费音乐批量下载B站免费视频和弹幕批量下载京东商品晒单图下载正方教务管理系统个人信息查询其它爬虫小工具downloader.py:文件下载小助手一个可以用于下载图片、视频、文件的小工具，有下载进度显示功能。稍加修改即可添加到自己的爬虫中。动态示意图：爬虫实战biqukan.py:《笔趣看》盗版小说网站，爬取小说工具第三方依赖库安装： pip3 install beautifulsoup4使用方法： python biqukan.pybaiduwenku.py: 百度文库word文章爬取原理说明：http://blog.csdn.net/c406495762/article/details/72331737代码不完善，没有进行打包，不具通用性，纯属娱乐。shuaia.py: 爬取《帅啊》网，帅哥图片《帅啊》网URL：http://www.shuaia.net/index.html原理说明：http://blog.csdn.net/c406495762/article/details/72597755第三方依赖库安装： pip3 install requests beautifulsoup4daili.py: 构建代理IP池原理说明：http://blog.csdn.net/c406495762/article/details/72793480carton: 使用Scrapy爬取《火影忍者》漫画代码可以爬取整个《火影忍者》漫画所有章节的内容，保存到本地。更改地址，可以爬取其他漫画。保存地址可以在settings.py中修改。动漫网站：http://comic.kukudm.com/原理说明：http://blog.csdn.net/c406495762/article/details/72858983hero.py: 《王者荣耀》推荐出装查询小助手网页爬取已经会了，想过爬取手机APP里的内容吗？原理说明：http://blog.csdn.net/c406495762/article/details/76850843financical.py: 财务报表下载小助手爬取的数据存入数据库会吗？《跟股神巴菲特学习炒股之财务报表入库(MySQL)》也许能给你一些思路。原理说明：http://blog.csdn.net/c406495762/article/details/77801899动态示意图：one_hour_spider:一小时入门Python3网络爬虫。原理说明:知乎：https://zhuanlan.zhihu.com/p/29809609CSDN：http://blog.csdn.net/c406495762/article/details/78123502本次实战内容有：网络小说下载(静态网站)-biqukan优美壁纸下载(动态网站)-unsplash视频下载douyin.py:抖音App视频下载抖音App的视频下载，就是普通的App爬取。原理说明:个人网站：http://cuijiahua.com/blog/2018/03/spider-5.htmldouyin_pro:抖音App视频下载（升级版）抖音App的视频下载，添加视频解析网站，支持无水印视频下载，使用第三方平台解析。原理说明:个人网站：http://cuijiahua.com/blog/2018/03/spider-5.htmldouyin:抖音App视频下载（升级版2）抖音App的视频下载，添加视频解析网站，支持无水印视频下载，通过url解析，无需第三方平台。原理说明:个人网站：http://cuijiahua.com/blog/2018/03/spider-5.html动态示意图：geetest.py:GEETEST验证码识别原理说明:无12306.py:用Python抢火车票简单代码可以自己慢慢丰富，蛮简单，有爬虫基础很好操作，没有原理说明。baiwan:百万英雄辅助答题效果图：原理说明：个人网站：http://cuijiahua.com/blog/2018/01/spider_3.html功能介绍：服务器端，使用Python（baiwan.py）通过抓包获得的接口获取答题数据，解析之后通过百度知道搜索接口匹配答案，将最终匹配的结果写入文件（file.txt)。手机抓包不会的朋友，可以看下我的早期手机APP抓包教程。Node.js（app.js）每隔1s读取一次file.txt文件，并将读取结果通过socket.io推送给客户端（index.html）。亲测答题延时在3s左右。声明：没做过后端和前端，花了一天时间，现学现卖弄好的，javascript也是现看现用，百度的程序，调试调试而已。可能有很多用法比较low的地方，用法不对，请勿见怪，有大牛感兴趣，可以自行完善。Netease:根据歌单下载网易云音乐效果图：原理说明：暂无功能介绍：根据music_list.txt文件里的歌单的信息下载网易云音乐，将自己喜欢的音乐进行批量下载。bilibili：B站视频和弹幕批量下载原理说明：暂无使用说明： python bilibili.py -d 猫 -k 猫 -p 10 三个参数： -d\t保存视频的文件夹名 -k\tB站搜索的关键字 -p\t下载搜索结果前多少页jingdong：京东商品晒单图下载效果图：原理说明：暂无使用说明： python jd.py -k 芒果  三个参数： -d\t保存图片的路径，默认为fd.py文件所在文件夹 -k\t搜索关键词 -n  \t下载商品的晒单图个数，即n个商店的晒单图zhengfang_system_spider：对正方教务管理系统个人课表，个人学生成绩，绩点等简单爬取效果图：原理说明：暂无使用说明： cd zhengfang_system_spider pip install -r requirements.txt python spider.py其它欢迎 Pull requests，感谢贡献。更多精彩，敬请期待！ 
36,Yorko/mlcourse.ai,https://github.com/Yorko/mlcourse.ai/blob/main/README.md,Python,"mlcourse.ai – Open Machine Learning Coursemlcourse.ai is an open Machine Learning course by OpenDataScience (ods.ai), led by Yury Kashnitsky (yorko). Having both a Ph.D. degree in applied math and a Kaggle Competitions Master tier, Yury aimed at designing an ML course with a perfect balance between theory and practice. Thus, the course meets you with math formulae in lectures, and a lot of practice in a form of assignments and  Kaggle Inclass competitions. Currently, the course is in a self-paced mode. Here we guide you through the self-paced mlcourse.ai.Bonus:Additionally, you can purchase a Bonus Assignments pack with the best non-demo versions of mlcourse.ai assignments. Select the \""Bonus Assignments\"" tier. Refer to the details of the deal on the main page mlcourse.ai.Mirrors (🇬🇧-only): mlcourse.ai (main site), Kaggle Dataset (same notebooks as Kaggle Notebooks)Self-paced passingYou are guided through 10 weeks of mlcourse.ai. For each week, from Pandas to Gradient Boosting, instructions are given on which articles to read, lectures to watch, what assignments to accomplish.ArticlesThis is the list of published articles on medium.com 🇬🇧, habr.com 🇷🇺. Also notebooks in Chinese are mentioned 🇨🇳 and links to Kaggle Notebooks (in English) are given. Icons are clickable.Exploratory Data Analysis with Pandas 🇬🇧 🇷🇺 🇨🇳, Kaggle NotebookVisual Data Analysis with Python 🇬🇧 🇷🇺 🇨🇳, Kaggle Notebooks: part1, part2Classification, Decision Trees and k Nearest Neighbors 🇬🇧 🇷🇺 🇨🇳, Kaggle NotebookLinear Classification and Regression 🇬🇧 🇷🇺 🇨🇳, Kaggle Notebooks: part1, part2, part3, part4, part5Bagging and Random Forest 🇬🇧 🇷🇺 🇨🇳, Kaggle Notebooks: part1, part2, part3Feature Engineering and Feature Selection 🇬🇧 🇷🇺 🇨🇳, Kaggle NotebookUnsupervised Learning: Principal Component Analysis and Clustering 🇬🇧 🇷🇺 🇨🇳, Kaggle NotebookVowpal Wabbit: Learning with Gigabytes of Data 🇬🇧 🇷🇺 🇨🇳, Kaggle NotebookTime Series Analysis with Python, part 1 🇬🇧 🇷🇺 🇨🇳. Predicting future with Facebook Prophet, part 2 🇬🇧, 🇨🇳 Kaggle Notebooks: part1, part2Gradient Boosting 🇬🇧 🇷🇺, 🇨🇳, Kaggle NotebookLecturesVideolectures are uploaded to this YouTube playlist.Introduction, video, slidesExploratory data analysis with Pandas, videoVisualization, main plots for EDA, videoDecision trees: theory and practical partLogistic regression: theoretical foundations, practical part (baselines in the \""Alice\"" competition)Ensembles and Random Forest – part 1. Classification metrics – part 2. Example of a business task, predicting a customer payment – part 3Linear regression and regularization - theory, LASSO & Ridge, LTV prediction - practiceUnsupervised learning - Principal Component Analysis and ClusteringStochastic Gradient Descent for classification and regression - part 1, part 2 TBATime series analysis with Python (ARIMA, Prophet) - videoGradient boosting: basic ideas - part 1, key ideas behind Xgboost, LightGBM, and CatBoost + practice - part 2AssignmentsThe following are demo-assignments. Additionally, within the \""Bonus Assignments\"" tier you can get access to non-demo assignments.Exploratory data analysis with Pandas, nbviewer, Kaggle Notebook, solutionAnalyzing cardiovascular disease data, nbviewer, Kaggle Notebook, solutionDecision trees with a toy task and the UCI Adult dataset, nbviewer, Kaggle Notebook, solutionSarcasm detection, Kaggle Notebook, solution. Linear Regression as an optimization problem, nbviewer, Kaggle NotebookLogistic Regression and Random Forest in the credit scoring problem, nbviewer, Kaggle Notebook, solutionExploring OLS, Lasso and Random Forest in a regression task, nbviewer, Kaggle Notebook, solutionUnsupervised learning, nbviewer, Kaggle Notebook, solutionImplementing online regressor, nbviewer, Kaggle Notebook, solutionTime series analysis, nbviewer, Kaggle Notebook, solutionBeating baseline in a competition, Kaggle NotebookBonus assignmentsAdditionally, you can purchase a Bonus Assignments pack with the best non-demo versions of mlcourse.ai assignments. Select the \""Bonus Assignments\"" tier on Patreon or a similar tier on Boosty (rus).                                          Details of the dealmlcourse.ai is still in self-paced mode but we offer you Bonus Assignments with solutions for a contribution of $17/month. The idea is that you pay for ~1-5 months while studying the course materials, but a single contribution is still fine and opens your access to the bonus pack.Note: the first payment is charged at the moment of joining the Tier Patreon, and the next payment is charged on the 1st day of the next month, thus it's better to purchase the pack in the 1st half of the month.mlcourse.ai is never supposed to go fully monetized (it's created in the wonderful open ODS.ai community and will remain open and free) but it'd help to cover some operational costs, and Yury also put in quite some effort into assembling all the best assignments into one pack. Please note that unlike the rest of the course content, Bonus Assignments are copyrighted. Informally, Yury's fine if you share the pack with 2-3 friends but public sharing of the Bonus Assignments pack is prohibited.  The bonus pack contains 10 assignments, in some of them you are challenged to beat a baseline in a Kaggle competition under thorough guidance (\""Alice\"" and \""Medium\"") or implement an algorithm from scratch -- efficient stochastic gradient descent classifier and gradient boosting.Kaggle competitionsCatch Me If You Can: Intruder Detection through Webpage Session Tracking. Kaggle InclassPredicting popularity of a Medium article. Kaggle InclassDotA 2 winner prediction. Kaggle InclassCiting mlcourse.aiIf you happen to cite mlcourse.ai in your work, you can use this BibTeX record:@misc{mlcourse_ai,    author = {Kashnitsky, Yury},    title = {mlcourse.ai – Open Machine Learning Course},    year = {2020},    publisher = {GitHub},    journal = {GitHub repository},    howpublished = {\\url{https://github.com/Yorko/mlcourse.ai}},}CommunityYou can join the Singularis.ai Slack community to ask questions on the course materials. The community is mostly Russian-speaking but questions in English are still welcome."
37,davidsandberg/facenet,https://github.com/davidsandberg/facenet/blob/master/README.md,Python,"Face Recognition using Tensorflow This is a TensorFlow implementation of the face recognizer described in the paper\""FaceNet: A Unified Embedding for Face Recognition and Clustering\"". The project also uses ideas from the paper \""Deep Face Recognition\"" from the Visual Geometry Group at Oxford.CompatibilityThe code is tested using Tensorflow r1.7 under Ubuntu 14.04 with Python 2.7 and Python 3.5. The test cases can be found here and the results can be found here.NewsDateUpdate2018-04-10Added new models trained on Casia-WebFace and VGGFace2 (see below). Note that the models uses fixed image standardization (see wiki).2018-03-31Added a new, more flexible input pipeline as well as a bunch of minor updates.2017-05-13Removed a bunch of older non-slim models. Moved the last bottleneck layer into the respective models. Corrected normalization of Center Loss.2017-05-06Added code to train a classifier on your own images. Renamed facenet_train.py to train_tripletloss.py and facenet_train_classifier.py to train_softmax.py.2017-03-02Added pretrained models that generate 128-dimensional embeddings.2017-02-22Updated to Tensorflow r1.0. Added Continuous Integration using Travis-CI.2017-02-03Added models where only trainable variables has been stored in the checkpoint. These are therefore significantly smaller.2017-01-27Added a model trained on a subset of the MS-Celeb-1M dataset. The LFW accuracy of this model is around 0.994.2017‑01‑02Updated to run with Tensorflow r0.12. Not sure if it runs with older versions of Tensorflow though.Pre-trained modelsModel nameLFW accuracyTraining datasetArchitecture20180408-1029000.9905CASIA-WebFaceInception ResNet v120180402-1147590.9965VGGFace2Inception ResNet v1NOTE: If you use any of the models, please do not forget to give proper credit to those providing the training dataset as well.InspirationThe code is heavily inspired by the OpenFace implementation.Training dataThe CASIA-WebFace dataset has been used for training. This training set consists of total of 453 453 images over 10 575 identities after face detection. Some performance improvement has been seen if the dataset has been filtered before training. Some more information about how this was done will come later.The best performing model has been trained on the VGGFace2 dataset consisting of ~3.3M faces and ~9000 classes.Pre-processingFace alignment using MTCNNOne problem with the above approach seems to be that the Dlib face detector misses some of the hard examples (partial occlusion, silhouettes, etc). This makes the training set too \""easy\"" which causes the model to perform worse on other benchmarks.To solve this, other face landmark detectors has been tested. One face landmark detector that has proven to work very well in this setting is theMulti-task CNN. A Matlab/Caffe implementation can be found here and this has been used for face alignment with very good results. A Python/Tensorflow implementation of MTCNN can be found here. This implementation does not give identical results to the Matlab/Caffe implementation but the performance is very similar.Running trainingCurrently, the best results are achieved by training the model using softmax loss. Details on how to train a model using softmax loss on the CASIA-WebFace dataset can be found on the page Classifier training of Inception-ResNet-v1 and .Pre-trained modelsInception-ResNet-v1 modelA couple of pretrained models are provided. They are trained using softmax loss with the Inception-Resnet-v1 model. The datasets has been aligned using MTCNN.PerformanceThe accuracy on LFW for the model 20180402-114759 is 0.99650+-0.00252. A description of how to run the test can be found on the page Validate on LFW. Note that the input images to the model need to be standardized using fixed image standardization (use the option --use_fixed_image_standardization when running e.g. validate_on_lfw.py)."
38,yandex-praktikum/calc_and_win,https://github.com/yandex-praktikum/calc_and_win/blob/master/README.md,Python,"calc_and_winРепозиторий игры \""Рассчитай и победи!\"""
39,facebook/prophet,https://github.com/facebook/prophet/blob/main/README.md,Python,"Prophet: Automatic Forecasting Procedure2023 Update: We discuss our plans for the future of Prophet in this blog post: facebook/prophet in 2023 and beyondProphet is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well.Prophet is open source software released by Facebook's Core Data Science team. It is available for download on CRAN and PyPI.Important linksHomepage: https://facebook.github.io/prophet/HTML documentation: https://facebook.github.io/prophet/docs/quick_start.htmlIssue tracker: https://github.com/facebook/prophet/issuesSource code repository: https://github.com/facebook/prophetContributing: https://facebook.github.io/prophet/docs/contributing.htmlProphet R package: https://cran.r-project.org/package=prophetProphet Python package: https://pypi.python.org/pypi/prophet/Release blogpost: https://research.facebook.com/blog/2017/2/prophet-forecasting-at-scale/Prophet paper: Sean J. Taylor, Benjamin Letham (2018) Forecasting at scale. The American Statistician 72(1):37-45 (https://peerj.com/preprints/3190.pdf).Installation in R - CRAN⚠️ The CRAN version of prophet is fairly outdated. To get the latest bug fixes and updated country holiday data, we suggest installing the latest release.Prophet is a CRAN package so you can use install.packages.install.packages('prophet')After installation, you can get started!Installation in R - Latest releaseinstall.packages('remotes')remotes::install_github('facebook/prophet@*release', subdir = 'R')Experimental backend - cmdstanrYou can also choose an experimental alternative stan backend called cmdstanr. Once you've installed prophet,follow these instructions to use cmdstanr instead of rstan as the backend:# R# We recommend running this in a fresh R session or restarting your current sessioninstall.packages(c(\""cmdstanr\"", \""posterior\""), repos = c(\""https://mc-stan.org/r-packages/\"", getOption(\""repos\"")))# If you haven't installed cmdstan before, run:cmdstanr::install_cmdstan()# Otherwise, you can point cmdstanr to your cmdstan path:cmdstanr::set_cmdstan_path(path = <your existing cmdstan>)# Set the R_STAN_BACKEND environment variableSys.setenv(R_STAN_BACKEND = \""CMDSTANR\"")WindowsOn Windows, R requires a compiler so you'll need to follow the instructions provided by rstan. The key step is installing Rtools before attempting to install the package.If you have custom Stan compiler settings, install from source rather than the CRAN binary.Installation in Python - PyPI releaseProphet is on PyPI, so you can use pip to install it.python -m pip install prophetFrom v0.6 onwards, Python 2 is no longer supported.As of v1.0, the package name on PyPI is \""prophet\""; prior to v1.0 it was \""fbprophet\"".As of v1.1, the minimum supported Python version is 3.7.After installation, you can get started!AnacondaProphet can also be installed through conda-forge.conda install -c conda-forge prophetInstallation in Python - Development versionTo get the latest code changes as they are merged, you can clone this repo and build from source manually. This is not guaranteed to be stable.git clone https://github.com/facebook/prophet.gitcd prophet/pythonpython -m pip install -e .By default, Prophet will use a fixed version of cmdstan (downloading and installing it if necessary) to compile the model executables. If this is undesired and you would like to use your own existing cmdstan installation, you can set the environment variable PROPHET_REPACKAGE_CMDSTAN to False:export PROPHET_REPACKAGE_CMDSTAN=False; python -m pip install -e .LinuxMake sure compilers (gcc, g++, build-essential) and Python development tools (python-dev, python3-dev) are installed. In Red Hat systems, install the packages gcc64 and gcc64-c++. If you are using a VM, be aware that you will need at least 4GB of memory to install prophet, and at least 2GB of memory to use prophet.WindowsUsing cmdstanpy with Windows requires a Unix-compatible C compiler such as mingw-gcc. If cmdstanpy is installed first, one can be installed via the cmdstanpy.install_cxx_toolchain command.ChangelogVersion 1.1.4 (2023.05.30)PythonWe now rely solely on holidays package for country holidays.Upgraded cmdstan version to 2.31.0, enabling Apple M1 support.Fixed bug with Windows installation caused by long paths.RUpdated holidays data based on holidays version 0.25.Version 1.1.2 (2023.01.20)PythonSped up .predict() by up to 10x by removing intermediate DataFrame creations.Sped up fourier series generation, leading to at least 1.5x speed improvement for train() and predict() pipelines.Fixed bug in how warm start values were being read.Wheels are now version-agnostic.RFixed a bug in construct_holiday_dataframe()Updated holidays data based on holidays version 0.18.Version 1.1.1 (2022.09.08)(Python) Improved runtime (3-7x) of uncertainty predictions via vectorization.Bugfixes relating to Python package versions and R holiday objects.Version 1.1 (2022.06.25)Replaced pystan2 dependency with cmdstan + cmdstanpy.Pre-packaged model binaries for Python package, uploaded binary distributions to PyPI.Improvements in the stan model code, cross-validation metric calculations, holidays.Version 1.0 (2021.03.28)Python package name changed from fbprophet to prophetFixed R Windows build issues to get latest version back on CRANImprovements in serialization, holidays, and R timezone handlingPlotting improvementsVersion 0.7 (2020.09.05)Built-in json serializationAdded \""flat\"" growth optionBugfixes related to holidays and pandasPlotting improvementsImprovements in cross validation, such as parallelization and directly specifying cutoffsVersion 0.6 (2020.03.03)Fix bugs related to upstream changes in holidays and pandas packages.Compile model during first use, not during install (to comply with CRAN policy)cmdstanpy backend now available in PythonPython 2 no longer supportedVersion 0.5 (2019.05.14)Conditional seasonalitiesImproved cross validation estimatesPlotly plot in PythonBugfixesVersion 0.4 (2018.12.18)Added holidays functionalityBugfixesVersion 0.3 (2018.06.01)Multiplicative seasonalityCross validation error metrics and visualizationsParameter to set range of potential changepointsUnified Stan model for both trend typesImproved future trend uncertainty for sub-daily dataBugfixesVersion 0.2.1 (2017.11.08)BugfixesVersion 0.2 (2017.09.02)Forecasting with sub-daily dataDaily seasonality, and custom seasonalitiesExtra regressorsAccess to posterior predictive samplesCross-validation functionSaturating minimumsBugfixesVersion 0.1.1 (2017.04.17)BugfixesNew options for detecting yearly and weekly seasonality (now the default)Version 0.1 (2017.02.23)Initial releaseLicenseProphet is licensed under the MIT license."
40,jackfrued/Python-100-Days,https://github.com/jackfrued/Python-100-Days/blob/master/README.md,Python,"Python - 100天从新手到大师作者：骆昊说明：从项目上线到获得8w+星标以来，一直收到反馈说基础部分（前15天的内容）对新手来说是比较困难的，建议有配套视频进行讲解。最近把基础部分的内容重新制作了一个名为“Python-Core-50-Courses”的项目，用更为简单通俗的方式重写了这部分内容并附带了视频讲解，初学者可以关注下这个新项目。如果需要Python基础视频，可以在“B站”搜索《Python零基础快速上手》，这套视频是我讲课的时候录制的随堂视频，画质尚可、音质一般，但是对初学者应该会有些帮助，欢迎大家留言、评论、发弹幕。学习之后觉得有收获的小伙伴可以“一键三连”来支持UP主（千锋Python）。国内用户如果访问GitHub比较慢的话，可以关注我的知乎号Python-Jack，上面的“从零开始学Python”专栏比较适合初学者，其他的专栏也在持续创作和更新中，欢迎大家关注并点赞评论。创作不易，感谢大家的打赏支持，这些钱不会用于个人消费（例如：购买咖啡），而是通过腾讯公益、美团公益、水滴筹等平台捐赠给需要帮助的人（点击了解捐赠情况）。需要加入QQ学习群的可以扫描下面的二维码，三个群加一个即可，不要重复进群。学习群会为大家提供学习资源和问题解答，如果有Python体验课和行业公开课会提前在群里通知大家，欢迎大家加入。项目“Day80~90”部分目前仍在创作中，因为作者平时也挤不出太多时间来写文档，因此更新的速度比较缓慢，感谢大家的理解。Python应用领域和职业发展分析简单的说，Python是一个“优雅”、“明确”、“简单”的编程语言。学习曲线低，非专业人士也能上手开源系统，拥有强大的生态圈解释型语言，完美的平台可移植性动态类型语言，支持面向对象和函数式编程代码规范程度高，可读性强Python在以下领域都有用武之地。后端开发 - Python / Java / Go / PHPDevOps - Python / Shell / Ruby数据采集 - Python / C++ / Java量化交易 - Python / C++ / R数据科学 - Python / R / Julia / Matlab机器学习 - Python / R / C++ / Julia自动化测试 - Python / Shell作为一名Python开发者，根据个人的喜好和职业规划，可以选择的就业领域也非常多。Python后端开发工程师（服务器、云平台、数据接口）Python运维工程师（自动化运维、SRE、DevOps）Python数据分析师（数据分析、商业智能、数字化运营）Python数据挖掘工程师（机器学习、深度学习、算法专家）Python爬虫工程师Python测试工程师（自动化测试、测试开发）说明：目前，数据分析和数据挖掘是非常热门的方向，因为不管是互联网行业还是传统行业都已经积累了大量的数据，各行各业都需要数据分析师从已有的数据中发现更多的商业价值，从而为企业的决策提供数据的支撑，这就是所谓的数据驱动决策。给初学者的几个建议：Make English as your working language. （让英语成为你的工作语言）Practice makes perfect. （熟能生巧）All experience comes from mistakes. （所有的经验都源于你犯过的错误）Don't be one of the leeches. （不要当伸手党）Either outstanding or out. （要么出众，要么出局）Day01~15 - Python语言基础Day01 - 初识PythonPython简介 - Python的历史 / Python的优缺点 / Python的应用领域搭建编程环境 - Windows环境 / Linux环境 / MacOS环境从终端运行Python程序 - Hello, world / print函数 / 运行程序使用IDLE - 交互式环境(REPL) / 编写多行代码 / 运行程序 / 退出IDLE注释 - 注释的作用 / 单行注释 / 多行注释Day02 - 语言元素程序和进制 - 指令和程序 / 冯诺依曼机 / 二进制和十进制 / 八进制和十六进制变量和类型 - 变量的命名 / 变量的使用 / input函数 / 检查变量类型 / 类型转换数字和字符串 - 整数 / 浮点数 / 复数 / 字符串 / 字符串基本操作 / 字符编码运算符 - 数学运算符 / 赋值运算符 / 比较运算符 / 逻辑运算符 / 身份运算符 / 运算符的优先级应用案例 - 华氏温度转换成摄氏温度 / 输入圆的半径计算周长和面积 / 输入年份判断是否是闰年Day03 - 分支结构分支结构的应用场景 - 条件 / 缩进 / 代码块 / 流程图if语句 - 简单的if / if-else结构 / if-elif-else结构 / 嵌套的if应用案例 - 用户身份验证 / 英制单位与公制单位互换 / 掷骰子决定做什么 / 百分制成绩转等级制 / 分段函数求值 / 输入三条边的长度如果能构成三角形就计算周长和面积Day04 - 循环结构循环结构的应用场景 - 条件 / 缩进 / 代码块 / 流程图while循环 - 基本结构 / break语句 / continue语句for循环 - 基本结构 / range类型 / 循环中的分支结构 / 嵌套的循环 / 提前结束程序应用案例 - 1~100求和 / 判断素数 / 猜数字游戏 / 打印九九表 / 打印三角形图案 / 猴子吃桃 / 百钱百鸡Day05 - 构造程序逻辑经典案例：水仙花数 / 百钱百鸡 / Craps赌博游戏练习题目：斐波那契数列 / 完美数 / 素数Day06 - 函数和模块的使用函数的作用 - 代码的坏味道 / 用函数封装功能模块定义函数 - def关键字 / 函数名 / 参数列表 / return语句 / 调用自定义函数调用函数 - Python内置函数 /  导入模块和函数函数的参数 - 默认参数 / 可变参数 / 关键字参数 / 命名关键字参数函数的返回值 - 没有返回值  / 返回单个值 / 返回多个值作用域问题 - 局部作用域 / 嵌套作用域 / 全局作用域 / 内置作用域 / 和作用域相关的关键字用模块管理函数 - 模块的概念 / 用自定义模块管理函数 / 命名冲突的时候会怎样（同一个模块和不同的模块）Day07 - 字符串和常用数据结构字符串的使用 - 计算长度 / 下标运算 / 切片 / 常用方法列表基本用法 - 定义列表 / 用下表访问元素 / 下标越界 / 添加元素 / 删除元素 / 修改元素 / 切片 / 循环遍历列表常用操作 - 连接 / 复制(复制元素和复制数组) / 长度 / 排序 / 倒转 / 查找生成列表 - 使用range创建数字列表 / 生成表达式 / 生成器元组的使用 - 定义元组 / 使用元组中的值 / 修改元组变量 / 元组和列表转换集合基本用法 - 集合和列表的区别 /  创建集合 / 添加元素 / 删除元素 /  清空集合常用操作 - 交集 / 并集 / 差集 / 对称差 / 子集 / 超集字典的基本用法 - 字典的特点 / 创建字典 / 添加元素 / 删除元素 / 取值 / 清空字典常用操作 - keys方法 / values方法 / items方法 / setdefault方法基础练习 - 跑马灯效果 / 列表找最大元素 / 统计考试成绩的平均分 / Fibonacci数列 / 杨辉三角综合案例 - 双色球选号 / 井字棋Day08 - 面向对象编程基础类和对象 - 什么是类 / 什么是对象 / 面向对象其他相关概念定义类 - 基本结构 / 属性和方法 / 构造器 / 析构器 / __str__方法使用对象 - 创建对象 / 给对象发消息面向对象的四大支柱 - 抽象 / 封装 / 继承 / 多态基础练习 - 定义学生类 / 定义时钟类 / 定义图形类 / 定义汽车类Day09 - 面向对象进阶属性 - 类属性 / 实例属性 / 属性访问器 / 属性修改器 / 属性删除器 / 使用__slots__类中的方法 - 实例方法 / 类方法 / 静态方法运算符重载 - __add__ / __sub__ / __or__ /__getitem__ / __setitem__ / __len__ / __repr__ / __gt__ / __lt__ / __le__ / __ge__ / __eq__ / __ne__ / __contains__类(的对象)之间的关系 - 关联 / 继承 / 依赖继承和多态 - 什么是继承 / 继承的语法 / 调用父类方法 / 方法重写 / 类型判定 / 多重继承 / 菱形继承(钻石继承)和C3算法综合案例 - 工资结算系统 / 图书自动折扣系统 / 自定义分数类Day10 - 图形用户界面和游戏开发使用tkinter开发GUI程序使用pygame三方库开发游戏应用“大球吃小球”游戏Day11 - 文件和异常读文件 - 读取整个文件 / 逐行读取 / 文件路径写文件 - 覆盖写入 / 追加写入 / 文本文件 / 二进制文件异常处理 - 异常机制的重要性 / try-except代码块 / else代码块 / finally代码块 / 内置异常类型 / 异常栈 / raise语句数据持久化 - CSV文件概述 / csv模块的应用 / JSON数据格式 / json模块的应用Day12 - 字符串和正则表达式字符串高级操作 - 转义字符 / 原始字符串 / 多行字符串 / in和not in运算符 / is_xxx方法 / join和split方法 / strip相关方法 / pyperclip模块 / 不变字符串和可变字符串 / StringIO的使用正则表达式入门 - 正则表达式的作用 / 元字符 / 转义 / 量词 / 分组 / 零宽断言 /贪婪匹配与惰性匹配懒惰 / 使用re模块实现正则表达式操作（匹配、搜索、替换、捕获）使用正则表达式 - re模块 / compile函数 / group和groups方法 / match方法 / search方法 / findall和finditer方法 / sub和subn方法 / split方法应用案例 - 使用正则表达式验证输入的字符串Day13 - 进程和线程进程和线程的概念 - 什么是进程 / 什么是线程 / 多线程的应用场景使用进程 - fork函数 / multiprocessing模块 / 进程池 / 进程间通信使用线程 -  threading模块 / Thread类 / RLock类 / Condition类 / 线程池Day14 - 网络编程入门和网络应用开发计算机网络基础 - 计算机网络发展史 / “TCP-IP”模型 / IP地址 / 端口 / 协议 / 其他相关概念网络应用模式 - “客户端-服务器”模式 / “浏览器-服务器”模式基于HTTP协议访问网络资源 - 网络API概述 / 访问URL / requests三方库 / 解析JSON格式数据Python网络编程 - 套接字的概念 / socket模块 /  socket函数 / 创建TCP服务器 / 创建TCP客户端 / 创建UDP服务器 / 创建UDP客户端电子邮件 - SMTP协议 / POP3协议 / IMAP协议 / smtplib模块 / poplib模块 / imaplib模块短信服务 - 调用短信服务网关Day15 - 图像和文档处理用Pillow处理图片 - 图片读写 / 图片合成 / 几何变换 / 色彩转换 / 滤镜效果读写Word文档 - 文本内容的处理 / 段落 / 页眉和页脚 / 样式的处理读写Excel文件 - xlrd / xlwt / openpyxlDay16~Day20 - Python语言进阶 常用数据结构函数的高级用法 - “一等公民” / 高阶函数 / Lambda函数 / 作用域和闭包 / 装饰器面向对象高级知识 - “三大支柱” / 类与类之间的关系 / 垃圾回收 / 魔术属性和方法 / 混入 / 元类 / 面向对象设计原则 / GoF设计模式迭代器和生成器 - 相关魔术方法 / 创建生成器的两种方式 /并发和异步编程 - 多线程 / 多进程 / 异步IO / async和awaitDay21~30 - Web前端入门用HTML标签承载页面内容用CSS渲染页面用JavaScript处理交互式行为jQuery入门和提高Vue.js入门Element的使用Bootstrap的使用Day31~35 - 玩转Linux操作系统操作系统发展史和Linux概述Linux基础命令Linux中的实用程序Linux的文件系统Vim编辑器的应用环境变量和Shell编程软件的安装和服务的配置网络访问和管理其他相关内容Day36~40 - 数据库基础和进阶关系型数据库概述MySQL的安装和使用SQL的使用DDL - 数据定义语言 - create / drop / alterDML - 数据操作语言 - insert / delete / updateDQL - 数据查询语言 - selectDCL - 数据控制语言 - grant / revokeMySQL新特性窗口函数的应用JSON数据类型相关知识数据完整性和一致性视图、函数、过程、触发器事务和锁执行计划和索引范式理论和反范式设计在Python中操作MySQLDay41~55 - 实战DjangoDay41 - Django快速上手Web应用工作机制HTTP请求和响应Django框架概述5分钟快速上手Day42 - 深入模型关系型数据库配置使用ORM完成对模型的CRUD操作管理后台的使用Django模型最佳实践模型定义参考Day43 - 静态资源和Ajax请求加载静态资源Ajax概述用Ajax实现投票功能Day44 - Cookie和Session实现用户跟踪cookie和session的关系Django框架对session的支持视图函数中的cookie读写操作Day45 - 报表和日志通过HttpResponse修改响应头使用StreamingHttpResponse处理大文件使用xlwt生成Excel报表使用reportlab生成PDF报表使用ECharts生成前端图表Day46 - 日志和调试工具栏配置日志配置Django-Debug-Toolbar优化ORM代码Day47 - 中间件的应用什么是中间件Django框架内置的中间件自定义中间件及其应用场景Day48 - 前后端分离开发入门返回JSON格式的数据用Vue.js渲染页面Day49 - RESTful架构和DRF入门Day50 - RESTful架构和DRF进阶Day51 - 使用缓存网站优化第一定律在Django项目中使用Redis提供缓存服务在视图函数中读写缓存使用装饰器实现页面缓存为数据接口提供缓存服务Day52 - 接入三方平台文件上传表单控件和图片文件预览服务器端如何处理上传的文件Day53 - 异步任务和定时任务网站优化第二定律配置消息队列服务在项目中使用Celery实现任务异步化在项目中使用Celery实现定时任务Day54 - 单元测试Day55 - 项目上线Python中的单元测试Django框架对单元测试的支持使用版本控制系统配置和使用uWSGI动静分离和Nginx配置配置HTTPS配置域名解析Day56~60 - 用FastAPI开发数据接口FastAPI五分钟上手请求和响应接入关系型数据库依赖注入中间件异步化虚拟化部署（Docker）项目实战：车辆违章查询项目Day61~65 - 爬虫开发Day61 - 网络数据采集概述网络爬虫的概念及其应用领域网络爬虫的合法性探讨开发网络爬虫的相关工具一个爬虫程序的构成Day62 - 数据抓取和解析使用requests三方库实现数据抓取页面解析的三种方式正则表达式解析XPath解析CSS选择器解析Day63 - Python中的并发编程多线程多进程异步I/ODay64 - 使用Selenium抓取网页动态内容Day65 - 爬虫框架Scrapy简介Day66~80 - 数据分析Day66 - 数据分析概述Day67 - 环境准备Day68 - NumPy的应用-1Day69 - NumPy的应用-2Day70 - Pandas的应用-1Day71 - Pandas的应用-2Day72 - Pandas的应用-3Day73 - Pandas的应用-4Day74 - Pandas的应用-5Day75 - 数据可视化-1Day76 - 数据可视化-2Day77 - 概率统计基础Day78 - 方差分析和参数估计Day79 - 相关和回归Day80 - 数据分析方法论Day81~90 - 机器学习和深度学习Day81 - 机器学习基础Day82 - k最近邻分类Day83 - 决策树Day84 - 贝叶斯分类Day85 - 支持向量机Day86 - K-均值聚类Day87 - 回归分析Day88 - 深度学习入门Day89 - PyTorch概述Day90 - PyTorch实战Day91~100 - 团队项目开发第91天：团队项目开发的问题和解决方案软件过程模型经典过程模型（瀑布模型）可行性分析（研究做还是不做），输出《可行性分析报告》。需求分析（研究做什么），输出《需求规格说明书》和产品界面原型图。概要设计和详细设计，输出概念模型图（ER图）、物理模型图、类图、时序图等。编码 / 测试。上线 / 维护。瀑布模型最大的缺点是无法拥抱需求变化，整套流程结束后才能看到产品，团队士气低落。敏捷开发（Scrum）- 产品所有者、Scrum Master、研发人员 - Sprint产品的Backlog（用户故事、产品原型）。计划会议（评估和预算）。日常开发（站立会议、番茄工作法、结对编程、测试先行、代码重构……）。修复bug（问题描述、重现步骤、测试人员、被指派人）。发布版本。评审会议（Showcase，用户需要参与）。回顾会议（对当前迭代周期做一个总结）。补充：敏捷软件开发宣言个体和互动 高于 流程和工具工作的软件 高于 详尽的文档客户合作 高于 合同谈判响应变化 高于 遵循计划角色：产品所有者（决定做什么，能对需求拍板的人）、团队负责人（解决各种问题，专注如何更好的工作，屏蔽外部对开发团队的影响）、开发团队（项目执行人员，具体指开发人员和测试人员）。准备工作：商业案例和资金、合同、憧憬、初始产品需求、初始发布计划、入股、组建团队。敏捷团队通常人数为8-10人。工作量估算：将开发任务量化，包括原型、Logo设计、UI设计、前端开发等，尽量把每个工作分解到最小任务量，最小任务量标准为工作时间不能超过两天，然后估算总体项目时间。把每个任务都贴在看板上面，看板上分三部分：to do（待完成）、in progress（进行中）和done（已完成）。项目团队组建团队的构成和角色说明：谢谢付祥英女士帮助我绘制了下面这张精美的公司组织架构图。编程规范和代码审查（flake8、pylint）Python中的一些“惯例”（请参考《Python惯例-如何编写Pythonic的代码》）影响代码可读性的原因：代码注释太少或者没有注释代码破坏了语言的最佳实践反模式编程（意大利面代码、复制-黏贴编程、自负编程、……）团队开发工具介绍版本控制：Git、Mercury缺陷管理：Gitlab、Redmine敏捷闭环工具：禅道、JIRA持续集成：Jenkins、Travis-CI请参考《团队项目开发的问题和解决方案》。项目选题和理解业务选题范围设定CMS（用户端）：新闻聚合网站、问答/分享社区、影评/书评网站等。MIS（用户端+管理端）：KMS、KPI考核系统、HRS、CRM系统、供应链系统、仓储管理系统等。App后台（管理端+数据接口）：二手交易类、报刊杂志类、小众电商类、新闻资讯类、旅游类、社交类、阅读类等。其他类型：自身行业背景和工作经验、业务容易理解和把控。需求理解、模块划分和任务分配需求理解：头脑风暴和竞品分析。模块划分：画思维导图（XMind），每个模块是一个枝节点，每个具体的功能是一个叶节点（用动词表述），需要确保每个叶节点无法再生出新节点，确定每个叶子节点的重要性、优先级和工作量。任务分配：由项目负责人根据上面的指标为每个团队成员分配任务。制定项目进度表（每日更新）模块功能人员状态完成工时计划开始实际开始计划结束实际结束备注评论添加评论王大锤正在进行50%42018/8/72018/8/7删除评论王大锤等待0%22018/8/72018/8/7查看评论白元芳正在进行20%42018/8/72018/8/7需要进行代码审查评论投票白元芳等待0%42018/8/82018/8/8OOAD和数据库设计UML（统一建模语言）的类图通过模型创建表（正向工程），例如在Django项目中可以通过下面的命令创建二维表。python manage.py makemigrations apppython manage.py migrate使用PowerDesigner绘制物理模型图。通过数据表创建模型（反向工程），例如在Django项目中可以通过下面的命令生成模型。python manage.py inspectdb > app/models.py第92天：Docker容器详解Docker简介安装Docker使用Docker创建容器（Nginx、MySQL、Redis、Gitlab、Jenkins）构建Docker镜像（Dockerfile的编写和相关指令）容器编排（Docker-compose）集群管理（Kubernetes）第93天：MySQL性能优化第94天：网络API接口设计第95天：[使用Django开发商业项目](./Day91-100/95.使用Django开发商业项\t目.md)项目开发中的公共问题数据库的配置（多数据库、主从复制、数据库路由）缓存的配置（分区缓存、键设置、超时设置、主从复制、故障恢复（哨兵））日志的配置分析和调试（Django-Debug-ToolBar）好用的Python模块（日期计算、图像处理、数据加密、三方API）REST API设计RESTful架构理解RESTful架构RESTful API设计指南RESTful API最佳实践API接口文档的撰写RAP2YAPIdjango-REST-framework的应用项目中的重点难点剖析使用缓存缓解数据库压力 - Redis使用消息队列做解耦合和削峰 - Celery + RabbitMQ第96天：软件测试和自动化测试单元测试测试的种类编写单元测试（unittest、pytest、nose2、tox、ddt、……）测试覆盖率（coverage）Django项目部署部署前的准备工作关键设置（SECRET_KEY / DEBUG / ALLOWED_HOSTS / 缓存 / 数据库）HTTPS / CSRF_COOKIE_SECUR  / SESSION_COOKIE_SECURE日志相关配置Linux常用命令回顾Linux常用服务的安装和配置uWSGI/Gunicorn和Nginx的使用Gunicorn和uWSGI的比较对于不需要大量定制化的简单应用程序，Gunicorn是一个不错的选择，uWSGI的学习曲线比Gunicorn要陡峭得多，Gunicorn的默认参数就已经能够适应大多数应用程序。uWSGI支持异构部署。由于Nginx本身支持uWSGI，在线上一般都将Nginx和uWSGI捆绑在一起部署，而且uWSGI属于功能齐全且高度定制的WSGI中间件。在性能上，Gunicorn和uWSGI其实表现相当。使用虚拟化技术（Docker）部署测试环境和生产环境性能测试AB的使用SQLslap的使用sysbench的使用自动化测试使用Shell和Python进行自动化测试使用Selenium实现自动化测试Selenium IDESelenium WebDriverSelenium Remote Control测试工具Robot Framework介绍第97天：电商网站技术要点剖析第98天：项目部署上线和性能调优MySQL数据库调优Web服务器性能优化Nginx负载均衡配置Keepalived实现高可用代码性能调优多线程异步化静态资源访问优化云存储CDN第99天：面试中的公共问题第100天：Python面试题实录"
41,TheAlgorithms/Python,https://github.com/TheAlgorithms/Python/blob/master/README.md,Python,          The Algorithms - Python                                                                  All algorithms implemented in Python - for educationImplementations are for learning purposes only. They may be less efficient than the implementations in the Python standard library. Use them at your discretion.Getting StartedRead through our Contribution Guidelines before you contribute.Community ChannelsWe are on Discord and Gitter! Community channels are a great way for you to ask questions and get help. Please join us!List of AlgorithmsSee our directory for easier navigation and a better overview of the project.
42,Significant-Gravitas/Auto-GPT,https://github.com/Significant-Gravitas/Auto-GPT/blob/master/README.md,Python,"Auto-GPT: An Autonomous GPT-4 Experiment💡 Get help - Q&A or Discord 💬🔴 USE stable not master 🔴Download the latest stable release from here: https://github.com/Significant-Gravitas/Auto-GPT/releases/latest.The master branch is under heavy development and may often be in a broken state.Auto-GPT is an experimental open-source application showcasing the capabilities of the GPT-4 language model. This program, driven by GPT-4, chains together LLM \""thoughts\"", to autonomously achieve whatever goal you set. As one of the first examples of GPT-4 running fully autonomously, Auto-GPT pushes the boundaries of what is possible with AI. Demo April 16th 2023               AutoGPTDemo_Subs_WithoutFinalScreen.mp4          Demo made by Blake Werlinger🚀 Features🌐 Internet access for searches and information gathering💾 Long-term and short-term memory management🧠 GPT-4 instances for text generation🔗 Access to popular websites and platforms🗃️ File storage and summarization with GPT-3.5🔌 Extensibility with PluginsQuickstartCheck out the wikiGet an OpenAI API KeyDownload the latest releaseFollow the installation instructionsConfigure any additional features you want, or install some pluginsRun the appPlease see the documentation for full setup instructions and configuration options.📖 Documentation⚙️ Setup💻 Usage🔌 PluginsConfiguration🔍 Web Search🧠 Memory🗣️ Voice (TTS)🖼️ Image Generation 💖 Help Fund Auto-GPT's Development 💖If you can spare a coffee, you can help to cover the costs of developing Auto-GPT and help to push the boundaries of fully autonomous AI!Your support is greatly appreciated. Development of this free, open-source project is made possible by all the contributors and sponsors. If you'd like to sponsor this project and have your avatar or company logo appear below click here.                                                                                                                                                                                                                                                                                                                                          ⚠️ LimitationsThis experiment aims to showcase the potential of GPT-4 but comes with some limitations:Not a polished application or product, just an experimentMay not perform well in complex, real-world business scenarios. In fact, if it actually does, please share your results!Quite expensive to run, so set and monitor your API key limits with OpenAI!🛡 DisclaimerThis project, Auto-GPT, is an experimental application and is provided \""as-is\"" without any warranty, express or implied. By using this software, you agree to assume all risks associated with its use, including but not limited to data loss, system failure, or any other issues that may arise.The developers and contributors of this project do not accept any responsibility or liability for any losses, damages, or other consequences that may occur as a result of using this software. You are solely responsible for any decisions and actions taken based on the information provided by Auto-GPT.Please note that the use of the GPT-4 language model can be expensive due to its token usage. By utilizing this project, you acknowledge that you are responsible for monitoring and managing your own token usage and the associated costs. It is highly recommended to check your OpenAI API usage regularly and set up any necessary limits or alerts to prevent unexpected charges.As an autonomous experiment, Auto-GPT may generate content or take actions that are not in line with real-world business practices or legal requirements. It is your responsibility to ensure that any actions or decisions made based on the output of this software comply with all applicable laws, regulations, and ethical standards. The developers and contributors of this project shall not be held responsible for any consequences arising from the use of this software.By using Auto-GPT, you agree to indemnify, defend, and hold harmless the developers, contributors, and any affiliated parties from and against any and all claims, damages, losses, liabilities, costs, and expenses (including reasonable attorneys' fees) arising from your use of this software or your violation of these terms.🐦 Connect with Us on TwitterStay up-to-date with the latest news, updates, and insights about Auto-GPT by following our Twitter accounts. Engage with the developer and the AI's own account for interesting discussions, project updates, and more.Developer: Follow @siggravitas for insights into the development process, project updates, and related topics from the creator of Entrepreneur-GPT.We look forward to connecting with you and hearing your thoughts, ideas, and experiences with Auto-GPT. Join us on Twitter and let's explore the future of AI together!        "
43,huggingface/transformers,https://github.com/huggingface/transformers/blob/main/README.md,Python,"                                                                                                                    English |        简体中文 |        繁體中文 |        한국어 |        Español |        日本語 |        हिन्दी        State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow    🤗 Transformers provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio.These models can be applied on:📝 Text, for tasks like text classification, information extraction, question answering, summarization, translation, text generation, in over 100 languages.🖼️ Images, for tasks like image classification, object detection, and segmentation.🗣️ Audio, for tasks like speech recognition and audio classification.Transformer models can also perform tasks on several modalities combined, such as table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.🤗 Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments.🤗 Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch and TensorFlow — with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other.Online demosYou can test most of our models directly on their pages from the model hub. We also offer private model hosting, versioning, & an inference API for public and private models.Here are a few examples:In Natural Language Processing:Masked word completion with BERTName Entity Recognition with ElectraText generation with GPT-2Natural Language Inference with RoBERTaSummarization with BARTQuestion answering with DistilBERTTranslation with T5In Computer Vision:Image classification with ViTObject Detection with DETRSemantic Segmentation with SegFormerPanoptic Segmentation with MaskFormerDepth Estimation with DPTVideo Classification with VideoMAEUniversal Segmentation with OneFormerIn Audio:Automatic Speech Recognition with Wav2Vec2Keyword Spotting with Wav2Vec2Audio Classification with Audio Spectrogram TransformerIn Multimodal tasks:Table Question Answering with TAPASVisual Question Answering with ViLTZero-shot Image Classification with CLIPDocument Question Answering with LayoutLMZero-shot Video Classification with X-CLIP100 projects using TransformersTransformers is more than a toolkit to use pretrained models: it's a community of projects built around it and theHugging Face Hub. We want Transformers to enable developers, researchers, students, professors, engineers, and anyoneelse to build their dream projects.In order to celebrate the 100,000 stars of transformers, we have decided to put the spotlight on thecommunity, and we have created the awesome-transformers page which lists 100incredible projects built in the vicinity of transformers.If you own or use a project that you believe should be part of the list, please open a PR to add it!If you are looking for custom support from the Hugging Face team    Quick tourTo immediately use a model on a given input (text, image, audio, ...), we provide the pipeline API. Pipelines group together a pretrained model with the preprocessing that was used during that model's training. Here is how to quickly use a pipeline to classify positive versus negative texts:>>> from transformers import pipeline# Allocate a pipeline for sentiment-analysis>>> classifier = pipeline('sentiment-analysis')>>> classifier('We are very happy to introduce pipeline to the transformers repository.')[{'label': 'POSITIVE', 'score': 0.9996980428695679}]The second line of code downloads and caches the pretrained model used by the pipeline, while the third evaluates it on the given text. Here the answer is \""positive\"" with a confidence of 99.97%.Many tasks have a pre-trained pipeline ready to go, in NLP but also in computer vision and speech. For example, we can easily extract detected objects in an image:>>> import requests>>> from PIL import Image>>> from transformers import pipeline# Download an image with cute cats>>> url = \""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample.png\"">>> image_data = requests.get(url, stream=True).raw>>> image = Image.open(image_data)# Allocate a pipeline for object detection>>> object_detector = pipeline('object-detection')>>> object_detector(image)[{'score': 0.9982201457023621,  'label': 'remote',  'box': {'xmin': 40, 'ymin': 70, 'xmax': 175, 'ymax': 117}}, {'score': 0.9960021376609802,  'label': 'remote',  'box': {'xmin': 333, 'ymin': 72, 'xmax': 368, 'ymax': 187}}, {'score': 0.9954745173454285,  'label': 'couch',  'box': {'xmin': 0, 'ymin': 1, 'xmax': 639, 'ymax': 473}}, {'score': 0.9988006353378296,  'label': 'cat',  'box': {'xmin': 13, 'ymin': 52, 'xmax': 314, 'ymax': 470}}, {'score': 0.9986783862113953,  'label': 'cat',  'box': {'xmin': 345, 'ymin': 23, 'xmax': 640, 'ymax': 368}}]Here we get a list of objects detected in the image, with a box surrounding the object and a confidence score. Here is the original image on the left, with the predictions displayed on the right:        You can learn more about the tasks supported by the pipeline API in this tutorial.In addition to pipeline, to download and use any of the pretrained models on your given task, all it takes is three lines of code. Here is the PyTorch version:>>> from transformers import AutoTokenizer, AutoModel>>> tokenizer = AutoTokenizer.from_pretrained(\""bert-base-uncased\"")>>> model = AutoModel.from_pretrained(\""bert-base-uncased\"")>>> inputs = tokenizer(\""Hello world!\"", return_tensors=\""pt\"")>>> outputs = model(**inputs)And here is the equivalent code for TensorFlow:>>> from transformers import AutoTokenizer, TFAutoModel>>> tokenizer = AutoTokenizer.from_pretrained(\""bert-base-uncased\"")>>> model = TFAutoModel.from_pretrained(\""bert-base-uncased\"")>>> inputs = tokenizer(\""Hello world!\"", return_tensors=\""tf\"")>>> outputs = model(**inputs)The tokenizer is responsible for all the preprocessing the pretrained model expects, and can be called directly on a single string (as in the above examples) or a list. It will output a dictionary that you can use in downstream code or simply directly pass to your model using the ** argument unpacking operator.The model itself is a regular Pytorch nn.Module or a TensorFlow tf.keras.Model (depending on your backend) which you can use as usual. This tutorial explains how to integrate such a model into a classic PyTorch or TensorFlow training loop, or how to use our Trainer API to quickly fine-tune on a new dataset.Why should I use transformers?Easy-to-use state-of-the-art models:High performance on natural language understanding & generation, computer vision, and audio tasks.Low barrier to entry for educators and practitioners.Few user-facing abstractions with just three classes to learn.A unified API for using all our pretrained models.Lower compute costs, smaller carbon footprint:Researchers can share trained models instead of always retraining.Practitioners can reduce compute time and production costs.Dozens of architectures with over 60,000 pretrained models across all modalities.Choose the right framework for every part of a model's lifetime:Train state-of-the-art models in 3 lines of code.Move a single model between TF2.0/PyTorch/JAX frameworks at will.Seamlessly pick the right framework for training, evaluation and production.Easily customize a model or an example to your needs:We provide examples for each architecture to reproduce the results published by its original authors.Model internals are exposed as consistently as possible.Model files can be used independently of the library for quick experiments.Why shouldn't I use transformers?This library is not a modular toolbox of building blocks for neural nets. The code in the model files is not refactored with additional abstractions on purpose, so that researchers can quickly iterate on each of the models without diving into additional abstractions/files.The training API is not intended to work on any model but is optimized to work with the models provided by the library. For generic machine learning loops, you should use another library (possibly, Accelerate).While we strive to present as many use cases as possible, the scripts in our examples folder are just that: examples. It is expected that they won't work out-of-the box on your specific problem and that you will be required to change a few lines of code to adapt them to your needs.InstallationWith pipThis repository is tested on Python 3.8+, Flax 0.4.1+, PyTorch 1.10+ and TensorFlow 2.6+.You should install 🤗 Transformers in a virtual environment. If you're unfamiliar with Python virtual environments, check out the user guide.First, create a virtual environment with the version of Python you're going to use and activate it.Then, you will need to install at least one of Flax, PyTorch or TensorFlow.Please refer to TensorFlow installation page, PyTorch installation page and/or Flax and Jax installation pages regarding the specific installation command for your platform.When one of those backends has been installed, 🤗 Transformers can be installed using pip as follows:pip install transformersIf you'd like to play with the examples or need the bleeding edge of the code and can't wait for a new release, you must install the library from source.With condaSince Transformers version v4.0.0, we now have a conda channel: huggingface.🤗 Transformers can be installed using conda as follows:conda install -c huggingface transformersFollow the installation pages of Flax, PyTorch or TensorFlow to see how to install them with conda.NOTE:  On Windows, you may be prompted to activate Developer Mode in order to benefit from caching. If this is not an option for you, please let us know in this issue.Model architecturesAll the model checkpoints provided by 🤗 Transformers are seamlessly integrated from the huggingface.co model hub where they are uploaded directly by users and organizations.Current number of checkpoints: 🤗 Transformers currently provides the following architectures (see here for a high-level summary of each them):ALBERT (from Google Research and the Toyota Technological Institute at Chicago) released with the paper ALBERT: A Lite BERT for Self-supervised Learning of Language Representations, by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut.ALIGN (from Google Research) released with the paper Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision by Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig.AltCLIP (from BAAI) released with the paper AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities by Chen, Zhongzhi and Liu, Guang and Zhang, Bo-Wen and Ye, Fulong and Yang, Qinghong and Wu, Ledell.Audio Spectrogram Transformer (from MIT) released with the paper AST: Audio Spectrogram Transformer by Yuan Gong, Yu-An Chung, James Glass.Autoformer (from Tsinghua University) released with the paper Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting by Haixu Wu, Jiehui Xu, Jianmin Wang, Mingsheng Long.Bark (from Suno) released in the repository suno-ai/bark by Suno AI team.BART (from Facebook) released with the paper BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension by Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov and Luke Zettlemoyer.BARThez (from École polytechnique) released with the paper BARThez: a Skilled Pretrained French Sequence-to-Sequence Model by Moussa Kamal Eddine, Antoine J.-P. Tixier, Michalis Vazirgiannis.BARTpho (from VinAI Research) released with the paper BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese by Nguyen Luong Tran, Duong Minh Le and Dat Quoc Nguyen.BEiT (from Microsoft) released with the paper BEiT: BERT Pre-Training of Image Transformers by Hangbo Bao, Li Dong, Furu Wei.BERT (from Google) released with the paper BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.BERT For Sequence Generation (from Google) released with the paper Leveraging Pre-trained Checkpoints for Sequence Generation Tasks by Sascha Rothe, Shashi Narayan, Aliaksei Severyn.BERTweet (from VinAI Research) released with the paper BERTweet: A pre-trained language model for English Tweets by Dat Quoc Nguyen, Thanh Vu and Anh Tuan Nguyen.BigBird-Pegasus (from Google Research) released with the paper Big Bird: Transformers for Longer Sequences by Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.BigBird-RoBERTa (from Google Research) released with the paper Big Bird: Transformers for Longer Sequences by Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.BioGpt (from Microsoft Research AI4Science) released with the paper BioGPT: generative pre-trained transformer for biomedical text generation and mining by Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon and Tie-Yan Liu.BiT (from Google AI) released with the paper Big Transfer (BiT): General Visual Representation Learning by Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, Neil Houlsby.Blenderbot (from Facebook) released with the paper Recipes for building an open-domain chatbot by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.BlenderbotSmall (from Facebook) released with the paper Recipes for building an open-domain chatbot by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.BLIP (from Salesforce) released with the paper BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation by Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi.BLIP-2 (from Salesforce) released with the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi.BLOOM (from BigScience workshop) released by the BigScience Workshop.BORT (from Alexa) released with the paper Optimal Subarchitecture Extraction For BERT by Adrian de Wynter and Daniel J. Perry.BridgeTower (from Harbin Institute of Technology/Microsoft Research Asia/Intel Labs) released with the paper BridgeTower: Building Bridges Between Encoders in Vision-Language Representation Learning by Xiao Xu, Chenfei Wu, Shachar Rosenman, Vasudev Lal, Wanxiang Che, Nan Duan.ByT5 (from Google Research) released with the paper ByT5: Towards a token-free future with pre-trained byte-to-byte models by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel.CamemBERT (from Inria/Facebook/Sorbonne) released with the paper CamemBERT: a Tasty French Language Model by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Suárez*, Yoann Dupont, Laurent Romary, Éric Villemonte de la Clergerie, Djamé Seddah and Benoît Sagot.CANINE (from Google Research) released with the paper CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation by Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting.Chinese-CLIP (from OFA-Sys) released with the paper Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese by An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, Chang Zhou.CLAP (from LAION-AI) released with the paper Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation by Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, Shlomo Dubnov.CLIP (from OpenAI) released with the paper Learning Transferable Visual Models From Natural Language Supervision by Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever.CLIPSeg (from University of Göttingen) released with the paper Image Segmentation Using Text and Image Prompts by Timo Lüddecke and Alexander Ecker.CodeGen (from Salesforce) released with the paper A Conversational Paradigm for Program Synthesis by Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong.Conditional DETR (from Microsoft Research Asia) released with the paper Conditional DETR for Fast Training Convergence by Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng, Houqiang Li, Yuhui Yuan, Lei Sun, Jingdong Wang.ConvBERT (from YituTech) released with the paper ConvBERT: Improving BERT with Span-based Dynamic Convolution by Zihang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, Shuicheng Yan.ConvNeXT (from Facebook AI) released with the paper A ConvNet for the 2020s by Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie.ConvNeXTV2 (from Facebook AI) released with the paper ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders by Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, Saining Xie.CPM (from Tsinghua University) released with the paper CPM: A Large-scale Generative Chinese Pre-trained Language Model by Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin, Yusheng Su, Haozhe Ji, Jian Guan, Fanchao Qi, Xiaozhi Wang, Yanan Zheng, Guoyang Zeng, Huanqi Cao, Shengqi Chen, Daixuan Li, Zhenbo Sun, Zhiyuan Liu, Minlie Huang, Wentao Han, Jie Tang, Juanzi Li, Xiaoyan Zhu, Maosong Sun.CPM-Ant (from OpenBMB) released by the OpenBMB.CTRL (from Salesforce) released with the paper CTRL: A Conditional Transformer Language Model for Controllable Generation by Nitish Shirish Keskar*, Bryan McCann*, Lav R. Varshney, Caiming Xiong and Richard Socher.CvT (from Microsoft) released with the paper CvT: Introducing Convolutions to Vision Transformers by Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, Lei Zhang.Data2Vec (from Facebook) released with the paper Data2Vec:  A General Framework for Self-supervised Learning in Speech, Vision and Language by Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, Michael Auli.DeBERTa (from Microsoft) released with the paper DeBERTa: Decoding-enhanced BERT with Disentangled Attention by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.DeBERTa-v2 (from Microsoft) released with the paper DeBERTa: Decoding-enhanced BERT with Disentangled Attention by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.Decision Transformer (from Berkeley/Facebook/Google) released with the paper Decision Transformer: Reinforcement Learning via Sequence Modeling by Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch.Deformable DETR (from SenseTime Research) released with the paper Deformable DETR: Deformable Transformers for End-to-End Object Detection by Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai.DeiT (from Facebook) released with the paper Training data-efficient image transformers & distillation through attention by Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Hervé Jégou.DePlot (from Google AI) released with the paper DePlot: One-shot visual language reasoning by plot-to-table translation by Fangyu Liu, Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Wenhu Chen, Nigel Collier, Yasemin Altun.DETA (from The University of Texas at Austin) released with the paper NMS Strikes Back by Jeffrey Ouyang-Zhang, Jang Hyun Cho, Xingyi Zhou, Philipp Krähenbühl.DETR (from Facebook) released with the paper End-to-End Object Detection with Transformers by Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko.DialoGPT (from Microsoft Research) released with the paper DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation by Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan.DiNAT (from SHI Labs) released with the paper Dilated Neighborhood Attention Transformer by Ali Hassani and Humphrey Shi.DINOv2 (from Meta AI) released with the paper DINOv2: Learning Robust Visual Features without Supervision by Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, Piotr Bojanowski.DistilBERT (from HuggingFace), released together with the paper DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2 into DistilGPT2, RoBERTa into DistilRoBERTa, Multilingual BERT into DistilmBERT and a German version of DistilBERT.DiT (from Microsoft Research) released with the paper DiT: Self-supervised Pre-training for Document Image Transformer by Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, Furu Wei.Donut (from NAVER), released together with the paper OCR-free Document Understanding Transformer by Geewook Kim, Teakgyu Hong, Moonbin Yim, Jeongyeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, Seunghyun Park.DPR (from Facebook) released with the paper Dense Passage Retrieval for Open-Domain Question Answering by Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih.DPT (from Intel Labs) released with the paper Vision Transformers for Dense Prediction by René Ranftl, Alexey Bochkovskiy, Vladlen Koltun.EfficientFormer (from Snap Research) released with the paper EfficientFormer: Vision Transformers at MobileNetSpeed by Yanyu Li, Geng Yuan, Yang Wen, Ju Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, Jian Ren.EfficientNet (from Google Brain) released with the paper EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks by Mingxing Tan, Quoc V. Le.ELECTRA (from Google Research/Stanford University) released with the paper ELECTRA: Pre-training text encoders as discriminators rather than generators by Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning.EnCodec (from Meta AI) released with the paper High Fidelity Neural Audio Compression by Alexandre Défossez, Jade Copet, Gabriel Synnaeve, Yossi Adi.EncoderDecoder (from Google Research) released with the paper Leveraging Pre-trained Checkpoints for Sequence Generation Tasks by Sascha Rothe, Shashi Narayan, Aliaksei Severyn.ERNIE (from Baidu) released with the paper ERNIE: Enhanced Representation through Knowledge Integration by Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, Hua Wu.ErnieM (from Baidu) released with the paper ERNIE-M: Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora by Xuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang.ESM (from Meta AI) are transformer protein language models.  ESM-1b was released with the paper Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences by Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, and Rob Fergus. ESM-1v was released with the paper Language models enable zero-shot prediction of the effects of mutations on protein function by Joshua Meier, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu and Alexander Rives. ESM-2 and ESMFold were released with the paper Language models of protein sequences at the scale of evolution enable accurate structure prediction by Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Sal Candido, Alexander Rives.Falcon (from Technology Innovation Institute) by Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme.FLAN-T5 (from Google AI) released in the repository google-research/t5x by Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason WeiFLAN-UL2 (from Google AI) released in the repository google-research/t5x by Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason WeiFlauBERT (from CNRS) released with the paper FlauBERT: Unsupervised Language Model Pre-training for French by Hang Le, Loïc Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen, Benoît Crabbé, Laurent Besacier, Didier Schwab.FLAVA (from Facebook AI) released with the paper FLAVA: A Foundational Language And Vision Alignment Model by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela.FNet (from Google Research) released with the paper FNet: Mixing Tokens with Fourier Transforms by James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon.FocalNet (from Microsoft Research) released with the paper Focal Modulation Networks by Jianwei Yang, Chunyuan Li, Xiyang Dai, Lu Yuan, Jianfeng Gao.Funnel Transformer (from CMU/Google Brain) released with the paper Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le.GIT (from Microsoft Research) released with the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, Lijuan Wang.GLPN (from KAIST) released with the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Doyeon Kim, Woonghyun Ga, Pyungwhan Ahn, Donggyu Joo, Sehwan Chun, Junmo Kim.GPT (from OpenAI) released with the paper Improving Language Understanding by Generative Pre-Training by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever.GPT Neo (from EleutherAI) released in the repository EleutherAI/gpt-neo by Sid Black, Stella Biderman, Leo Gao, Phil Wang and Connor Leahy.GPT NeoX (from EleutherAI) released with the paper GPT-NeoX-20B: An Open-Source Autoregressive Language Model by Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, Samuel WeinbachGPT NeoX Japanese (from ABEJA) released by Shinya Otani, Takayoshi Makabe, Anuj Arora, and Kyo Hattori.GPT-2 (from OpenAI) released with the paper Language Models are Unsupervised Multitask Learners by Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei** and Ilya Sutskever**.GPT-J (from EleutherAI) released in the repository kingoflolz/mesh-transformer-jax by Ben Wang and Aran Komatsuzaki.GPT-Sw3 (from AI-Sweden) released with the paper Lessons Learned from GPT-SW3: Building the First Large-Scale Generative Language Model for Swedish by Ariel Ekgren, Amaru Cuba Gyllensten, Evangelia Gogoulou, Alice Heiman, Severine Verlinden, Joey Öhman, Fredrik Carlsson, Magnus Sahlgren.GPTBigCode (from BigCode) released with the paper SantaCoder: don't reach for the stars! by Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, Joel Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco De Toni, Bernardo García del Río, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries, Leandro von Werra.GPTSAN-japanese released in the repository tanreinama/GPTSAN by Toshiyuki Sakamoto(tanreinama).Graphormer (from Microsoft) released with the paper Do Transformers Really Perform Bad for Graph Representation? by Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, Tie-Yan Liu.GroupViT (from UCSD, NVIDIA) released with the paper GroupViT: Semantic Segmentation Emerges from Text Supervision by Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, Xiaolong Wang.Hubert (from Facebook) released with the paper HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed.I-BERT (from Berkeley) released with the paper I-BERT: Integer-only BERT Quantization by Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W. Mahoney, Kurt Keutzer.ImageGPT (from OpenAI) released with the paper Generative Pretraining from Pixels by Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, Ilya Sutskever.Informer (from Beihang University, UC Berkeley, Rutgers University, SEDD Company) released with the paper Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting by Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang.InstructBLIP (from Salesforce) released with the paper InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning by Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi.Jukebox (from OpenAI) released with the paper Jukebox: A Generative Model for Music by Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, Ilya Sutskever.LayoutLM (from Microsoft Research Asia) released with the paper LayoutLM: Pre-training of Text and Layout for Document Image Understanding by Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou.LayoutLMv2 (from Microsoft Research Asia) released with the paper LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding by Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, Lidong Zhou.LayoutLMv3 (from Microsoft Research Asia) released with the paper LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking by Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, Furu Wei.LayoutXLM (from Microsoft Research Asia) released with the paper LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding by Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Furu Wei.LED (from AllenAI) released with the paper Longformer: The Long-Document Transformer by Iz Beltagy, Matthew E. Peters, Arman Cohan.LeViT (from Meta AI) released with the paper LeViT: A Vision Transformer in ConvNet's Clothing for Faster Inference by Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Hervé Jégou, Matthijs Douze.LiLT (from South China University of Technology) released with the paper LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding by Jiapeng Wang, Lianwen Jin, Kai Ding.LLaMA (from The FAIR team of Meta AI) released with the paper LLaMA: Open and Efficient Foundation Language Models by Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample.Llama2 (from The FAIR team of Meta AI) released with the paper Llama2: Open Foundation and Fine-Tuned Chat Models by Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushka rMishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing EllenTan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom.Longformer (from AllenAI) released with the paper Longformer: The Long-Document Transformer by Iz Beltagy, Matthew E. Peters, Arman Cohan.LongT5 (from Google AI) released with the paper LongT5: Efficient Text-To-Text Transformer for Long Sequences by Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, Yinfei Yang.LUKE (from Studio Ousia) released with the paper LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention by Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, Yuji Matsumoto.LXMERT (from UNC Chapel Hill) released with the paper LXMERT: Learning Cross-Modality Encoder Representations from Transformers for Open-Domain Question Answering by Hao Tan and Mohit Bansal.M-CTC-T (from Facebook) released with the paper Pseudo-Labeling For Massively Multilingual Speech Recognition by Loren Lugosch, Tatiana Likhomanenko, Gabriel Synnaeve, and Ronan Collobert.M2M100 (from Facebook) released with the paper Beyond English-Centric Multilingual Machine Translation by Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, Armand Joulin.MarianMT Machine translation models trained using OPUS data by Jörg Tiedemann. The Marian Framework is being developed by the Microsoft Translator Team.MarkupLM (from Microsoft Research Asia) released with the paper MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document Understanding by Junlong Li, Yiheng Xu, Lei Cui, Furu Wei.Mask2Former (from FAIR and UIUC) released with the paper Masked-attention Mask Transformer for Universal Image Segmentation by Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, Rohit Girdhar.MaskFormer (from Meta and UIUC) released with the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation by Bowen Cheng, Alexander G. Schwing, Alexander Kirillov.MatCha (from Google AI) released with the paper MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering by Fangyu Liu, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Yasemin Altun, Nigel Collier, Julian Martin Eisenschlos.mBART (from Facebook) released with the paper Multilingual Denoising Pre-training for Neural Machine Translation by Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, Luke Zettlemoyer.mBART-50 (from Facebook) released with the paper Multilingual Translation with Extensible Multilingual Pretraining and Finetuning by Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, Angela Fan.MEGA (from Meta/USC/CMU/SJTU) released with the paper Mega: Moving Average Equipped Gated Attention by Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer.Megatron-BERT (from NVIDIA) released with the paper Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper and Bryan Catanzaro.Megatron-GPT2 (from NVIDIA) released with the paper Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper and Bryan Catanzaro.MGP-STR (from Alibaba Research) released with the paper Multi-Granularity Prediction for Scene Text Recognition by Peng Wang, Cheng Da, and Cong Yao.mLUKE (from Studio Ousia) released with the paper mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models by Ryokan Ri, Ikuya Yamada, and Yoshimasa Tsuruoka.MMS (from Facebook) released with the paper Scaling Speech Technology to 1,000+ Languages by Vineel Pratap, Andros Tjandra, Bowen Shi, Paden Tomasello, Arun Babu, Sayani Kundu, Ali Elkahky, Zhaoheng Ni, Apoorv Vyas, Maryam Fazel-Zarandi, Alexei Baevski, Yossi Adi, Xiaohui Zhang, Wei-Ning Hsu, Alexis Conneau, Michael Auli.MobileBERT (from CMU/Google Brain) released with the paper MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices by Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou.MobileNetV1 (from Google Inc.) released with the paper MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications by Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam.MobileNetV2 (from Google Inc.) released with the paper MobileNetV2: Inverted Residuals and Linear Bottlenecks by Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen.MobileViT (from Apple) released with the paper MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer by Sachin Mehta and Mohammad Rastegari.MobileViTV2 (from Apple) released with the paper Separable Self-attention for Mobile Vision Transformers by Sachin Mehta and Mohammad Rastegari.MPNet (from Microsoft Research) released with the paper MPNet: Masked and Permuted Pre-training for Language Understanding by Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu.MPT (from MosaiML) released with the repository llm-foundry by the MosaicML NLP Team.MRA (from the University of Wisconsin - Madison) released with the paper Multi Resolution Analysis (MRA) for Approximate Self-Attention by Zhanpeng Zeng, Sourav Pal, Jeffery Kline, Glenn M Fung, Vikas Singh.MT5 (from Google AI) released with the paper mT5: A massively multilingual pre-trained text-to-text transformer by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel.MusicGen (from Meta) released with the paper Simple and Controllable Music Generation by Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi and Alexandre Défossez.MVP (from RUC AI Box) released with the paper MVP: Multi-task Supervised Pre-training for Natural Language Generation by Tianyi Tang, Junyi Li, Wayne Xin Zhao and Ji-Rong Wen.NAT (from SHI Labs) released with the paper Neighborhood Attention Transformer by Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and Humphrey Shi.Nezha (from Huawei Noah’s Ark Lab) released with the paper NEZHA: Neural Contextualized Representation for Chinese Language Understanding by Junqiu Wei, Xiaozhe Ren, Xiaoguang Li, Wenyong Huang, Yi Liao, Yasheng Wang, Jiashu Lin, Xin Jiang, Xiao Chen and Qun Liu.NLLB (from Meta) released with the paper No Language Left Behind: Scaling Human-Centered Machine Translation by the NLLB team.NLLB-MOE (from Meta) released with the paper No Language Left Behind: Scaling Human-Centered Machine Translation by the NLLB team.Nyströmformer (from the University of Wisconsin - Madison) released with the paper Nyströmformer: A Nyström-Based Algorithm for Approximating Self-Attention by Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, Vikas Singh.OneFormer (from SHI Labs) released with the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov, Humphrey Shi.OpenLlama (from s-JoL) released in Open-Llama.OPT (from Meta AI) released with the paper OPT: Open Pre-trained Transformer Language Models by Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen et al.OWL-ViT (from Google AI) released with the paper Simple Open-Vocabulary Object Detection with Vision Transformers by Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby.Pegasus (from Google) released with the paper PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu.PEGASUS-X (from Google) released with the paper Investigating Efficiently Extending Transformers for Long Input Summarization by Jason Phang, Yao Zhao, and Peter J. Liu.Perceiver IO (from Deepmind) released with the paper Perceiver IO: A General Architecture for Structured Inputs & Outputs by Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier Hénaff, Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, João Carreira.PhoBERT (from VinAI Research) released with the paper PhoBERT: Pre-trained language models for Vietnamese by Dat Quoc Nguyen and Anh Tuan Nguyen.Pix2Struct (from Google) released with the paper Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding by Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, Kristina Toutanova.PLBart (from UCLA NLP) released with the paper Unified Pre-training for Program Understanding and Generation by Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, Kai-Wei Chang.PoolFormer (from Sea AI Labs) released with the paper MetaFormer is Actually What You Need for Vision by Yu, Weihao and Luo, Mi and Zhou, Pan and Si, Chenyang and Zhou, Yichen and Wang, Xinchao and Feng, Jiashi and Yan, Shuicheng.ProphetNet (from Microsoft Research) released with the paper ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang and Ming Zhou.PVT (from Nanjing University, The University of Hong Kong etc.) released with the paper Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions by Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao.QDQBert (from NVIDIA) released with the paper Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation by Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev and Paulius Micikevicius.RAG (from Facebook) released with the paper Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela.REALM (from Google Research) released with the paper REALM: Retrieval-Augmented Language Model Pre-Training by Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat and Ming-Wei Chang.Reformer (from Google Research) released with the paper Reformer: The Efficient Transformer by Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya.RegNet (from META Platforms) released with the paper Designing Network Design Space by Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, Piotr Dollár.RemBERT (from Google Research) released with the paper Rethinking embedding coupling in pre-trained language models by Hyung Won Chung, Thibault Févry, Henry Tsai, M. Johnson, Sebastian Ruder.ResNet (from Microsoft Research) released with the paper Deep Residual Learning for Image Recognition by Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun.RoBERTa (from Facebook), released together with the paper RoBERTa: A Robustly Optimized BERT Pretraining Approach by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.RoBERTa-PreLayerNorm (from Facebook) released with the paper fairseq: A Fast, Extensible Toolkit for Sequence Modeling by Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli.RoCBert (from WeChatAI) released with the paper RoCBert: Robust Chinese Bert with Multimodal Contrastive Pretraining by HuiSu, WeiweiShi, XiaoyuShen, XiaoZhou, TuoJi, JiaruiFang, JieZhou.RoFormer (from ZhuiyiTechnology), released together with the paper RoFormer: Enhanced Transformer with Rotary Position Embedding by Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu.RWKV (from Bo Peng), released on this repo by Bo Peng.SegFormer (from NVIDIA) released with the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping Luo.Segment Anything (from Meta AI) released with the paper Segment Anything by Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alex Berg, Wan-Yen Lo, Piotr Dollar, Ross Girshick.SEW (from ASAPP) released with the paper Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition by Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.SEW-D (from ASAPP) released with the paper Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition by Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.SpeechT5 (from Microsoft Research) released with the paper SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing by Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei.SpeechToTextTransformer (from Facebook), released together with the paper fairseq S2T: Fast Speech-to-Text Modeling with fairseq by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Dmytro Okhonko, Juan Pino.SpeechToTextTransformer2 (from Facebook), released together with the paper Large-Scale Self- and Semi-Supervised Learning for Speech Translation by Changhan Wang, Anne Wu, Juan Pino, Alexei Baevski, Michael Auli, Alexis Conneau.Splinter (from Tel Aviv University), released together with the paper Few-Shot Question Answering by Pretraining Span Selection by Ori Ram, Yuval Kirstain, Jonathan Berant, Amir Globerson, Omer Levy.SqueezeBERT (from Berkeley) released with the paper SqueezeBERT: What can computer vision teach NLP about efficient neural networks? by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer.SwiftFormer (from MBZUAI) released with the paper SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications by Abdelrahman Shaker, Muhammad Maaz, Hanoona Rasheed, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan.Swin Transformer (from Microsoft) released with the paper Swin Transformer: Hierarchical Vision Transformer using Shifted Windows by Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo.Swin Transformer V2 (from Microsoft) released with the paper Swin Transformer V2: Scaling Up Capacity and Resolution by Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, Baining Guo.Swin2SR (from University of Würzburg) released with the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Marcos V. Conde, Ui-Jin Choi, Maxime Burchi, Radu Timofte.SwitchTransformers (from Google) released with the paper Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity by William Fedus, Barret Zoph, Noam Shazeer.T5 (from Google AI) released with the paper Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu.T5v1.1 (from Google AI) released in the repository google-research/text-to-text-transfer-transformer by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu.Table Transformer (from Microsoft Research) released with the paper PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents by Brandon Smock, Rohith Pesala, Robin Abraham.TAPAS (from Google AI) released with the paper TAPAS: Weakly Supervised Table Parsing via Pre-training by Jonathan Herzig, Paweł Krzysztof Nowak, Thomas Müller, Francesco Piccinno and Julian Martin Eisenschlos.TAPEX (from Microsoft Research) released with the paper TAPEX: Table Pre-training via Learning a Neural SQL Executor by Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou.Time Series Transformer (from HuggingFace).TimeSformer (from Facebook) released with the paper Is Space-Time Attention All You Need for Video Understanding? by Gedas Bertasius, Heng Wang, Lorenzo Torresani.Trajectory Transformer (from the University of California at Berkeley) released with the paper Offline Reinforcement Learning as One Big Sequence Modeling Problem by Michael Janner, Qiyang Li, Sergey LevineTransformer-XL (from Google/CMU) released with the paper Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context by Zihang Dai*, Zhilin Yang*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.TrOCR (from Microsoft), released together with the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, Furu Wei.TVLT (from UNC Chapel Hill) released with the paper TVLT: Textless Vision-Language Transformer by Zineng Tang, Jaemin Cho, Yixin Nie, Mohit Bansal.UL2 (from Google Research) released with the paper Unifying Language Learning Paradigms by Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, Donald MetzlerUMT5 (from Google Research) released with the paper UniMax: Fairer and More Effective Language Sampling for Large-Scale Multilingual Pretraining by Hyung Won Chung, Xavier Garcia, Adam Roberts, Yi Tay, Orhan Firat, Sharan Narang, Noah Constant.UniSpeech (from Microsoft Research) released with the paper UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael Zeng, Xuedong Huang.UniSpeechSat (from Microsoft Research) released with the paper UNISPEECH-SAT: UNIVERSAL SPEECH REPRESENTATION LEARNING WITH SPEAKER AWARE PRE-TRAINING by Sanyuan Chen, Yu Wu, Chengyi Wang, Zhengyang Chen, Zhuo Chen, Shujie Liu, Jian Wu, Yao Qian, Furu Wei, Jinyu Li, Xiangzhan Yu.UPerNet (from Peking University) released with the paper Unified Perceptual Parsing for Scene Understanding by Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, Jian Sun.VAN (from Tsinghua University and Nankai University) released with the paper Visual Attention Network by Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, Shi-Min Hu.VideoMAE (from Multimedia Computing Group, Nanjing University) released with the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Zhan Tong, Yibing Song, Jue Wang, Limin Wang.ViLT (from NAVER AI Lab/Kakao Enterprise/Kakao Brain) released with the paper ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision by Wonjae Kim, Bokyung Son, Ildoo Kim.Vision Transformer (ViT) (from Google AI) released with the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.VisualBERT (from UCLA NLP) released with the paper VisualBERT: A Simple and Performant Baseline for Vision and Language by Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang.ViT Hybrid (from Google AI) released with the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.ViTMAE (from Meta AI) released with the paper Masked Autoencoders Are Scalable Vision Learners by Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick.ViTMSN (from Meta AI) released with the paper Masked Siamese Networks for Label-Efficient Learning by Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes, Pascal Vincent, Armand Joulin, Michael Rabbat, Nicolas Ballas.ViViT (from Google Research) released with the paper ViViT: A Video Vision Transformer by Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lučić, Cordelia Schmid.Wav2Vec2 (from Facebook AI) released with the paper wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.Wav2Vec2-Conformer (from Facebook AI) released with the paper FAIRSEQ S2T: Fast Speech-to-Text Modeling with FAIRSEQ by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Sravya Popuri, Dmytro Okhonko, Juan Pino.Wav2Vec2Phoneme (from Facebook AI) released with the paper Simple and Effective Zero-shot Cross-lingual Phoneme Recognition by Qiantong Xu, Alexei Baevski, Michael Auli.WavLM (from Microsoft Research) released with the paper WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing by Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Furu Wei.Whisper (from OpenAI) released with the paper Robust Speech Recognition via Large-Scale Weak Supervision by Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever.X-CLIP (from Microsoft Research) released with the paper Expanding Language-Image Pretrained Models for General Video Recognition by Bolin Ni, Houwen Peng, Minghao Chen, Songyang Zhang, Gaofeng Meng, Jianlong Fu, Shiming Xiang, Haibin Ling.X-MOD (from Meta AI) released with the paper Lifting the Curse of Multilinguality by Pre-training Modular Transformers by Jonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James Cross, Sebastian Riedel, Mikel Artetxe.XGLM (From Facebook AI) released with the paper Few-shot Learning with Multilingual Language Models by Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, Xian Li.XLM (from Facebook) released together with the paper Cross-lingual Language Model Pretraining by Guillaume Lample and Alexis Conneau.XLM-ProphetNet (from Microsoft Research) released with the paper ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang and Ming Zhou.XLM-RoBERTa (from Facebook AI), released together with the paper Unsupervised Cross-lingual Representation Learning at Scale by Alexis Conneau*, Kartikay Khandelwal*, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov.XLM-RoBERTa-XL (from Facebook AI), released together with the paper Larger-Scale Transformers for Multilingual Masked Language Modeling by Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, Alexis Conneau.XLM-V (from Meta AI) released with the paper XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models by Davis Liang, Hila Gonen, Yuning Mao, Rui Hou, Naman Goyal, Marjan Ghazvininejad, Luke Zettlemoyer, Madian Khabsa.XLNet (from Google/CMU) released with the paper ​XLNet: Generalized Autoregressive Pretraining for Language Understanding by Zhilin Yang*, Zihang Dai*, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.XLS-R (from Facebook AI) released with the paper XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale by Arun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman Goyal, Kritika Singh, Patrick von Platen, Yatharth Saraf, Juan Pino, Alexei Baevski, Alexis Conneau, Michael Auli.XLSR-Wav2Vec2 (from Facebook AI) released with the paper Unsupervised Cross-Lingual Representation Learning For Speech Recognition by Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, Michael Auli.YOLOS (from Huazhong University of Science & Technology) released with the paper You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Yuxin Fang, Bencheng Liao, Xinggang Wang, Jiemin Fang, Jiyang Qi, Rui Wu, Jianwei Niu, Wenyu Liu.YOSO (from the University of Wisconsin - Madison) released with the paper You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling by Zhanpeng Zeng, Yunyang Xiong, Sathya N. Ravi, Shailesh Acharya, Glenn Fung, Vikas Singh.Want to contribute a new model? We have added a detailed guide and templates to guide you in the process of adding a new model. You can find them in the templates folder of the repository. Be sure to check the contributing guidelines and contact the maintainers or open an issue to collect feedbacks before starting your PR.To check if each model has an implementation in Flax, PyTorch or TensorFlow, or has an associated tokenizer backed by the 🤗 Tokenizers library, refer to this table.These implementations have been tested on several datasets (see the example scripts) and should match the performance of the original implementations. You can find more details on performance in the Examples section of the documentation.Learn moreSectionDescriptionDocumentationFull API documentation and tutorialsTask summaryTasks supported by 🤗 TransformersPreprocessing tutorialUsing the Tokenizer class to prepare data for the modelsTraining and fine-tuningUsing the models provided by 🤗 Transformers in a PyTorch/TensorFlow training loop and the Trainer APIQuick tour: Fine-tuning/usage scriptsExample scripts for fine-tuning models on a wide range of tasksModel sharing and uploadingUpload and share your fine-tuned models with the communityCitationWe now have a paper you can cite for the 🤗 Transformers library:@inproceedings{wolf-etal-2020-transformers,    title = \""Transformers: State-of-the-Art Natural Language Processing\"",    author = \""Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush\"",    booktitle = \""Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\"",    month = oct,    year = \""2020\"",    address = \""Online\"",    publisher = \""Association for Computational Linguistics\"",    url = \""https://www.aclweb.org/anthology/2020.emnlp-demos.6\"",    pages = \""38--45\""}"
44,shadowsocks/shadowsocks,https://github.com/shadowsocks/shadowsocks/blob/rm/README.md,Python,Removed according to regulations.
45,zero-to-mastery/start-here-guidelines,https://github.com/zero-to-mastery/start-here-guidelines/blob/master/README.md,Python,"One rule of this community:We don't care if you break things. This is a playground, and we encourage failing often. Use this as a practice ground, and enjoy contributing to projects you create with your fellow students. Many students have gained real-world experience \""working in teams\"" by working on these projects.A Guide to Get Started (used to be the 4 step guide)Check out Andrei's videos on github if you haven't watched it already.On the GitHub page for this repository, click on the button \""Fork.\""Clone your forked repository to your computer:For example, run this command inside your terminal:git clone https://github.com/<your-github-username>/start-here-guidelines.gitReplace <your-github-username>!Learn more about forking and cloning a repo.Move to project directory:cd start-here-guidelinesBefore you make any changes, keep your fork in sync to avoid merge conflicts:git remote add upstream https://github.com/zero-to-mastery/start-here-guidelines.gitgit pull upstream masterIf you run into a merge conflict, you have to resolve the conflict. There are a lot of guides online, or you can watch this tutorial.After adding the upstream and checking that all files are up to date, we now will create new branch before editing any files. There are two ways to do so:git checkout -b <branch-name>git branch <branch-name>git switch <branch-name>On your computer, open your text editor, and add your name to the CONTRIBUTORS.md file.⚠️ IMPORTANT NOTE #1: Add your name somewhere in the middle. Not at the top or bottom in order to avoid the chance of you getting a merge conflict!⚠️ IMPORTANT NOTE #2: Please do NOT edit or remove other people from the list, even to fix their indentation etc. This will likely prevent your PR from being merged.Add the changes with git add, git commit (write a good commit message, if possible):git add CONTRIBUTORS.mdgit commit -m \""Add <your-github-username>\""Replace <your-github-username>!Push your changes to your repository:git push origin <branch-name>Go to the GitHub page of your fork, and make a pull request:Read more about pull requests on the GitHub help pages.Wait until Zerobot or one of the maintainers merges your pull request. If there are any conflicts, you will get a notification and be required to resolve the conflict.Go join a project and start contributing or create your own group apps. Don't be shy and enjoy creating things together (We have over 20 projects for all levels of programmers)! Check out this guide for more information on selecting a project.To see the Zero to Mastery Icon in your GitHub profile, follow these steps (you must complete steps 1 and 2 for this to work).Anatomy of an open-source project:Every open-source community is different.Spending years on one open-source project means you’ve gotten to know one open-source project. Move to a different project, and you might find the vocabulary, norms, and communication styles are completely different.That being said, many open-source projects follow a similar organizational structure. Understanding the different community roles and overall process will help you get quickly oriented to any new project.A typical open-source project has the following types of people:Author: The person(s) or organization that created the project.Owner: The person(s) who has administrative ownership over the organization or repository (not always the same as the original author).Maintainers: Contributors who are responsible for driving the vision and managing the organizational aspects of the project (may also be authors or owners of the project).Contributors: Everyone who has contributed something back to the project.Community Members: People who use the project. They might be active in conversations or express their opinion on the project’s direction.Bigger projects may also have subcommittees or working groups focused on different tasks, such as tooling, triage, community moderation, and event organizing. Look on a project’s website for a “team” page or in the repository for governance documentation to find this information.A project also has documentation. These files are usually listed in the top level of a repository.LICENSE: By definition, every open-source project must have an open-source license. If the project does not have a license, it is not open source.README: The README is the instruction manual that welcomes new community members to the project. It explains why the project is useful and how to get started.CONTRIBUTING: Whereas READMEs help people use the project, contributing docs help people contribute to the project. It explains what types of contributions are needed and how the process works. While not every project has a CONTRIBUTING file, its presence signals that this is a welcoming project to contribute to.CODE_OF_CONDUCT: The code of conduct sets ground rules for participants’ behavior and helps to facilitate a friendly, welcoming environment. While not every project has a CODE_OF_CONDUCT file, its presence signals that this is a welcoming project to contribute to.Other documentation: There might be additional documentation such as tutorials, walkthroughs, or governance policies, especially on bigger projects.Finally, open-source projects use the following tools to organize discussion. Reading through the archives will give you a good picture of how the community thinks and works.Issue tracker: Where people discuss issues related to the project.Pull requests: Where people discuss and review changes that are in progress.Discussion forums or mailing lists: Some projects may use these channels for conversational topics (for example, “How do I…“ or “What do you think about…“ instead of bug reports or feature requests). Others use the issue tracker for all conversations.Synchronous chat channel: Some projects use chat channels (such as Discord or IRC) for casual conversation, collaboration, and quick exchanges.Get all the ZTM Courses, for one monthly subscription here."
46,Ebazhanov/linkedin-skill-assessments-quizzes,https://github.com/Ebazhanov/linkedin-skill-assessments-quizzes/blob/main/README.md,Python,"Linkedin Skill assessments - Answers⚠️ DISCLAIMER: The owners of this repository are not liable for any illegal usage of the content provided in this repository. The content is provided for informational and educational purposes only, and any actions taken by users of this repository are the responsibility of the user. By accessing this repository, you agree to hold the owners harmless from any claims, damages, or expenses arising from the use of the information provided.[ Go to see the last contributor ]🙏 PLEASEAlways add explanation (or reference link) to your answers. Use online grammar checker.That would help anyone to better learn new concepts!🎉 AnnouncementsColumn Translation have links to quizzes in different languages like Es, Fr, It and De.If you want to meet each other or discuss quiz related problems or maybe ask for skills endorsement just join the Discord chat.Playground before taking quiz using:MD2Practice (Web App)Skill Assessments Quizzes (Web App)LinkedIn Quizzed with Kodyfire (Terminal)Want to contribute? Here is the source code.❓ Need help?Open new issue🔥 Open in VS Code view here or thereTable of ContentsLinkedin-quiz-questionsPassed/FailedTranslated  in ...QuestionsAnswersYour resource for answers. In case you have doubts please contact this person or add them to review your PR.Accounting❗needs updating  5049@tujinwei, @mervynteo, @johnfelipeAdobe-Acrobat  2722Adobe-Illustrator❗needs updating  7674Adobe-InDesign❗needs updating  4240Adobe-Lightroom❗needs updating  2020Adobe-Photoshop❗needs updating  9393@declarckAdobe Premiere Pro  4836Adobe XD  1613After Effects❗needs updating  2413Agile Methodologies❗needs updating  116116@its0x08Android  7272@surajsahani, @mr-shoxruxbek, @ItSNeverLateAngular  7965@vanekbr, @aamita96ArcGIS Products55AutoCAD❗needs updating  7775@djayorAutodesk Fusion 360❗needs updating  3725@djayor, @tm-sanjayAutodesk Maya3030@marifogluAWS  9999@jokerkeny, @Amsal1AWS-Lambda❗needs updating  5149Bash  7877@D4RIO, @Amsal1C#6161@LiviuSosu, @RamonMartinezNieto, @declarckC++❗needs updating7373@Amsal1, @Amsal1C (Programming Language)8383@makifay, @Amsal1, @its0x08CSS122116@BHARGAVPATEL1244Cybersecurity❗needs updating10196Django7171@PROCW.NET Framework6359@declarckEclipse❗needs updating3628Front-end Development6868@vanekbr, @ShankS3, @declarckGit134134@Emanuele-emGo (Programming Language)4040@ruslanbes, @monkrusGoogle Ads2925Google Analytics8282Google Cloud Platform (GCP)5250@antra0497Hadoop7154HTML129128@declarckIT Operations5454@asahioceanJava130130@sumanas27, @ruslanbes, @PROCWJavascript131131@taletski, @PROCW, @msteiner96, @declarckjQuery8477@declarckJSON❗needs updating8786@iHamzaKhanzadaKeynote140Kotlin7878@ItSNeverLate, @HusseinhjLinux8278@D4RIO, @Amsal1Logic Pro8278Machine Learning9898@aaronwangj, @antra0497MATLAB7070@tm-sanjayMaven5350Microsoft Access3028@drmegalomaniacMicrosoft Azure5553@tomtreffke, @ziasistaniMicrosoft Excel❗needs updating109107@gazihasanrahmanMicrosoft Outlook7956Microsoft Power Automate1402@mervynteoMicrosoft Power BI8180@vittorio-giattiMicrosoft Power Point8577@ckulloMicrosoft Project❗needs updating4443Microsoft Word❗needs updating7877MongoDB7777MySQL9797@ruslanbesnode.js7976@pbachmanNoSQL5655objective-c4038OOP10282@declarck, @gaurovgiriPHP8979@ruslanbes, @msteiner96Pro Tools22Python176176@tik9, @Amsal1, @declarck, @TSG405QuickBooks❗needs updating6739R5252@gregglindReact.js100100@RobTables @bandinoplaREST API6565Revit❗needs updating140Ruby on Rails5959@gudataRust3232@BobbyByrne @Emanuele-emScala5248Search Engine Optimization (SEO)8181SharePoint❗needs updating5338Sketchup22SOLIDWORKS❗needs updating5757@BHARGAVPATEL1244Spring Framework6767Swift6767Transact-SQL (T-SQL)4542@beefydog, @BenVlodgiUnity❗needs updating4746@uno-sebastianVisual Basic for Applications (VBA)❗needs updating3634@AdamKaczor6250Visio3535Windows Server6857WordPress8073@ruslanbes, @Amsal1XML4342@ruslanbesContributors ✨Thanks goes to these wonderful people (emoji key):            Evgenii💻 🖋      Sergei Stadnik💻 🔍 🤔 📖      Santhosh💻      Jacob Dsa💻 🖋      Aaron Meese💻 🖋      arqarq💻 🖋      Amit Yadav💻 🖋              Javokhir Nazarov💻 🖋      saurav kumar🖋      Chetan🖋      Amir Hossein Shekari🎨 🖋 💻      SergDaut🎨      Nilotpal Pramanik🎨 💻 🖋 💼 📖 🔣 💡      Abhishek Kumar🎨              Monu Gupta🎨      KARTIKEYA GUPTA💻 🖋      kenkyusha💻 🖋      juandavidtowers💻 🖋      cyber-netics💻 🖋      jtrisw💻 🖋      Renato Regalado💻 🖋              Matthew💻 🖋      Jan S.💻 🖋      Manoli💻 🖋      Faraz tanveer💻 🖋      mohnishkarri💻 🖋 🎨      andyzhu💻 🖋      Vishal Kushwah💻 🖋              Yurii Yakymenko💻 🖋      Swetabh Suman💻 🖋      AJAY DANDGE💻 🖋      Mehmet Yesin🎨      Lok Chun Wai🎨      Adria de Juan🎨      GL-Man🎨              Jheel Patel🎨      Sameer Waskar🎨      Alexander Andrews🎨      Alexander Maxwell🎨      Slava🎨      Mayur Khatri🎨      Mascantosh💻 🖋 📢 🤔              Kivanc Enes🎨      Ritika Das🎨      Zer07793🎨      Andrew Cheung🎨      Sadha🎨      tainenko🎨 💻      github-star-coder🎨              Danilo Oliveira🎨      lordeko🎨      Shubham Kumar🎨 💻      testtree🎨      Cheryl Murphy🎨 💻      Bipin Thomas🎨      Abdulrahman Hisham🎨              Dakshitha Dissanayaka🎨      BADR KACIMI🎨      Alex Wang🎨      Maxim🎨      GordonGrant🎨 💻      Ephrem Demelash🎨      JonOrcutt🎨              topdev10🎨      cookwellwebsite🎨      xren935🎨      Nemo Frenkel🎨      MD SAIF ALAM🎨      Boris López Araya🎨      Larry Chiem🎨              Muhammad Bilal Ilyas🎨      AliMilani🎨 💻      Suraj Sahani🎨      FlyingSquirrel🎨      Erick Tijero🎨      Jaskaran Kukreja🎨      MichaelL🎨              MagicLegend🎨      Dereck Bearsong🎨      Pappu Kumar Pashi🎨      Venkata Kishore Tavva🎨      Rafat Touqir Rafsun🎨      Snehesh Dutta🎨      Timo Körner🎨 💻              alexxxan🎨      GGJason🎨      LeeAnna Ewing🎨 🤔      kamal Jyotwal🎨      Bob-Johns🎨 💻 🖋      yunussalmanlyit🎨 💻      chilcot🎨 💻              Jacky Li💻 🖋 🎨      Sarthak Trivedi🎨      Ayush Aggarwal🎨 💻      Nic Ballarini🎨      Luigi Zambetti🎨 💻      govindhaswin🎨      Addy Roy💻 🎨              Akshat Tamrakar🎨 💻      Sai Bhargava Ramu🎨      Gurkan💻      Spencer Hayes-Laverdiere💻      Aniket Soni💻      tanmay5792💻      Dina Taklit💻 🎨 🖋              Dushyant Singh💻      Ravi Prakash Singh💻      Nihal Joshi💻      Guy Klages💻      Arvind🎨 💻      mujeeb91💻      joserca🎨 💻              Prateek Agrawal💻      Teoh Tze Chuin(サラ)💻 🎨      Jayant Jain💻      Ayush Sahu💻      Hridya Krishna R💻 🎨      Rahul Bali💻 🎨      S.ZHeng🎨 💻 💼              Shriya Madan🎨 💻      mahalrupi🎨      Lucas Lermagne🎨      Jeff Deutsch🎨 💻      Betoxx1🎨      Wingman4l7🎨      Martin Espericueta🎨              Mh-Tahir🎨      Zdravko Šplajt🎨 💻      Ms3105🎨 💻 🖋      Ambika Sidhesware💻      mundoguero💻      Darkus24🖋      Sou-786🖋 🎨              Banurekha🖋      ShiraStarL🎨      Ilya Komarov🎨      DemigodMs🖋 📖      Mekha Hridya🎨 🔍      Andrey Safonov🎨 🔍      Tommaso🎨 💻              Jessica Salbert💻 🎨      JAYANTH DOLAI💻 🎨      silverstroom💻 🎨 💼      Furkan Sayım💻 🎨      Sukumar Chandrasekaran🎨      Yejin Park🎨 💻      Ali Nooshabadi🎨 💻              imitavor🎨 💻      Salih Kilicli🎨 💻      Marcelo Meneses🎨 💻      Anton Krekotun🎨 🚧 🖋 💻 📖 💼      Arnav Sarma💻 💡 🎨      meghatiku💻 🎨      Anshu Trivedi🎨              Taylor Dorsett💻 🖋 🎨      Havit Rovik💻      pushpapune💻 🎨      Ramtin Radfar🎨 🤔 💼 💵 💻 🖋 💬      Abdulmajeed Isa💻 🎨      vikassaxena02🎨      RobTables🎨 💻 💼              Daniel🎨 💻 💼 🔍      Zahid Ali💻 🎨      Chad Chai💻 🎨      Marco Biedermann💻 🎨 💼 🤔      Srinidhi Murthy🎨      Miao Cai💻 🎨      Dionicio Diaz🎨 💻              Mir Monoarul Alam🎨      Shawn Ohn💻 🎨      Amanbolat Balabekov🎨 💻      black-mamba-code💻      Jian-forks🎨 💻      shivani patel🎨      Akash Chowrasia🎨              yairg98🎨      Jay Gajjar🎨      coolerbooler💻      Md Zinnatul Islam Morol🎨      shresthashok550🎨 📖      Alan Pallath📖      Adrian Wong💻              vsDizzy💻 🎨      Frex Cuadillera🎨 💻      ashish570💻 🎨      ruchpeanuts💻 🎨      Artmasque🎨 💻      Amirhossein Mojiri Foroushani🎨      for💻 🎨              Luke🎨 💻      Hector Espinoza🎨      Adrián Buenfil🎨 💻      Amit Kumar🎨      schoppfe🎨 💻      Sofiyal C🎨 💻      spitlisk💻 🎨              PRAVIN SHARMA🎨      NIDZAAA1🎨 💻      John Mai🎨 💻      kimsoyeong🎨      Dona Ghosh💻      Ryan Hill🎨 💻      j42z🎨 💻              Ashish Sangale🎨 💻      Derek Yang🎨 💻      mohsinmsm🎨 💻      Gokulkrish2302💻      Bhaavishek💻 🎨      Louis Liao🎨      sengc92🎨 💻              Alex Marvin🎨      Balkrishna Bhatt🎨 💻      Evaldas Lavrinovičius🎨 💻      Adam Erchegyi🎨 💻      Truman Hung🎨 💻      rzamora11🎨      gaurav0224🎨              Lee GyeongJun🎨      Mirek🎨 💻      surajm245🎨      ArisLaode🎨 💻      RaviDhoriya🎨 💻      sarai-84🎨 💻      Vishnu🎨 💻              Muhammad Minhaj💻      Chandrika Deb🎨 💻      Gitgit101-bit💻 🎨      Hedi Sellami💻 🎨      saurabhvaish93💻 🎨      Nikola Begovic💻 🎨      Wang💻 🎨              Manuel Eusebio de Paz Carmona🎨      Basim Al-Jawahery🎨 💻      RAJA AHMED🎨 💻      Abhik Lodh💻      Md. Pial Ahamed💻 🎨      Hassan Shahzad💻 🎨      Christian Sosa Gago💻              Hasnain Rasheed💻 🎨      T-Radford💻      dahiyashish💻 🎨      RahulSharma468💻 🎨      Jumpod Plekhongthu💻 🎨      Thomas Young-Audet💻 🎨      VinayagamBabu💻 🎨              Deniz Koç💻 🎨      Azhar Khan💻 🎨 🖋 📖 🔣 🚧      Jacob Short💻 🎨      Uchimura85💻 🎨      Leo Nugraha💻 🎨 📖      Mujtaba Mehdi📖 🖋      Jim-ds💻 🎨              Sreehari K💻 🎨      Florian Martinez💻 🎨      Aaron💻 🎨      apoage🎨      Ignacio Guillermo Martinez 💻 🎨      AirlineDog🎨 💻      Mekel🎨 💻              hmosharrof🎨 💻      Ben Emamian💻 🎨      babeshark💻 🎨      Leonardo Jaques💻 🎨      Stefanos Apkarian💻 🎨      Ayhan Albayrak💻 🎨      KidusMT💻 🎨              hectormarroquin20💻 🎨      Edelweiss35💻 🎨      MihaiD💻 🎨      AnveshReddyAnnem💻 🎨      Hyunjae Park💻 🎨      Rajiv Albino💻 🎨      Atishay💻              Yusuf Naheem🎨      Windu🎨 💻      Superv1sor💻 🎨      Karine (:🎨 💻      Eduard Pech🎨 💻      jjeshwani🎨 💻      Steve🎨 💻              Aleigh Ohslund💻      Abhinav Suman🎨 💻      Hamza Ehtesham Farooq🎨 💻      IamNotPeterPan💻 💵 🎨      Cetger🎨      pkonopacki🎨      Yang Yang🎨 💻              Muhammad Shoaib Sarwar💻      Murilo Henrique💻 🎨      emilianoalvz🎨 💻      Sumana Saha🎨 💻      Yurii17K🎨 💻      Rupesh Bhandari🎨 💻      salmos3718💻              John Baker🎨 💻      SanjaySathiraju🎨 💻      Donat Kabashi🎨      Arul Prasad J🎨 💻      Qi Chen🎨 💻      Maksym Dmyterko🎨 💻      ilovepullrequests💻              Samira Maleki🎨 💻      NIKITA MAHOVIYA💻      jesuisdev.Net🎨 💻      Ashraf Nazar🎨      Naveed Ahmad🎨      Ajmain Naqib🎨 💻      Avinash Tingre💻 🎨              nicktids🎨      Keith Dinh💻 🎨      André Ferreira💻 🎨      eliottkespi💻 🎨      praveenpno💻 🎨      vitowidigdo💻 🎨      Devesh Pratap Singh💻 🎨              Dario Rodriguez💻 🎨      charmander_didi💻 🎨      PHBasin💻 🎨      Ritvik Singh Chauhan💻 🎨      Riya P Mathew💻 🎨      Stephanie Cherubin💻 🎨      BenitesGui💻 🎨              FarikBear💻 🎨      Dmytro Havrilov💻 🎨      Parvesh Monu💻 🎨      Dipen Panchasara💻 🎨      gudata🎨 💻      gawadeditor💻 🎨      Kirill Taletski🎨 💻              Saajan🎨 💻      Kushagra S🎨 💻      Oanh Le🎨 💻      Frane Medvidović🎨 💻      Yorman🎨 💻      Bill Chan🎨 💻      Pratik Lomte🎨 💻              LOC LAM🎨 💻      TUSAR RANJAN MAHAPATRA💻      BhargavKanjarla💻      Karel De Smet💻 🎨      sidisan🎨      ygnzayarphyo🎨 💻      svansteelandt💻              Kebechet🎨      Daniel Selvan D🎨 💻      Mahdi Razavi🎨 💻      Niklas Tiede💻 🎨      narutubaderddin💻 🎨      dylandhood💻      Dheeraj Gupta💻              Pieter Claerhout💻 🎨      Shivam Agnihotri💻      RanjithReddy-Narra💻      Nikita Wadhwani🎨 💻      rsholokh💻 🎨      Ayaan Hossain💻 🎨      Rajesh Swarna💻              Deniz Etkar🎨 💻      pro335💻 🎨      Jakub Radzik💻 🎨      Hamza Khanzada💻      ARNON🎨      Vikram Singh💻      Shoxruxbek💻 🎨              Amit Khatri💻 🎨      Wali Ullah🎨 💻      Amit11794💻 🎨      metis-macys-66898💻 🎨      Faisal Maqbool🎨 💻      Kumar Neeraj💻 🎨      Maurizio Marini🎨 💻              Saket Kothari🎨 💻      Szymon Zborowski🎨 💻      iks3000🎨 💻      Ehsan Seyedi🎨 💻      vanekbr🎨 💻      Princy_M🎨 💻      Shijie Zhou🎨 💻              lakshyamcs16🎨 💻      Filippo Facco🎨 💻      mendel5🎨 💻      Patryk🎨 💻      VishwaSangani🎨 💻      Alvin Zhao🎨 💻      Lazar Gugleta🎨 💻              vmicho🎨 💻      Sikandar Ali🎨 💻      Raja Babu🎨 💻      faizajahanzeb💻      Guil_AiT🎨 💻      Kushal Das🎨 💻      Luis Bonilla🎨 💻              jovan1013🎨 💻      Damian🎨 💻      Yash Gupta💻      lolcatnip🎨 💻      Ikko Ashimine🎨 💻      Farukh🎨 💻      Moksedul💻 🎨              Navneet Kumar🎨 💻      Saqib AlMalik💻      fahimrahman🎨 💻      vaibhav patil🎨 💻      Rahul Madan🎨 💻      kartik Kaklotar🎨 💻      ASAHI OCEAN🎨 💻              Daniel Jungbluth🎨 💻      Rajdeep Singh Borana🎨 💻      ankitha19💻      Linh Tran💻      islamarr💻 🎨      Mohamed Sabith🎨 💻      Miguel Angel Cruz Acosta🎨 💻              Adebayo Ilerioluwa 🎨      Markus🎨 💻      dkonyayev🎨 💻      Kevin A Mathew🎨 💻      David Melo🎨 🔣      DFW1N🎨 💻      Sohaib Ayub🎨 💻              Navvy🎨 💻      bloodiator2🎨 💻      Hanji🎨 💻      arthur74🎨 💻      Sri Subathra Devi B🎨 💻      Akif Aydogmus🎨 💻      Umer Javaid🎨 💻              Norio Umata🎨 💻      Gazi Hasan Rahman🎨 💻      Keith Nguyen🎨 💻      Megalomaniac🎨 💻      ShankS3🎨 💻      Farhad Alishov🎨 💻      Ronak J Vanpariya🎨 💻              azrael0learza🎨 💻      Pavel Rahman🎨 💻      chuabern🎨 💻      Rahul Tirkey🎨 💻      Ruslan Bes🎨 💻 💡 🚧 🖋 🔣 🚇      Bohdan🎨 💻      Juzdzewski🎨 💻              Grigor Minasyan🎨 💻      alvintwc🎨 💻      Anand Natarajan🎨 💻      Kashan Ali🎨 💻      Thomas Meshail🎨 💻      Son Pham🎨      Michael French💡              Yash Mishra📖      Miguel Rodriguez🎨 💻      Philipp Bachmann🎨 💻      sunny🎨 💻      Siddharth Chatterjee🎨 💻      Michael Naghavipour🎨 💻      Sahil Garg🎨 💻              MicroLion🎨 💻      wctwc🎨 💻      Rohan Sharma🔣      AshishBodla🎨 💻      Taras Pysarskyi🎨 💻      Luqman Bello O.🎨 💻      DyingDown🎨 💻              Diego Chapedelaine🎨 💻      Richlee🎨 💻      Asif Habib🎨 💻      Mazharul Hossain🎨 💻      toni🎨 💻      Pragyanshu Rai🎨 💻      Matthew Eller🎨 💻              AbhiBiju🎨 💻      Roman Zhornytskiy🎨 💻      Lucas Camino🎨 💻      João Vitor Casarin🎨 💻      Evgeniy Shay🎨 💻      Ehsan Barkhordar🎨 💻      Gabriel🎨 💻              Shibu Mohapatra🎨 💻      Pavel Kirkovsky🎨 💻      Tahir Gul🎨 💻      imDevSalman🎨 💻      Jordan Donaldson🎨 💻      js-venus🎨 💻      Faisal Shaikh🎨 💻              ashishbpatil🎨 💻      Tri Le🎨 💻      tomtreffke🎨 💻      Salah Eddine Lalami🎨 💻      Mattias Xu🎨 💻      Manas Gupta🎨 💻      wolfsong62🎨 💻              Mehdi Mirzaei🎨 💻      Van Ba Khanh🎨 💻      Sel Embee🎨 💻      Suvradip Paul🎨 💻      Sharique🎨      Seabass🎨 💻      Penny Liu🎨 💻              jatinder bhola🎨 💻      misterqbit🎨 💻      Daniel-VS9🎨 💻      Shruthi🎨 💻      beefydog🎨 💻      Suraj Kumar🎨 💻      hrishikeshps🎨 💻              Sudarshan🎨 💻      Divyansh💻 🎨      Zyaire🎨 💻      Omar Belkady🎨 💻      alexiismua🎨 💻      Eduarda Alves🎨      pycoach🎨 💻              Ruhul🎨 💻      pmoustopoulos🎨 💻      Lee Hui Ting💻 🎨      bodi1981🎨 💻      Devaraat Joshi🎨 💻      Johnny🎨 💻      rogue-coder🎨 💻              viiktr🎨      Lalit Mohan💻      João Sousa💻      言葉之靈💻 🎨      RJLABS💻      brittney0522🎨 💻      sham🎨 💻              Glenn Goossens💻 🎨      Cyber Hawk🎨 💻 🖋 💼      Ankit Yadav🎨 💻      verbality💻      Mohammed Siddiqui🎨 💻      AdamKaczor6250🎨 💻      Ramón Martinez Nieto🎨 💻              Grzegorz Dziubak🎨 💻      Ayoub BERDEDDOUCH🎨 💻      nikola-fadv🎨 💻      Akarsh Agrawal🎨 💻      Mitra Mirshafiee🎨 💻      Parker Stephens🎨 💻      alrenee99💻              Karthick Vankayala💻      Iryna 🎨 💻      palanugrah💻      Gwinbleind🎨 💻      Randy Bobandy🎨 💻      Bek Rozikoff💻      davnguye🎨 💻              Neel Patel💻      ehudbehar🎨 💻      nicholas-cod3r🎨 💻      michaelfranki🎨      Esther White🎨 💻      prathmeshpb🎨 💻      Victor Lin🎨 💻              Christine C. Yin🎨 💻      GitLearner-begin🎨 💻      Mesrop Andreasyan🎨 💻      Nathan Garcia🎨      commonsw04🎨 💻      Md. Rashad Tanjim🎨 💻      Ali Malek💻              PAODLT🎨 💻      Nikhil Bobade🎨 💻      hyuckjin21💻      Itasha Modi🎨 💻      Nikitha Reddy🎨 💻      Mahshooq Zubair🎨 💻      Subham Das💻              Onkar Birajdar🎨 💻      Nick Titomichelakis🎨 💻      Christian Leo-Pernold🎨      Matthew Marquise🎨 💻      baronfac🎨 💻      Abhishek Tilwar🎨 💻      DavidsDvm🎨 💻              Parth Parikh🎨 💻      Hector Castro🎨 💻      Rikky Arisendi🎨 💻      Ali HamXa🎨 💻      Frank.wu🎨 💻      Jatin Kumar🎨 💻 📖      masterHAWK99🎨 💻              Pushp Jain🎨 💻      Ashutosh Rout🎨 💻      Atharva Deshpande🎨 💻      Teodor Ciripescu🎨 💻      Anmol Bansal🎨 💻      Nikhil Kumar Macharla🎨 💻      Dexter🎨 💻              Aaron🎨 💻      Yogita Jaswani🎨 💻 📖 🖋      StoryDev🎨 💻      Mesut Doğansoy🎨 💻      Paras Dhawan🎨 💻      Emanuel Zhupa🎨 💻      Aaradhyaa717🎨 💻              jaacko-torus🎨 💻      mBlack💻      kalrayashwin📖 🖋 🎨 💻      Seraph💻 🎨      ZhiHong Chua🎨 💻      Amsal Khan🎨 💻 📖 🖋      Raghav Rastogi🎨 💻              Tzila📖      Shahriar Nasim Nafi📖      AG🎨 💻      Mojtaba Kamyabi🎨 💻      Ahmad Abdulrahman🎨 💻      Eclipse🎨 💻      Anshu Pal🎨 💻              Denis🎨 💻      mehmet sayin📖      WebDEV🎨 💻      Sam Komesarook🎨 💻      Kiran Ghimire🎨 💻      Joshua Davis🎨 💻      Muhammad-Huzaifa-Siddiqui💻              tobeornottobeadev🎨 💻      VAIBHAV SINGHAL🎨 💻      Keiran Pillman🎨 💻      Max Donchenko🎨 💻      sgonsal🎨 💻      diksha137🎨 💻      Vignesh🎨 💻              Gabriel França🎨 💻      Joseph🎨 💻      Bruno Rafael🎨 💻      vcamarre🎨 💻      thibault ketterer🎨 💻 🚧      VictorGonzalezToledo🎨 💻      1911510996🎨 💻              invidu🎨 💻      Nurul Furqon🎨 💻      David Asbill🎨 💻      Niko Birbilis🎨 💻      Mugundan Kottursuresh🎨      agrsachin81🎨 💻      Othmane El Alami🎨 💻              Syed Atif Ali🎨 💻      lakhanjindam🎨 💻      youssef hamdane🎨 💻      starfaerie🎨 💻      rodrigo0107🎨 💻      Michał Gralak🎨 💻      Jewel Mahmud🎨 💻              cwilson830🎨 💻      buun1030🎨 💻      Reda-ELOUAHABI🎨 💻      saad-aksa🎨 💻      Emdadul Haque🎨 💻      PROCW🎨 💻      cccppp1🎨 💻              Joanna Baile🎨 💻      Ahmed Saber🎨 💻      Masoud Keshavarz🎨 💻      mortazavian🎨 💻      Aniket Pandey🎨 💻      Vijay Nirmal🎨 💻      Daniel Carvallo💻              menaechmi🎨 💻      azenyx🎨 💻      Ahmet Özrahat🎨 💻      Abdulrahman Abouzaid🎨 💻      jmgnorbec🎨 💻      palinko91🎨 💻      Laisson R. Silveira🎨 💻              BHARGAVPATEL1244🎨 💻      Candide U🎨 💻      Sitansh Rajput🎨 💻      Houda Mouttalib🎨 💻      MumuTW🎨 💻      Suave Bajaj🎨 💻      Mehdi Parsaei🎨 💻              Dinko Osrecki🎨 💻      Dhia Djobbi🎨 💻      Mahmoud Galal🎨 💻      Anh Minh🎨 💻      Suvesh K🎨 💻      Petar Todorov🎨 💻      Alexander Nguyen🎨 💻              Morteza Jalalvand🎨 💻      Claudson Martins🎨 💻      Matt Jacobson🎨 💻      Rafael Belokurows🎨 💻       Thomas Gamauf🎨 💻      Rishabh Mahajan🎨 💻      rakeshpdgupta23🎨 💻              Shashidharknaik🎨 💻      taleleuma🎨 💻      Florian Bühler🎨 💻      Raihan Bin Wahid🎨 💻      MOHAMMED NASSER🎨 💻      federico🎨 💻      Andre Violante🎨 💻              tcunningham98🎨 💻      Jan Grießer🎨 💻      Serkan Alc🎨 💻 🖋      Jez McKean🎨 💻      meisam alifallahi🎨 💻      Mehul Thakkar🎨 💻      Saksham Soni🎨 💻              Pedro Peregrina🎨 💻      Mintu Choudhary🎨 💻      lucianmoldovanu🎨 💻      John C. Scott🎨 💻      Mia D.🎨 💻      EwenBernard🎨 💻      M. Reza Nasirloo🎨 💻              Jay Agrawal🎨 💻      DeShay🎨 💻      Jay206-Programmer🎨 💻      Elender🎨 💻 🖋      Bobby Byrne🎨 💻      Pirci🎨 💻      Hasanuzzaman🎨 💻              Josh Kautz🎨 💻      Brofar🎨 💻      Mina Karam🎨 💻      Duncan O N🎨 💻      Sean Tumulak-Nguyen🎨 💻      Artur Trześniewski🎨 💻      JJaammeessM🎨 💻              shubham agarwal🎨 💻      Michele Righi🎨 💻      Panagiotis Kontos🎨 💻      sumitbathla🎨 💻      Deepak Mathur🎨 💻      Juho Nykänen🎨 💻      Santiago González Siordia🎨 💻              SRIJITA MALLICK🎨 💻      Samriddhi B🎨 💻      Nitzan Papini🎨 💻      Mario Sanz🎨 💻      Crab^4🎨 💻      Pablo🎨 💻      Gordon Pham-Nguyen🎨 💻              Kristoffer🎨 💻      chrisblach🎨 💻      Gábor🎨 💻      Lina🎨 💻      Harrison Watts🎨 💻      Mario Petričko🎨 💻      Ben8120🎨 💻              Giovanna🎨 💻      Minal Ahuja🎨 💻      mossfarmer🎨 💻      ThaC0derDre🎨 💻      itware🎨 💻      Michael Walker🎨 💻      Tom Jacob Chirayil🎨 💻              Sachin Kumar🎨 💻      adi-ray🎨 💻      Dr-Blank-alt🎨 💻      Bogdan Cazacu🎨 💻      Gilson Urbano🎨 💻      Nina🎨 💻      Anthony🎨 💻              manushimjani🎨 💻      Michael Reyes🎨 💻      Rachel Kennelly🎨 💻      Aakash Garg🎨 💻      Daniel Livingston🎨 💻      alexrojco🎨 💻      Minh Nguyen🎨 💻              Mahesh Dattatraya Babar🎨 💻      Jin Zihang🎨 💻      Bikramjit Ganguly🎨 💻      QuestionableGuise🎨 💻      liq19ch🎨 💻      Bruno Rocha🎨 💻      Anand Dyavanapalli💻 🖋              crucian-afk🎨 💻      0xgainz🎨 💻      weirdfsh🎨 💻      Valan Baptist Mathuranayagam🎨 💻      Paul Kaefer🎨 💻      Yu-Hsiang Wang🎨 💻      Javad Adib🎨 💻              davidliu0930🎨 💻      Achilleas John Yfantis🎨 💻      Omkar Shivadekar🎨 💻 🖋 🐛      ToanTran🎨 💻      Gautam Naik🎨 💻      Marc🎨 💻      twix20🎨 💻              Kristian S.🎨 💻      Aleksey Khoroshilov🎨 💻      arjunsrsr🎨 💻      Ali Haider🎨 💻      Trisha Dring🎨 💻      Andre Marzulo🎨 💻      Krishna Modi🎨 💻              Rosemary Li🎨 💻      Alex Weller🎨 💻      Tam Nguyen🎨 💻      aquintelaoliveira🎨 💻      Norbert Brett🎨 💻      rocsogd🎨 💻      0nyr🎨 💻              rethkevin🎨 💻      RickHeadle🎨 💻      Leandre🎨 💻      Natnael Sisay🎨 💻      sbbu🎨 💻      wael🎨 💻      Fabricio Tramontano Pirini🎨 💻              Alexander Stoyanov🎨 💻      Dezx20🎨 💻      southparkkids🎨 💻      bmstar🎨 💻      kiagam🎨 💻      Juan Castillo🎨 💻      FFenne🎨 💻              Jose Toledo🎨 💻      Pat McGhen🎨 💻      Eiko Wagenknecht💻 🖋 🔣      Alan Chalmers🎨 💻      Jean Didier🎨 💻      Andy🎨 💻      pestadieu🎨 💻              Kanishka Chakraborty🎨 💻      Nandha🎨 💻      Vahid Mafi🎨 💻 🔣 🖋 💼      Akshay Ashok🎨 💻      0x08🎨 💻      Sandeep Mishra🎨 💻      Evann Regnault🎨 💻              Lenny Zeitoun🎨 💻      Eden Boaron🎨 💻      TroyBTC🎨 💻      Aby Sebastian🎨 💻      Matthew Dunn🎨 💻      ckullo🎨 💻 🖋 🔣      Mohamed Mamdouh🎨 💻              Youssef Bazina🎨 💻      Frederico Kückelhaus💻      Nushan Kodikara💻      Zach Cooper💻      Roy🎨 💻      Saurav Panchal🎨 💻      totallynotdavid🎨 💻              goosepirate🎨 💻 💡 💼      KAUTH🎨 💻      Hari Kiran Vusirikala🎨 💻      Sounak Dey🎨 💻      zia💼 🎨 💻      Reza Davari🎨 💻      AkshayAjaykumar🎨 💻              x24870🎨 💻      Ko Phone🎨 💻      Nabstar3🎨 💻      Mateusz🎨 💻      Yunus Emre Emik💻      Abhinav Sinha🎨 💻      Hung Nguyen🎨 💻              Maselino💻      Shuktika Mahanty💻      Mikołaj Gawroński🎨 💻      Hussein Habibi Juybari🎨 💻      Sean-McArthur🎨 💻      Osman F Bayram🎨 💻      Benjamin Thomas Blodgett🎨 💻              Chuanlong-Zang🎨 💻      julian🎨 💻      francisco🎨 💻      aalihhiader9211🎨 💻      Muhammad Zunair🎨 💻      Liya🎨 💻      BegadTarek🎨 💻              etorobot🎨 💻      Hussam Khan🎨 💻      Saikat Chakraborty🎨 💻      Nicholas Quisler🎨 💻      Evang Poul🎨 💻      Gregg Lind🎨 💻      Deepak Kumar🎨 💻              Callum Leslie🎨 💻      Curtis Barnard Jr.🎨 💻      Deepanshukaim🎨 💻      Manthan Ank🎨 💻      hossein varmazyar🎨 💻      Brayan Muñoz V.🎨 💻      Kamil Rasheed Siddiqui💻 🎨              mutt0-ds🎨 💻      egbertjk🎨 💻      Majid Zojaji🎨 💻      Sean Chen🎨 💻      Herbert Milhomme🎨 💻      A3🎨 💻      Killian🎨 💻              Coakeow🎨 💻      ྅༻ Ǭɀħ ༄༆ཉ🎨 💻      Pratik Solanki🎨 💻      Sunny🎨 💻      ssge🎨 💻      Bernat Frangi🎨 💻      Jeevan Rupacha🎨 💻              amirandap🎨 💻      Deepakshi Mittal🎨 💻      Abhijeet Parida🎨 💻      Khaled Riyad🎨 💻      Pratap parui🎨 💻      Prajit Panday🎨 💻      PipeSierra🎨 💻              Collins Oden🎨 💻      Kshitij Dwivedi🎨 💻      Bernardia Vitri Arumsari🎨 💻      Ömer Faruk Taşdemir🎨 💻      Spencer Stith🎨 💻      Porsche Rodjanasak🎨 💻      Shakeel Sharif🎨 💻              Victoria Cheng🎨 💻      Denis🎨 💻      Anand Prakash Tiwari🎨 💻      danijeljw-rpc🎨 💻      Ahmed H Ebrahim🎨 💻      Virginia Gardner🎨 💻      Jhironsel Diaz A.🎨 💻              Yunus Kidem🎨 💻      MT🎨 💻      Dinesh Zaldekar🎨 💻      adi🎨 💻      Farhan Shaikh🎨 💻      Elvis Salvatierra🎨 💻      Kaushik-Iyer🎨 💻              HocAndres🎨 💻      VictorHugoAguilarAguilar🎨 💻      Murat Can Abay🎨 💻      Chris🎨 💻      Shivam7-1🎨 💻      Paipai13🎨 💻      Shambles-io🎨 💻              Abhishek K M🎨 💻      Ezequiel Cuevas🎨 💻      Plamen Ivanov🎨 💻      Yuji🎨 💻      Jean-Philippe Lebœuf🎨 💻 🔣      Naufan🎨 💻      jadnov🎨 💻              vaxtangens🎨 💻      subashkonar13🎨 💻      Rushi Javiya🎨 💻      Mert Gül🎨 💻      Lily🎨 💻      Kalinoff🎨 💻      Joel Tony🎨 💻              Peter🎨 💻      Roozbeh Zarei🎨 💻      Shen🎨 💻      Joonsoo.LEE🎨 💻      Fede.Breg🎨 💻      Rui Costa🎨 💻      João Gustavo Bispo🎨 💻              Sami-I🎨 💻      Tsvetoslav Tsvetkov🎨 💻      Olabode Olaniyi David🎨 💻      theRuslan🎨 💻      leighboz🎨 💻      Frank Sossi🎨 💻      Tomasz Adamski🎨 💻              Mansoor M. Sathir🎨 💻      Golamrabbi Azad🎨 💻      Nahian Ahmed🎨 💻      Rafael de Jesus Silva Monteiro🎨 💻      Odionyebuchukwu Jude🎨 💻      The Nithin Balaji🎨 💻      Knackii🎨 💻              vittorio-giatti🎨 💻      Guilherme de Carvalho Lima Rebouças🎨 💻      aaref shami🎨 💻      Andrey Dryupin🎨 💻      Muhanned Noman🎨 💻      Jan Silva🎨 💻      emanuele-em🎨 💻 🖋              Sanjay TM🎨 💻      Joe Markberg / code editor🎨 💻      Julien Quiaios🎨 💻      Eric Ramirez Santis🎨 💻      M🎨 💻      Malcata🎨 💻      Athul Muralidharan🎨 💻              Dariusz Ochota🎨 💻      CHANDAN CHOUDHURY🎨 💻      Deep🎨 💻      Ahmet İstemihan ÖZTÜRK🎨 💻      TIM🎨 💻      jakeg814🎨 💻      Leonidos🎨 💻              Abhinandu V Nair🎨 💻      charafeddine01🎨 💻      Jasper🎨 💻      Manish Goyal🎨 💻      SATYAM_SINGH🎨 💻      Four🎨 💻      Vaishnavi Amira Yada🎨 💻              ShriKrushna Bhagwat🎨 💻      Rohit Nandagawali🎨 💻      felipe🎨 💻 🚧 🖋 ✅ 🧑‍🏫      Saurabh Mudgal🎨 💻      szenadam🎨 💻      Shubhendra Singh🎨 💻      Yoosuf Sayyid💻 🎨              Güven Çetinerler🎨 💻      Luke Jefferies🎨 💻      Chris🎨 💻      Lúcio Aguiar💻      Enuma029💻      yktsang01💻      maximumn3rd🎨 💻              Jon Galletero🎨 💻      Thaddeus  Thomas🎨 💻      Aakash Kumar💻 🎨      Ali M🎨 💻      OskyEdz🎨 💻      Ravi Gupta🎨 💻      Rafa Raizer🎨 💻              Abdullah Al Muzaki🎨 💻      Rahul Faujdar🎨 💻      Abhishek Verma🎨 💻      Ashutosh Shinde🎨 💻      Ganesh Rai🎨 💻      StefanTrpkovic🎨 💻      Erik Blanca🎨 💻              Vedant Madane🎨 💻      Antra Tripathi🎨 💻      Ethan Knights🎨 💻      Alexandru Boncut🎨 💻      Pablo Bandinopla🎨 💻 🚧 🖋      Robz-99🎨 💻      Harpal Singh🎨 💻              paulboundy99🎨 💻      Mubashir Ahmed🎨 💻      Rohan Hari🎨 💻      Erik Henrique 🎨 💻      Leandro Matheus🎨 💻      Deepak🎨 💻      AlishaSingh🎨 💻              Lynn Latt Yati🎨 💻      San Shwe🎨 💻      SKR🎨 💻      msbunnyjaguar🎨 💻      Mohamad Zabiulla🎨 💻      Hatim Zahid🎨 💻      Rauzan Sumara🎨 💻              Hosein1358🎨 💻      Mohit🎨 💻      Ali🎨 💻      Avinash1765🎨 💻      Sai Teja Madha🎨 💻      Monsur Ahmed Shafiq🎨 💻      xuxianjin-dev🎨 💻              chetna🎨 💻      Gul Zaib🎨 💻      Natalia🎨 💻      Dionísio Braga🎨 💻      Pritish Rajpurohit🎨 💻      incanlove🎨 💻      Innocent🎨 💻              Devin Almonor🎨 💻      antonyveyre🎨 💻      Beltz Anhxton🎨 💻      Mehdi🎨 💻      Muhammad Usman🎨 💻      Patrick Dantas🎨 💻      Tak Vannak🎨 💻              Ramzi RADDAOUI🎨 💻      Konstantin-Glukhov🎨 💻      uguroban🎨 💻      Humberto Alves🎨 💻      JuangZendrato🎨 💻      James Oluwaleye🎨 💻      Wasi Sadman🎨 💻              Pavle Mijatovic🎨 💻      Luiz H. S. Bispo🎨 💻      Сухас Дхолз🎨 💻      Alvaro Trujillo🎨 💻      Everton 🎨 💻      jfrozas🎨 💻      Shuaaib Badran🎨 💻              Shivam Jha🎨 💻      Mohamed Tayeh🎨 💻      Makendran G🎨 💻      mayank singh tomar🎨 💻      hossam sadany🎨 💻      Harshbardhan Singh💻 🎨      Fawad Jawaid Malik🎨 💻              Tina Lacatis🎨 💻      TeddyCuoreDolce🎨 💻      bchooxg🎨 💻      Alisha Takkar🎨 💻      Gianluigi🎨 💻      Mehran Javaherian🎨 💻      Benjamin Ololade Adedokun🎨 💻              Md. Abdul Mutalib🎨 💻      Aadil Arsh.S.R🎨 💻      J. Nathan Allen🎨 💻      Kieran Krug🎨 💻      Seth Addo🎨 💻      Satvik Singh Rathore🎨 💻      dangoth🎨 💻              Maxim🎨 💻      Phuong-Cat Ngo🎨 💻      Frenchtoast0🎨 💻      Rakshith🎨 💻      Vaibhav Arora🎨 💻      zghp🎨 💻      Bedovan🎨 💻              chiaramistro🎨 💻      him2016🎨 💻      HarshitSachdeva🎨 💻      Sadaf Saleem🎨 💻      Aaroh Srivastava🎨 💻      eloygplaza🎨 💻      Gaurav Kumar Verma🎨 💻              AndreaCUS🎨 💻      Simran🎨 💻      Prashant Bhapkar🎨 💻      mhaendler🎨 💻      Gauri Maheshwari🎨 💻      4Lajf🎨 💻      Tanmoy Sengupta🎨 💻              Sharad Tripathi🎨 💻      Niraj Chavan🎨 💻      Luisa Gualda🎨 💻      Monika-Sivakumar-3🎨 💻      harryfensome🎨 💻      Shubham Choubey🎨 💻      Ashwini Patil🎨 💻              cleversonlira🎨 💻      Nurmukhammed🎨 💻      workspace-utkarsh🎨 💻      Santosh Phadtare🎨 💻      Prashant Warghude🎨 💻      Umang Dakh🎨 💻      Shalini Chavan🎨 💻              vinit gurjar🎨 💻      Vishal Kumar🎨 💻      Wonhyeong Seo🎨 💻      Achwale Prajwal Namdevrao🎨 💻      Ankan Banerjee🎨 💻      bhaumikankan🎨 💻      JamesMacroZhang🎨 💻              Pedro Lopes🎨 💻      dia🎨 💻      tayyabhussain2910🎨 💻      Rajdeep Shrivastava 🎨 💻      Mukul Kumar🎨 💻      Mayank N🎨 💻      jdelucca🎨 💻              Sneha Mittal🎨 💻      Sarika Kushwaha🎨 💻      farzad-khb🎨 💻      Elijah Shackelford🎨 💻      The-Only-Raminator🎨 💻      Keerthana Kasthuril🎨 💻      Viachaslau Auchynnikau🎨 💻              Mohammad Osman Rasooli🎨 💻      mvedovato🎨 💻      Sonali Rajput🎨 💻      Isha Dhek🎨 💻      Ramshad Cheriyeri Peediyakkal🎨 💻      Micah🎨 💻      gauravshukla2203🎨 💻              sndmurthy🎨 💻      Shivam-Singh🎨 💻      M. Ammar Khan🎨 💻      chandolakul🎨 💻      bhatnagar221🎨 💻      Adrian Nieściur🎨 💻      nezi311🎨 💻              scottajevans🎨 💻      Marcelo Antunes Soares Fantini🎨 💻      Axel De Acetis🎨 💻      Drishti Sah🎨 💻      VipulDhillon🎨 💻      Urmi Jana🎨 💻      Ayush Mokal🎨 💻              Damola Olutoke🎨 💻      Max🎨 💻      Lakshmi N🎨 💻      ArtemReva🎨 💻      Ujjwal Aggarwal🎨 💻      Mo🎨 💻      Brian🎨 💻              chamley🎨 💻      Simone Baptiste🎨 💻      Shekhar Thakur🎨 💻      Smith🎨 💻      codernoob1🎨 💻      lok84🎨 💻      Tobias Riemenschneider🎨 💻              Tharsanan1🎨 💻      ANURAG SINGH🎨 💻      Yash Sant🎨 💻      Krishiv Patel🎨 💻      GGGalaxy🎨 💻      pardeepdhillon661🎨 💻      anujd64🎨 💻              Pedro Pereira🎨 💻      Master_Saptak🎨 💻      SURANJAN DAS🎨 💻      Tripura kant🎨 💻      shabzkhan🎨 💻      Mustafa Poya🎨 💻      Roshan Jha🎨 💻              GuillaumeLarue🎨 💻      Tomasz Rodak🎨 💻      Junil Kim🎨 💻      Surbhi Mayank🎨 💻      Nemanja Lekic🎨 💻      HemantMalokar🎨 💻      Felipe M. López🎨 💻              bibliofilo🎨 💻      GauthamG2🎨 💻      02_t🎨 💻      Yusuf Abdul-razaq🎨 💻      Vladimir🎨 💻      Sai Chandra K🎨 💻      Soroush Bonab🎨 💻              Giide0n🎨 💻      GG🎨 💻      Dáger Zúñiga🎨 💻      rsk2🎨 💻      Storozhev DJ🎨 💻      Jeevan🎨 💻      Andy Johnson🎨 💻              Aníbal Pozo🎨 💻      Jovane de Castro🎨 💻      Muhammad Hamza Amir🎨 💻      tharaka-mts🎨 💻      Ali KHYAR🎨 💻      Caio Araujo🎨 💻      Oscar Dyremyhr🎨 💻              arteality🎨 💻      Daniel Drexlmaier🎨 💻      Marco Monti🎨 💻      mikeycrystal🎨 💻      Veljanovskii🎨 💻      Ivan Gorbachev🎨 💻      Sahil Rawat🎨 💻              Hasitha Suneth🎨 💻      Yerko Vera Lezama🎨 💻      Ivan Penchev🎨 💻      Tanver Islam Tonmoy🎨 💻      Xun Cao🎨 💻      Nayan Babariya🎨 💻      Priyanshu Maurya🎨 💻              Dylan Tintenfich🎨 💻      Ron Strauss🎨 💻      Mohammed AlBanna🎨 💻      Mukund M🎨 💻      Franklin Ohaegbulam🎨 💻      Nisarg Shah🎨 💻      Unik Dahal🎨 💻              Readily🎨 💻      Alexandre Poitevin🎨 💻      Scaramir🎨 💻      Pruthvi🎨 💻      Kalmanq🎨 💻      Alfatah Nesab🎨 💻      arudesai🎨 💻              Adryenne🎨 💻      El mehdi oudaoud🎨 💻      Jayant Goel🎨 💻      Tsuki🎨 💻      Peter Lemanski🎨 💻      Annurag-byte🎨 💻      Anthony Vu🎨 💻              Vitaly Nikolaychuk🎨 💻      Nathan🎨 💻      Evgenii Petukhov🎨 💻      Loris Guerra🎨 💻      fakhriaunur🎨 💻      Mehdi HYANI🎨 💻      Sarvex Jatasra🎨 💻              santimanuelr🎨 💻      Evgeniy Rezanov🎨 💻      Sonia M🎨 💻      Grzegorz Kmita🎨 💻      Manuel Carita🎨 💻      Felipe Cisternas Alvarez🎨 💻      Guo Ci🎨 💻              Marcos Silva🎨 💻      KK🎨 💻      Shubhanjan Medhi🎨 💻      ArthurFerreiraRodrigues🎨 💻      PabloHermun🎨 💻      disha-baldawa🎨 💻      StaroMoon🎨 💻              Amila T Kumarasekara🎨 💻      Amoh Prince🎨 💻      AngeloGC🎨 💻      Ebube Glory Ogbonda🎨 💻      Prahalad Belavadi📖      Antoni Sarnowski-Trypka🎨 💻      Alberto Pasqualetto🎨 💻              Amir Babaei🎨 💻      Syed Abdul Hannan🎨 💻      Srajan Rai🎨 💻      Clarence Moore🎨 💻      Nguyen Anh Tuan🎨 💻      dar2dar2🎨 💻      Ameer Ibrahim🎨 💻              Tiago Lugatto🎨 💻      raremiroir🎨 💻      Moobie🎨 💻      AlicanDursun🎨 💻      bbalsam🎨 💻      Luboš Hájek🎨 💻      mrshahzeb7🎨 💻              Wesley Scholl🎨 💻      Lawrence Turcotte🎨 💻      Michael DiPaolo🎨 💻      Smart-Codi🎨 💻      Vivek Kumar🎨 💻      Igor Moiseev🎨 💻      Bård Pedersen🎨 💻              HOA PHAN🎨 💻      GaborModra🎨 💻      vivek-114🎨 💻      Robin🎨 💻      Alex🎨 💻      John Ehrlinger🎨 💻      Roman Zhuravlov🎨 💻              Jordan Moss🎨 💻      RaeShelly🎨 💻      gmollard🎨 💻      Md Kaif Khan🎨 💻      Pablo Romera🎨 💻      Erik Bustos🎨 💻      trogfield🎨 💻              simon-aichhorn🎨 💻      Tufan GÜLEÇ🎨 💻      Uğur Berkecan Ünlü🎨 💻      Revanth Naik🎨 💻      Lia Pires🎨 💻      Igor Mestechkin🎨 💻      Anirudh Karanth🎨 💻              KBobovskiy🎨 💻      zhatiayua🎨 💻 🖋      David Cardona🎨 💻      Paulo Castilho🎨 💻      Sebastiano Picchi🎨 💻      pjotar🎨 💻      Rimel CHERIF💻              Arsal uddin🖋      Dmitry Kasporsky💻      SoftwareDev1014🎨 💻      @Robvred🎨 💻      Kasun Shanaka💻      Ahmad M.🎨 💻      Alex Kozin🎨 💻              Mandy Meindersma🎨 💻      LEGALISE PIRACY🎨 💻      Alex Logvin🎨 💻      Aria Dahl🎨 💻      Mustafa Arifoglu🎨 💻      Yevhen Leshchenko🎨 💻      Anubhav Adhikari🎨 💻              Noah Tatko🎨 💻      Mohit Gadhavi🎨 💻      Pedro Basílio🎨 💻      RealSanjeev🎨 💻      Akash Hazra🎨 💻      Christoph Dahlen🎨 💻      Vincent du Plessis🎨 💻              Karen Tamrazyan🎨 💻      Mirza Younus Baig🎨 💻      Ashish Kumar🎨 💻      Unknown6334🎨 💻      flowaz🎨 💻      zi-aikra🎨 💻      PAYAL PM🎨 💻              Lennart Lösche🎨 💻      Yummy-Yums🎨 💻      Njuacha Hubert Mikulowski🎨 💻      Hussein Esmail🎨 💻      Bilgehan Bezir🎨 💻      Muhammed Shittu🎨 💻      Clément FERNANDES🎨 💻              JaCKoP619🎨 💻      userutf8🎨 💻      Mohamed Ubaid🎨 💻      Justin Yates🎨 💻      mohammad ali🎨 💻      Madhav Singh🎨 💻      RgbMouse69🎨 💻              Nicholas Leask🎨 💻      parthav0🎨 💻      Sigma🎨 💻      Evelina Becheva🎨 💻      Akshit Gulyan🎨 💻      Arpita Jana🎨 💻      Praveen Kumar🎨 💻              Mohammad Sami🎨 💻      eddiestefanescu🎨 💻      Ramesh Yadav🎨 💻      Sarthak Joshi🎨 💻      Nikhil12300🎨 💻      Yevgen🎨 💻      Leo🎨 💻              laurent b🎨 💻      Mettchen🎨 💻      Ali Mahdavi🎨 💻      Lucas Dondo🎨 💻      Siddhesh Agarwal🎨 💻      slimerPuncher🎨 💻      saritashh🎨 💻              Iulian-Valeriu Cioată🎨 💻      Szabolcs Nagy🎨 💻      Jarle Kvile🎨 💻      劉耀升 Vic Liu🎨 💻      Suryansh🎨 💻      Matthew Oosthuyse🎨 💻      Florin Zamfir🎨 💻              Melek🎨 💻      moesocio🎨 💻      Alan James🎨 💻      Mai Thanh Phương🎨 💻      Neville Dabre🎨 💻      Maksym🎨 💻      tamanna900🎨 💻              Adithya Awati🎨 💻      This project follows the all-contributors specification.Contributions of any kind welcome![ Go back to the top of the page ]Contributor Over TimeStargazers over timeVisualisation of this repository by Gourcehttps://www.youtube.com/watch?v=24cZVytc5D4"
47,ageitgey/face_recognition,https://github.com/ageitgey/face_recognition/blob/master/README.md,Python,"Face RecognitionYou can also read a translated version of this file in Chinese 简体中文版 or in Korean 한국어 or in Japanese 日本語.Recognize and manipulate faces from Python or from the command line withthe world's simplest face recognition library.Built using dlib's state-of-the-art face recognitionbuilt with deep learning. The model has an accuracy of 99.38% on theLabeled Faces in the Wild benchmark.This also provides a simple face_recognition command line tool that letsyou do face recognition on a folder of images from the command line!FeaturesFind faces in picturesFind all the faces that appear in a picture:import face_recognitionimage = face_recognition.load_image_file(\""your_file.jpg\"")face_locations = face_recognition.face_locations(image)Find and manipulate facial features in picturesGet the locations and outlines of each person's eyes, nose, mouth and chin.import face_recognitionimage = face_recognition.load_image_file(\""your_file.jpg\"")face_landmarks_list = face_recognition.face_landmarks(image)Finding facial features is super useful for lots of important stuff. But you can also use it for really stupid stufflike applying digital make-up (think 'Meitu'):Identify faces in picturesRecognize who appears in each photo.import face_recognitionknown_image = face_recognition.load_image_file(\""biden.jpg\"")unknown_image = face_recognition.load_image_file(\""unknown.jpg\"")biden_encoding = face_recognition.face_encodings(known_image)[0]unknown_encoding = face_recognition.face_encodings(unknown_image)[0]results = face_recognition.compare_faces([biden_encoding], unknown_encoding)You can even use this library with other Python libraries to do real-time face recognition:See this example for the code.Online DemosUser-contributed shared Jupyter notebook demo (not officially supported): InstallationRequirementsPython 3.3+ or Python 2.7macOS or Linux (Windows not officially supported, but might work)Installation Options:Installing on Mac or LinuxFirst, make sure you have dlib already installed with Python bindings:How to install dlib from source on macOS or UbuntuThen, make sure you have cmake installed:brew install cmakeFinally, install this module from pypi using pip3 (or pip2 for Python 2):pip3 install face_recognitionAlternatively, you can try this library with Docker, see this section.If you are having trouble with installation, you can also try out apre-configured VM.Installing on an Nvidia Jetson Nano boardJetson Nano installation instructionsPlease follow the instructions in the article carefully. There is current a bug in the CUDA libraries on the Jetson Nano that will cause this library to fail silently if you don't follow the instructions in the article to comment out a line in dlib and recompile it.Installing on Raspberry Pi 2+Raspberry Pi 2+ installation instructionsInstalling on FreeBSDpkg install graphics/py-face_recognitionInstalling on WindowsWhile Windows isn't officially supported, helpful users have posted instructions on how to install this library:@masoudr's Windows 10 installation guide (dlib + face_recognition)Installing a pre-configured Virtual Machine imageDownload the pre-configured VM image (for VMware Player or VirtualBox).UsageCommand-Line InterfaceWhen you install face_recognition, you get two simple command-lineprograms:face_recognition - Recognize faces in a photograph or folder full forphotographs.face_detection - Find faces in a photograph or folder full for photographs.face_recognition command line toolThe face_recognition command lets you recognize faces in a photograph orfolder full  for photographs.First, you need to provide a folder with one picture of each person youalready know. There should be one image file for each person with thefiles named according to who is in the picture:Next, you need a second folder with the files you want to identify:Then in you simply run the command face_recognition, passing inthe folder of known people and the folder (or single image) with unknownpeople and it tells you who is in each image:$ face_recognition ./pictures_of_people_i_know/ ./unknown_pictures//unknown_pictures/unknown.jpg,Barack Obama/face_recognition_test/unknown_pictures/unknown.jpg,unknown_personThere's one line in the output for each face. The data is comma-separatedwith the filename and the name of the person found.An unknown_person is a face in the image that didn't match anyone inyour folder of known people.face_detection command line toolThe face_detection command lets you find the location (pixel coordinatates)of any faces in an image.Just run the command face_detection, passing in a folder of imagesto check (or a single image):$ face_detection  ./folder_with_pictures/examples/image1.jpg,65,215,169,112examples/image2.jpg,62,394,211,244examples/image2.jpg,95,941,244,792It prints one line for each face that was detected. The coordinatesreported are the top, right, bottom and left coordinates of the face (in pixels).Adjusting Tolerance / SensitivityIf you are getting multiple matches for the same person, it might be thatthe people in your photos look very similar and a lower tolerance valueis needed to make face comparisons more strict.You can do that with the --tolerance parameter. The default tolerancevalue is 0.6 and lower numbers make face comparisons more strict:$ face_recognition --tolerance 0.54 ./pictures_of_people_i_know/ ./unknown_pictures//unknown_pictures/unknown.jpg,Barack Obama/face_recognition_test/unknown_pictures/unknown.jpg,unknown_personIf you want to see the face distance calculated for each match in orderto adjust the tolerance setting, you can use --show-distance true:$ face_recognition --show-distance true ./pictures_of_people_i_know/ ./unknown_pictures//unknown_pictures/unknown.jpg,Barack Obama,0.378542298956785/face_recognition_test/unknown_pictures/unknown.jpg,unknown_person,NoneMore ExamplesIf you simply want to know the names of the people in each photograph but don'tcare about file names, you could do this:$ face_recognition ./pictures_of_people_i_know/ ./unknown_pictures/ | cut -d ',' -f2Barack Obamaunknown_personSpeeding up Face RecognitionFace recognition can be done in parallel if you have a computer withmultiple CPU cores. For example, if your system has 4 CPU cores, you canprocess about 4 times as many images in the same amount of time by usingall your CPU cores in parallel.If you are using Python 3.4 or newer, pass in a --cpus <number_of_cpu_cores_to_use> parameter:$ face_recognition --cpus 4 ./pictures_of_people_i_know/ ./unknown_pictures/You can also pass in --cpus -1 to use all CPU cores in your system.Python ModuleYou can import the face_recognition module and then easily manipulatefaces with just a couple of lines of code. It's super easy!API Docs: https://face-recognition.readthedocs.io.Automatically find all the faces in an imageimport face_recognitionimage = face_recognition.load_image_file(\""my_picture.jpg\"")face_locations = face_recognition.face_locations(image)# face_locations is now an array listing the co-ordinates of each face!See this exampleto try it out.You can also opt-in to a somewhat more accurate deep-learning-based face detection model.Note: GPU acceleration (via NVidia's CUDA library) is required for goodperformance with this model. You'll also want to enable CUDA supportwhen compliling dlib.import face_recognitionimage = face_recognition.load_image_file(\""my_picture.jpg\"")face_locations = face_recognition.face_locations(image, model=\""cnn\"")# face_locations is now an array listing the co-ordinates of each face!See this exampleto try it out.If you have a lot of images and a GPU, you can alsofind faces in batches.Automatically locate the facial features of a person in an imageimport face_recognitionimage = face_recognition.load_image_file(\""my_picture.jpg\"")face_landmarks_list = face_recognition.face_landmarks(image)# face_landmarks_list is now an array with the locations of each facial feature in each face.# face_landmarks_list[0]['left_eye'] would be the location and outline of the first person's left eye.See this exampleto try it out.Recognize faces in images and identify who they areimport face_recognitionpicture_of_me = face_recognition.load_image_file(\""me.jpg\"")my_face_encoding = face_recognition.face_encodings(picture_of_me)[0]# my_face_encoding now contains a universal 'encoding' of my facial features that can be compared to any other picture of a face!unknown_picture = face_recognition.load_image_file(\""unknown.jpg\"")unknown_face_encoding = face_recognition.face_encodings(unknown_picture)[0]# Now we can see the two face encodings are of the same person with `compare_faces`!results = face_recognition.compare_faces([my_face_encoding], unknown_face_encoding)if results[0] == True:    print(\""It's a picture of me!\"")else:    print(\""It's not a picture of me!\"")See this exampleto try it out.Python Code ExamplesAll the examples are available here.Face DetectionFind faces in a photographFind faces in a photograph (using deep learning)Find faces in batches of images w/ GPU (using deep learning)Blur all the faces in a live video using your webcam (Requires OpenCV to be installed)Facial FeaturesIdentify specific facial features in a photographApply (horribly ugly) digital make-upFacial RecognitionFind and recognize unknown faces in a photograph based on photographs of known peopleIdentify and draw boxes around each person in a photoCompare faces by numeric face distance instead of only True/False matchesRecognize faces in live video using your webcam - Simple / Slower Version (Requires OpenCV to be installed)Recognize faces in live video using your webcam - Faster Version (Requires OpenCV to be installed)Recognize faces in a video file and write out new video file (Requires OpenCV to be installed)Recognize faces on a Raspberry Pi w/ cameraRun a web service to recognize faces via HTTP (Requires Flask to be installed)Recognize faces with a K-nearest neighbors classifierTrain multiple images per person then recognize faces using a SVMCreating a Standalone ExecutableIf you want to create a standalone executable that can run without the need to install python or face_recognition, you can use PyInstaller. However, it requires some custom configuration to work with this library. See this issue for how to do it.Articles and Guides that cover face_recognitionMy article on how Face Recognition works: Modern Face Recognition with Deep LearningCovers the algorithms and how they generally workFace recognition with OpenCV, Python, and deep learning by Adrian RosebrockCovers how to use face recognition in practiceRaspberry Pi Face Recognition by Adrian RosebrockCovers how to use this on a Raspberry PiFace clustering with Python by Adrian RosebrockCovers how to automatically cluster photos based on who appears in each photo using unsupervised learningHow Face Recognition WorksIf you want to learn how face location and recognition work instead ofdepending on a black box library, read my article.CaveatsThe face recognition model is trained on adults and does not work very well on children. It tends to mixup children quite easy using the default comparison threshold of 0.6.Accuracy may vary between ethnic groups. Please see this wiki page for more details.Deployment to Cloud Hosts (Heroku, AWS, etc)Since face_recognition depends on dlib which is written in C++, it can be tricky to deploy an appusing it to a cloud hosting provider like Heroku or AWS.To make things easier, there's an example Dockerfile in this repo that shows how to run an app built withface_recognition in a Docker container. With that, you should be able to deployto any service that supports Docker images.You can try the Docker image locally by running: docker-compose up --buildThere are also several prebuilt Docker images.Linux users with a GPU (drivers >= 384.81) and Nvidia-Docker installed can run the example on the GPU: Open the docker-compose.yml file and uncomment the dockerfile: Dockerfile.gpu and runtime: nvidia lines.Having problems?If you run into problems, please read the Common Errors section of the wiki before filing a github issue.ThanksMany, many thanks to Davis King (@nulhom)for creating dlib and for providing the trained facial feature detection and face encoding modelsused in this library. For more information on the ResNet that powers the face encodings, check outhis blog post.Thanks to everyone who works on all the awesome Python data science libraries like numpy, scipy, scikit-image,pillow, etc, etc that makes this kind of stuff so easy and fun in Python.Thanks to Cookiecutter and theaudreyr/cookiecutter-pypackage project templatefor making Python project packaging way more tolerable."
48,swisskyrepo/PayloadsAllTheThings,https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/README.md,Python,"Payloads All The ThingsA list of useful payloads and bypasses for Web Application Security.Feel free to improve with your payloads and techniques !I ❤️ pull requests :)You can also contribute with a 🍻 IRL, or using the sponsor buttonAn alternative display version is available at PayloadsAllTheThingsWeb.  📖 DocumentationEvery section contains the following files, you can use the _template_vuln folder to create a new chapter:README.md - vulnerability description and how to exploit it, including several payloadsIntruder - a set of files to give to Burp IntruderImages - pictures for the README.mdFiles - some files referenced in the README.mdYou might also like the Methodology and Resources folder :Methodology and ResourcesActive Directory Attack.mdCloud - AWS Pentest.mdCloud - Azure Pentest.mdCobalt Strike - Cheatsheet.mdLinux - Evasion.mdLinux - Persistence.mdLinux - Privilege Escalation.mdMetasploit - Cheatsheet.mdMethodology and enumeration.mdNetwork Pivoting Techniques.mdNetwork Discovery.mdReverse Shell Cheatsheet.mdSubdomains Enumeration.mdWindows - AMSI Bypass.mdWindows - DPAPI.mdWindows - Download and Execute.mdWindows - Mimikatz.mdWindows - Persistence.mdWindows - Privilege Escalation.mdWindows - Using credentials.mdYou want more ? Check the Books and Youtube videos selections.👨‍💻 ContributionsBe sure to read CONTRIBUTING.md  Thanks again for your contribution! ❤️🧙‍♂️ SponsorsThis project is proudly sponsored by these companies."
49,xtekky/gpt4free,https://github.com/xtekky/gpt4free/blob/main/README.md,Python,"By using this repository or any code related to it, you agree to the legal notice. The author is not responsible for any copies, forks, or reuploads made by other users. This is the author's only account and repository. To prevent impersonation or irresponsible actions, you may comply with the GNU GPL license this Repository uses.This (quite censored) New Version of gpt4free, was just released, it may contain bugs, open an issue or contribute a PR when encountering one, some features were disabled.Docker is for now not available but I would be happy if someone contributes a PR. The g4f GUI will be uploaded soon enough.Newpypi package:pip install -U g4fTable of Contents:Getting StartedPrerequisitesSetting up the projectUsageThe g4f Packageinterference openai-proxy apiModelsgpt-3.5 / gpt-4Other ModelsRelated gpt4free projectsContributeChatGPT cloneCopyrightCopyright NoticeStar HistoryGetting StartedPrerequisites:Download and install Python (Version 3.x is recommended).Setting up the project:Install using pypipip install -U g4forClone the GitHub repository:git clone https://github.com/xtekky/gpt4free.gitNavigate to the project directory:cd gpt4free(Recommended) Create a virtual environment to manage Python packages for your project:python3 -m venv venvActivate the virtual environment:On Windows:.\\venv\\Scripts\\activateOn macOS and Linux:source venv/bin/activateInstall the required Python packages from requirements.txt:pip install -r requirements.txtCreate a test.py file in the root folder and start using the repo, further Instructions are belowimport g4f...UsageThe g4f Packageimport g4fprint(g4f.provider.Ails.params)  # supported args# Automatic selection of provider# streamed completionresponse = g4f.ChatCompletion.create(    model=\""gpt-3.5-turbo\"",    messages=[{\""role\"": \""user\"", \""content\"": \""Hello world\""}],    stream=True,)for message in response:    print(message, flush=True, end='')# normal responseresponse = g4f.ChatCompletion.create(    model=g4f.models.gpt_4,    messages=[{\""role\"": \""user\"", \""content\"": \""hi\""}],)  # alterative model settingprint(response)# Set with providerresponse = g4f.ChatCompletion.create(    model=\""gpt-3.5-turbo\"",    provider=g4f.provider.DeepAi,    messages=[{\""role\"": \""user\"", \""content\"": \""Hello world\""}],    stream=True,)for message in response:    print(message)providers:from g4f.provider import (    Acytoo,    Aichat,    Ails,    AiService,    AItianhu,    Bard,    Bing,    ChatgptAi,    ChatgptLogin,    DeepAi,    GetGpt)# usage:response = g4f.ChatCompletion.create(..., provider=ProviderName)interference openai-proxy api (use with openai python package)get requirements:pip install -r interference/requirements.txtrun server:python3 -m interference.appimport openaiopenai.api_key = \""\""openai.api_base = \""http://localhost:1337\""def main():    chat_completion = openai.ChatCompletion.create(        model=\""gpt-3.5-turbo\"",        messages=[{\""role\"": \""user\"", \""content\"": \""write a poem about a tree\""}],        stream=True,    )    if isinstance(chat_completion, dict):        # not stream        print(chat_completion.choices[0].message.content)    else:        # stream        for token in chat_completion:            content = token[\""choices\""][0][\""delta\""].get(\""content\"")            if content != None:                print(content, end=\""\"", flush=True)if __name__ == \""__main__\"":    main()Modelsgpt-3.5 / gpt-4WebsiteProvidergpt-3.5gpt-4StreamingStatusAuthwww.aitianhu.comg4f.provider.AItianhu✔️❌❌❌chat.acytoo.comg4f.provider.Acytoo✔️❌❌❌aiservice.vercel.appg4f.provider.AiService✔️❌❌❌chat-gpt.orgg4f.provider.Aichat✔️❌❌❌ai.lsg4f.provider.Ails✔️❌✔️❌bard.google.comg4f.provider.Bard❌❌❌✔️bing.comg4f.provider.Bing❌✔️❌❌chatgpt.aig4f.provider.ChatgptAi❌✔️❌❌chatgptlogin.acg4f.provider.ChatgptLogin✔️❌❌❌deepai.orgg4f.provider.DeepAi✔️❌✔️❌chat.dfehub.comg4f.provider.DfeHub✔️❌✔️❌free.easychat.workg4f.provider.EasyChat✔️❌✔️❌forefront.comg4f.provider.Forefront✔️❌✔️❌chat.getgpt.worldg4f.provider.GetGpt✔️❌✔️❌gpt-gm.h2o.aig4f.provider.H2o❌❌✔️❌liaobots.comg4f.provider.Liaobots✔️✔️✔️✔️supertest.lockchat.appg4f.provider.Lockchat✔️✔️✔️❌opchatgpts.netg4f.provider.Opchatgpts✔️❌❌❌backend.raycast.comg4f.provider.Raycast✔️✔️✔️✔️theb.aig4f.provider.Theb✔️❌✔️❌play.vercel.aig4f.provider.Vercel✔️❌❌❌wewordle.orgg4f.provider.Wewordle✔️❌❌❌you.comg4f.provider.You✔️❌❌❌chat9.yqcloud.topg4f.provider.Yqcloud✔️❌❌❌Other ModelsModelBase ProviderProviderWebsitepalmGoogleg4f.provider.Bardbard.google.comh2ogpt-gm-oasst1-en-2048-falcon-7b-v3Huggingfaceg4f.provider.H2owww.h2o.aih2ogpt-gm-oasst1-en-2048-falcon-40b-v1Huggingfaceg4f.provider.H2owww.h2o.aih2ogpt-gm-oasst1-en-2048-open-llama-13bHuggingfaceg4f.provider.H2owww.h2o.aiclaude-instant-v1Anthropicg4f.provider.Vercelsdk.vercel.aiclaude-v1Anthropicg4f.provider.Vercelsdk.vercel.aiclaude-v2Anthropicg4f.provider.Vercelsdk.vercel.aicommand-light-nightlyCohereg4f.provider.Vercelsdk.vercel.aicommand-nightlyCohereg4f.provider.Vercelsdk.vercel.aigpt-neox-20bHuggingfaceg4f.provider.Vercelsdk.vercel.aioasst-sft-1-pythia-12bHuggingfaceg4f.provider.Vercelsdk.vercel.aioasst-sft-4-pythia-12b-epoch-3.5Huggingfaceg4f.provider.Vercelsdk.vercel.aisantacoderHuggingfaceg4f.provider.Vercelsdk.vercel.aibloomHuggingfaceg4f.provider.Vercelsdk.vercel.aiflan-t5-xxlHuggingfaceg4f.provider.Vercelsdk.vercel.aicode-davinci-002OpenAIg4f.provider.Vercelsdk.vercel.aigpt-3.5-turbo-16kOpenAIg4f.provider.Vercelsdk.vercel.aigpt-3.5-turbo-16k-0613OpenAIg4f.provider.Vercelsdk.vercel.aigpt-4-0613OpenAIg4f.provider.Vercelsdk.vercel.aitext-ada-001OpenAIg4f.provider.Vercelsdk.vercel.aitext-babbage-001OpenAIg4f.provider.Vercelsdk.vercel.aitext-curie-001OpenAIg4f.provider.Vercelsdk.vercel.aitext-davinci-002OpenAIg4f.provider.Vercelsdk.vercel.aitext-davinci-003OpenAIg4f.provider.Vercelsdk.vercel.aillama13b-v2-chatReplicateg4f.provider.Vercelsdk.vercel.aillama7b-v2-chatReplicateg4f.provider.Vercelsdk.vercel.aiRelated gpt4free projects            🎁 Projects      ⭐ Stars      📚 Forks      🛎 Issues      📬 Pull requests                  gpt4free                                      gpt4free-ts                                      ChatGPT-Clone                                      ChatGpt Discord Bot                                      LangChain gpt4free                                      ChatGpt Telegram Bot                              Contributeto add another provider, its very simple:create a new file in g4f/provider with the name of the ProviderImplement a class that extends BaseProvider.from .base_provider import BaseProviderfrom ..typing import CreateResult, Anyclass HogeService(BaseProvider):    url = \""http://hoge.com\""    working = True    supports_gpt_35_turbo = True    @staticmethod    def create_completion(        model: str,        messages: list[dict[str, str]],        stream: bool,        **kwargs: Any,    ) -> CreateResult:        passHere, you can adjust the settings, for example if the website does support streaming, set working to True...Write code to request the provider in create_completion and yield the response, even if its a one-time response, do not hesitate to look at other providers for inspirationAdd the Provider Name in g4f/provider/init.pyfrom .base_provider import BaseProviderfrom .HogeService import HogeService__all__ = [  HogeService,]You are done !, test the provider by calling it:import g4fresponse = g4f.ChatCompletion.create(model='gpt-3.5-turbo', provider=g4f.provider.PROVIDERNAME,                                    messages=[{\""role\"": \""user\"", \""content\"": \""test\""}], stream=g4f.provider.PROVIDERNAME.supports_stream)for message in response:    print(message, flush=True, end='')ChatGPT cloneCurrently implementing new features and trying to scale it, please be patient it may be unstablehttps://chat.g4f.ai/chatThis site was developed by me and includes gpt-4/3.5, internet access and gpt-jailbreak's like DANRun locally here: https://github.com/xtekky/chatgpt-cloneCopyright:This program is licensed under the GNU GPL v3Copyright Notice:xtekky/gpt4free: Copyright (C) 2023 xtekkyThis program is free software: you can redistribute it and/or modifyit under the terms of the GNU General Public License as published bythe Free Software Foundation, either version 3 of the License, or(at your option) any later version.This program is distributed in the hope that it will be useful,but WITHOUT ANY WARRANTY; without even the implied warranty ofMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See theGNU General Public License for more details.You should have received a copy of the GNU General Public Licensealong with this program.  If not, see <https://www.gnu.org/licenses/>.Star History         "
50,jackfrued/Python-100-Days,https://github.com/jackfrued/Python-100-Days/blob/master/README.md,Python,"Python - 100天从新手到大师作者：骆昊说明：从项目上线到获得8w+星标以来，一直收到反馈说基础部分（前15天的内容）对新手来说是比较困难的，建议有配套视频进行讲解。最近把基础部分的内容重新制作了一个名为“Python-Core-50-Courses”的项目，用更为简单通俗的方式重写了这部分内容并附带了视频讲解，初学者可以关注下这个新项目。如果需要Python基础视频，可以在“B站”搜索《Python零基础快速上手》，这套视频是我讲课的时候录制的随堂视频，画质尚可、音质一般，但是对初学者应该会有些帮助，欢迎大家留言、评论、发弹幕。学习之后觉得有收获的小伙伴可以“一键三连”来支持UP主（千锋Python）。国内用户如果访问GitHub比较慢的话，可以关注我的知乎号Python-Jack，上面的“从零开始学Python”专栏比较适合初学者，其他的专栏也在持续创作和更新中，欢迎大家关注并点赞评论。创作不易，感谢大家的打赏支持，这些钱不会用于个人消费（例如：购买咖啡），而是通过腾讯公益、美团公益、水滴筹等平台捐赠给需要帮助的人（点击了解捐赠情况）。需要加入QQ学习群的可以扫描下面的二维码，三个群加一个即可，不要重复进群。学习群会为大家提供学习资源和问题解答，如果有Python体验课和行业公开课会提前在群里通知大家，欢迎大家加入。项目“Day80~90”部分目前仍在创作中，因为作者平时也挤不出太多时间来写文档，因此更新的速度比较缓慢，感谢大家的理解。Python应用领域和职业发展分析简单的说，Python是一个“优雅”、“明确”、“简单”的编程语言。学习曲线低，非专业人士也能上手开源系统，拥有强大的生态圈解释型语言，完美的平台可移植性动态类型语言，支持面向对象和函数式编程代码规范程度高，可读性强Python在以下领域都有用武之地。后端开发 - Python / Java / Go / PHPDevOps - Python / Shell / Ruby数据采集 - Python / C++ / Java量化交易 - Python / C++ / R数据科学 - Python / R / Julia / Matlab机器学习 - Python / R / C++ / Julia自动化测试 - Python / Shell作为一名Python开发者，根据个人的喜好和职业规划，可以选择的就业领域也非常多。Python后端开发工程师（服务器、云平台、数据接口）Python运维工程师（自动化运维、SRE、DevOps）Python数据分析师（数据分析、商业智能、数字化运营）Python数据挖掘工程师（机器学习、深度学习、算法专家）Python爬虫工程师Python测试工程师（自动化测试、测试开发）说明：目前，数据分析和数据挖掘是非常热门的方向，因为不管是互联网行业还是传统行业都已经积累了大量的数据，各行各业都需要数据分析师从已有的数据中发现更多的商业价值，从而为企业的决策提供数据的支撑，这就是所谓的数据驱动决策。给初学者的几个建议：Make English as your working language. （让英语成为你的工作语言）Practice makes perfect. （熟能生巧）All experience comes from mistakes. （所有的经验都源于你犯过的错误）Don't be one of the leeches. （不要当伸手党）Either outstanding or out. （要么出众，要么出局）Day01~15 - Python语言基础Day01 - 初识PythonPython简介 - Python的历史 / Python的优缺点 / Python的应用领域搭建编程环境 - Windows环境 / Linux环境 / MacOS环境从终端运行Python程序 - Hello, world / print函数 / 运行程序使用IDLE - 交互式环境(REPL) / 编写多行代码 / 运行程序 / 退出IDLE注释 - 注释的作用 / 单行注释 / 多行注释Day02 - 语言元素程序和进制 - 指令和程序 / 冯诺依曼机 / 二进制和十进制 / 八进制和十六进制变量和类型 - 变量的命名 / 变量的使用 / input函数 / 检查变量类型 / 类型转换数字和字符串 - 整数 / 浮点数 / 复数 / 字符串 / 字符串基本操作 / 字符编码运算符 - 数学运算符 / 赋值运算符 / 比较运算符 / 逻辑运算符 / 身份运算符 / 运算符的优先级应用案例 - 华氏温度转换成摄氏温度 / 输入圆的半径计算周长和面积 / 输入年份判断是否是闰年Day03 - 分支结构分支结构的应用场景 - 条件 / 缩进 / 代码块 / 流程图if语句 - 简单的if / if-else结构 / if-elif-else结构 / 嵌套的if应用案例 - 用户身份验证 / 英制单位与公制单位互换 / 掷骰子决定做什么 / 百分制成绩转等级制 / 分段函数求值 / 输入三条边的长度如果能构成三角形就计算周长和面积Day04 - 循环结构循环结构的应用场景 - 条件 / 缩进 / 代码块 / 流程图while循环 - 基本结构 / break语句 / continue语句for循环 - 基本结构 / range类型 / 循环中的分支结构 / 嵌套的循环 / 提前结束程序应用案例 - 1~100求和 / 判断素数 / 猜数字游戏 / 打印九九表 / 打印三角形图案 / 猴子吃桃 / 百钱百鸡Day05 - 构造程序逻辑经典案例：水仙花数 / 百钱百鸡 / Craps赌博游戏练习题目：斐波那契数列 / 完美数 / 素数Day06 - 函数和模块的使用函数的作用 - 代码的坏味道 / 用函数封装功能模块定义函数 - def关键字 / 函数名 / 参数列表 / return语句 / 调用自定义函数调用函数 - Python内置函数 /  导入模块和函数函数的参数 - 默认参数 / 可变参数 / 关键字参数 / 命名关键字参数函数的返回值 - 没有返回值  / 返回单个值 / 返回多个值作用域问题 - 局部作用域 / 嵌套作用域 / 全局作用域 / 内置作用域 / 和作用域相关的关键字用模块管理函数 - 模块的概念 / 用自定义模块管理函数 / 命名冲突的时候会怎样（同一个模块和不同的模块）Day07 - 字符串和常用数据结构字符串的使用 - 计算长度 / 下标运算 / 切片 / 常用方法列表基本用法 - 定义列表 / 用下表访问元素 / 下标越界 / 添加元素 / 删除元素 / 修改元素 / 切片 / 循环遍历列表常用操作 - 连接 / 复制(复制元素和复制数组) / 长度 / 排序 / 倒转 / 查找生成列表 - 使用range创建数字列表 / 生成表达式 / 生成器元组的使用 - 定义元组 / 使用元组中的值 / 修改元组变量 / 元组和列表转换集合基本用法 - 集合和列表的区别 /  创建集合 / 添加元素 / 删除元素 /  清空集合常用操作 - 交集 / 并集 / 差集 / 对称差 / 子集 / 超集字典的基本用法 - 字典的特点 / 创建字典 / 添加元素 / 删除元素 / 取值 / 清空字典常用操作 - keys方法 / values方法 / items方法 / setdefault方法基础练习 - 跑马灯效果 / 列表找最大元素 / 统计考试成绩的平均分 / Fibonacci数列 / 杨辉三角综合案例 - 双色球选号 / 井字棋Day08 - 面向对象编程基础类和对象 - 什么是类 / 什么是对象 / 面向对象其他相关概念定义类 - 基本结构 / 属性和方法 / 构造器 / 析构器 / __str__方法使用对象 - 创建对象 / 给对象发消息面向对象的四大支柱 - 抽象 / 封装 / 继承 / 多态基础练习 - 定义学生类 / 定义时钟类 / 定义图形类 / 定义汽车类Day09 - 面向对象进阶属性 - 类属性 / 实例属性 / 属性访问器 / 属性修改器 / 属性删除器 / 使用__slots__类中的方法 - 实例方法 / 类方法 / 静态方法运算符重载 - __add__ / __sub__ / __or__ /__getitem__ / __setitem__ / __len__ / __repr__ / __gt__ / __lt__ / __le__ / __ge__ / __eq__ / __ne__ / __contains__类(的对象)之间的关系 - 关联 / 继承 / 依赖继承和多态 - 什么是继承 / 继承的语法 / 调用父类方法 / 方法重写 / 类型判定 / 多重继承 / 菱形继承(钻石继承)和C3算法综合案例 - 工资结算系统 / 图书自动折扣系统 / 自定义分数类Day10 - 图形用户界面和游戏开发使用tkinter开发GUI程序使用pygame三方库开发游戏应用“大球吃小球”游戏Day11 - 文件和异常读文件 - 读取整个文件 / 逐行读取 / 文件路径写文件 - 覆盖写入 / 追加写入 / 文本文件 / 二进制文件异常处理 - 异常机制的重要性 / try-except代码块 / else代码块 / finally代码块 / 内置异常类型 / 异常栈 / raise语句数据持久化 - CSV文件概述 / csv模块的应用 / JSON数据格式 / json模块的应用Day12 - 字符串和正则表达式字符串高级操作 - 转义字符 / 原始字符串 / 多行字符串 / in和not in运算符 / is_xxx方法 / join和split方法 / strip相关方法 / pyperclip模块 / 不变字符串和可变字符串 / StringIO的使用正则表达式入门 - 正则表达式的作用 / 元字符 / 转义 / 量词 / 分组 / 零宽断言 /贪婪匹配与惰性匹配懒惰 / 使用re模块实现正则表达式操作（匹配、搜索、替换、捕获）使用正则表达式 - re模块 / compile函数 / group和groups方法 / match方法 / search方法 / findall和finditer方法 / sub和subn方法 / split方法应用案例 - 使用正则表达式验证输入的字符串Day13 - 进程和线程进程和线程的概念 - 什么是进程 / 什么是线程 / 多线程的应用场景使用进程 - fork函数 / multiprocessing模块 / 进程池 / 进程间通信使用线程 -  threading模块 / Thread类 / RLock类 / Condition类 / 线程池Day14 - 网络编程入门和网络应用开发计算机网络基础 - 计算机网络发展史 / “TCP-IP”模型 / IP地址 / 端口 / 协议 / 其他相关概念网络应用模式 - “客户端-服务器”模式 / “浏览器-服务器”模式基于HTTP协议访问网络资源 - 网络API概述 / 访问URL / requests三方库 / 解析JSON格式数据Python网络编程 - 套接字的概念 / socket模块 /  socket函数 / 创建TCP服务器 / 创建TCP客户端 / 创建UDP服务器 / 创建UDP客户端电子邮件 - SMTP协议 / POP3协议 / IMAP协议 / smtplib模块 / poplib模块 / imaplib模块短信服务 - 调用短信服务网关Day15 - 图像和文档处理用Pillow处理图片 - 图片读写 / 图片合成 / 几何变换 / 色彩转换 / 滤镜效果读写Word文档 - 文本内容的处理 / 段落 / 页眉和页脚 / 样式的处理读写Excel文件 - xlrd / xlwt / openpyxlDay16~Day20 - Python语言进阶 常用数据结构函数的高级用法 - “一等公民” / 高阶函数 / Lambda函数 / 作用域和闭包 / 装饰器面向对象高级知识 - “三大支柱” / 类与类之间的关系 / 垃圾回收 / 魔术属性和方法 / 混入 / 元类 / 面向对象设计原则 / GoF设计模式迭代器和生成器 - 相关魔术方法 / 创建生成器的两种方式 /并发和异步编程 - 多线程 / 多进程 / 异步IO / async和awaitDay21~30 - Web前端入门用HTML标签承载页面内容用CSS渲染页面用JavaScript处理交互式行为jQuery入门和提高Vue.js入门Element的使用Bootstrap的使用Day31~35 - 玩转Linux操作系统操作系统发展史和Linux概述Linux基础命令Linux中的实用程序Linux的文件系统Vim编辑器的应用环境变量和Shell编程软件的安装和服务的配置网络访问和管理其他相关内容Day36~40 - 数据库基础和进阶关系型数据库概述MySQL的安装和使用SQL的使用DDL - 数据定义语言 - create / drop / alterDML - 数据操作语言 - insert / delete / updateDQL - 数据查询语言 - selectDCL - 数据控制语言 - grant / revokeMySQL新特性窗口函数的应用JSON数据类型相关知识数据完整性和一致性视图、函数、过程、触发器事务和锁执行计划和索引范式理论和反范式设计在Python中操作MySQLDay41~55 - 实战DjangoDay41 - Django快速上手Web应用工作机制HTTP请求和响应Django框架概述5分钟快速上手Day42 - 深入模型关系型数据库配置使用ORM完成对模型的CRUD操作管理后台的使用Django模型最佳实践模型定义参考Day43 - 静态资源和Ajax请求加载静态资源Ajax概述用Ajax实现投票功能Day44 - Cookie和Session实现用户跟踪cookie和session的关系Django框架对session的支持视图函数中的cookie读写操作Day45 - 报表和日志通过HttpResponse修改响应头使用StreamingHttpResponse处理大文件使用xlwt生成Excel报表使用reportlab生成PDF报表使用ECharts生成前端图表Day46 - 日志和调试工具栏配置日志配置Django-Debug-Toolbar优化ORM代码Day47 - 中间件的应用什么是中间件Django框架内置的中间件自定义中间件及其应用场景Day48 - 前后端分离开发入门返回JSON格式的数据用Vue.js渲染页面Day49 - RESTful架构和DRF入门Day50 - RESTful架构和DRF进阶Day51 - 使用缓存网站优化第一定律在Django项目中使用Redis提供缓存服务在视图函数中读写缓存使用装饰器实现页面缓存为数据接口提供缓存服务Day52 - 接入三方平台文件上传表单控件和图片文件预览服务器端如何处理上传的文件Day53 - 异步任务和定时任务网站优化第二定律配置消息队列服务在项目中使用Celery实现任务异步化在项目中使用Celery实现定时任务Day54 - 单元测试Day55 - 项目上线Python中的单元测试Django框架对单元测试的支持使用版本控制系统配置和使用uWSGI动静分离和Nginx配置配置HTTPS配置域名解析Day56~60 - 用FastAPI开发数据接口FastAPI五分钟上手请求和响应接入关系型数据库依赖注入中间件异步化虚拟化部署（Docker）项目实战：车辆违章查询项目Day61~65 - 爬虫开发Day61 - 网络数据采集概述网络爬虫的概念及其应用领域网络爬虫的合法性探讨开发网络爬虫的相关工具一个爬虫程序的构成Day62 - 数据抓取和解析使用requests三方库实现数据抓取页面解析的三种方式正则表达式解析XPath解析CSS选择器解析Day63 - Python中的并发编程多线程多进程异步I/ODay64 - 使用Selenium抓取网页动态内容Day65 - 爬虫框架Scrapy简介Day66~80 - 数据分析Day66 - 数据分析概述Day67 - 环境准备Day68 - NumPy的应用-1Day69 - NumPy的应用-2Day70 - Pandas的应用-1Day71 - Pandas的应用-2Day72 - Pandas的应用-3Day73 - Pandas的应用-4Day74 - Pandas的应用-5Day75 - 数据可视化-1Day76 - 数据可视化-2Day77 - 概率统计基础Day78 - 方差分析和参数估计Day79 - 相关和回归Day80 - 数据分析方法论Day81~90 - 机器学习和深度学习Day81 - 机器学习基础Day82 - k最近邻分类Day83 - 决策树Day84 - 贝叶斯分类Day85 - 支持向量机Day86 - K-均值聚类Day87 - 回归分析Day88 - 深度学习入门Day89 - PyTorch概述Day90 - PyTorch实战Day91~100 - 团队项目开发第91天：团队项目开发的问题和解决方案软件过程模型经典过程模型（瀑布模型）可行性分析（研究做还是不做），输出《可行性分析报告》。需求分析（研究做什么），输出《需求规格说明书》和产品界面原型图。概要设计和详细设计，输出概念模型图（ER图）、物理模型图、类图、时序图等。编码 / 测试。上线 / 维护。瀑布模型最大的缺点是无法拥抱需求变化，整套流程结束后才能看到产品，团队士气低落。敏捷开发（Scrum）- 产品所有者、Scrum Master、研发人员 - Sprint产品的Backlog（用户故事、产品原型）。计划会议（评估和预算）。日常开发（站立会议、番茄工作法、结对编程、测试先行、代码重构……）。修复bug（问题描述、重现步骤、测试人员、被指派人）。发布版本。评审会议（Showcase，用户需要参与）。回顾会议（对当前迭代周期做一个总结）。补充：敏捷软件开发宣言个体和互动 高于 流程和工具工作的软件 高于 详尽的文档客户合作 高于 合同谈判响应变化 高于 遵循计划角色：产品所有者（决定做什么，能对需求拍板的人）、团队负责人（解决各种问题，专注如何更好的工作，屏蔽外部对开发团队的影响）、开发团队（项目执行人员，具体指开发人员和测试人员）。准备工作：商业案例和资金、合同、憧憬、初始产品需求、初始发布计划、入股、组建团队。敏捷团队通常人数为8-10人。工作量估算：将开发任务量化，包括原型、Logo设计、UI设计、前端开发等，尽量把每个工作分解到最小任务量，最小任务量标准为工作时间不能超过两天，然后估算总体项目时间。把每个任务都贴在看板上面，看板上分三部分：to do（待完成）、in progress（进行中）和done（已完成）。项目团队组建团队的构成和角色说明：谢谢付祥英女士帮助我绘制了下面这张精美的公司组织架构图。编程规范和代码审查（flake8、pylint）Python中的一些“惯例”（请参考《Python惯例-如何编写Pythonic的代码》）影响代码可读性的原因：代码注释太少或者没有注释代码破坏了语言的最佳实践反模式编程（意大利面代码、复制-黏贴编程、自负编程、……）团队开发工具介绍版本控制：Git、Mercury缺陷管理：Gitlab、Redmine敏捷闭环工具：禅道、JIRA持续集成：Jenkins、Travis-CI请参考《团队项目开发的问题和解决方案》。项目选题和理解业务选题范围设定CMS（用户端）：新闻聚合网站、问答/分享社区、影评/书评网站等。MIS（用户端+管理端）：KMS、KPI考核系统、HRS、CRM系统、供应链系统、仓储管理系统等。App后台（管理端+数据接口）：二手交易类、报刊杂志类、小众电商类、新闻资讯类、旅游类、社交类、阅读类等。其他类型：自身行业背景和工作经验、业务容易理解和把控。需求理解、模块划分和任务分配需求理解：头脑风暴和竞品分析。模块划分：画思维导图（XMind），每个模块是一个枝节点，每个具体的功能是一个叶节点（用动词表述），需要确保每个叶节点无法再生出新节点，确定每个叶子节点的重要性、优先级和工作量。任务分配：由项目负责人根据上面的指标为每个团队成员分配任务。制定项目进度表（每日更新）模块功能人员状态完成工时计划开始实际开始计划结束实际结束备注评论添加评论王大锤正在进行50%42018/8/72018/8/7删除评论王大锤等待0%22018/8/72018/8/7查看评论白元芳正在进行20%42018/8/72018/8/7需要进行代码审查评论投票白元芳等待0%42018/8/82018/8/8OOAD和数据库设计UML（统一建模语言）的类图通过模型创建表（正向工程），例如在Django项目中可以通过下面的命令创建二维表。python manage.py makemigrations apppython manage.py migrate使用PowerDesigner绘制物理模型图。通过数据表创建模型（反向工程），例如在Django项目中可以通过下面的命令生成模型。python manage.py inspectdb > app/models.py第92天：Docker容器详解Docker简介安装Docker使用Docker创建容器（Nginx、MySQL、Redis、Gitlab、Jenkins）构建Docker镜像（Dockerfile的编写和相关指令）容器编排（Docker-compose）集群管理（Kubernetes）第93天：MySQL性能优化第94天：网络API接口设计第95天：[使用Django开发商业项目](./Day91-100/95.使用Django开发商业项\t目.md)项目开发中的公共问题数据库的配置（多数据库、主从复制、数据库路由）缓存的配置（分区缓存、键设置、超时设置、主从复制、故障恢复（哨兵））日志的配置分析和调试（Django-Debug-ToolBar）好用的Python模块（日期计算、图像处理、数据加密、三方API）REST API设计RESTful架构理解RESTful架构RESTful API设计指南RESTful API最佳实践API接口文档的撰写RAP2YAPIdjango-REST-framework的应用项目中的重点难点剖析使用缓存缓解数据库压力 - Redis使用消息队列做解耦合和削峰 - Celery + RabbitMQ第96天：软件测试和自动化测试单元测试测试的种类编写单元测试（unittest、pytest、nose2、tox、ddt、……）测试覆盖率（coverage）Django项目部署部署前的准备工作关键设置（SECRET_KEY / DEBUG / ALLOWED_HOSTS / 缓存 / 数据库）HTTPS / CSRF_COOKIE_SECUR  / SESSION_COOKIE_SECURE日志相关配置Linux常用命令回顾Linux常用服务的安装和配置uWSGI/Gunicorn和Nginx的使用Gunicorn和uWSGI的比较对于不需要大量定制化的简单应用程序，Gunicorn是一个不错的选择，uWSGI的学习曲线比Gunicorn要陡峭得多，Gunicorn的默认参数就已经能够适应大多数应用程序。uWSGI支持异构部署。由于Nginx本身支持uWSGI，在线上一般都将Nginx和uWSGI捆绑在一起部署，而且uWSGI属于功能齐全且高度定制的WSGI中间件。在性能上，Gunicorn和uWSGI其实表现相当。使用虚拟化技术（Docker）部署测试环境和生产环境性能测试AB的使用SQLslap的使用sysbench的使用自动化测试使用Shell和Python进行自动化测试使用Selenium实现自动化测试Selenium IDESelenium WebDriverSelenium Remote Control测试工具Robot Framework介绍第97天：电商网站技术要点剖析第98天：项目部署上线和性能调优MySQL数据库调优Web服务器性能优化Nginx负载均衡配置Keepalived实现高可用代码性能调优多线程异步化静态资源访问优化云存储CDN第99天：面试中的公共问题第100天：Python面试题实录"
51,TheAlgorithms/Python,https://github.com/TheAlgorithms/Python/blob/master/README.md,Python,          The Algorithms - Python                                                                  All algorithms implemented in Python - for educationImplementations are for learning purposes only. They may be less efficient than the implementations in the Python standard library. Use them at your discretion.Getting StartedRead through our Contribution Guidelines before you contribute.Community ChannelsWe are on Discord and Gitter! Community channels are a great way for you to ask questions and get help. Please join us!List of AlgorithmsSee our directory for easier navigation and a better overview of the project.
52,Significant-Gravitas/Auto-GPT,https://github.com/Significant-Gravitas/Auto-GPT/blob/master/README.md,Python,"Auto-GPT: An Autonomous GPT-4 Experiment💡 Get help - Q&A or Discord 💬🔴 USE stable not master 🔴Download the latest stable release from here: https://github.com/Significant-Gravitas/Auto-GPT/releases/latest.The master branch is under heavy development and may often be in a broken state.Auto-GPT is an experimental open-source application showcasing the capabilities of the GPT-4 language model. This program, driven by GPT-4, chains together LLM \""thoughts\"", to autonomously achieve whatever goal you set. As one of the first examples of GPT-4 running fully autonomously, Auto-GPT pushes the boundaries of what is possible with AI. Demo April 16th 2023               AutoGPTDemo_Subs_WithoutFinalScreen.mp4          Demo made by Blake Werlinger🚀 Features🌐 Internet access for searches and information gathering💾 Long-term and short-term memory management🧠 GPT-4 instances for text generation🔗 Access to popular websites and platforms🗃️ File storage and summarization with GPT-3.5🔌 Extensibility with PluginsQuickstartCheck out the wikiGet an OpenAI API KeyDownload the latest releaseFollow the installation instructionsConfigure any additional features you want, or install some pluginsRun the appPlease see the documentation for full setup instructions and configuration options.📖 Documentation⚙️ Setup💻 Usage🔌 PluginsConfiguration🔍 Web Search🧠 Memory🗣️ Voice (TTS)🖼️ Image Generation 💖 Help Fund Auto-GPT's Development 💖If you can spare a coffee, you can help to cover the costs of developing Auto-GPT and help to push the boundaries of fully autonomous AI!Your support is greatly appreciated. Development of this free, open-source project is made possible by all the contributors and sponsors. If you'd like to sponsor this project and have your avatar or company logo appear below click here.                                                                                                                                                                                                                                                                                                                                          ⚠️ LimitationsThis experiment aims to showcase the potential of GPT-4 but comes with some limitations:Not a polished application or product, just an experimentMay not perform well in complex, real-world business scenarios. In fact, if it actually does, please share your results!Quite expensive to run, so set and monitor your API key limits with OpenAI!🛡 DisclaimerThis project, Auto-GPT, is an experimental application and is provided \""as-is\"" without any warranty, express or implied. By using this software, you agree to assume all risks associated with its use, including but not limited to data loss, system failure, or any other issues that may arise.The developers and contributors of this project do not accept any responsibility or liability for any losses, damages, or other consequences that may occur as a result of using this software. You are solely responsible for any decisions and actions taken based on the information provided by Auto-GPT.Please note that the use of the GPT-4 language model can be expensive due to its token usage. By utilizing this project, you acknowledge that you are responsible for monitoring and managing your own token usage and the associated costs. It is highly recommended to check your OpenAI API usage regularly and set up any necessary limits or alerts to prevent unexpected charges.As an autonomous experiment, Auto-GPT may generate content or take actions that are not in line with real-world business practices or legal requirements. It is your responsibility to ensure that any actions or decisions made based on the output of this software comply with all applicable laws, regulations, and ethical standards. The developers and contributors of this project shall not be held responsible for any consequences arising from the use of this software.By using Auto-GPT, you agree to indemnify, defend, and hold harmless the developers, contributors, and any affiliated parties from and against any and all claims, damages, losses, liabilities, costs, and expenses (including reasonable attorneys' fees) arising from your use of this software or your violation of these terms.🐦 Connect with Us on TwitterStay up-to-date with the latest news, updates, and insights about Auto-GPT by following our Twitter accounts. Engage with the developer and the AI's own account for interesting discussions, project updates, and more.Developer: Follow @siggravitas for insights into the development process, project updates, and related topics from the creator of Entrepreneur-GPT.We look forward to connecting with you and hearing your thoughts, ideas, and experiences with Auto-GPT. Join us on Twitter and let's explore the future of AI together!        "
53,huggingface/transformers,https://github.com/huggingface/transformers/blob/main/README.md,Python,"                                                                                                                    English |        简体中文 |        繁體中文 |        한국어 |        Español |        日本語 |        हिन्दी        State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow    🤗 Transformers provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio.These models can be applied on:📝 Text, for tasks like text classification, information extraction, question answering, summarization, translation, text generation, in over 100 languages.🖼️ Images, for tasks like image classification, object detection, and segmentation.🗣️ Audio, for tasks like speech recognition and audio classification.Transformer models can also perform tasks on several modalities combined, such as table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.🤗 Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments.🤗 Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch and TensorFlow — with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other.Online demosYou can test most of our models directly on their pages from the model hub. We also offer private model hosting, versioning, & an inference API for public and private models.Here are a few examples:In Natural Language Processing:Masked word completion with BERTName Entity Recognition with ElectraText generation with GPT-2Natural Language Inference with RoBERTaSummarization with BARTQuestion answering with DistilBERTTranslation with T5In Computer Vision:Image classification with ViTObject Detection with DETRSemantic Segmentation with SegFormerPanoptic Segmentation with MaskFormerDepth Estimation with DPTVideo Classification with VideoMAEUniversal Segmentation with OneFormerIn Audio:Automatic Speech Recognition with Wav2Vec2Keyword Spotting with Wav2Vec2Audio Classification with Audio Spectrogram TransformerIn Multimodal tasks:Table Question Answering with TAPASVisual Question Answering with ViLTZero-shot Image Classification with CLIPDocument Question Answering with LayoutLMZero-shot Video Classification with X-CLIP100 projects using TransformersTransformers is more than a toolkit to use pretrained models: it's a community of projects built around it and theHugging Face Hub. We want Transformers to enable developers, researchers, students, professors, engineers, and anyoneelse to build their dream projects.In order to celebrate the 100,000 stars of transformers, we have decided to put the spotlight on thecommunity, and we have created the awesome-transformers page which lists 100incredible projects built in the vicinity of transformers.If you own or use a project that you believe should be part of the list, please open a PR to add it!If you are looking for custom support from the Hugging Face team    Quick tourTo immediately use a model on a given input (text, image, audio, ...), we provide the pipeline API. Pipelines group together a pretrained model with the preprocessing that was used during that model's training. Here is how to quickly use a pipeline to classify positive versus negative texts:>>> from transformers import pipeline# Allocate a pipeline for sentiment-analysis>>> classifier = pipeline('sentiment-analysis')>>> classifier('We are very happy to introduce pipeline to the transformers repository.')[{'label': 'POSITIVE', 'score': 0.9996980428695679}]The second line of code downloads and caches the pretrained model used by the pipeline, while the third evaluates it on the given text. Here the answer is \""positive\"" with a confidence of 99.97%.Many tasks have a pre-trained pipeline ready to go, in NLP but also in computer vision and speech. For example, we can easily extract detected objects in an image:>>> import requests>>> from PIL import Image>>> from transformers import pipeline# Download an image with cute cats>>> url = \""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample.png\"">>> image_data = requests.get(url, stream=True).raw>>> image = Image.open(image_data)# Allocate a pipeline for object detection>>> object_detector = pipeline('object-detection')>>> object_detector(image)[{'score': 0.9982201457023621,  'label': 'remote',  'box': {'xmin': 40, 'ymin': 70, 'xmax': 175, 'ymax': 117}}, {'score': 0.9960021376609802,  'label': 'remote',  'box': {'xmin': 333, 'ymin': 72, 'xmax': 368, 'ymax': 187}}, {'score': 0.9954745173454285,  'label': 'couch',  'box': {'xmin': 0, 'ymin': 1, 'xmax': 639, 'ymax': 473}}, {'score': 0.9988006353378296,  'label': 'cat',  'box': {'xmin': 13, 'ymin': 52, 'xmax': 314, 'ymax': 470}}, {'score': 0.9986783862113953,  'label': 'cat',  'box': {'xmin': 345, 'ymin': 23, 'xmax': 640, 'ymax': 368}}]Here we get a list of objects detected in the image, with a box surrounding the object and a confidence score. Here is the original image on the left, with the predictions displayed on the right:        You can learn more about the tasks supported by the pipeline API in this tutorial.In addition to pipeline, to download and use any of the pretrained models on your given task, all it takes is three lines of code. Here is the PyTorch version:>>> from transformers import AutoTokenizer, AutoModel>>> tokenizer = AutoTokenizer.from_pretrained(\""bert-base-uncased\"")>>> model = AutoModel.from_pretrained(\""bert-base-uncased\"")>>> inputs = tokenizer(\""Hello world!\"", return_tensors=\""pt\"")>>> outputs = model(**inputs)And here is the equivalent code for TensorFlow:>>> from transformers import AutoTokenizer, TFAutoModel>>> tokenizer = AutoTokenizer.from_pretrained(\""bert-base-uncased\"")>>> model = TFAutoModel.from_pretrained(\""bert-base-uncased\"")>>> inputs = tokenizer(\""Hello world!\"", return_tensors=\""tf\"")>>> outputs = model(**inputs)The tokenizer is responsible for all the preprocessing the pretrained model expects, and can be called directly on a single string (as in the above examples) or a list. It will output a dictionary that you can use in downstream code or simply directly pass to your model using the ** argument unpacking operator.The model itself is a regular Pytorch nn.Module or a TensorFlow tf.keras.Model (depending on your backend) which you can use as usual. This tutorial explains how to integrate such a model into a classic PyTorch or TensorFlow training loop, or how to use our Trainer API to quickly fine-tune on a new dataset.Why should I use transformers?Easy-to-use state-of-the-art models:High performance on natural language understanding & generation, computer vision, and audio tasks.Low barrier to entry for educators and practitioners.Few user-facing abstractions with just three classes to learn.A unified API for using all our pretrained models.Lower compute costs, smaller carbon footprint:Researchers can share trained models instead of always retraining.Practitioners can reduce compute time and production costs.Dozens of architectures with over 60,000 pretrained models across all modalities.Choose the right framework for every part of a model's lifetime:Train state-of-the-art models in 3 lines of code.Move a single model between TF2.0/PyTorch/JAX frameworks at will.Seamlessly pick the right framework for training, evaluation and production.Easily customize a model or an example to your needs:We provide examples for each architecture to reproduce the results published by its original authors.Model internals are exposed as consistently as possible.Model files can be used independently of the library for quick experiments.Why shouldn't I use transformers?This library is not a modular toolbox of building blocks for neural nets. The code in the model files is not refactored with additional abstractions on purpose, so that researchers can quickly iterate on each of the models without diving into additional abstractions/files.The training API is not intended to work on any model but is optimized to work with the models provided by the library. For generic machine learning loops, you should use another library (possibly, Accelerate).While we strive to present as many use cases as possible, the scripts in our examples folder are just that: examples. It is expected that they won't work out-of-the box on your specific problem and that you will be required to change a few lines of code to adapt them to your needs.InstallationWith pipThis repository is tested on Python 3.8+, Flax 0.4.1+, PyTorch 1.10+ and TensorFlow 2.6+.You should install 🤗 Transformers in a virtual environment. If you're unfamiliar with Python virtual environments, check out the user guide.First, create a virtual environment with the version of Python you're going to use and activate it.Then, you will need to install at least one of Flax, PyTorch or TensorFlow.Please refer to TensorFlow installation page, PyTorch installation page and/or Flax and Jax installation pages regarding the specific installation command for your platform.When one of those backends has been installed, 🤗 Transformers can be installed using pip as follows:pip install transformersIf you'd like to play with the examples or need the bleeding edge of the code and can't wait for a new release, you must install the library from source.With condaSince Transformers version v4.0.0, we now have a conda channel: huggingface.🤗 Transformers can be installed using conda as follows:conda install -c huggingface transformersFollow the installation pages of Flax, PyTorch or TensorFlow to see how to install them with conda.NOTE:  On Windows, you may be prompted to activate Developer Mode in order to benefit from caching. If this is not an option for you, please let us know in this issue.Model architecturesAll the model checkpoints provided by 🤗 Transformers are seamlessly integrated from the huggingface.co model hub where they are uploaded directly by users and organizations.Current number of checkpoints: 🤗 Transformers currently provides the following architectures (see here for a high-level summary of each them):ALBERT (from Google Research and the Toyota Technological Institute at Chicago) released with the paper ALBERT: A Lite BERT for Self-supervised Learning of Language Representations, by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut.ALIGN (from Google Research) released with the paper Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision by Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig.AltCLIP (from BAAI) released with the paper AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities by Chen, Zhongzhi and Liu, Guang and Zhang, Bo-Wen and Ye, Fulong and Yang, Qinghong and Wu, Ledell.Audio Spectrogram Transformer (from MIT) released with the paper AST: Audio Spectrogram Transformer by Yuan Gong, Yu-An Chung, James Glass.Autoformer (from Tsinghua University) released with the paper Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting by Haixu Wu, Jiehui Xu, Jianmin Wang, Mingsheng Long.Bark (from Suno) released in the repository suno-ai/bark by Suno AI team.BART (from Facebook) released with the paper BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension by Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov and Luke Zettlemoyer.BARThez (from École polytechnique) released with the paper BARThez: a Skilled Pretrained French Sequence-to-Sequence Model by Moussa Kamal Eddine, Antoine J.-P. Tixier, Michalis Vazirgiannis.BARTpho (from VinAI Research) released with the paper BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese by Nguyen Luong Tran, Duong Minh Le and Dat Quoc Nguyen.BEiT (from Microsoft) released with the paper BEiT: BERT Pre-Training of Image Transformers by Hangbo Bao, Li Dong, Furu Wei.BERT (from Google) released with the paper BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.BERT For Sequence Generation (from Google) released with the paper Leveraging Pre-trained Checkpoints for Sequence Generation Tasks by Sascha Rothe, Shashi Narayan, Aliaksei Severyn.BERTweet (from VinAI Research) released with the paper BERTweet: A pre-trained language model for English Tweets by Dat Quoc Nguyen, Thanh Vu and Anh Tuan Nguyen.BigBird-Pegasus (from Google Research) released with the paper Big Bird: Transformers for Longer Sequences by Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.BigBird-RoBERTa (from Google Research) released with the paper Big Bird: Transformers for Longer Sequences by Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.BioGpt (from Microsoft Research AI4Science) released with the paper BioGPT: generative pre-trained transformer for biomedical text generation and mining by Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon and Tie-Yan Liu.BiT (from Google AI) released with the paper Big Transfer (BiT): General Visual Representation Learning by Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, Neil Houlsby.Blenderbot (from Facebook) released with the paper Recipes for building an open-domain chatbot by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.BlenderbotSmall (from Facebook) released with the paper Recipes for building an open-domain chatbot by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.BLIP (from Salesforce) released with the paper BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation by Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi.BLIP-2 (from Salesforce) released with the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi.BLOOM (from BigScience workshop) released by the BigScience Workshop.BORT (from Alexa) released with the paper Optimal Subarchitecture Extraction For BERT by Adrian de Wynter and Daniel J. Perry.BridgeTower (from Harbin Institute of Technology/Microsoft Research Asia/Intel Labs) released with the paper BridgeTower: Building Bridges Between Encoders in Vision-Language Representation Learning by Xiao Xu, Chenfei Wu, Shachar Rosenman, Vasudev Lal, Wanxiang Che, Nan Duan.ByT5 (from Google Research) released with the paper ByT5: Towards a token-free future with pre-trained byte-to-byte models by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel.CamemBERT (from Inria/Facebook/Sorbonne) released with the paper CamemBERT: a Tasty French Language Model by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Suárez*, Yoann Dupont, Laurent Romary, Éric Villemonte de la Clergerie, Djamé Seddah and Benoît Sagot.CANINE (from Google Research) released with the paper CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation by Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting.Chinese-CLIP (from OFA-Sys) released with the paper Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese by An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, Chang Zhou.CLAP (from LAION-AI) released with the paper Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation by Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, Shlomo Dubnov.CLIP (from OpenAI) released with the paper Learning Transferable Visual Models From Natural Language Supervision by Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever.CLIPSeg (from University of Göttingen) released with the paper Image Segmentation Using Text and Image Prompts by Timo Lüddecke and Alexander Ecker.CodeGen (from Salesforce) released with the paper A Conversational Paradigm for Program Synthesis by Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong.Conditional DETR (from Microsoft Research Asia) released with the paper Conditional DETR for Fast Training Convergence by Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng, Houqiang Li, Yuhui Yuan, Lei Sun, Jingdong Wang.ConvBERT (from YituTech) released with the paper ConvBERT: Improving BERT with Span-based Dynamic Convolution by Zihang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, Shuicheng Yan.ConvNeXT (from Facebook AI) released with the paper A ConvNet for the 2020s by Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie.ConvNeXTV2 (from Facebook AI) released with the paper ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders by Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, Saining Xie.CPM (from Tsinghua University) released with the paper CPM: A Large-scale Generative Chinese Pre-trained Language Model by Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin, Yusheng Su, Haozhe Ji, Jian Guan, Fanchao Qi, Xiaozhi Wang, Yanan Zheng, Guoyang Zeng, Huanqi Cao, Shengqi Chen, Daixuan Li, Zhenbo Sun, Zhiyuan Liu, Minlie Huang, Wentao Han, Jie Tang, Juanzi Li, Xiaoyan Zhu, Maosong Sun.CPM-Ant (from OpenBMB) released by the OpenBMB.CTRL (from Salesforce) released with the paper CTRL: A Conditional Transformer Language Model for Controllable Generation by Nitish Shirish Keskar*, Bryan McCann*, Lav R. Varshney, Caiming Xiong and Richard Socher.CvT (from Microsoft) released with the paper CvT: Introducing Convolutions to Vision Transformers by Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, Lei Zhang.Data2Vec (from Facebook) released with the paper Data2Vec:  A General Framework for Self-supervised Learning in Speech, Vision and Language by Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, Michael Auli.DeBERTa (from Microsoft) released with the paper DeBERTa: Decoding-enhanced BERT with Disentangled Attention by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.DeBERTa-v2 (from Microsoft) released with the paper DeBERTa: Decoding-enhanced BERT with Disentangled Attention by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.Decision Transformer (from Berkeley/Facebook/Google) released with the paper Decision Transformer: Reinforcement Learning via Sequence Modeling by Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch.Deformable DETR (from SenseTime Research) released with the paper Deformable DETR: Deformable Transformers for End-to-End Object Detection by Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai.DeiT (from Facebook) released with the paper Training data-efficient image transformers & distillation through attention by Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Hervé Jégou.DePlot (from Google AI) released with the paper DePlot: One-shot visual language reasoning by plot-to-table translation by Fangyu Liu, Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Wenhu Chen, Nigel Collier, Yasemin Altun.DETA (from The University of Texas at Austin) released with the paper NMS Strikes Back by Jeffrey Ouyang-Zhang, Jang Hyun Cho, Xingyi Zhou, Philipp Krähenbühl.DETR (from Facebook) released with the paper End-to-End Object Detection with Transformers by Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko.DialoGPT (from Microsoft Research) released with the paper DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation by Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan.DiNAT (from SHI Labs) released with the paper Dilated Neighborhood Attention Transformer by Ali Hassani and Humphrey Shi.DINOv2 (from Meta AI) released with the paper DINOv2: Learning Robust Visual Features without Supervision by Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, Piotr Bojanowski.DistilBERT (from HuggingFace), released together with the paper DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2 into DistilGPT2, RoBERTa into DistilRoBERTa, Multilingual BERT into DistilmBERT and a German version of DistilBERT.DiT (from Microsoft Research) released with the paper DiT: Self-supervised Pre-training for Document Image Transformer by Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, Furu Wei.Donut (from NAVER), released together with the paper OCR-free Document Understanding Transformer by Geewook Kim, Teakgyu Hong, Moonbin Yim, Jeongyeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, Seunghyun Park.DPR (from Facebook) released with the paper Dense Passage Retrieval for Open-Domain Question Answering by Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih.DPT (from Intel Labs) released with the paper Vision Transformers for Dense Prediction by René Ranftl, Alexey Bochkovskiy, Vladlen Koltun.EfficientFormer (from Snap Research) released with the paper EfficientFormer: Vision Transformers at MobileNetSpeed by Yanyu Li, Geng Yuan, Yang Wen, Ju Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, Jian Ren.EfficientNet (from Google Brain) released with the paper EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks by Mingxing Tan, Quoc V. Le.ELECTRA (from Google Research/Stanford University) released with the paper ELECTRA: Pre-training text encoders as discriminators rather than generators by Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning.EnCodec (from Meta AI) released with the paper High Fidelity Neural Audio Compression by Alexandre Défossez, Jade Copet, Gabriel Synnaeve, Yossi Adi.EncoderDecoder (from Google Research) released with the paper Leveraging Pre-trained Checkpoints for Sequence Generation Tasks by Sascha Rothe, Shashi Narayan, Aliaksei Severyn.ERNIE (from Baidu) released with the paper ERNIE: Enhanced Representation through Knowledge Integration by Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, Hua Wu.ErnieM (from Baidu) released with the paper ERNIE-M: Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora by Xuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang.ESM (from Meta AI) are transformer protein language models.  ESM-1b was released with the paper Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences by Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, and Rob Fergus. ESM-1v was released with the paper Language models enable zero-shot prediction of the effects of mutations on protein function by Joshua Meier, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu and Alexander Rives. ESM-2 and ESMFold were released with the paper Language models of protein sequences at the scale of evolution enable accurate structure prediction by Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Sal Candido, Alexander Rives.Falcon (from Technology Innovation Institute) by Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme.FLAN-T5 (from Google AI) released in the repository google-research/t5x by Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason WeiFLAN-UL2 (from Google AI) released in the repository google-research/t5x by Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason WeiFlauBERT (from CNRS) released with the paper FlauBERT: Unsupervised Language Model Pre-training for French by Hang Le, Loïc Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen, Benoît Crabbé, Laurent Besacier, Didier Schwab.FLAVA (from Facebook AI) released with the paper FLAVA: A Foundational Language And Vision Alignment Model by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela.FNet (from Google Research) released with the paper FNet: Mixing Tokens with Fourier Transforms by James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon.FocalNet (from Microsoft Research) released with the paper Focal Modulation Networks by Jianwei Yang, Chunyuan Li, Xiyang Dai, Lu Yuan, Jianfeng Gao.Funnel Transformer (from CMU/Google Brain) released with the paper Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le.GIT (from Microsoft Research) released with the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, Lijuan Wang.GLPN (from KAIST) released with the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Doyeon Kim, Woonghyun Ga, Pyungwhan Ahn, Donggyu Joo, Sehwan Chun, Junmo Kim.GPT (from OpenAI) released with the paper Improving Language Understanding by Generative Pre-Training by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever.GPT Neo (from EleutherAI) released in the repository EleutherAI/gpt-neo by Sid Black, Stella Biderman, Leo Gao, Phil Wang and Connor Leahy.GPT NeoX (from EleutherAI) released with the paper GPT-NeoX-20B: An Open-Source Autoregressive Language Model by Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, Samuel WeinbachGPT NeoX Japanese (from ABEJA) released by Shinya Otani, Takayoshi Makabe, Anuj Arora, and Kyo Hattori.GPT-2 (from OpenAI) released with the paper Language Models are Unsupervised Multitask Learners by Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei** and Ilya Sutskever**.GPT-J (from EleutherAI) released in the repository kingoflolz/mesh-transformer-jax by Ben Wang and Aran Komatsuzaki.GPT-Sw3 (from AI-Sweden) released with the paper Lessons Learned from GPT-SW3: Building the First Large-Scale Generative Language Model for Swedish by Ariel Ekgren, Amaru Cuba Gyllensten, Evangelia Gogoulou, Alice Heiman, Severine Verlinden, Joey Öhman, Fredrik Carlsson, Magnus Sahlgren.GPTBigCode (from BigCode) released with the paper SantaCoder: don't reach for the stars! by Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, Joel Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco De Toni, Bernardo García del Río, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries, Leandro von Werra.GPTSAN-japanese released in the repository tanreinama/GPTSAN by Toshiyuki Sakamoto(tanreinama).Graphormer (from Microsoft) released with the paper Do Transformers Really Perform Bad for Graph Representation? by Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, Tie-Yan Liu.GroupViT (from UCSD, NVIDIA) released with the paper GroupViT: Semantic Segmentation Emerges from Text Supervision by Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, Xiaolong Wang.Hubert (from Facebook) released with the paper HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed.I-BERT (from Berkeley) released with the paper I-BERT: Integer-only BERT Quantization by Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W. Mahoney, Kurt Keutzer.ImageGPT (from OpenAI) released with the paper Generative Pretraining from Pixels by Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, Ilya Sutskever.Informer (from Beihang University, UC Berkeley, Rutgers University, SEDD Company) released with the paper Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting by Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang.InstructBLIP (from Salesforce) released with the paper InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning by Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi.Jukebox (from OpenAI) released with the paper Jukebox: A Generative Model for Music by Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, Ilya Sutskever.LayoutLM (from Microsoft Research Asia) released with the paper LayoutLM: Pre-training of Text and Layout for Document Image Understanding by Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou.LayoutLMv2 (from Microsoft Research Asia) released with the paper LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding by Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, Lidong Zhou.LayoutLMv3 (from Microsoft Research Asia) released with the paper LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking by Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, Furu Wei.LayoutXLM (from Microsoft Research Asia) released with the paper LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding by Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Furu Wei.LED (from AllenAI) released with the paper Longformer: The Long-Document Transformer by Iz Beltagy, Matthew E. Peters, Arman Cohan.LeViT (from Meta AI) released with the paper LeViT: A Vision Transformer in ConvNet's Clothing for Faster Inference by Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Hervé Jégou, Matthijs Douze.LiLT (from South China University of Technology) released with the paper LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding by Jiapeng Wang, Lianwen Jin, Kai Ding.LLaMA (from The FAIR team of Meta AI) released with the paper LLaMA: Open and Efficient Foundation Language Models by Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample.Llama2 (from The FAIR team of Meta AI) released with the paper Llama2: Open Foundation and Fine-Tuned Chat Models by Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushka rMishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing EllenTan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom.Longformer (from AllenAI) released with the paper Longformer: The Long-Document Transformer by Iz Beltagy, Matthew E. Peters, Arman Cohan.LongT5 (from Google AI) released with the paper LongT5: Efficient Text-To-Text Transformer for Long Sequences by Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, Yinfei Yang.LUKE (from Studio Ousia) released with the paper LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention by Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, Yuji Matsumoto.LXMERT (from UNC Chapel Hill) released with the paper LXMERT: Learning Cross-Modality Encoder Representations from Transformers for Open-Domain Question Answering by Hao Tan and Mohit Bansal.M-CTC-T (from Facebook) released with the paper Pseudo-Labeling For Massively Multilingual Speech Recognition by Loren Lugosch, Tatiana Likhomanenko, Gabriel Synnaeve, and Ronan Collobert.M2M100 (from Facebook) released with the paper Beyond English-Centric Multilingual Machine Translation by Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, Armand Joulin.MarianMT Machine translation models trained using OPUS data by Jörg Tiedemann. The Marian Framework is being developed by the Microsoft Translator Team.MarkupLM (from Microsoft Research Asia) released with the paper MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document Understanding by Junlong Li, Yiheng Xu, Lei Cui, Furu Wei.Mask2Former (from FAIR and UIUC) released with the paper Masked-attention Mask Transformer for Universal Image Segmentation by Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, Rohit Girdhar.MaskFormer (from Meta and UIUC) released with the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation by Bowen Cheng, Alexander G. Schwing, Alexander Kirillov.MatCha (from Google AI) released with the paper MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering by Fangyu Liu, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Yasemin Altun, Nigel Collier, Julian Martin Eisenschlos.mBART (from Facebook) released with the paper Multilingual Denoising Pre-training for Neural Machine Translation by Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, Luke Zettlemoyer.mBART-50 (from Facebook) released with the paper Multilingual Translation with Extensible Multilingual Pretraining and Finetuning by Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, Angela Fan.MEGA (from Meta/USC/CMU/SJTU) released with the paper Mega: Moving Average Equipped Gated Attention by Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer.Megatron-BERT (from NVIDIA) released with the paper Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper and Bryan Catanzaro.Megatron-GPT2 (from NVIDIA) released with the paper Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper and Bryan Catanzaro.MGP-STR (from Alibaba Research) released with the paper Multi-Granularity Prediction for Scene Text Recognition by Peng Wang, Cheng Da, and Cong Yao.mLUKE (from Studio Ousia) released with the paper mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models by Ryokan Ri, Ikuya Yamada, and Yoshimasa Tsuruoka.MMS (from Facebook) released with the paper Scaling Speech Technology to 1,000+ Languages by Vineel Pratap, Andros Tjandra, Bowen Shi, Paden Tomasello, Arun Babu, Sayani Kundu, Ali Elkahky, Zhaoheng Ni, Apoorv Vyas, Maryam Fazel-Zarandi, Alexei Baevski, Yossi Adi, Xiaohui Zhang, Wei-Ning Hsu, Alexis Conneau, Michael Auli.MobileBERT (from CMU/Google Brain) released with the paper MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices by Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou.MobileNetV1 (from Google Inc.) released with the paper MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications by Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam.MobileNetV2 (from Google Inc.) released with the paper MobileNetV2: Inverted Residuals and Linear Bottlenecks by Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen.MobileViT (from Apple) released with the paper MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer by Sachin Mehta and Mohammad Rastegari.MobileViTV2 (from Apple) released with the paper Separable Self-attention for Mobile Vision Transformers by Sachin Mehta and Mohammad Rastegari.MPNet (from Microsoft Research) released with the paper MPNet: Masked and Permuted Pre-training for Language Understanding by Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu.MPT (from MosaiML) released with the repository llm-foundry by the MosaicML NLP Team.MRA (from the University of Wisconsin - Madison) released with the paper Multi Resolution Analysis (MRA) for Approximate Self-Attention by Zhanpeng Zeng, Sourav Pal, Jeffery Kline, Glenn M Fung, Vikas Singh.MT5 (from Google AI) released with the paper mT5: A massively multilingual pre-trained text-to-text transformer by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel.MusicGen (from Meta) released with the paper Simple and Controllable Music Generation by Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi and Alexandre Défossez.MVP (from RUC AI Box) released with the paper MVP: Multi-task Supervised Pre-training for Natural Language Generation by Tianyi Tang, Junyi Li, Wayne Xin Zhao and Ji-Rong Wen.NAT (from SHI Labs) released with the paper Neighborhood Attention Transformer by Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and Humphrey Shi.Nezha (from Huawei Noah’s Ark Lab) released with the paper NEZHA: Neural Contextualized Representation for Chinese Language Understanding by Junqiu Wei, Xiaozhe Ren, Xiaoguang Li, Wenyong Huang, Yi Liao, Yasheng Wang, Jiashu Lin, Xin Jiang, Xiao Chen and Qun Liu.NLLB (from Meta) released with the paper No Language Left Behind: Scaling Human-Centered Machine Translation by the NLLB team.NLLB-MOE (from Meta) released with the paper No Language Left Behind: Scaling Human-Centered Machine Translation by the NLLB team.Nyströmformer (from the University of Wisconsin - Madison) released with the paper Nyströmformer: A Nyström-Based Algorithm for Approximating Self-Attention by Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, Vikas Singh.OneFormer (from SHI Labs) released with the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov, Humphrey Shi.OpenLlama (from s-JoL) released in Open-Llama.OPT (from Meta AI) released with the paper OPT: Open Pre-trained Transformer Language Models by Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen et al.OWL-ViT (from Google AI) released with the paper Simple Open-Vocabulary Object Detection with Vision Transformers by Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby.Pegasus (from Google) released with the paper PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu.PEGASUS-X (from Google) released with the paper Investigating Efficiently Extending Transformers for Long Input Summarization by Jason Phang, Yao Zhao, and Peter J. Liu.Perceiver IO (from Deepmind) released with the paper Perceiver IO: A General Architecture for Structured Inputs & Outputs by Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier Hénaff, Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, João Carreira.PhoBERT (from VinAI Research) released with the paper PhoBERT: Pre-trained language models for Vietnamese by Dat Quoc Nguyen and Anh Tuan Nguyen.Pix2Struct (from Google) released with the paper Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding by Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, Kristina Toutanova.PLBart (from UCLA NLP) released with the paper Unified Pre-training for Program Understanding and Generation by Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, Kai-Wei Chang.PoolFormer (from Sea AI Labs) released with the paper MetaFormer is Actually What You Need for Vision by Yu, Weihao and Luo, Mi and Zhou, Pan and Si, Chenyang and Zhou, Yichen and Wang, Xinchao and Feng, Jiashi and Yan, Shuicheng.ProphetNet (from Microsoft Research) released with the paper ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang and Ming Zhou.PVT (from Nanjing University, The University of Hong Kong etc.) released with the paper Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions by Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao.QDQBert (from NVIDIA) released with the paper Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation by Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev and Paulius Micikevicius.RAG (from Facebook) released with the paper Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela.REALM (from Google Research) released with the paper REALM: Retrieval-Augmented Language Model Pre-Training by Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat and Ming-Wei Chang.Reformer (from Google Research) released with the paper Reformer: The Efficient Transformer by Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya.RegNet (from META Platforms) released with the paper Designing Network Design Space by Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, Piotr Dollár.RemBERT (from Google Research) released with the paper Rethinking embedding coupling in pre-trained language models by Hyung Won Chung, Thibault Févry, Henry Tsai, M. Johnson, Sebastian Ruder.ResNet (from Microsoft Research) released with the paper Deep Residual Learning for Image Recognition by Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun.RoBERTa (from Facebook), released together with the paper RoBERTa: A Robustly Optimized BERT Pretraining Approach by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.RoBERTa-PreLayerNorm (from Facebook) released with the paper fairseq: A Fast, Extensible Toolkit for Sequence Modeling by Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli.RoCBert (from WeChatAI) released with the paper RoCBert: Robust Chinese Bert with Multimodal Contrastive Pretraining by HuiSu, WeiweiShi, XiaoyuShen, XiaoZhou, TuoJi, JiaruiFang, JieZhou.RoFormer (from ZhuiyiTechnology), released together with the paper RoFormer: Enhanced Transformer with Rotary Position Embedding by Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu.RWKV (from Bo Peng), released on this repo by Bo Peng.SegFormer (from NVIDIA) released with the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping Luo.Segment Anything (from Meta AI) released with the paper Segment Anything by Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alex Berg, Wan-Yen Lo, Piotr Dollar, Ross Girshick.SEW (from ASAPP) released with the paper Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition by Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.SEW-D (from ASAPP) released with the paper Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition by Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.SpeechT5 (from Microsoft Research) released with the paper SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing by Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei.SpeechToTextTransformer (from Facebook), released together with the paper fairseq S2T: Fast Speech-to-Text Modeling with fairseq by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Dmytro Okhonko, Juan Pino.SpeechToTextTransformer2 (from Facebook), released together with the paper Large-Scale Self- and Semi-Supervised Learning for Speech Translation by Changhan Wang, Anne Wu, Juan Pino, Alexei Baevski, Michael Auli, Alexis Conneau.Splinter (from Tel Aviv University), released together with the paper Few-Shot Question Answering by Pretraining Span Selection by Ori Ram, Yuval Kirstain, Jonathan Berant, Amir Globerson, Omer Levy.SqueezeBERT (from Berkeley) released with the paper SqueezeBERT: What can computer vision teach NLP about efficient neural networks? by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer.SwiftFormer (from MBZUAI) released with the paper SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications by Abdelrahman Shaker, Muhammad Maaz, Hanoona Rasheed, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan.Swin Transformer (from Microsoft) released with the paper Swin Transformer: Hierarchical Vision Transformer using Shifted Windows by Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo.Swin Transformer V2 (from Microsoft) released with the paper Swin Transformer V2: Scaling Up Capacity and Resolution by Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, Baining Guo.Swin2SR (from University of Würzburg) released with the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Marcos V. Conde, Ui-Jin Choi, Maxime Burchi, Radu Timofte.SwitchTransformers (from Google) released with the paper Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity by William Fedus, Barret Zoph, Noam Shazeer.T5 (from Google AI) released with the paper Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu.T5v1.1 (from Google AI) released in the repository google-research/text-to-text-transfer-transformer by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu.Table Transformer (from Microsoft Research) released with the paper PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents by Brandon Smock, Rohith Pesala, Robin Abraham.TAPAS (from Google AI) released with the paper TAPAS: Weakly Supervised Table Parsing via Pre-training by Jonathan Herzig, Paweł Krzysztof Nowak, Thomas Müller, Francesco Piccinno and Julian Martin Eisenschlos.TAPEX (from Microsoft Research) released with the paper TAPEX: Table Pre-training via Learning a Neural SQL Executor by Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou.Time Series Transformer (from HuggingFace).TimeSformer (from Facebook) released with the paper Is Space-Time Attention All You Need for Video Understanding? by Gedas Bertasius, Heng Wang, Lorenzo Torresani.Trajectory Transformer (from the University of California at Berkeley) released with the paper Offline Reinforcement Learning as One Big Sequence Modeling Problem by Michael Janner, Qiyang Li, Sergey LevineTransformer-XL (from Google/CMU) released with the paper Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context by Zihang Dai*, Zhilin Yang*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.TrOCR (from Microsoft), released together with the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, Furu Wei.TVLT (from UNC Chapel Hill) released with the paper TVLT: Textless Vision-Language Transformer by Zineng Tang, Jaemin Cho, Yixin Nie, Mohit Bansal.UL2 (from Google Research) released with the paper Unifying Language Learning Paradigms by Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, Donald MetzlerUMT5 (from Google Research) released with the paper UniMax: Fairer and More Effective Language Sampling for Large-Scale Multilingual Pretraining by Hyung Won Chung, Xavier Garcia, Adam Roberts, Yi Tay, Orhan Firat, Sharan Narang, Noah Constant.UniSpeech (from Microsoft Research) released with the paper UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael Zeng, Xuedong Huang.UniSpeechSat (from Microsoft Research) released with the paper UNISPEECH-SAT: UNIVERSAL SPEECH REPRESENTATION LEARNING WITH SPEAKER AWARE PRE-TRAINING by Sanyuan Chen, Yu Wu, Chengyi Wang, Zhengyang Chen, Zhuo Chen, Shujie Liu, Jian Wu, Yao Qian, Furu Wei, Jinyu Li, Xiangzhan Yu.UPerNet (from Peking University) released with the paper Unified Perceptual Parsing for Scene Understanding by Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, Jian Sun.VAN (from Tsinghua University and Nankai University) released with the paper Visual Attention Network by Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, Shi-Min Hu.VideoMAE (from Multimedia Computing Group, Nanjing University) released with the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Zhan Tong, Yibing Song, Jue Wang, Limin Wang.ViLT (from NAVER AI Lab/Kakao Enterprise/Kakao Brain) released with the paper ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision by Wonjae Kim, Bokyung Son, Ildoo Kim.Vision Transformer (ViT) (from Google AI) released with the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.VisualBERT (from UCLA NLP) released with the paper VisualBERT: A Simple and Performant Baseline for Vision and Language by Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang.ViT Hybrid (from Google AI) released with the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.ViTMAE (from Meta AI) released with the paper Masked Autoencoders Are Scalable Vision Learners by Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick.ViTMSN (from Meta AI) released with the paper Masked Siamese Networks for Label-Efficient Learning by Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes, Pascal Vincent, Armand Joulin, Michael Rabbat, Nicolas Ballas.ViViT (from Google Research) released with the paper ViViT: A Video Vision Transformer by Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lučić, Cordelia Schmid.Wav2Vec2 (from Facebook AI) released with the paper wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.Wav2Vec2-Conformer (from Facebook AI) released with the paper FAIRSEQ S2T: Fast Speech-to-Text Modeling with FAIRSEQ by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Sravya Popuri, Dmytro Okhonko, Juan Pino.Wav2Vec2Phoneme (from Facebook AI) released with the paper Simple and Effective Zero-shot Cross-lingual Phoneme Recognition by Qiantong Xu, Alexei Baevski, Michael Auli.WavLM (from Microsoft Research) released with the paper WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing by Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Furu Wei.Whisper (from OpenAI) released with the paper Robust Speech Recognition via Large-Scale Weak Supervision by Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever.X-CLIP (from Microsoft Research) released with the paper Expanding Language-Image Pretrained Models for General Video Recognition by Bolin Ni, Houwen Peng, Minghao Chen, Songyang Zhang, Gaofeng Meng, Jianlong Fu, Shiming Xiang, Haibin Ling.X-MOD (from Meta AI) released with the paper Lifting the Curse of Multilinguality by Pre-training Modular Transformers by Jonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James Cross, Sebastian Riedel, Mikel Artetxe.XGLM (From Facebook AI) released with the paper Few-shot Learning with Multilingual Language Models by Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, Xian Li.XLM (from Facebook) released together with the paper Cross-lingual Language Model Pretraining by Guillaume Lample and Alexis Conneau.XLM-ProphetNet (from Microsoft Research) released with the paper ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang and Ming Zhou.XLM-RoBERTa (from Facebook AI), released together with the paper Unsupervised Cross-lingual Representation Learning at Scale by Alexis Conneau*, Kartikay Khandelwal*, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov.XLM-RoBERTa-XL (from Facebook AI), released together with the paper Larger-Scale Transformers for Multilingual Masked Language Modeling by Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, Alexis Conneau.XLM-V (from Meta AI) released with the paper XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models by Davis Liang, Hila Gonen, Yuning Mao, Rui Hou, Naman Goyal, Marjan Ghazvininejad, Luke Zettlemoyer, Madian Khabsa.XLNet (from Google/CMU) released with the paper ​XLNet: Generalized Autoregressive Pretraining for Language Understanding by Zhilin Yang*, Zihang Dai*, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.XLS-R (from Facebook AI) released with the paper XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale by Arun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman Goyal, Kritika Singh, Patrick von Platen, Yatharth Saraf, Juan Pino, Alexei Baevski, Alexis Conneau, Michael Auli.XLSR-Wav2Vec2 (from Facebook AI) released with the paper Unsupervised Cross-Lingual Representation Learning For Speech Recognition by Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, Michael Auli.YOLOS (from Huazhong University of Science & Technology) released with the paper You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Yuxin Fang, Bencheng Liao, Xinggang Wang, Jiemin Fang, Jiyang Qi, Rui Wu, Jianwei Niu, Wenyu Liu.YOSO (from the University of Wisconsin - Madison) released with the paper You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling by Zhanpeng Zeng, Yunyang Xiong, Sathya N. Ravi, Shailesh Acharya, Glenn Fung, Vikas Singh.Want to contribute a new model? We have added a detailed guide and templates to guide you in the process of adding a new model. You can find them in the templates folder of the repository. Be sure to check the contributing guidelines and contact the maintainers or open an issue to collect feedbacks before starting your PR.To check if each model has an implementation in Flax, PyTorch or TensorFlow, or has an associated tokenizer backed by the 🤗 Tokenizers library, refer to this table.These implementations have been tested on several datasets (see the example scripts) and should match the performance of the original implementations. You can find more details on performance in the Examples section of the documentation.Learn moreSectionDescriptionDocumentationFull API documentation and tutorialsTask summaryTasks supported by 🤗 TransformersPreprocessing tutorialUsing the Tokenizer class to prepare data for the modelsTraining and fine-tuningUsing the models provided by 🤗 Transformers in a PyTorch/TensorFlow training loop and the Trainer APIQuick tour: Fine-tuning/usage scriptsExample scripts for fine-tuning models on a wide range of tasksModel sharing and uploadingUpload and share your fine-tuned models with the communityCitationWe now have a paper you can cite for the 🤗 Transformers library:@inproceedings{wolf-etal-2020-transformers,    title = \""Transformers: State-of-the-Art Natural Language Processing\"",    author = \""Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush\"",    booktitle = \""Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\"",    month = oct,    year = \""2020\"",    address = \""Online\"",    publisher = \""Association for Computational Linguistics\"",    url = \""https://www.aclweb.org/anthology/2020.emnlp-demos.6\"",    pages = \""38--45\""}"
54,Ebazhanov/linkedin-skill-assessments-quizzes,https://github.com/Ebazhanov/linkedin-skill-assessments-quizzes/blob/main/README.md,Python,"Linkedin Skill assessments - Answers⚠️ DISCLAIMER: The owners of this repository are not liable for any illegal usage of the content provided in this repository. The content is provided for informational and educational purposes only, and any actions taken by users of this repository are the responsibility of the user. By accessing this repository, you agree to hold the owners harmless from any claims, damages, or expenses arising from the use of the information provided.[ Go to see the last contributor ]🙏 PLEASEAlways add explanation (or reference link) to your answers. Use online grammar checker.That would help anyone to better learn new concepts!🎉 AnnouncementsColumn Translation have links to quizzes in different languages like Es, Fr, It and De.If you want to meet each other or discuss quiz related problems or maybe ask for skills endorsement just join the Discord chat.Playground before taking quiz using:MD2Practice (Web App)Skill Assessments Quizzes (Web App)LinkedIn Quizzed with Kodyfire (Terminal)Want to contribute? Here is the source code.❓ Need help?Open new issue🔥 Open in VS Code view here or thereTable of ContentsLinkedin-quiz-questionsPassed/FailedTranslated  in ...QuestionsAnswersYour resource for answers. In case you have doubts please contact this person or add them to review your PR.Accounting❗needs updating  5049@tujinwei, @mervynteo, @johnfelipeAdobe-Acrobat  2722Adobe-Illustrator❗needs updating  7674Adobe-InDesign❗needs updating  4240Adobe-Lightroom❗needs updating  2020Adobe-Photoshop❗needs updating  9393@declarckAdobe Premiere Pro  4836Adobe XD  1613After Effects❗needs updating  2413Agile Methodologies❗needs updating  116116@its0x08Android  7272@surajsahani, @mr-shoxruxbek, @ItSNeverLateAngular  7965@vanekbr, @aamita96ArcGIS Products55AutoCAD❗needs updating  7775@djayorAutodesk Fusion 360❗needs updating  3725@djayor, @tm-sanjayAutodesk Maya3030@marifogluAWS  9999@jokerkeny, @Amsal1AWS-Lambda❗needs updating  5149Bash  7877@D4RIO, @Amsal1C#6161@LiviuSosu, @RamonMartinezNieto, @declarckC++❗needs updating7373@Amsal1, @Amsal1C (Programming Language)8383@makifay, @Amsal1, @its0x08CSS122116@BHARGAVPATEL1244Cybersecurity❗needs updating10196Django7171@PROCW.NET Framework6359@declarckEclipse❗needs updating3628Front-end Development6868@vanekbr, @ShankS3, @declarckGit134134@Emanuele-emGo (Programming Language)4040@ruslanbes, @monkrusGoogle Ads2925Google Analytics8282Google Cloud Platform (GCP)5250@antra0497Hadoop7154HTML129128@declarckIT Operations5454@asahioceanJava130130@sumanas27, @ruslanbes, @PROCWJavascript131131@taletski, @PROCW, @msteiner96, @declarckjQuery8477@declarckJSON❗needs updating8786@iHamzaKhanzadaKeynote140Kotlin7878@ItSNeverLate, @HusseinhjLinux8278@D4RIO, @Amsal1Logic Pro8278Machine Learning9898@aaronwangj, @antra0497MATLAB7070@tm-sanjayMaven5350Microsoft Access3028@drmegalomaniacMicrosoft Azure5553@tomtreffke, @ziasistaniMicrosoft Excel❗needs updating109107@gazihasanrahmanMicrosoft Outlook7956Microsoft Power Automate1402@mervynteoMicrosoft Power BI8180@vittorio-giattiMicrosoft Power Point8577@ckulloMicrosoft Project❗needs updating4443Microsoft Word❗needs updating7877MongoDB7777MySQL9797@ruslanbesnode.js7976@pbachmanNoSQL5655objective-c4038OOP10282@declarck, @gaurovgiriPHP8979@ruslanbes, @msteiner96Pro Tools22Python176176@tik9, @Amsal1, @declarck, @TSG405QuickBooks❗needs updating6739R5252@gregglindReact.js100100@RobTables @bandinoplaREST API6565Revit❗needs updating140Ruby on Rails5959@gudataRust3232@BobbyByrne @Emanuele-emScala5248Search Engine Optimization (SEO)8181SharePoint❗needs updating5338Sketchup22SOLIDWORKS❗needs updating5757@BHARGAVPATEL1244Spring Framework6767Swift6767Transact-SQL (T-SQL)4542@beefydog, @BenVlodgiUnity❗needs updating4746@uno-sebastianVisual Basic for Applications (VBA)❗needs updating3634@AdamKaczor6250Visio3535Windows Server6857WordPress8073@ruslanbes, @Amsal1XML4342@ruslanbesContributors ✨Thanks goes to these wonderful people (emoji key):            Evgenii💻 🖋      Sergei Stadnik💻 🔍 🤔 📖      Santhosh💻      Jacob Dsa💻 🖋      Aaron Meese💻 🖋      arqarq💻 🖋      Amit Yadav💻 🖋              Javokhir Nazarov💻 🖋      saurav kumar🖋      Chetan🖋      Amir Hossein Shekari🎨 🖋 💻      SergDaut🎨      Nilotpal Pramanik🎨 💻 🖋 💼 📖 🔣 💡      Abhishek Kumar🎨              Monu Gupta🎨      KARTIKEYA GUPTA💻 🖋      kenkyusha💻 🖋      juandavidtowers💻 🖋      cyber-netics💻 🖋      jtrisw💻 🖋      Renato Regalado💻 🖋              Matthew💻 🖋      Jan S.💻 🖋      Manoli💻 🖋      Faraz tanveer💻 🖋      mohnishkarri💻 🖋 🎨      andyzhu💻 🖋      Vishal Kushwah💻 🖋              Yurii Yakymenko💻 🖋      Swetabh Suman💻 🖋      AJAY DANDGE💻 🖋      Mehmet Yesin🎨      Lok Chun Wai🎨      Adria de Juan🎨      GL-Man🎨              Jheel Patel🎨      Sameer Waskar🎨      Alexander Andrews🎨      Alexander Maxwell🎨      Slava🎨      Mayur Khatri🎨      Mascantosh💻 🖋 📢 🤔              Kivanc Enes🎨      Ritika Das🎨      Zer07793🎨      Andrew Cheung🎨      Sadha🎨      tainenko🎨 💻      github-star-coder🎨              Danilo Oliveira🎨      lordeko🎨      Shubham Kumar🎨 💻      testtree🎨      Cheryl Murphy🎨 💻      Bipin Thomas🎨      Abdulrahman Hisham🎨              Dakshitha Dissanayaka🎨      BADR KACIMI🎨      Alex Wang🎨      Maxim🎨      GordonGrant🎨 💻      Ephrem Demelash🎨      JonOrcutt🎨              topdev10🎨      cookwellwebsite🎨      xren935🎨      Nemo Frenkel🎨      MD SAIF ALAM🎨      Boris López Araya🎨      Larry Chiem🎨              Muhammad Bilal Ilyas🎨      AliMilani🎨 💻      Suraj Sahani🎨      FlyingSquirrel🎨      Erick Tijero🎨      Jaskaran Kukreja🎨      MichaelL🎨              MagicLegend🎨      Dereck Bearsong🎨      Pappu Kumar Pashi🎨      Venkata Kishore Tavva🎨      Rafat Touqir Rafsun🎨      Snehesh Dutta🎨      Timo Körner🎨 💻              alexxxan🎨      GGJason🎨      LeeAnna Ewing🎨 🤔      kamal Jyotwal🎨      Bob-Johns🎨 💻 🖋      yunussalmanlyit🎨 💻      chilcot🎨 💻              Jacky Li💻 🖋 🎨      Sarthak Trivedi🎨      Ayush Aggarwal🎨 💻      Nic Ballarini🎨      Luigi Zambetti🎨 💻      govindhaswin🎨      Addy Roy💻 🎨              Akshat Tamrakar🎨 💻      Sai Bhargava Ramu🎨      Gurkan💻      Spencer Hayes-Laverdiere💻      Aniket Soni💻      tanmay5792💻      Dina Taklit💻 🎨 🖋              Dushyant Singh💻      Ravi Prakash Singh💻      Nihal Joshi💻      Guy Klages💻      Arvind🎨 💻      mujeeb91💻      joserca🎨 💻              Prateek Agrawal💻      Teoh Tze Chuin(サラ)💻 🎨      Jayant Jain💻      Ayush Sahu💻      Hridya Krishna R💻 🎨      Rahul Bali💻 🎨      S.ZHeng🎨 💻 💼              Shriya Madan🎨 💻      mahalrupi🎨      Lucas Lermagne🎨      Jeff Deutsch🎨 💻      Betoxx1🎨      Wingman4l7🎨      Martin Espericueta🎨              Mh-Tahir🎨      Zdravko Šplajt🎨 💻      Ms3105🎨 💻 🖋      Ambika Sidhesware💻      mundoguero💻      Darkus24🖋      Sou-786🖋 🎨              Banurekha🖋      ShiraStarL🎨      Ilya Komarov🎨      DemigodMs🖋 📖      Mekha Hridya🎨 🔍      Andrey Safonov🎨 🔍      Tommaso🎨 💻              Jessica Salbert💻 🎨      JAYANTH DOLAI💻 🎨      silverstroom💻 🎨 💼      Furkan Sayım💻 🎨      Sukumar Chandrasekaran🎨      Yejin Park🎨 💻      Ali Nooshabadi🎨 💻              imitavor🎨 💻      Salih Kilicli🎨 💻      Marcelo Meneses🎨 💻      Anton Krekotun🎨 🚧 🖋 💻 📖 💼      Arnav Sarma💻 💡 🎨      meghatiku💻 🎨      Anshu Trivedi🎨              Taylor Dorsett💻 🖋 🎨      Havit Rovik💻      pushpapune💻 🎨      Ramtin Radfar🎨 🤔 💼 💵 💻 🖋 💬      Abdulmajeed Isa💻 🎨      vikassaxena02🎨      RobTables🎨 💻 💼              Daniel🎨 💻 💼 🔍      Zahid Ali💻 🎨      Chad Chai💻 🎨      Marco Biedermann💻 🎨 💼 🤔      Srinidhi Murthy🎨      Miao Cai💻 🎨      Dionicio Diaz🎨 💻              Mir Monoarul Alam🎨      Shawn Ohn💻 🎨      Amanbolat Balabekov🎨 💻      black-mamba-code💻      Jian-forks🎨 💻      shivani patel🎨      Akash Chowrasia🎨              yairg98🎨      Jay Gajjar🎨      coolerbooler💻      Md Zinnatul Islam Morol🎨      shresthashok550🎨 📖      Alan Pallath📖      Adrian Wong💻              vsDizzy💻 🎨      Frex Cuadillera🎨 💻      ashish570💻 🎨      ruchpeanuts💻 🎨      Artmasque🎨 💻      Amirhossein Mojiri Foroushani🎨      for💻 🎨              Luke🎨 💻      Hector Espinoza🎨      Adrián Buenfil🎨 💻      Amit Kumar🎨      schoppfe🎨 💻      Sofiyal C🎨 💻      spitlisk💻 🎨              PRAVIN SHARMA🎨      NIDZAAA1🎨 💻      John Mai🎨 💻      kimsoyeong🎨      Dona Ghosh💻      Ryan Hill🎨 💻      j42z🎨 💻              Ashish Sangale🎨 💻      Derek Yang🎨 💻      mohsinmsm🎨 💻      Gokulkrish2302💻      Bhaavishek💻 🎨      Louis Liao🎨      sengc92🎨 💻              Alex Marvin🎨      Balkrishna Bhatt🎨 💻      Evaldas Lavrinovičius🎨 💻      Adam Erchegyi🎨 💻      Truman Hung🎨 💻      rzamora11🎨      gaurav0224🎨              Lee GyeongJun🎨      Mirek🎨 💻      surajm245🎨      ArisLaode🎨 💻      RaviDhoriya🎨 💻      sarai-84🎨 💻      Vishnu🎨 💻              Muhammad Minhaj💻      Chandrika Deb🎨 💻      Gitgit101-bit💻 🎨      Hedi Sellami💻 🎨      saurabhvaish93💻 🎨      Nikola Begovic💻 🎨      Wang💻 🎨              Manuel Eusebio de Paz Carmona🎨      Basim Al-Jawahery🎨 💻      RAJA AHMED🎨 💻      Abhik Lodh💻      Md. Pial Ahamed💻 🎨      Hassan Shahzad💻 🎨      Christian Sosa Gago💻              Hasnain Rasheed💻 🎨      T-Radford💻      dahiyashish💻 🎨      RahulSharma468💻 🎨      Jumpod Plekhongthu💻 🎨      Thomas Young-Audet💻 🎨      VinayagamBabu💻 🎨              Deniz Koç💻 🎨      Azhar Khan💻 🎨 🖋 📖 🔣 🚧      Jacob Short💻 🎨      Uchimura85💻 🎨      Leo Nugraha💻 🎨 📖      Mujtaba Mehdi📖 🖋      Jim-ds💻 🎨              Sreehari K💻 🎨      Florian Martinez💻 🎨      Aaron💻 🎨      apoage🎨      Ignacio Guillermo Martinez 💻 🎨      AirlineDog🎨 💻      Mekel🎨 💻              hmosharrof🎨 💻      Ben Emamian💻 🎨      babeshark💻 🎨      Leonardo Jaques💻 🎨      Stefanos Apkarian💻 🎨      Ayhan Albayrak💻 🎨      KidusMT💻 🎨              hectormarroquin20💻 🎨      Edelweiss35💻 🎨      MihaiD💻 🎨      AnveshReddyAnnem💻 🎨      Hyunjae Park💻 🎨      Rajiv Albino💻 🎨      Atishay💻              Yusuf Naheem🎨      Windu🎨 💻      Superv1sor💻 🎨      Karine (:🎨 💻      Eduard Pech🎨 💻      jjeshwani🎨 💻      Steve🎨 💻              Aleigh Ohslund💻      Abhinav Suman🎨 💻      Hamza Ehtesham Farooq🎨 💻      IamNotPeterPan💻 💵 🎨      Cetger🎨      pkonopacki🎨      Yang Yang🎨 💻              Muhammad Shoaib Sarwar💻      Murilo Henrique💻 🎨      emilianoalvz🎨 💻      Sumana Saha🎨 💻      Yurii17K🎨 💻      Rupesh Bhandari🎨 💻      salmos3718💻              John Baker🎨 💻      SanjaySathiraju🎨 💻      Donat Kabashi🎨      Arul Prasad J🎨 💻      Qi Chen🎨 💻      Maksym Dmyterko🎨 💻      ilovepullrequests💻              Samira Maleki🎨 💻      NIKITA MAHOVIYA💻      jesuisdev.Net🎨 💻      Ashraf Nazar🎨      Naveed Ahmad🎨      Ajmain Naqib🎨 💻      Avinash Tingre💻 🎨              nicktids🎨      Keith Dinh💻 🎨      André Ferreira💻 🎨      eliottkespi💻 🎨      praveenpno💻 🎨      vitowidigdo💻 🎨      Devesh Pratap Singh💻 🎨              Dario Rodriguez💻 🎨      charmander_didi💻 🎨      PHBasin💻 🎨      Ritvik Singh Chauhan💻 🎨      Riya P Mathew💻 🎨      Stephanie Cherubin💻 🎨      BenitesGui💻 🎨              FarikBear💻 🎨      Dmytro Havrilov💻 🎨      Parvesh Monu💻 🎨      Dipen Panchasara💻 🎨      gudata🎨 💻      gawadeditor💻 🎨      Kirill Taletski🎨 💻              Saajan🎨 💻      Kushagra S🎨 💻      Oanh Le🎨 💻      Frane Medvidović🎨 💻      Yorman🎨 💻      Bill Chan🎨 💻      Pratik Lomte🎨 💻              LOC LAM🎨 💻      TUSAR RANJAN MAHAPATRA💻      BhargavKanjarla💻      Karel De Smet💻 🎨      sidisan🎨      ygnzayarphyo🎨 💻      svansteelandt💻              Kebechet🎨      Daniel Selvan D🎨 💻      Mahdi Razavi🎨 💻      Niklas Tiede💻 🎨      narutubaderddin💻 🎨      dylandhood💻      Dheeraj Gupta💻              Pieter Claerhout💻 🎨      Shivam Agnihotri💻      RanjithReddy-Narra💻      Nikita Wadhwani🎨 💻      rsholokh💻 🎨      Ayaan Hossain💻 🎨      Rajesh Swarna💻              Deniz Etkar🎨 💻      pro335💻 🎨      Jakub Radzik💻 🎨      Hamza Khanzada💻      ARNON🎨      Vikram Singh💻      Shoxruxbek💻 🎨              Amit Khatri💻 🎨      Wali Ullah🎨 💻      Amit11794💻 🎨      metis-macys-66898💻 🎨      Faisal Maqbool🎨 💻      Kumar Neeraj💻 🎨      Maurizio Marini🎨 💻              Saket Kothari🎨 💻      Szymon Zborowski🎨 💻      iks3000🎨 💻      Ehsan Seyedi🎨 💻      vanekbr🎨 💻      Princy_M🎨 💻      Shijie Zhou🎨 💻              lakshyamcs16🎨 💻      Filippo Facco🎨 💻      mendel5🎨 💻      Patryk🎨 💻      VishwaSangani🎨 💻      Alvin Zhao🎨 💻      Lazar Gugleta🎨 💻              vmicho🎨 💻      Sikandar Ali🎨 💻      Raja Babu🎨 💻      faizajahanzeb💻      Guil_AiT🎨 💻      Kushal Das🎨 💻      Luis Bonilla🎨 💻              jovan1013🎨 💻      Damian🎨 💻      Yash Gupta💻      lolcatnip🎨 💻      Ikko Ashimine🎨 💻      Farukh🎨 💻      Moksedul💻 🎨              Navneet Kumar🎨 💻      Saqib AlMalik💻      fahimrahman🎨 💻      vaibhav patil🎨 💻      Rahul Madan🎨 💻      kartik Kaklotar🎨 💻      ASAHI OCEAN🎨 💻              Daniel Jungbluth🎨 💻      Rajdeep Singh Borana🎨 💻      ankitha19💻      Linh Tran💻      islamarr💻 🎨      Mohamed Sabith🎨 💻      Miguel Angel Cruz Acosta🎨 💻              Adebayo Ilerioluwa 🎨      Markus🎨 💻      dkonyayev🎨 💻      Kevin A Mathew🎨 💻      David Melo🎨 🔣      DFW1N🎨 💻      Sohaib Ayub🎨 💻              Navvy🎨 💻      bloodiator2🎨 💻      Hanji🎨 💻      arthur74🎨 💻      Sri Subathra Devi B🎨 💻      Akif Aydogmus🎨 💻      Umer Javaid🎨 💻              Norio Umata🎨 💻      Gazi Hasan Rahman🎨 💻      Keith Nguyen🎨 💻      Megalomaniac🎨 💻      ShankS3🎨 💻      Farhad Alishov🎨 💻      Ronak J Vanpariya🎨 💻              azrael0learza🎨 💻      Pavel Rahman🎨 💻      chuabern🎨 💻      Rahul Tirkey🎨 💻      Ruslan Bes🎨 💻 💡 🚧 🖋 🔣 🚇      Bohdan🎨 💻      Juzdzewski🎨 💻              Grigor Minasyan🎨 💻      alvintwc🎨 💻      Anand Natarajan🎨 💻      Kashan Ali🎨 💻      Thomas Meshail🎨 💻      Son Pham🎨      Michael French💡              Yash Mishra📖      Miguel Rodriguez🎨 💻      Philipp Bachmann🎨 💻      sunny🎨 💻      Siddharth Chatterjee🎨 💻      Michael Naghavipour🎨 💻      Sahil Garg🎨 💻              MicroLion🎨 💻      wctwc🎨 💻      Rohan Sharma🔣      AshishBodla🎨 💻      Taras Pysarskyi🎨 💻      Luqman Bello O.🎨 💻      DyingDown🎨 💻              Diego Chapedelaine🎨 💻      Richlee🎨 💻      Asif Habib🎨 💻      Mazharul Hossain🎨 💻      toni🎨 💻      Pragyanshu Rai🎨 💻      Matthew Eller🎨 💻              AbhiBiju🎨 💻      Roman Zhornytskiy🎨 💻      Lucas Camino🎨 💻      João Vitor Casarin🎨 💻      Evgeniy Shay🎨 💻      Ehsan Barkhordar🎨 💻      Gabriel🎨 💻              Shibu Mohapatra🎨 💻      Pavel Kirkovsky🎨 💻      Tahir Gul🎨 💻      imDevSalman🎨 💻      Jordan Donaldson🎨 💻      js-venus🎨 💻      Faisal Shaikh🎨 💻              ashishbpatil🎨 💻      Tri Le🎨 💻      tomtreffke🎨 💻      Salah Eddine Lalami🎨 💻      Mattias Xu🎨 💻      Manas Gupta🎨 💻      wolfsong62🎨 💻              Mehdi Mirzaei🎨 💻      Van Ba Khanh🎨 💻      Sel Embee🎨 💻      Suvradip Paul🎨 💻      Sharique🎨      Seabass🎨 💻      Penny Liu🎨 💻              jatinder bhola🎨 💻      misterqbit🎨 💻      Daniel-VS9🎨 💻      Shruthi🎨 💻      beefydog🎨 💻      Suraj Kumar🎨 💻      hrishikeshps🎨 💻              Sudarshan🎨 💻      Divyansh💻 🎨      Zyaire🎨 💻      Omar Belkady🎨 💻      alexiismua🎨 💻      Eduarda Alves🎨      pycoach🎨 💻              Ruhul🎨 💻      pmoustopoulos🎨 💻      Lee Hui Ting💻 🎨      bodi1981🎨 💻      Devaraat Joshi🎨 💻      Johnny🎨 💻      rogue-coder🎨 💻              viiktr🎨      Lalit Mohan💻      João Sousa💻      言葉之靈💻 🎨      RJLABS💻      brittney0522🎨 💻      sham🎨 💻              Glenn Goossens💻 🎨      Cyber Hawk🎨 💻 🖋 💼      Ankit Yadav🎨 💻      verbality💻      Mohammed Siddiqui🎨 💻      AdamKaczor6250🎨 💻      Ramón Martinez Nieto🎨 💻              Grzegorz Dziubak🎨 💻      Ayoub BERDEDDOUCH🎨 💻      nikola-fadv🎨 💻      Akarsh Agrawal🎨 💻      Mitra Mirshafiee🎨 💻      Parker Stephens🎨 💻      alrenee99💻              Karthick Vankayala💻      Iryna 🎨 💻      palanugrah💻      Gwinbleind🎨 💻      Randy Bobandy🎨 💻      Bek Rozikoff💻      davnguye🎨 💻              Neel Patel💻      ehudbehar🎨 💻      nicholas-cod3r🎨 💻      michaelfranki🎨      Esther White🎨 💻      prathmeshpb🎨 💻      Victor Lin🎨 💻              Christine C. Yin🎨 💻      GitLearner-begin🎨 💻      Mesrop Andreasyan🎨 💻      Nathan Garcia🎨      commonsw04🎨 💻      Md. Rashad Tanjim🎨 💻      Ali Malek💻              PAODLT🎨 💻      Nikhil Bobade🎨 💻      hyuckjin21💻      Itasha Modi🎨 💻      Nikitha Reddy🎨 💻      Mahshooq Zubair🎨 💻      Subham Das💻              Onkar Birajdar🎨 💻      Nick Titomichelakis🎨 💻      Christian Leo-Pernold🎨      Matthew Marquise🎨 💻      baronfac🎨 💻      Abhishek Tilwar🎨 💻      DavidsDvm🎨 💻              Parth Parikh🎨 💻      Hector Castro🎨 💻      Rikky Arisendi🎨 💻      Ali HamXa🎨 💻      Frank.wu🎨 💻      Jatin Kumar🎨 💻 📖      masterHAWK99🎨 💻              Pushp Jain🎨 💻      Ashutosh Rout🎨 💻      Atharva Deshpande🎨 💻      Teodor Ciripescu🎨 💻      Anmol Bansal🎨 💻      Nikhil Kumar Macharla🎨 💻      Dexter🎨 💻              Aaron🎨 💻      Yogita Jaswani🎨 💻 📖 🖋      StoryDev🎨 💻      Mesut Doğansoy🎨 💻      Paras Dhawan🎨 💻      Emanuel Zhupa🎨 💻      Aaradhyaa717🎨 💻              jaacko-torus🎨 💻      mBlack💻      kalrayashwin📖 🖋 🎨 💻      Seraph💻 🎨      ZhiHong Chua🎨 💻      Amsal Khan🎨 💻 📖 🖋      Raghav Rastogi🎨 💻              Tzila📖      Shahriar Nasim Nafi📖      AG🎨 💻      Mojtaba Kamyabi🎨 💻      Ahmad Abdulrahman🎨 💻      Eclipse🎨 💻      Anshu Pal🎨 💻              Denis🎨 💻      mehmet sayin📖      WebDEV🎨 💻      Sam Komesarook🎨 💻      Kiran Ghimire🎨 💻      Joshua Davis🎨 💻      Muhammad-Huzaifa-Siddiqui💻              tobeornottobeadev🎨 💻      VAIBHAV SINGHAL🎨 💻      Keiran Pillman🎨 💻      Max Donchenko🎨 💻      sgonsal🎨 💻      diksha137🎨 💻      Vignesh🎨 💻              Gabriel França🎨 💻      Joseph🎨 💻      Bruno Rafael🎨 💻      vcamarre🎨 💻      thibault ketterer🎨 💻 🚧      VictorGonzalezToledo🎨 💻      1911510996🎨 💻              invidu🎨 💻      Nurul Furqon🎨 💻      David Asbill🎨 💻      Niko Birbilis🎨 💻      Mugundan Kottursuresh🎨      agrsachin81🎨 💻      Othmane El Alami🎨 💻              Syed Atif Ali🎨 💻      lakhanjindam🎨 💻      youssef hamdane🎨 💻      starfaerie🎨 💻      rodrigo0107🎨 💻      Michał Gralak🎨 💻      Jewel Mahmud🎨 💻              cwilson830🎨 💻      buun1030🎨 💻      Reda-ELOUAHABI🎨 💻      saad-aksa🎨 💻      Emdadul Haque🎨 💻      PROCW🎨 💻      cccppp1🎨 💻              Joanna Baile🎨 💻      Ahmed Saber🎨 💻      Masoud Keshavarz🎨 💻      mortazavian🎨 💻      Aniket Pandey🎨 💻      Vijay Nirmal🎨 💻      Daniel Carvallo💻              menaechmi🎨 💻      azenyx🎨 💻      Ahmet Özrahat🎨 💻      Abdulrahman Abouzaid🎨 💻      jmgnorbec🎨 💻      palinko91🎨 💻      Laisson R. Silveira🎨 💻              BHARGAVPATEL1244🎨 💻      Candide U🎨 💻      Sitansh Rajput🎨 💻      Houda Mouttalib🎨 💻      MumuTW🎨 💻      Suave Bajaj🎨 💻      Mehdi Parsaei🎨 💻              Dinko Osrecki🎨 💻      Dhia Djobbi🎨 💻      Mahmoud Galal🎨 💻      Anh Minh🎨 💻      Suvesh K🎨 💻      Petar Todorov🎨 💻      Alexander Nguyen🎨 💻              Morteza Jalalvand🎨 💻      Claudson Martins🎨 💻      Matt Jacobson🎨 💻      Rafael Belokurows🎨 💻       Thomas Gamauf🎨 💻      Rishabh Mahajan🎨 💻      rakeshpdgupta23🎨 💻              Shashidharknaik🎨 💻      taleleuma🎨 💻      Florian Bühler🎨 💻      Raihan Bin Wahid🎨 💻      MOHAMMED NASSER🎨 💻      federico🎨 💻      Andre Violante🎨 💻              tcunningham98🎨 💻      Jan Grießer🎨 💻      Serkan Alc🎨 💻 🖋      Jez McKean🎨 💻      meisam alifallahi🎨 💻      Mehul Thakkar🎨 💻      Saksham Soni🎨 💻              Pedro Peregrina🎨 💻      Mintu Choudhary🎨 💻      lucianmoldovanu🎨 💻      John C. Scott🎨 💻      Mia D.🎨 💻      EwenBernard🎨 💻      M. Reza Nasirloo🎨 💻              Jay Agrawal🎨 💻      DeShay🎨 💻      Jay206-Programmer🎨 💻      Elender🎨 💻 🖋      Bobby Byrne🎨 💻      Pirci🎨 💻      Hasanuzzaman🎨 💻              Josh Kautz🎨 💻      Brofar🎨 💻      Mina Karam🎨 💻      Duncan O N🎨 💻      Sean Tumulak-Nguyen🎨 💻      Artur Trześniewski🎨 💻      JJaammeessM🎨 💻              shubham agarwal🎨 💻      Michele Righi🎨 💻      Panagiotis Kontos🎨 💻      sumitbathla🎨 💻      Deepak Mathur🎨 💻      Juho Nykänen🎨 💻      Santiago González Siordia🎨 💻              SRIJITA MALLICK🎨 💻      Samriddhi B🎨 💻      Nitzan Papini🎨 💻      Mario Sanz🎨 💻      Crab^4🎨 💻      Pablo🎨 💻      Gordon Pham-Nguyen🎨 💻              Kristoffer🎨 💻      chrisblach🎨 💻      Gábor🎨 💻      Lina🎨 💻      Harrison Watts🎨 💻      Mario Petričko🎨 💻      Ben8120🎨 💻              Giovanna🎨 💻      Minal Ahuja🎨 💻      mossfarmer🎨 💻      ThaC0derDre🎨 💻      itware🎨 💻      Michael Walker🎨 💻      Tom Jacob Chirayil🎨 💻              Sachin Kumar🎨 💻      adi-ray🎨 💻      Dr-Blank-alt🎨 💻      Bogdan Cazacu🎨 💻      Gilson Urbano🎨 💻      Nina🎨 💻      Anthony🎨 💻              manushimjani🎨 💻      Michael Reyes🎨 💻      Rachel Kennelly🎨 💻      Aakash Garg🎨 💻      Daniel Livingston🎨 💻      alexrojco🎨 💻      Minh Nguyen🎨 💻              Mahesh Dattatraya Babar🎨 💻      Jin Zihang🎨 💻      Bikramjit Ganguly🎨 💻      QuestionableGuise🎨 💻      liq19ch🎨 💻      Bruno Rocha🎨 💻      Anand Dyavanapalli💻 🖋              crucian-afk🎨 💻      0xgainz🎨 💻      weirdfsh🎨 💻      Valan Baptist Mathuranayagam🎨 💻      Paul Kaefer🎨 💻      Yu-Hsiang Wang🎨 💻      Javad Adib🎨 💻              davidliu0930🎨 💻      Achilleas John Yfantis🎨 💻      Omkar Shivadekar🎨 💻 🖋 🐛      ToanTran🎨 💻      Gautam Naik🎨 💻      Marc🎨 💻      twix20🎨 💻              Kristian S.🎨 💻      Aleksey Khoroshilov🎨 💻      arjunsrsr🎨 💻      Ali Haider🎨 💻      Trisha Dring🎨 💻      Andre Marzulo🎨 💻      Krishna Modi🎨 💻              Rosemary Li🎨 💻      Alex Weller🎨 💻      Tam Nguyen🎨 💻      aquintelaoliveira🎨 💻      Norbert Brett🎨 💻      rocsogd🎨 💻      0nyr🎨 💻              rethkevin🎨 💻      RickHeadle🎨 💻      Leandre🎨 💻      Natnael Sisay🎨 💻      sbbu🎨 💻      wael🎨 💻      Fabricio Tramontano Pirini🎨 💻              Alexander Stoyanov🎨 💻      Dezx20🎨 💻      southparkkids🎨 💻      bmstar🎨 💻      kiagam🎨 💻      Juan Castillo🎨 💻      FFenne🎨 💻              Jose Toledo🎨 💻      Pat McGhen🎨 💻      Eiko Wagenknecht💻 🖋 🔣      Alan Chalmers🎨 💻      Jean Didier🎨 💻      Andy🎨 💻      pestadieu🎨 💻              Kanishka Chakraborty🎨 💻      Nandha🎨 💻      Vahid Mafi🎨 💻 🔣 🖋 💼      Akshay Ashok🎨 💻      0x08🎨 💻      Sandeep Mishra🎨 💻      Evann Regnault🎨 💻              Lenny Zeitoun🎨 💻      Eden Boaron🎨 💻      TroyBTC🎨 💻      Aby Sebastian🎨 💻      Matthew Dunn🎨 💻      ckullo🎨 💻 🖋 🔣      Mohamed Mamdouh🎨 💻              Youssef Bazina🎨 💻      Frederico Kückelhaus💻      Nushan Kodikara💻      Zach Cooper💻      Roy🎨 💻      Saurav Panchal🎨 💻      totallynotdavid🎨 💻              goosepirate🎨 💻 💡 💼      KAUTH🎨 💻      Hari Kiran Vusirikala🎨 💻      Sounak Dey🎨 💻      zia💼 🎨 💻      Reza Davari🎨 💻      AkshayAjaykumar🎨 💻              x24870🎨 💻      Ko Phone🎨 💻      Nabstar3🎨 💻      Mateusz🎨 💻      Yunus Emre Emik💻      Abhinav Sinha🎨 💻      Hung Nguyen🎨 💻              Maselino💻      Shuktika Mahanty💻      Mikołaj Gawroński🎨 💻      Hussein Habibi Juybari🎨 💻      Sean-McArthur🎨 💻      Osman F Bayram🎨 💻      Benjamin Thomas Blodgett🎨 💻              Chuanlong-Zang🎨 💻      julian🎨 💻      francisco🎨 💻      aalihhiader9211🎨 💻      Muhammad Zunair🎨 💻      Liya🎨 💻      BegadTarek🎨 💻              etorobot🎨 💻      Hussam Khan🎨 💻      Saikat Chakraborty🎨 💻      Nicholas Quisler🎨 💻      Evang Poul🎨 💻      Gregg Lind🎨 💻      Deepak Kumar🎨 💻              Callum Leslie🎨 💻      Curtis Barnard Jr.🎨 💻      Deepanshukaim🎨 💻      Manthan Ank🎨 💻      hossein varmazyar🎨 💻      Brayan Muñoz V.🎨 💻      Kamil Rasheed Siddiqui💻 🎨              mutt0-ds🎨 💻      egbertjk🎨 💻      Majid Zojaji🎨 💻      Sean Chen🎨 💻      Herbert Milhomme🎨 💻      A3🎨 💻      Killian🎨 💻              Coakeow🎨 💻      ྅༻ Ǭɀħ ༄༆ཉ🎨 💻      Pratik Solanki🎨 💻      Sunny🎨 💻      ssge🎨 💻      Bernat Frangi🎨 💻      Jeevan Rupacha🎨 💻              amirandap🎨 💻      Deepakshi Mittal🎨 💻      Abhijeet Parida🎨 💻      Khaled Riyad🎨 💻      Pratap parui🎨 💻      Prajit Panday🎨 💻      PipeSierra🎨 💻              Collins Oden🎨 💻      Kshitij Dwivedi🎨 💻      Bernardia Vitri Arumsari🎨 💻      Ömer Faruk Taşdemir🎨 💻      Spencer Stith🎨 💻      Porsche Rodjanasak🎨 💻      Shakeel Sharif🎨 💻              Victoria Cheng🎨 💻      Denis🎨 💻      Anand Prakash Tiwari🎨 💻      danijeljw-rpc🎨 💻      Ahmed H Ebrahim🎨 💻      Virginia Gardner🎨 💻      Jhironsel Diaz A.🎨 💻              Yunus Kidem🎨 💻      MT🎨 💻      Dinesh Zaldekar🎨 💻      adi🎨 💻      Farhan Shaikh🎨 💻      Elvis Salvatierra🎨 💻      Kaushik-Iyer🎨 💻              HocAndres🎨 💻      VictorHugoAguilarAguilar🎨 💻      Murat Can Abay🎨 💻      Chris🎨 💻      Shivam7-1🎨 💻      Paipai13🎨 💻      Shambles-io🎨 💻              Abhishek K M🎨 💻      Ezequiel Cuevas🎨 💻      Plamen Ivanov🎨 💻      Yuji🎨 💻      Jean-Philippe Lebœuf🎨 💻 🔣      Naufan🎨 💻      jadnov🎨 💻              vaxtangens🎨 💻      subashkonar13🎨 💻      Rushi Javiya🎨 💻      Mert Gül🎨 💻      Lily🎨 💻      Kalinoff🎨 💻      Joel Tony🎨 💻              Peter🎨 💻      Roozbeh Zarei🎨 💻      Shen🎨 💻      Joonsoo.LEE🎨 💻      Fede.Breg🎨 💻      Rui Costa🎨 💻      João Gustavo Bispo🎨 💻              Sami-I🎨 💻      Tsvetoslav Tsvetkov🎨 💻      Olabode Olaniyi David🎨 💻      theRuslan🎨 💻      leighboz🎨 💻      Frank Sossi🎨 💻      Tomasz Adamski🎨 💻              Mansoor M. Sathir🎨 💻      Golamrabbi Azad🎨 💻      Nahian Ahmed🎨 💻      Rafael de Jesus Silva Monteiro🎨 💻      Odionyebuchukwu Jude🎨 💻      The Nithin Balaji🎨 💻      Knackii🎨 💻              vittorio-giatti🎨 💻      Guilherme de Carvalho Lima Rebouças🎨 💻      aaref shami🎨 💻      Andrey Dryupin🎨 💻      Muhanned Noman🎨 💻      Jan Silva🎨 💻      emanuele-em🎨 💻 🖋              Sanjay TM🎨 💻      Joe Markberg / code editor🎨 💻      Julien Quiaios🎨 💻      Eric Ramirez Santis🎨 💻      M🎨 💻      Malcata🎨 💻      Athul Muralidharan🎨 💻              Dariusz Ochota🎨 💻      CHANDAN CHOUDHURY🎨 💻      Deep🎨 💻      Ahmet İstemihan ÖZTÜRK🎨 💻      TIM🎨 💻      jakeg814🎨 💻      Leonidos🎨 💻              Abhinandu V Nair🎨 💻      charafeddine01🎨 💻      Jasper🎨 💻      Manish Goyal🎨 💻      SATYAM_SINGH🎨 💻      Four🎨 💻      Vaishnavi Amira Yada🎨 💻              ShriKrushna Bhagwat🎨 💻      Rohit Nandagawali🎨 💻      felipe🎨 💻 🚧 🖋 ✅ 🧑‍🏫      Saurabh Mudgal🎨 💻      szenadam🎨 💻      Shubhendra Singh🎨 💻      Yoosuf Sayyid💻 🎨              Güven Çetinerler🎨 💻      Luke Jefferies🎨 💻      Chris🎨 💻      Lúcio Aguiar💻      Enuma029💻      yktsang01💻      maximumn3rd🎨 💻              Jon Galletero🎨 💻      Thaddeus  Thomas🎨 💻      Aakash Kumar💻 🎨      Ali M🎨 💻      OskyEdz🎨 💻      Ravi Gupta🎨 💻      Rafa Raizer🎨 💻              Abdullah Al Muzaki🎨 💻      Rahul Faujdar🎨 💻      Abhishek Verma🎨 💻      Ashutosh Shinde🎨 💻      Ganesh Rai🎨 💻      StefanTrpkovic🎨 💻      Erik Blanca🎨 💻              Vedant Madane🎨 💻      Antra Tripathi🎨 💻      Ethan Knights🎨 💻      Alexandru Boncut🎨 💻      Pablo Bandinopla🎨 💻 🚧 🖋      Robz-99🎨 💻      Harpal Singh🎨 💻              paulboundy99🎨 💻      Mubashir Ahmed🎨 💻      Rohan Hari🎨 💻      Erik Henrique 🎨 💻      Leandro Matheus🎨 💻      Deepak🎨 💻      AlishaSingh🎨 💻              Lynn Latt Yati🎨 💻      San Shwe🎨 💻      SKR🎨 💻      msbunnyjaguar🎨 💻      Mohamad Zabiulla🎨 💻      Hatim Zahid🎨 💻      Rauzan Sumara🎨 💻              Hosein1358🎨 💻      Mohit🎨 💻      Ali🎨 💻      Avinash1765🎨 💻      Sai Teja Madha🎨 💻      Monsur Ahmed Shafiq🎨 💻      xuxianjin-dev🎨 💻              chetna🎨 💻      Gul Zaib🎨 💻      Natalia🎨 💻      Dionísio Braga🎨 💻      Pritish Rajpurohit🎨 💻      incanlove🎨 💻      Innocent🎨 💻              Devin Almonor🎨 💻      antonyveyre🎨 💻      Beltz Anhxton🎨 💻      Mehdi🎨 💻      Muhammad Usman🎨 💻      Patrick Dantas🎨 💻      Tak Vannak🎨 💻              Ramzi RADDAOUI🎨 💻      Konstantin-Glukhov🎨 💻      uguroban🎨 💻      Humberto Alves🎨 💻      JuangZendrato🎨 💻      James Oluwaleye🎨 💻      Wasi Sadman🎨 💻              Pavle Mijatovic🎨 💻      Luiz H. S. Bispo🎨 💻      Сухас Дхолз🎨 💻      Alvaro Trujillo🎨 💻      Everton 🎨 💻      jfrozas🎨 💻      Shuaaib Badran🎨 💻              Shivam Jha🎨 💻      Mohamed Tayeh🎨 💻      Makendran G🎨 💻      mayank singh tomar🎨 💻      hossam sadany🎨 💻      Harshbardhan Singh💻 🎨      Fawad Jawaid Malik🎨 💻              Tina Lacatis🎨 💻      TeddyCuoreDolce🎨 💻      bchooxg🎨 💻      Alisha Takkar🎨 💻      Gianluigi🎨 💻      Mehran Javaherian🎨 💻      Benjamin Ololade Adedokun🎨 💻              Md. Abdul Mutalib🎨 💻      Aadil Arsh.S.R🎨 💻      J. Nathan Allen🎨 💻      Kieran Krug🎨 💻      Seth Addo🎨 💻      Satvik Singh Rathore🎨 💻      dangoth🎨 💻              Maxim🎨 💻      Phuong-Cat Ngo🎨 💻      Frenchtoast0🎨 💻      Rakshith🎨 💻      Vaibhav Arora🎨 💻      zghp🎨 💻      Bedovan🎨 💻              chiaramistro🎨 💻      him2016🎨 💻      HarshitSachdeva🎨 💻      Sadaf Saleem🎨 💻      Aaroh Srivastava🎨 💻      eloygplaza🎨 💻      Gaurav Kumar Verma🎨 💻              AndreaCUS🎨 💻      Simran🎨 💻      Prashant Bhapkar🎨 💻      mhaendler🎨 💻      Gauri Maheshwari🎨 💻      4Lajf🎨 💻      Tanmoy Sengupta🎨 💻              Sharad Tripathi🎨 💻      Niraj Chavan🎨 💻      Luisa Gualda🎨 💻      Monika-Sivakumar-3🎨 💻      harryfensome🎨 💻      Shubham Choubey🎨 💻      Ashwini Patil🎨 💻              cleversonlira🎨 💻      Nurmukhammed🎨 💻      workspace-utkarsh🎨 💻      Santosh Phadtare🎨 💻      Prashant Warghude🎨 💻      Umang Dakh🎨 💻      Shalini Chavan🎨 💻              vinit gurjar🎨 💻      Vishal Kumar🎨 💻      Wonhyeong Seo🎨 💻      Achwale Prajwal Namdevrao🎨 💻      Ankan Banerjee🎨 💻      bhaumikankan🎨 💻      JamesMacroZhang🎨 💻              Pedro Lopes🎨 💻      dia🎨 💻      tayyabhussain2910🎨 💻      Rajdeep Shrivastava 🎨 💻      Mukul Kumar🎨 💻      Mayank N🎨 💻      jdelucca🎨 💻              Sneha Mittal🎨 💻      Sarika Kushwaha🎨 💻      farzad-khb🎨 💻      Elijah Shackelford🎨 💻      The-Only-Raminator🎨 💻      Keerthana Kasthuril🎨 💻      Viachaslau Auchynnikau🎨 💻              Mohammad Osman Rasooli🎨 💻      mvedovato🎨 💻      Sonali Rajput🎨 💻      Isha Dhek🎨 💻      Ramshad Cheriyeri Peediyakkal🎨 💻      Micah🎨 💻      gauravshukla2203🎨 💻              sndmurthy🎨 💻      Shivam-Singh🎨 💻      M. Ammar Khan🎨 💻      chandolakul🎨 💻      bhatnagar221🎨 💻      Adrian Nieściur🎨 💻      nezi311🎨 💻              scottajevans🎨 💻      Marcelo Antunes Soares Fantini🎨 💻      Axel De Acetis🎨 💻      Drishti Sah🎨 💻      VipulDhillon🎨 💻      Urmi Jana🎨 💻      Ayush Mokal🎨 💻              Damola Olutoke🎨 💻      Max🎨 💻      Lakshmi N🎨 💻      ArtemReva🎨 💻      Ujjwal Aggarwal🎨 💻      Mo🎨 💻      Brian🎨 💻              chamley🎨 💻      Simone Baptiste🎨 💻      Shekhar Thakur🎨 💻      Smith🎨 💻      codernoob1🎨 💻      lok84🎨 💻      Tobias Riemenschneider🎨 💻              Tharsanan1🎨 💻      ANURAG SINGH🎨 💻      Yash Sant🎨 💻      Krishiv Patel🎨 💻      GGGalaxy🎨 💻      pardeepdhillon661🎨 💻      anujd64🎨 💻              Pedro Pereira🎨 💻      Master_Saptak🎨 💻      SURANJAN DAS🎨 💻      Tripura kant🎨 💻      shabzkhan🎨 💻      Mustafa Poya🎨 💻      Roshan Jha🎨 💻              GuillaumeLarue🎨 💻      Tomasz Rodak🎨 💻      Junil Kim🎨 💻      Surbhi Mayank🎨 💻      Nemanja Lekic🎨 💻      HemantMalokar🎨 💻      Felipe M. López🎨 💻              bibliofilo🎨 💻      GauthamG2🎨 💻      02_t🎨 💻      Yusuf Abdul-razaq🎨 💻      Vladimir🎨 💻      Sai Chandra K🎨 💻      Soroush Bonab🎨 💻              Giide0n🎨 💻      GG🎨 💻      Dáger Zúñiga🎨 💻      rsk2🎨 💻      Storozhev DJ🎨 💻      Jeevan🎨 💻      Andy Johnson🎨 💻              Aníbal Pozo🎨 💻      Jovane de Castro🎨 💻      Muhammad Hamza Amir🎨 💻      tharaka-mts🎨 💻      Ali KHYAR🎨 💻      Caio Araujo🎨 💻      Oscar Dyremyhr🎨 💻              arteality🎨 💻      Daniel Drexlmaier🎨 💻      Marco Monti🎨 💻      mikeycrystal🎨 💻      Veljanovskii🎨 💻      Ivan Gorbachev🎨 💻      Sahil Rawat🎨 💻              Hasitha Suneth🎨 💻      Yerko Vera Lezama🎨 💻      Ivan Penchev🎨 💻      Tanver Islam Tonmoy🎨 💻      Xun Cao🎨 💻      Nayan Babariya🎨 💻      Priyanshu Maurya🎨 💻              Dylan Tintenfich🎨 💻      Ron Strauss🎨 💻      Mohammed AlBanna🎨 💻      Mukund M🎨 💻      Franklin Ohaegbulam🎨 💻      Nisarg Shah🎨 💻      Unik Dahal🎨 💻              Readily🎨 💻      Alexandre Poitevin🎨 💻      Scaramir🎨 💻      Pruthvi🎨 💻      Kalmanq🎨 💻      Alfatah Nesab🎨 💻      arudesai🎨 💻              Adryenne🎨 💻      El mehdi oudaoud🎨 💻      Jayant Goel🎨 💻      Tsuki🎨 💻      Peter Lemanski🎨 💻      Annurag-byte🎨 💻      Anthony Vu🎨 💻              Vitaly Nikolaychuk🎨 💻      Nathan🎨 💻      Evgenii Petukhov🎨 💻      Loris Guerra🎨 💻      fakhriaunur🎨 💻      Mehdi HYANI🎨 💻      Sarvex Jatasra🎨 💻              santimanuelr🎨 💻      Evgeniy Rezanov🎨 💻      Sonia M🎨 💻      Grzegorz Kmita🎨 💻      Manuel Carita🎨 💻      Felipe Cisternas Alvarez🎨 💻      Guo Ci🎨 💻              Marcos Silva🎨 💻      KK🎨 💻      Shubhanjan Medhi🎨 💻      ArthurFerreiraRodrigues🎨 💻      PabloHermun🎨 💻      disha-baldawa🎨 💻      StaroMoon🎨 💻              Amila T Kumarasekara🎨 💻      Amoh Prince🎨 💻      AngeloGC🎨 💻      Ebube Glory Ogbonda🎨 💻      Prahalad Belavadi📖      Antoni Sarnowski-Trypka🎨 💻      Alberto Pasqualetto🎨 💻              Amir Babaei🎨 💻      Syed Abdul Hannan🎨 💻      Srajan Rai🎨 💻      Clarence Moore🎨 💻      Nguyen Anh Tuan🎨 💻      dar2dar2🎨 💻      Ameer Ibrahim🎨 💻              Tiago Lugatto🎨 💻      raremiroir🎨 💻      Moobie🎨 💻      AlicanDursun🎨 💻      bbalsam🎨 💻      Luboš Hájek🎨 💻      mrshahzeb7🎨 💻              Wesley Scholl🎨 💻      Lawrence Turcotte🎨 💻      Michael DiPaolo🎨 💻      Smart-Codi🎨 💻      Vivek Kumar🎨 💻      Igor Moiseev🎨 💻      Bård Pedersen🎨 💻              HOA PHAN🎨 💻      GaborModra🎨 💻      vivek-114🎨 💻      Robin🎨 💻      Alex🎨 💻      John Ehrlinger🎨 💻      Roman Zhuravlov🎨 💻              Jordan Moss🎨 💻      RaeShelly🎨 💻      gmollard🎨 💻      Md Kaif Khan🎨 💻      Pablo Romera🎨 💻      Erik Bustos🎨 💻      trogfield🎨 💻              simon-aichhorn🎨 💻      Tufan GÜLEÇ🎨 💻      Uğur Berkecan Ünlü🎨 💻      Revanth Naik🎨 💻      Lia Pires🎨 💻      Igor Mestechkin🎨 💻      Anirudh Karanth🎨 💻              KBobovskiy🎨 💻      zhatiayua🎨 💻 🖋      David Cardona🎨 💻      Paulo Castilho🎨 💻      Sebastiano Picchi🎨 💻      pjotar🎨 💻      Rimel CHERIF💻              Arsal uddin🖋      Dmitry Kasporsky💻      SoftwareDev1014🎨 💻      @Robvred🎨 💻      Kasun Shanaka💻      Ahmad M.🎨 💻      Alex Kozin🎨 💻              Mandy Meindersma🎨 💻      LEGALISE PIRACY🎨 💻      Alex Logvin🎨 💻      Aria Dahl🎨 💻      Mustafa Arifoglu🎨 💻      Yevhen Leshchenko🎨 💻      Anubhav Adhikari🎨 💻              Noah Tatko🎨 💻      Mohit Gadhavi🎨 💻      Pedro Basílio🎨 💻      RealSanjeev🎨 💻      Akash Hazra🎨 💻      Christoph Dahlen🎨 💻      Vincent du Plessis🎨 💻              Karen Tamrazyan🎨 💻      Mirza Younus Baig🎨 💻      Ashish Kumar🎨 💻      Unknown6334🎨 💻      flowaz🎨 💻      zi-aikra🎨 💻      PAYAL PM🎨 💻              Lennart Lösche🎨 💻      Yummy-Yums🎨 💻      Njuacha Hubert Mikulowski🎨 💻      Hussein Esmail🎨 💻      Bilgehan Bezir🎨 💻      Muhammed Shittu🎨 💻      Clément FERNANDES🎨 💻              JaCKoP619🎨 💻      userutf8🎨 💻      Mohamed Ubaid🎨 💻      Justin Yates🎨 💻      mohammad ali🎨 💻      Madhav Singh🎨 💻      RgbMouse69🎨 💻              Nicholas Leask🎨 💻      parthav0🎨 💻      Sigma🎨 💻      Evelina Becheva🎨 💻      Akshit Gulyan🎨 💻      Arpita Jana🎨 💻      Praveen Kumar🎨 💻              Mohammad Sami🎨 💻      eddiestefanescu🎨 💻      Ramesh Yadav🎨 💻      Sarthak Joshi🎨 💻      Nikhil12300🎨 💻      Yevgen🎨 💻      Leo🎨 💻              laurent b🎨 💻      Mettchen🎨 💻      Ali Mahdavi🎨 💻      Lucas Dondo🎨 💻      Siddhesh Agarwal🎨 💻      slimerPuncher🎨 💻      saritashh🎨 💻              Iulian-Valeriu Cioată🎨 💻      Szabolcs Nagy🎨 💻      Jarle Kvile🎨 💻      劉耀升 Vic Liu🎨 💻      Suryansh🎨 💻      Matthew Oosthuyse🎨 💻      Florin Zamfir🎨 💻              Melek🎨 💻      moesocio🎨 💻      Alan James🎨 💻      Mai Thanh Phương🎨 💻      Neville Dabre🎨 💻      Maksym🎨 💻      tamanna900🎨 💻              Adithya Awati🎨 💻      This project follows the all-contributors specification.Contributions of any kind welcome![ Go back to the top of the page ]Contributor Over TimeStargazers over timeVisualisation of this repository by Gourcehttps://www.youtube.com/watch?v=24cZVytc5D4"
55,matterport/Mask_RCNN,https://github.com/matterport/Mask_RCNN/blob/master/README.md,Python,"Mask R-CNN for Object Detection and SegmentationThis is an implementation of Mask R-CNN on Python 3, Keras, and TensorFlow. The model generates bounding boxes and segmentation masks for each instance of an object in the image. It's based on Feature Pyramid Network (FPN) and a ResNet101 backbone.The repository includes:Source code of Mask R-CNN built on FPN and ResNet101.Training code for MS COCOPre-trained weights for MS COCOJupyter notebooks to visualize the detection pipeline at every stepParallelModel class for multi-GPU trainingEvaluation on MS COCO metrics (AP)Example of training on your own datasetThe code is documented and designed to be easy to extend. If you use it in your research, please consider citing this repository (bibtex below). If you work on 3D vision, you might find our recently released Matterport3D dataset useful as well.This dataset was created from 3D-reconstructed spaces captured by our customers who agreed to make them publicly available for academic use. You can see more examples here.Getting Starteddemo.ipynb Is the easiest way to start. It shows an example of using a model pre-trained on MS COCO to segment objects in your own images.It includes code to run object detection and instance segmentation on arbitrary images.train_shapes.ipynb shows how to train Mask R-CNN on your own dataset. This notebook introduces a toy dataset (Shapes) to demonstrate training on a new dataset.(model.py, utils.py, config.py): These files contain the main Mask RCNN implementation.inspect_data.ipynb. This notebook visualizes the different pre-processing stepsto prepare the training data.inspect_model.ipynb This notebook goes in depth into the steps performed to detect and segment objects. It provides visualizations of every step of the pipeline.inspect_weights.ipynbThis notebooks inspects the weights of a trained model and looks for anomalies and odd patterns.Step by Step DetectionTo help with debugging and understanding the model, there are 3 notebooks(inspect_data.ipynb, inspect_model.ipynb,inspect_weights.ipynb) that provide a lot of visualizations and allow running the model step by step to inspect the output at each point. Here are a few examples:1. Anchor sorting and filteringVisualizes every step of the first stage Region Proposal Network and displays positive and negative anchors along with anchor box refinement.2. Bounding Box RefinementThis is an example of final detection boxes (dotted lines) and the refinement applied to them (solid lines) in the second stage.3. Mask GenerationExamples of generated masks. These then get scaled and placed on the image in the right location.4.Layer activationsOften it's useful to inspect the activations at different layers to look for signs of trouble (all zeros or random noise).5. Weight HistogramsAnother useful debugging tool is to inspect the weight histograms. These are included in the inspect_weights.ipynb notebook.6. Logging to TensorBoardTensorBoard is another great debugging and visualization tool. The model is configured to log losses and save weights at the end of every epoch.6. Composing the different pieces into a final resultTraining on MS COCOWe're providing pre-trained weights for MS COCO to make it easier to start. You canuse those weights as a starting point to train your own variation on the network.Training and evaluation code is in samples/coco/coco.py. You can import thismodule in Jupyter notebook (see the provided notebooks for examples) or youcan run it directly from the command line as such:# Train a new model starting from pre-trained COCO weightspython3 samples/coco/coco.py train --dataset=/path/to/coco/ --model=coco# Train a new model starting from ImageNet weightspython3 samples/coco/coco.py train --dataset=/path/to/coco/ --model=imagenet# Continue training a model that you had trained earlierpython3 samples/coco/coco.py train --dataset=/path/to/coco/ --model=/path/to/weights.h5# Continue training the last model you trained. This will find# the last trained weights in the model directory.python3 samples/coco/coco.py train --dataset=/path/to/coco/ --model=lastYou can also run the COCO evaluation code with:# Run COCO evaluation on the last trained modelpython3 samples/coco/coco.py evaluate --dataset=/path/to/coco/ --model=lastThe training schedule, learning rate, and other parameters should be set in samples/coco/coco.py.Training on Your Own DatasetStart by reading this blog post about the balloon color splash sample. It covers the process starting from annotating images to training to using the results in a sample application.In summary, to train the model on your own dataset you'll need to extend two classes:ConfigThis class contains the default configuration. Subclass it and modify the attributes you need to change.DatasetThis class provides a consistent way to work with any dataset.It allows you to use new datasets for training without having to changethe code of the model. It also supports loading multiple datasets at thesame time, which is useful if the objects you want to detect are notall available in one dataset.See examples in samples/shapes/train_shapes.ipynb, samples/coco/coco.py, samples/balloon/balloon.py, and samples/nucleus/nucleus.py.Differences from the Official PaperThis implementation follows the Mask RCNN paper for the most part, but there are a few cases where we deviated in favor of code simplicity and generalization. These are some of the differences we're aware of. If you encounter other differences, please do let us know.Image Resizing: To support training multiple images per batch we resize all images to the same size. For example, 1024x1024px on MS COCO. We preserve the aspect ratio, so if an image is not square we pad it with zeros. In the paper the resizing is done such that the smallest side is 800px and the largest is trimmed at 1000px.Bounding Boxes: Some datasets provide bounding boxes and some provide masks only. To support training on multiple datasets we opted to ignore the bounding boxes that come with the dataset and generate them on the fly instead. We pick the smallest box that encapsulates all the pixels of the mask as the bounding box. This simplifies the implementation and also makes it easy to apply image augmentations that would otherwise be harder to apply to bounding boxes, such as image rotation.To validate this approach, we compared our computed bounding boxes to those provided by the COCO dataset.We found that ~2% of bounding boxes differed by 1px or more, ~0.05% differed by 5px or more,and only 0.01% differed by 10px or more.Learning Rate: The paper uses a learning rate of 0.02, but we found that to betoo high, and often causes the weights to explode, especially when using a small batchsize. It might be related to differences between how Caffe and TensorFlow computegradients (sum vs mean across batches and GPUs). Or, maybe the official model uses gradientclipping to avoid this issue. We do use gradient clipping, but don't set it too aggressively.We found that smaller learning rates converge faster anyway so we go with that.CitationUse this bibtex to cite this repository:@misc{matterport_maskrcnn_2017,  title={Mask R-CNN for object detection and instance segmentation on Keras and TensorFlow},  author={Waleed Abdulla},  year={2017},  publisher={Github},  journal={GitHub repository},  howpublished={\\url{https://github.com/matterport/Mask_RCNN}},}ContributingContributions to this repository are welcome. Examples of things you can contribute:Speed Improvements. Like re-writing some Python code in TensorFlow or Cython.Training on other datasets.Accuracy Improvements.Visualizations and examples.You can also join our team and help us build even more projects like this one.RequirementsPython 3.4, TensorFlow 1.3, Keras 2.0.8 and other common packages listed in requirements.txt.MS COCO Requirements:To train or test on MS COCO, you'll also need:pycocotools (installation instructions below)MS COCO DatasetDownload the 5K minivaland the 35K validation-minus-minivalsubsets. More details in the original Faster R-CNN implementation.If you use Docker, the code has been verified to work onthis Docker container.InstallationClone this repositoryInstall dependenciespip3 install -r requirements.txtRun setup from the repository root directorypython3 setup.py installDownload pre-trained COCO weights (mask_rcnn_coco.h5) from the releases page.(Optional) To train or test on MS COCO install pycocotools from one of these repos. They are forks of the original pycocotools with fixes for Python3 and Windows (the official repo doesn't seem to be active anymore).Linux: https://github.com/waleedka/cocoWindows: https://github.com/philferriere/cocoapi.You must have the Visual C++ 2015 build tools on your path (see the repo for additional details)Projects Using this ModelIf you extend this model to other datasets or build projects that use it, we'd love to hear from you.4K Video Demo by Karol Majek.Images to OSM: Improve OpenStreetMap by adding baseball, soccer, tennis, football, and basketball fields.Splash of Color. A blog post explaining how to train this model from scratch and use it to implement a color splash effect.Segmenting Nuclei in Microscopy Images. Built for the 2018 Data Science BowlCode is in the samples/nucleus directory.Detection and Segmentation for Surgery Robots by the NUS Control & Mechatronics Lab.Reconstructing 3D buildings from aerial LiDARA proof of concept project by Esri, in collaboration with Nvidia and Miami-Dade County. Along with a great write up and code by Dmitry Kudinov, Daniel Hedges, and Omar Maher.Usiigaci: Label-free Cell Tracking in Phase Contrast MicroscopyA project from Japan to automatically track cells in a microfluidics platform. Paper is pending, but the source code is released. Characterization of Arctic Ice-Wedge Polygons in Very High Spatial Resolution Aerial ImageryResearch project to understand the complex processes between degradations in the Arctic and climate change. By Weixing Zhang, Chandi Witharana, Anna Liljedahl, and Mikhail Kanevskiy.Mask-RCNN ShinyA computer vision class project by HU Shiyu to apply the color pop effect on people with beautiful results.Mapping Challenge: Convert satellite imagery to maps for use by humanitarian organisations.GRASS GIS Addon to generate vector masks from geospatial imagery. Based on a Master's thesis by Ondřej Pešek."
56,hankcs/HanLP,https://github.com/hankcs/HanLP/blob/doc-zh/README.md,Python,"HanLP: Han Language Processing                                                                                   English |    日本語 |    文档 |    论文 |    论坛 |    docker |    ▶️在线运行面向生产环境的多语种自然语言处理工具包，基于PyTorch和TensorFlow 2.x双引擎，目标是普及落地最前沿的NLP技术。HanLP具备功能完善、精度准确、性能高效、语料时新、架构清晰、可自定义的特点。借助世界上最大的多语种语料库，HanLP2.1支持包括简繁中英日俄法德在内的130种语言上的10种联合任务以及多种单任务。HanLP预训练了十几种任务上的数十个模型并且正在持续迭代语料库与模型：功能RESTful多任务单任务模型标注标准分词教程教程教程tok粗分、细分词性标注教程教程教程posCTB、PKU、863命名实体识别教程教程教程nerPKU、MSRA、OntoNotes依存句法分析教程教程教程depSD、UD、PMT成分句法分析教程教程教程conChinese Tree Bank语义依存分析教程教程教程sdpCSDP语义角色标注教程教程教程srlChinese Proposition Bank抽象意义表示教程暂无教程amrCAMR指代消解教程暂无暂无暂无OntoNotes语义文本相似度教程暂无教程sts暂无文本风格转换教程暂无暂无暂无暂无关键词短语提取教程暂无暂无暂无暂无抽取式自动摘要教程暂无暂无暂无暂无生成式自动摘要教程暂无暂无暂无暂无文本语法纠错教程暂无暂无暂无暂无文本分类教程暂无暂无暂无暂无情感分析教程暂无暂无暂无[-1,+1]语种检测教程暂无教程暂无ISO 639-1编码词干提取、词法语法特征提取请参考英文教程；词向量和完形填空请参考相应文档。简繁转换、拼音、新词发现、文本聚类请参考1.x教程。量体裁衣，HanLP提供RESTful和native两种API，分别面向轻量级和海量级两种场景。无论何种API何种语言，HanLP接口在语义上保持一致，在代码上坚持开源。如果您在研究中使用了HanLP，请引用我们的EMNLP论文。轻量级RESTful API仅数KB，适合敏捷开发、移动APP等场景。简单易用，无需GPU配环境，秒速安装。语料更多、模型更大、精度更高，强烈推荐。服务器GPU算力有限，匿名用户配额较少，建议申请免费公益API秘钥auth。Pythonpip install hanlp_restful创建客户端，填入服务器地址和秘钥：from hanlp_restful import HanLPClientHanLP = HanLPClient('https://www.hanlp.com/api', auth=None, language='zh') # auth不填则匿名，zh中文，mul多语种Golang安装 go get -u github.com/hankcs/gohanlp@main ，创建客户端，填入服务器地址和秘钥：HanLP := hanlp.HanLPClient(hanlp.WithAuth(\""\""),hanlp.WithLanguage(\""zh\"")) // auth不填则匿名，zh中文，mul多语种Java在pom.xml中添加依赖：<dependency>    <groupId>com.hankcs.hanlp.restful</groupId>    <artifactId>hanlp-restful</artifactId>    <version>0.0.12</version></dependency>创建客户端，填入服务器地址和秘钥：HanLPClient HanLP = new HanLPClient(\""https://www.hanlp.com/api\"", null, \""zh\""); // auth不填则匿名，zh中文，mul多语种快速上手无论何种开发语言，调用parse接口，传入一篇文章，得到HanLP精准的分析结果。HanLP.parse(\""2021年HanLPv2.1为生产环境带来次世代最先进的多语种NLP技术。阿婆主来到北京立方庭参观自然语义科技公司。\"")更多功能包括语义相似度、风格转换、指代消解等，请参考文档和测试用例。海量级native API依赖PyTorch、TensorFlow等深度学习技术，适合专业NLP工程师、研究者以及本地海量数据场景。要求Python 3.6至3.10，支持Windows，推荐*nix。可以在CPU上运行，推荐GPU/TPU。安装PyTorch版：pip install hanlpHanLP每次发布都通过了Linux、macOS和Windows上Python3.6至3.10的单元测试，不存在安装问题。HanLP发布的模型分为多任务和单任务两种，多任务速度快省显存，单任务精度高更灵活。多任务模型HanLP的工作流程为加载模型然后将其当作函数调用，例如下列联合多任务模型：import hanlpHanLP = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH) # 世界最大中文语料库HanLP(['2021年HanLPv2.1为生产环境带来次世代最先进的多语种NLP技术。', '阿婆主来到北京立方庭参观自然语义科技公司。'])Native API的输入单位为句子，需使用多语种分句模型或基于规则的分句函数先行分句。RESTful和native两种API的语义设计完全一致，用户可以无缝互换。简洁的接口也支持灵活的参数，常用的技巧有：灵活的tasks任务调度，任务越少，速度越快，详见教程。在内存有限的场景下，用户还可以删除不需要的任务达到模型瘦身的效果。高效的trie树自定义词典，以及强制、合并、校正3种规则，请参考demo和文档。规则系统的效果将无缝应用到后续统计模型，从而快速适应新领域。单任务模型根据我们的最新研究，多任务学习的优势在于速度和显存，然而精度往往不如单任务模型。所以，HanLP预训练了许多单任务模型并设计了优雅的流水线模式将其组装起来。import hanlpHanLP = hanlp.pipeline() \\    .append(hanlp.utils.rules.split_sentence, output_key='sentences') \\    .append(hanlp.load('FINE_ELECTRA_SMALL_ZH'), output_key='tok') \\    .append(hanlp.load('CTB9_POS_ELECTRA_SMALL'), output_key='pos') \\    .append(hanlp.load('MSRA_NER_ELECTRA_SMALL_ZH'), output_key='ner', input_key='tok') \\    .append(hanlp.load('CTB9_DEP_ELECTRA_SMALL', conll=0), output_key='dep', input_key='tok')\\    .append(hanlp.load('CTB9_CON_ELECTRA_SMALL'), output_key='con', input_key='tok')HanLP('2021年HanLPv2.1为生产环境带来次世代最先进的多语种NLP技术。阿婆主来到北京立方庭参观自然语义科技公司。')更多功能，请参考demo和文档了解更多模型与用法。输出格式无论何种API何种开发语言何种自然语言，HanLP的输出统一为json格式兼容dict的Document:{  \""tok/fine\"": [    [\""2021年\"", \""HanLPv2.1\"", \""为\"", \""生产\"", \""环境\"", \""带来\"", \""次\"", \""世代\"", \""最\"", \""先进\"", \""的\"", \""多\"", \""语种\"", \""NLP\"", \""技术\"", \""。\""],    [\""阿婆主\"", \""来到\"", \""北京\"", \""立方庭\"", \""参观\"", \""自然\"", \""语义\"", \""科技\"", \""公司\"", \""。\""]  ],  \""tok/coarse\"": [    [\""2021年\"", \""HanLPv2.1\"", \""为\"", \""生产\"", \""环境\"", \""带来\"", \""次世代\"", \""最\"", \""先进\"", \""的\"", \""多语种\"", \""NLP\"", \""技术\"", \""。\""],    [\""阿婆主\"", \""来到\"", \""北京立方庭\"", \""参观\"", \""自然语义科技公司\"", \""。\""]  ],  \""pos/ctb\"": [    [\""NT\"", \""NR\"", \""P\"", \""NN\"", \""NN\"", \""VV\"", \""JJ\"", \""NN\"", \""AD\"", \""JJ\"", \""DEG\"", \""CD\"", \""NN\"", \""NR\"", \""NN\"", \""PU\""],    [\""NN\"", \""VV\"", \""NR\"", \""NR\"", \""VV\"", \""NN\"", \""NN\"", \""NN\"", \""NN\"", \""PU\""]  ],  \""pos/pku\"": [    [\""t\"", \""nx\"", \""p\"", \""vn\"", \""n\"", \""v\"", \""b\"", \""n\"", \""d\"", \""a\"", \""u\"", \""a\"", \""n\"", \""nx\"", \""n\"", \""w\""],    [\""n\"", \""v\"", \""ns\"", \""ns\"", \""v\"", \""n\"", \""n\"", \""n\"", \""n\"", \""w\""]  ],  \""pos/863\"": [    [\""nt\"", \""w\"", \""p\"", \""v\"", \""n\"", \""v\"", \""a\"", \""nt\"", \""d\"", \""a\"", \""u\"", \""a\"", \""n\"", \""ws\"", \""n\"", \""w\""],    [\""n\"", \""v\"", \""ns\"", \""n\"", \""v\"", \""n\"", \""n\"", \""n\"", \""n\"", \""w\""]  ],  \""ner/pku\"": [    [],    [[\""北京立方庭\"", \""ns\"", 2, 4], [\""自然语义科技公司\"", \""nt\"", 5, 9]]  ],  \""ner/msra\"": [    [[\""2021年\"", \""DATE\"", 0, 1], [\""HanLPv2.1\"", \""ORGANIZATION\"", 1, 2]],    [[\""北京\"", \""LOCATION\"", 2, 3], [\""立方庭\"", \""LOCATION\"", 3, 4], [\""自然语义科技公司\"", \""ORGANIZATION\"", 5, 9]]  ],  \""ner/ontonotes\"": [    [[\""2021年\"", \""DATE\"", 0, 1], [\""HanLPv2.1\"", \""ORG\"", 1, 2]],    [[\""北京立方庭\"", \""FAC\"", 2, 4], [\""自然语义科技公司\"", \""ORG\"", 5, 9]]  ],  \""srl\"": [    [[[\""2021年\"", \""ARGM-TMP\"", 0, 1], [\""HanLPv2.1\"", \""ARG0\"", 1, 2], [\""为生产环境\"", \""ARG2\"", 2, 5], [\""带来\"", \""PRED\"", 5, 6], [\""次世代最先进的多语种NLP技术\"", \""ARG1\"", 6, 15]], [[\""最\"", \""ARGM-ADV\"", 8, 9], [\""先进\"", \""PRED\"", 9, 10], [\""技术\"", \""ARG0\"", 14, 15]]],    [[[\""阿婆主\"", \""ARG0\"", 0, 1], [\""来到\"", \""PRED\"", 1, 2], [\""北京立方庭\"", \""ARG1\"", 2, 4]], [[\""阿婆主\"", \""ARG0\"", 0, 1], [\""参观\"", \""PRED\"", 4, 5], [\""自然语义科技公司\"", \""ARG1\"", 5, 9]]]  ],  \""dep\"": [    [[6, \""tmod\""], [6, \""nsubj\""], [6, \""prep\""], [5, \""nn\""], [3, \""pobj\""], [0, \""root\""], [8, \""amod\""], [15, \""nn\""], [10, \""advmod\""], [15, \""rcmod\""], [10, \""assm\""], [13, \""nummod\""], [15, \""nn\""], [15, \""nn\""], [6, \""dobj\""], [6, \""punct\""]],    [[2, \""nsubj\""], [0, \""root\""], [4, \""nn\""], [2, \""dobj\""], [2, \""conj\""], [9, \""nn\""], [9, \""nn\""], [9, \""nn\""], [5, \""dobj\""], [2, \""punct\""]]  ],  \""sdp\"": [    [[[6, \""Time\""]], [[6, \""Exp\""]], [[5, \""mPrep\""]], [[5, \""Desc\""]], [[6, \""Datv\""]], [[13, \""dDesc\""]], [[0, \""Root\""], [8, \""Desc\""], [13, \""Desc\""]], [[15, \""Time\""]], [[10, \""mDegr\""]], [[15, \""Desc\""]], [[10, \""mAux\""]], [[8, \""Quan\""], [13, \""Quan\""]], [[15, \""Desc\""]], [[15, \""Nmod\""]], [[6, \""Pat\""]], [[6, \""mPunc\""]]],    [[[2, \""Agt\""], [5, \""Agt\""]], [[0, \""Root\""]], [[4, \""Loc\""]], [[2, \""Lfin\""]], [[2, \""ePurp\""]], [[8, \""Nmod\""]], [[9, \""Nmod\""]], [[9, \""Nmod\""]], [[5, \""Datv\""]], [[5, \""mPunc\""]]]  ],  \""con\"": [    [\""TOP\"", [[\""IP\"", [[\""NP\"", [[\""NT\"", [\""2021年\""]]]], [\""NP\"", [[\""NR\"", [\""HanLPv2.1\""]]]], [\""VP\"", [[\""PP\"", [[\""P\"", [\""为\""]], [\""NP\"", [[\""NN\"", [\""生产\""]], [\""NN\"", [\""环境\""]]]]]], [\""VP\"", [[\""VV\"", [\""带来\""]], [\""NP\"", [[\""ADJP\"", [[\""NP\"", [[\""ADJP\"", [[\""JJ\"", [\""次\""]]]], [\""NP\"", [[\""NN\"", [\""世代\""]]]]]], [\""ADVP\"", [[\""AD\"", [\""最\""]]]], [\""VP\"", [[\""JJ\"", [\""先进\""]]]]]], [\""DEG\"", [\""的\""]], [\""NP\"", [[\""QP\"", [[\""CD\"", [\""多\""]]]], [\""NP\"", [[\""NN\"", [\""语种\""]]]]]], [\""NP\"", [[\""NR\"", [\""NLP\""]], [\""NN\"", [\""技术\""]]]]]]]]]], [\""PU\"", [\""。\""]]]]]],    [\""TOP\"", [[\""IP\"", [[\""NP\"", [[\""NN\"", [\""阿婆主\""]]]], [\""VP\"", [[\""VP\"", [[\""VV\"", [\""来到\""]], [\""NP\"", [[\""NR\"", [\""北京\""]], [\""NR\"", [\""立方庭\""]]]]]], [\""VP\"", [[\""VV\"", [\""参观\""]], [\""NP\"", [[\""NN\"", [\""自然\""]], [\""NN\"", [\""语义\""]], [\""NN\"", [\""科技\""]], [\""NN\"", [\""公司\""]]]]]]]], [\""PU\"", [\""。\""]]]]]]  ]}特别地，Python RESTful和native API支持基于等宽字体的可视化，能够直接将语言学结构在控制台内可视化出来：HanLP(['2021年HanLPv2.1为生产环境带来次世代最先进的多语种NLP技术。', '阿婆主来到北京立方庭参观自然语义科技公司。']).pretty_print()Dep Tree    \tToken    \tRelati\tPoS\tTok      \tNER Type        \tTok      \tSRL PA1     \tTok      \tSRL PA2     \tTok      \tPoS    3       4       5       6       7       8       9 ────────────\t─────────\t──────\t───\t─────────\t────────────────\t─────────\t────────────\t─────────\t────────────\t─────────\t───────────────────────────────────────────────────────── ┌─────────►\t2021年    \ttmod  \tNT \t2021年    \t───►DATE        \t2021年    \t───►ARGM-TMP\t2021年    \t            \t2021年    \tNT ───────────────────────────────────────────►NP ───┐    │┌────────►\tHanLPv2.1\tnsubj \tNR \tHanLPv2.1\t───►ORGANIZATION\tHanLPv2.1\t───►ARG0    \tHanLPv2.1\t            \tHanLPv2.1\tNR ───────────────────────────────────────────►NP────┤    ││┌─►┌─────\t为        \tprep  \tP  \t为        \t                \t为        \t◄─┐         \t为        \t            \t为        \tP ───────────┐                                       │    │││  │  ┌─►\t生产       \tnn    \tNN \t生产       \t                \t生产       \t  ├►ARG2    \t生产       \t            \t生产       \tNN ──┐       ├────────────────────────►PP ───┐       │    │││  └─►└──\t环境       \tpobj  \tNN \t环境       \t                \t环境       \t◄─┘         \t环境       \t            \t环境       \tNN ──┴►NP ───┘                               │       │   ┌┼┴┴────────\t带来       \troot  \tVV \t带来       \t                \t带来       \t╟──►PRED    \t带来       \t            \t带来       \tVV ──────────────────────────────────┐       │       │   ││       ┌─►\t次        \tamod  \tJJ \t次        \t                \t次        \t◄─┐         \t次        \t            \t次        \tJJ ───►ADJP──┐                       │       ├►VP────┤   ││  ┌───►└──\t世代       \tnn    \tNN \t世代       \t                \t世代       \t  │         \t世代       \t            \t世代       \tNN ───►NP ───┴►NP ───┐               │       │       │   ││  │    ┌─►\t最        \tadvmod\tAD \t最        \t                \t最        \t  │         \t最        \t───►ARGM-ADV\t最        \tAD ───────────►ADVP──┼►ADJP──┐       ├►VP ───┘       ├►IP││  │┌──►├──\t先进       \trcmod \tJJ \t先进       \t                \t先进       \t  │         \t先进       \t╟──►PRED    \t先进       \tJJ ───────────►VP ───┘       │       │               │   ││  ││   └─►\t的        \tassm  \tDEG\t的        \t                \t的        \t  ├►ARG1    \t的        \t            \t的        \tDEG──────────────────────────┤       │               │   ││  ││   ┌─►\t多        \tnummod\tCD \t多        \t                \t多        \t  │         \t多        \t            \t多        \tCD ───►QP ───┐               ├►NP ───┘               │   ││  ││┌─►└──\t语种       \tnn    \tNN \t语种       \t                \t语种       \t  │         \t语种       \t            \t语种       \tNN ───►NP ───┴────────►NP────┤                       │   ││  │││  ┌─►\tNLP      \tnn    \tNR \tNLP      \t                \tNLP      \t  │         \tNLP      \t            \tNLP      \tNR ──┐                       │                       │   │└─►└┴┴──┴──\t技术       \tdobj  \tNN \t技术       \t                \t技术       \t◄─┘         \t技术       \t───►ARG0    \t技术       \tNN ──┴────────────────►NP ───┘                       │   └──────────►\t。        \tpunct \tPU \t。        \t                \t。        \t            \t。        \t            \t。        \tPU ──────────────────────────────────────────────────┘   Dep Tree    \tTok\tRelat\tPo\tTok\tNER Type        \tTok\tSRL PA1 \tTok\tSRL PA2 \tTok\tPo    3       4       5       6 ────────────\t───\t─────\t──\t───\t────────────────\t───\t────────\t───\t────────\t───\t────────────────────────────────         ┌─►\t阿婆主\tnsubj\tNN\t阿婆主\t                \t阿婆主\t───►ARG0\t阿婆主\t───►ARG0\t阿婆主\tNN───────────────────►NP ───┐   ┌┬────┬──┴──\t来到 \troot \tVV\t来到 \t                \t来到 \t╟──►PRED\t来到 \t        \t来到 \tVV──────────┐               │   ││    │  ┌─►\t北京 \tnn   \tNR\t北京 \t───►LOCATION    \t北京 \t◄─┐     \t北京 \t        \t北京 \tNR──┐       ├►VP ───┐       │   ││    └─►└──\t立方庭\tdobj \tNR\t立方庭\t───►LOCATION    \t立方庭\t◄─┴►ARG1\t立方庭\t        \t立方庭\tNR──┴►NP ───┘       │       │   │└─►┌───────\t参观 \tconj \tVV\t参观 \t                \t参观 \t        \t参观 \t╟──►PRED\t参观 \tVV──────────┐       ├►VP────┤   │   │  ┌───►\t自然 \tnn   \tNN\t自然 \t◄─┐             \t自然 \t        \t自然 \t◄─┐     \t自然 \tNN──┐       │       │       ├►IP│   │  │┌──►\t语义 \tnn   \tNN\t语义 \t  │             \t语义 \t        \t语义 \t  │     \t语义 \tNN  │       ├►VP ───┘       │   │   │  ││┌─►\t科技 \tnn   \tNN\t科技 \t  ├►ORGANIZATION\t科技 \t        \t科技 \t  ├►ARG1\t科技 \tNN  ├►NP ───┘               │   │   └─►└┴┴──\t公司 \tdobj \tNN\t公司 \t◄─┘             \t公司 \t        \t公司 \t◄─┘     \t公司 \tNN──┘                       │   └──────────►\t。  \tpunct\tPU\t。  \t                \t。  \t        \t。  \t        \t。  \tPU──────────────────────────┘   关于标注集含义，请参考《语言学标注规范》及《格式规范》。我们购买、标注或采用了世界上量级最大、种类最多的语料库用于联合多语种多任务学习，所以HanLP的标注集也是覆盖面最广的。训练你自己的领域模型写深度学习模型一点都不难，难的是复现较高的准确率。下列代码展示了如何在sighan2005 PKU语料库上花6分钟训练一个超越学术界state-of-the-art的中文分词模型。tokenizer = TransformerTaggingTokenizer()save_dir = 'data/model/cws/sighan2005_pku_bert_base_96.73'tokenizer.fit(    SIGHAN2005_PKU_TRAIN_ALL,    SIGHAN2005_PKU_TEST,  # Conventionally, no devset is used. See Tian et al. (2020).    save_dir,    'bert-base-chinese',    max_seq_len=300,    char_level=True,    hard_constraint=True,    sampler_builder=SortingSamplerBuilder(batch_size=32),    epochs=3,    adam_epsilon=1e-6,    warmup_steps=0.1,    weight_decay=0.01,    word_dropout=0.1,    seed=1660853059,)tokenizer.evaluate(SIGHAN2005_PKU_TEST, save_dir)其中，由于指定了随机数种子，结果一定是96.73。不同于那些虚假宣传的学术论文或商业项目，HanLP保证所有结果可复现。如果你有任何质疑，我们将当作最高优先级的致命性bug第一时间排查问题。请参考demo了解更多训练脚本。性能langcorporamodeltokposnerdepconsrlsdplemfeaamrfinecoarsectbpku863udpkumsraontonotesSemEval16DMPASPSDmulUD2.7OntoNotes5small98.62----93.23--74.4279.1076.8570.63-91.1993.6785.3487.7184.51-base98.97----90.32--80.3278.7471.2373.63-92.6096.0481.1985.0882.13-zhopensmall97.25-96.66-----95.0084.5787.6273.4084.57------base97.50-97.07-----96.0487.1189.8477.7887.11------closesmall96.7095.9396.8797.5695.05-96.2295.7476.7984.4488.1375.8174.28------base97.5296.4496.9997.5995.29-96.4895.7277.7785.2988.5776.5273.76------ernie96.9597.2996.7697.6495.22-97.3196.4777.9585.6789.1778.5174.10------根据我们的最新研究，单任务学习的性能往往优于多任务学习。在乎精度甚于速度的话，建议使用单任务模型。HanLP采用的数据预处理与拆分比例与流行方法未必相同，比如HanLP采用了完整版的MSRA命名实体识别语料，而非大众使用的阉割版；HanLP使用了语法覆盖更广的Stanford Dependencies标准，而非学术界沿用的Zhang and Clark (2008)标准；HanLP提出了均匀分割CTB的方法，而不采用学术界不均匀且遗漏了51个黄金文件的方法。HanLP开源了一整套语料预处理脚本与相应语料库，力图推动中文NLP的透明化。总之，HanLP只做我们认为正确、先进的事情，而不一定是流行、权威的事情。引用如果你在研究中使用了HanLP，请按如下格式引用：@inproceedings{he-choi-2021-stem,    title = \""The Stem Cell Hypothesis: Dilemma behind Multi-Task Learning with Transformer Encoders\"",    author = \""He, Han and Choi, Jinho D.\"",    booktitle = \""Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\"",    month = nov,    year = \""2021\"",    address = \""Online and Punta Cana, Dominican Republic\"",    publisher = \""Association for Computational Linguistics\"",    url = \""https://aclanthology.org/2021.emnlp-main.451\"",    pages = \""5555--5577\"",    abstract = \""Multi-task learning with transformer encoders (MTL) has emerged as a powerful technique to improve performance on closely-related tasks for both accuracy and efficiency while a question still remains whether or not it would perform as well on tasks that are distinct in nature. We first present MTL results on five NLP tasks, POS, NER, DEP, CON, and SRL, and depict its deficiency over single-task learning. We then conduct an extensive pruning analysis to show that a certain set of attention heads get claimed by most tasks during MTL, who interfere with one another to fine-tune those heads for their own objectives. Based on this finding, we propose the Stem Cell Hypothesis to reveal the existence of attention heads naturally talented for many tasks that cannot be jointly trained to create adequate embeddings for all of those tasks. Finally, we design novel parameter-free probes to justify our hypothesis and demonstrate how attention heads are transformed across the five tasks during MTL through label analysis.\"",}License源代码HanLP源代码的授权协议为 Apache License 2.0，可免费用做商业用途。请在产品说明中附加HanLP的链接和授权协议。HanLP受版权法保护，侵权必究。自然语义（青岛）科技有限公司HanLP从v1.7版起独立运作，由自然语义（青岛）科技有限公司作为项目主体，主导后续版本的开发，并拥有后续版本的版权。大快搜索HanLP v1.3~v1.65版由大快搜索主导开发，继续完全开源，大快搜索拥有相关版权。上海林原公司HanLP 早期得到了上海林原公司的大力支持，并拥有1.28及前序版本的版权，相关版本也曾在上海林原公司网站发布。预训练模型机器学习模型的授权在法律上没有定论，但本着尊重开源语料库原始授权的精神，如不特别说明，HanLP的多语种模型授权沿用CC BY-NC-SA 4.0，中文模型授权为仅供研究与教学使用。Referenceshttps://hanlp.hankcs.com/docs/references.html"
57,XX-net/XX-Net,https://github.com/XX-net/XX-Net/blob/master/README.md,Python,"🚀 XX-Net (翻墙VPN)这是一个可靠的翻墙系统，已经连续运行 8 年！我们不去研究墙有什么缺陷，因为所有的缺陷都会被慢慢的补上。我们的策略是化身为普通流量，完全无法区分，最终隐身在茫茫的网络连接中。。。🔌 功能特性支持多平台： Android/iOS/Windows/Mac/Linux采用独特的混淆算法，让您的流量在网络中无法被识别开源绿色软件，无需安装，可以支持多台设备同时连接模拟Chrome浏览器行为，完全无法识别，稳定翻墙内置 ChatGPT，每个套餐赠送 ChatGPT-3.5 一百万token官网下载: https://xx-net.comTelegram: https://t.me/xxnetshareTwitter: https://twitter.com/XXNetDev中文帮助文档      English Document      فارسی صفحه اصلی最新公告：2023-08-15新版 5.5.0, 提升连接性能5.1.0，内置ChatGPT原来是4.x.x 老版本的，需要重新下载新版安装，不能应用内升级。提示：有问题请先看Wiki文档提问 前，请先看最近讨论主题 ，避免重复发问。"
58,donnemartin/data-science-ipython-notebooks,https://github.com/donnemartin/data-science-ipython-notebooks/blob/master/README.md,Python,"      data-science-ipython-notebooksIndexdeep-learningtensorflowtheanokerascaffescikit-learnstatistical-inference-scipypandasmatplotlibnumpypython-datakaggle-and-business-analysessparkmapreduce-pythonamazon web servicescommand linesmiscnotebook-installationcreditscontributingcontact-infolicense  deep-learningIPython Notebook(s) demonstrating deep learning functionality.  tensor-flow-tutorialsAdditional TensorFlow tutorials:pkmital/tensorflow_tutorialsnlintz/TensorFlow-Tutorialsalrojo/tensorflow-tutorialBinRoot/TensorFlow-Booktuanavu/tensorflow-basic-tutorialsNotebookDescriptiontsf-basicsLearn basic operations in TensorFlow, a library for various kinds of perceptual and language understanding tasks from Google.tsf-linearImplement linear regression in TensorFlow.tsf-logisticImplement logistic regression in TensorFlow.tsf-nnImplement nearest neighboars in TensorFlow.tsf-alexImplement AlexNet in TensorFlow.tsf-cnnImplement convolutional neural networks in TensorFlow.tsf-mlpImplement multilayer perceptrons in TensorFlow.tsf-rnnImplement recurrent neural networks in TensorFlow.tsf-gpuLearn about basic multi-GPU computation in TensorFlow.tsf-gvizLearn about graph visualization in TensorFlow.tsf-lvizLearn about loss visualization in TensorFlow.tensor-flow-exercisesNotebookDescriptiontsf-not-mnistLearn simple data curation by creating a pickle with formatted datasets for training, development and testing in TensorFlow.tsf-fully-connectedProgressively train deeper and more accurate models using logistic regression and neural networks in TensorFlow.tsf-regularizationExplore regularization techniques by training fully connected networks to classify notMNIST characters in TensorFlow.tsf-convolutionsCreate convolutional neural networks in TensorFlow.tsf-word2vecTrain a skip-gram model over Text8 data in TensorFlow.tsf-lstmTrain a LSTM character model over Text8 data in TensorFlow.  theano-tutorialsNotebookDescriptiontheano-introIntro to Theano, which allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. It can use GPUs and perform efficient symbolic differentiation.theano-scanLearn scans, a mechanism to perform loops in a Theano graph.theano-logisticImplement logistic regression in Theano.theano-rnnImplement recurrent neural networks in Theano.theano-mlpImplement multilayer perceptrons in Theano.  keras-tutorialsNotebookDescriptionkerasKeras is an open source neural network library written in Python. It is capable of running on top of either Tensorflow or Theano.setupLearn about the tutorial goals and how to set up your Keras environment.intro-deep-learning-annGet an intro to deep learning with Keras and Artificial Neural Networks (ANN).theanoLearn about Theano by working with weights matrices and gradients.keras-ottoLearn about Keras by looking at the Kaggle Otto challenge.ann-mnistReview a simple implementation of ANN for MNIST using Keras.conv-netsLearn about Convolutional Neural Networks (CNNs) with Keras.conv-net-1Recognize handwritten digits from MNIST using Keras - Part 1.conv-net-2Recognize handwritten digits from MNIST using Keras - Part 2.keras-modelsUse pre-trained models such as VGG16, VGG19, ResNet50, and Inception v3 with Keras.auto-encodersLearn about Autoencoders with Keras.rnn-lstmLearn about Recurrent Neural Networks (RNNs) with Keras.lstm-sentence-genLearn about RNNs using Long Short Term Memory (LSTM) networks with Keras.deep-learning-miscNotebookDescriptiondeep-dreamCaffe-based computer vision program which uses a convolutional neural network to find and enhance patterns in images.  scikit-learnIPython Notebook(s) demonstrating scikit-learn functionality.NotebookDescriptionintroIntro notebook to scikit-learn.  Scikit-learn adds Python support for large, multi-dimensional arrays and matrices, along with a large library of high-level mathematical functions to operate on these arrays.knnImplement k-nearest neighbors in scikit-learn.linear-regImplement linear regression in scikit-learn.svmImplement support vector machine classifiers with and without kernels in scikit-learn.random-forestImplement random forest classifiers and regressors in scikit-learn.k-meansImplement k-means clustering in scikit-learn.pcaImplement principal component analysis in scikit-learn.gmmImplement Gaussian mixture models in scikit-learn.validationImplement validation and model selection in scikit-learn.  statistical-inference-scipyIPython Notebook(s) demonstrating statistical inference with SciPy functionality.NotebookDescriptionscipySciPy is a collection of mathematical algorithms and convenience functions built on the Numpy extension of Python. It adds significant power to the interactive Python session by providing the user with high-level commands and classes for manipulating and visualizing data.effect-sizeExplore statistics that quantify effect size by analyzing the difference in height between men and women.  Uses data from the Behavioral Risk Factor Surveillance System (BRFSS) to estimate the mean and standard deviation of height for adult women and men in the United States.samplingExplore random sampling by analyzing the average weight of men and women in the United States using BRFSS data.hypothesisExplore hypothesis testing by analyzing the difference of first-born babies compared with others.  pandasIPython Notebook(s) demonstrating pandas functionality.NotebookDescriptionpandasSoftware library written for data manipulation and analysis in Python. Offers data structures and operations for manipulating numerical tables and time series.github-data-wranglingLearn how to load, clean, merge, and feature engineer by analyzing GitHub data from the Viz repo.Introduction-to-PandasIntroduction to Pandas.Introducing-Pandas-ObjectsLearn about Pandas objects.Data Indexing and SelectionLearn about data indexing and selection in Pandas.Operations-in-PandasLearn about operating on data in Pandas.Missing-ValuesLearn about handling missing data in Pandas.Hierarchical-IndexingLearn about hierarchical indexing in Pandas.Concat-And-AppendLearn about combining datasets: concat and append in Pandas.Merge-and-JoinLearn about combining datasets: merge and join in Pandas.Aggregation-and-GroupingLearn about aggregation and grouping in Pandas.Pivot-TablesLearn about pivot tables in Pandas.Working-With-StringsLearn about vectorized string operations in Pandas.Working-with-Time-SeriesLearn about working with time series in pandas.Performance-Eval-and-QueryLearn about high-performance Pandas: eval() and query() in Pandas.  matplotlibIPython Notebook(s) demonstrating matplotlib functionality.NotebookDescriptionmatplotlibPython 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms.matplotlib-appliedApply matplotlib visualizations to Kaggle competitions for exploratory data analysis.  Learn how to create bar plots, histograms, subplot2grid, normalized plots, scatter plots, subplots, and kernel density estimation plots.Introduction-To-MatplotlibIntroduction to Matplotlib.Simple-Line-PlotsLearn about simple line plots in Matplotlib.Simple-Scatter-PlotsLearn about simple scatter plots in Matplotlib.Errorbars.ipynbLearn about visualizing errors in Matplotlib.Density-and-Contour-PlotsLearn about density and contour plots in Matplotlib.Histograms-and-BinningsLearn about histograms, binnings, and density in Matplotlib.Customizing-LegendsLearn about customizing plot legends in Matplotlib.Customizing-ColorbarsLearn about customizing colorbars in Matplotlib.Multiple-SubplotsLearn about multiple subplots in Matplotlib.Text-and-AnnotationLearn about text and annotation in Matplotlib.Customizing-TicksLearn about customizing ticks in Matplotlib.Settings-and-StylesheetsLearn about customizing Matplotlib: configurations and stylesheets.Three-Dimensional-PlottingLearn about three-dimensional plotting in Matplotlib.Geographic-Data-With-BasemapLearn about geographic data with basemap in Matplotlib.Visualization-With-SeabornLearn about visualization with Seaborn.  numpyIPython Notebook(s) demonstrating NumPy functionality.NotebookDescriptionnumpyAdds Python support for large, multi-dimensional arrays and matrices, along with a large library of high-level mathematical functions to operate on these arrays.Introduction-to-NumPyIntroduction to NumPy.Understanding-Data-TypesLearn about data types in Python.The-Basics-Of-NumPy-ArraysLearn about the basics of NumPy arrays.Computation-on-arrays-ufuncsLearn about computations on NumPy arrays: universal functions.Computation-on-arrays-aggregatesLearn about aggregations: min, max, and everything in between in NumPy.Computation-on-arrays-broadcastingLearn about computation on arrays: broadcasting in NumPy.Boolean-Arrays-and-MasksLearn about comparisons, masks, and boolean logic in NumPy.Fancy-IndexingLearn about fancy indexing in NumPy.SortingLearn about sorting arrays in NumPy.Structured-Data-NumPyLearn about structured data: NumPy's structured arrays.  python-dataIPython Notebook(s) demonstrating Python functionality geared towards data analysis.NotebookDescriptiondata structuresLearn Python basics with tuples, lists, dicts, sets.data structure utilitiesLearn Python operations such as slice, range, xrange, bisect, sort, sorted, reversed, enumerate, zip, list comprehensions.functionsLearn about more advanced Python features: Functions as objects, lambda functions, closures, *args, **kwargs currying, generators, generator expressions, itertools.datetimeLearn how to work with Python dates and times: datetime, strftime, strptime, timedelta.loggingLearn about Python logging with RotatingFileHandler and TimedRotatingFileHandler.pdbLearn how to debug in Python with the interactive source code debugger.unit testsLearn how to test in Python with Nose unit tests.  kaggle-and-business-analysesIPython Notebook(s) used in kaggle competitions and business analyses.NotebookDescriptiontitanicPredict survival on the Titanic.  Learn data cleaning, exploratory data analysis, and machine learning.churn-analysisPredict customer churn.  Exercise logistic regression, gradient boosting classifers, support vector machines, random forests, and k-nearest-neighbors.  Includes discussions of confusion matrices, ROC plots, feature importances, prediction probabilities, and calibration/descrimination.  sparkIPython Notebook(s) demonstrating spark and HDFS functionality.NotebookDescriptionsparkIn-memory cluster computing framework, up to 100 times faster for certain applications and is well suited for machine learning algorithms.hdfsReliably stores very large files across machines in a large cluster.  mapreduce-pythonIPython Notebook(s) demonstrating Hadoop MapReduce with mrjob functionality.NotebookDescriptionmapreduce-pythonRuns MapReduce jobs in Python, executing jobs locally or on Hadoop clusters. Demonstrates Hadoop Streaming in Python code with unit test and mrjob config file to analyze Amazon S3 bucket logs on Elastic MapReduce.  Disco is another python-based alternative.  awsIPython Notebook(s) demonstrating Amazon Web Services (AWS) and AWS tools functionality.Also check out:SAWS: A Supercharged AWS command line interface (CLI).Awesome AWS: A curated list of libraries, open source repos, guides, blogs, and other resources.NotebookDescriptionbotoOfficial AWS SDK for Python.s3cmdInteracts with S3 through the command line.s3distcpCombines smaller files and aggregates them together by taking in a pattern and target file.  S3DistCp can also be used to transfer large volumes of data from S3 to your Hadoop cluster.s3-parallel-putUploads multiple files to S3 in parallel.redshiftActs as a fast data warehouse built on top of technology from massive parallel processing (MPP).kinesisStreams data in real time with the ability to process thousands of data streams per second.lambdaRuns code in response to events, automatically managing compute resources.  commandsIPython Notebook(s) demonstrating various command lines for Linux, Git, etc.NotebookDescriptionlinuxUnix-like and mostly POSIX-compliant computer operating system.  Disk usage, splitting files, grep, sed, curl, viewing running processes, terminal syntax highlighting, and Vim.anacondaDistribution of the Python programming language for large-scale data processing, predictive analytics, and scientific computing, that aims to simplify package management and deployment.ipython notebookWeb-based interactive computational environment where you can combine code execution, text, mathematics, plots and rich media into a single document.gitDistributed revision control system with an emphasis on speed, data integrity, and support for distributed, non-linear workflows.rubyUsed to interact with the AWS command line and for Jekyll, a blog framework that can be hosted on GitHub Pages.jekyllSimple, blog-aware, static site generator for personal, project, or organization sites.  Renders Markdown or Textile and Liquid templates, and produces a complete, static website ready to be served by Apache HTTP Server, Nginx or another web server.pelicanPython-based alternative to Jekyll.djangoHigh-level Python Web framework that encourages rapid development and clean, pragmatic design. It can be useful to share reports/analyses and for blogging. Lighter-weight alternatives include Pyramid, Flask, Tornado, and Bottle.miscIPython Notebook(s) demonstrating miscellaneous functionality.NotebookDescriptionregexRegular expression cheat sheet useful in data wrangling.algorithmiaAlgorithmia is a marketplace for algorithms. This notebook showcases 4 different algorithms: Face Detection, Content Summarizer, Latent Dirichlet Allocation and Optical Character Recognition.notebook-installationanacondaAnaconda is a free distribution of the Python programming language for large-scale data processing, predictive analytics, and scientific computing that aims to simplify package management and deployment.Follow instructions to install Anaconda or the more lightweight miniconda.dev-setupFor detailed instructions, scripts, and tools to set up your development environment for data analysis, check out the dev-setup repo.running-notebooksTo view interactive content or to modify elements within the IPython notebooks, you must first clone or download the repository then run the notebook.  More information on IPython Notebooks can be found here.$ git clone https://github.com/donnemartin/data-science-ipython-notebooks.git$ cd data-science-ipython-notebooks$ jupyter notebookNotebooks tested with Python 2.7.x.creditsPython for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython by Wes McKinneyPyCon 2015 Scikit-learn Tutorial by Jake VanderPlasPython Data Science Handbook by Jake VanderPlasParallel Machine Learning with scikit-learn and IPython by Olivier GriselStatistical Interference Using Computational Methods in Python by Allen DowneyTensorFlow Examples by Aymeric DamienTensorFlow Tutorials by Parag K MitalTensorFlow Tutorials by Nathan LintzTensorFlow Tutorials by Alexander R JohansenTensorFlow Book by Nishant ShuklaSummer School 2015 by mila-udemKeras tutorials by Valerio MaggioKaggleYhat BlogcontributingContributions are welcome!  For bug reports or requests please submit an issue.contact-infoFeel free to contact me to discuss any issues, questions, or comments.Email: donne.martin@gmail.comTwitter: @donne_martinGitHub: donnemartinLinkedIn: donnemartinWebsite: donnemartin.comlicenseThis repository contains a variety of content; some developed by Donne Martin, and some from third-parties.  The third-party content is distributed under the license provided by those parties.The content developed by Donne Martin is distributed under the following license:I am providing code and resources in this repository to you under an open source license.  Because this is my personal repository, the license you receive to my code and resources is from me and not my employer (Facebook).Copyright 2015 Donne MartinLicensed under the Apache License, Version 2.0 (the \""License\"");you may not use this file except in compliance with the License.You may obtain a copy of the License at   http://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an \""AS IS\"" BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License."
59,jackfrued/Python-100-Days,https://github.com/jackfrued/Python-100-Days/blob/master/README.md,Python,"Python - 100天从新手到大师作者：骆昊说明：从项目上线到获得8w+星标以来，一直收到反馈说基础部分（前15天的内容）对新手来说是比较困难的，建议有配套视频进行讲解。最近把基础部分的内容重新制作了一个名为“Python-Core-50-Courses”的项目，用更为简单通俗的方式重写了这部分内容并附带了视频讲解，初学者可以关注下这个新项目。如果需要Python基础视频，可以在“B站”搜索《Python零基础快速上手》，这套视频是我讲课的时候录制的随堂视频，画质尚可、音质一般，但是对初学者应该会有些帮助，欢迎大家留言、评论、发弹幕。学习之后觉得有收获的小伙伴可以“一键三连”来支持UP主（千锋Python）。国内用户如果访问GitHub比较慢的话，可以关注我的知乎号Python-Jack，上面的“从零开始学Python”专栏比较适合初学者，其他的专栏也在持续创作和更新中，欢迎大家关注并点赞评论。创作不易，感谢大家的打赏支持，这些钱不会用于个人消费（例如：购买咖啡），而是通过腾讯公益、美团公益、水滴筹等平台捐赠给需要帮助的人（点击了解捐赠情况）。需要加入QQ学习群的可以扫描下面的二维码，三个群加一个即可，不要重复进群。学习群会为大家提供学习资源和问题解答，如果有Python体验课和行业公开课会提前在群里通知大家，欢迎大家加入。项目“Day80~90”部分目前仍在创作中，因为作者平时也挤不出太多时间来写文档，因此更新的速度比较缓慢，感谢大家的理解。Python应用领域和职业发展分析简单的说，Python是一个“优雅”、“明确”、“简单”的编程语言。学习曲线低，非专业人士也能上手开源系统，拥有强大的生态圈解释型语言，完美的平台可移植性动态类型语言，支持面向对象和函数式编程代码规范程度高，可读性强Python在以下领域都有用武之地。后端开发 - Python / Java / Go / PHPDevOps - Python / Shell / Ruby数据采集 - Python / C++ / Java量化交易 - Python / C++ / R数据科学 - Python / R / Julia / Matlab机器学习 - Python / R / C++ / Julia自动化测试 - Python / Shell作为一名Python开发者，根据个人的喜好和职业规划，可以选择的就业领域也非常多。Python后端开发工程师（服务器、云平台、数据接口）Python运维工程师（自动化运维、SRE、DevOps）Python数据分析师（数据分析、商业智能、数字化运营）Python数据挖掘工程师（机器学习、深度学习、算法专家）Python爬虫工程师Python测试工程师（自动化测试、测试开发）说明：目前，数据分析和数据挖掘是非常热门的方向，因为不管是互联网行业还是传统行业都已经积累了大量的数据，各行各业都需要数据分析师从已有的数据中发现更多的商业价值，从而为企业的决策提供数据的支撑，这就是所谓的数据驱动决策。给初学者的几个建议：Make English as your working language. （让英语成为你的工作语言）Practice makes perfect. （熟能生巧）All experience comes from mistakes. （所有的经验都源于你犯过的错误）Don't be one of the leeches. （不要当伸手党）Either outstanding or out. （要么出众，要么出局）Day01~15 - Python语言基础Day01 - 初识PythonPython简介 - Python的历史 / Python的优缺点 / Python的应用领域搭建编程环境 - Windows环境 / Linux环境 / MacOS环境从终端运行Python程序 - Hello, world / print函数 / 运行程序使用IDLE - 交互式环境(REPL) / 编写多行代码 / 运行程序 / 退出IDLE注释 - 注释的作用 / 单行注释 / 多行注释Day02 - 语言元素程序和进制 - 指令和程序 / 冯诺依曼机 / 二进制和十进制 / 八进制和十六进制变量和类型 - 变量的命名 / 变量的使用 / input函数 / 检查变量类型 / 类型转换数字和字符串 - 整数 / 浮点数 / 复数 / 字符串 / 字符串基本操作 / 字符编码运算符 - 数学运算符 / 赋值运算符 / 比较运算符 / 逻辑运算符 / 身份运算符 / 运算符的优先级应用案例 - 华氏温度转换成摄氏温度 / 输入圆的半径计算周长和面积 / 输入年份判断是否是闰年Day03 - 分支结构分支结构的应用场景 - 条件 / 缩进 / 代码块 / 流程图if语句 - 简单的if / if-else结构 / if-elif-else结构 / 嵌套的if应用案例 - 用户身份验证 / 英制单位与公制单位互换 / 掷骰子决定做什么 / 百分制成绩转等级制 / 分段函数求值 / 输入三条边的长度如果能构成三角形就计算周长和面积Day04 - 循环结构循环结构的应用场景 - 条件 / 缩进 / 代码块 / 流程图while循环 - 基本结构 / break语句 / continue语句for循环 - 基本结构 / range类型 / 循环中的分支结构 / 嵌套的循环 / 提前结束程序应用案例 - 1~100求和 / 判断素数 / 猜数字游戏 / 打印九九表 / 打印三角形图案 / 猴子吃桃 / 百钱百鸡Day05 - 构造程序逻辑经典案例：水仙花数 / 百钱百鸡 / Craps赌博游戏练习题目：斐波那契数列 / 完美数 / 素数Day06 - 函数和模块的使用函数的作用 - 代码的坏味道 / 用函数封装功能模块定义函数 - def关键字 / 函数名 / 参数列表 / return语句 / 调用自定义函数调用函数 - Python内置函数 /  导入模块和函数函数的参数 - 默认参数 / 可变参数 / 关键字参数 / 命名关键字参数函数的返回值 - 没有返回值  / 返回单个值 / 返回多个值作用域问题 - 局部作用域 / 嵌套作用域 / 全局作用域 / 内置作用域 / 和作用域相关的关键字用模块管理函数 - 模块的概念 / 用自定义模块管理函数 / 命名冲突的时候会怎样（同一个模块和不同的模块）Day07 - 字符串和常用数据结构字符串的使用 - 计算长度 / 下标运算 / 切片 / 常用方法列表基本用法 - 定义列表 / 用下表访问元素 / 下标越界 / 添加元素 / 删除元素 / 修改元素 / 切片 / 循环遍历列表常用操作 - 连接 / 复制(复制元素和复制数组) / 长度 / 排序 / 倒转 / 查找生成列表 - 使用range创建数字列表 / 生成表达式 / 生成器元组的使用 - 定义元组 / 使用元组中的值 / 修改元组变量 / 元组和列表转换集合基本用法 - 集合和列表的区别 /  创建集合 / 添加元素 / 删除元素 /  清空集合常用操作 - 交集 / 并集 / 差集 / 对称差 / 子集 / 超集字典的基本用法 - 字典的特点 / 创建字典 / 添加元素 / 删除元素 / 取值 / 清空字典常用操作 - keys方法 / values方法 / items方法 / setdefault方法基础练习 - 跑马灯效果 / 列表找最大元素 / 统计考试成绩的平均分 / Fibonacci数列 / 杨辉三角综合案例 - 双色球选号 / 井字棋Day08 - 面向对象编程基础类和对象 - 什么是类 / 什么是对象 / 面向对象其他相关概念定义类 - 基本结构 / 属性和方法 / 构造器 / 析构器 / __str__方法使用对象 - 创建对象 / 给对象发消息面向对象的四大支柱 - 抽象 / 封装 / 继承 / 多态基础练习 - 定义学生类 / 定义时钟类 / 定义图形类 / 定义汽车类Day09 - 面向对象进阶属性 - 类属性 / 实例属性 / 属性访问器 / 属性修改器 / 属性删除器 / 使用__slots__类中的方法 - 实例方法 / 类方法 / 静态方法运算符重载 - __add__ / __sub__ / __or__ /__getitem__ / __setitem__ / __len__ / __repr__ / __gt__ / __lt__ / __le__ / __ge__ / __eq__ / __ne__ / __contains__类(的对象)之间的关系 - 关联 / 继承 / 依赖继承和多态 - 什么是继承 / 继承的语法 / 调用父类方法 / 方法重写 / 类型判定 / 多重继承 / 菱形继承(钻石继承)和C3算法综合案例 - 工资结算系统 / 图书自动折扣系统 / 自定义分数类Day10 - 图形用户界面和游戏开发使用tkinter开发GUI程序使用pygame三方库开发游戏应用“大球吃小球”游戏Day11 - 文件和异常读文件 - 读取整个文件 / 逐行读取 / 文件路径写文件 - 覆盖写入 / 追加写入 / 文本文件 / 二进制文件异常处理 - 异常机制的重要性 / try-except代码块 / else代码块 / finally代码块 / 内置异常类型 / 异常栈 / raise语句数据持久化 - CSV文件概述 / csv模块的应用 / JSON数据格式 / json模块的应用Day12 - 字符串和正则表达式字符串高级操作 - 转义字符 / 原始字符串 / 多行字符串 / in和not in运算符 / is_xxx方法 / join和split方法 / strip相关方法 / pyperclip模块 / 不变字符串和可变字符串 / StringIO的使用正则表达式入门 - 正则表达式的作用 / 元字符 / 转义 / 量词 / 分组 / 零宽断言 /贪婪匹配与惰性匹配懒惰 / 使用re模块实现正则表达式操作（匹配、搜索、替换、捕获）使用正则表达式 - re模块 / compile函数 / group和groups方法 / match方法 / search方法 / findall和finditer方法 / sub和subn方法 / split方法应用案例 - 使用正则表达式验证输入的字符串Day13 - 进程和线程进程和线程的概念 - 什么是进程 / 什么是线程 / 多线程的应用场景使用进程 - fork函数 / multiprocessing模块 / 进程池 / 进程间通信使用线程 -  threading模块 / Thread类 / RLock类 / Condition类 / 线程池Day14 - 网络编程入门和网络应用开发计算机网络基础 - 计算机网络发展史 / “TCP-IP”模型 / IP地址 / 端口 / 协议 / 其他相关概念网络应用模式 - “客户端-服务器”模式 / “浏览器-服务器”模式基于HTTP协议访问网络资源 - 网络API概述 / 访问URL / requests三方库 / 解析JSON格式数据Python网络编程 - 套接字的概念 / socket模块 /  socket函数 / 创建TCP服务器 / 创建TCP客户端 / 创建UDP服务器 / 创建UDP客户端电子邮件 - SMTP协议 / POP3协议 / IMAP协议 / smtplib模块 / poplib模块 / imaplib模块短信服务 - 调用短信服务网关Day15 - 图像和文档处理用Pillow处理图片 - 图片读写 / 图片合成 / 几何变换 / 色彩转换 / 滤镜效果读写Word文档 - 文本内容的处理 / 段落 / 页眉和页脚 / 样式的处理读写Excel文件 - xlrd / xlwt / openpyxlDay16~Day20 - Python语言进阶 常用数据结构函数的高级用法 - “一等公民” / 高阶函数 / Lambda函数 / 作用域和闭包 / 装饰器面向对象高级知识 - “三大支柱” / 类与类之间的关系 / 垃圾回收 / 魔术属性和方法 / 混入 / 元类 / 面向对象设计原则 / GoF设计模式迭代器和生成器 - 相关魔术方法 / 创建生成器的两种方式 /并发和异步编程 - 多线程 / 多进程 / 异步IO / async和awaitDay21~30 - Web前端入门用HTML标签承载页面内容用CSS渲染页面用JavaScript处理交互式行为jQuery入门和提高Vue.js入门Element的使用Bootstrap的使用Day31~35 - 玩转Linux操作系统操作系统发展史和Linux概述Linux基础命令Linux中的实用程序Linux的文件系统Vim编辑器的应用环境变量和Shell编程软件的安装和服务的配置网络访问和管理其他相关内容Day36~40 - 数据库基础和进阶关系型数据库概述MySQL的安装和使用SQL的使用DDL - 数据定义语言 - create / drop / alterDML - 数据操作语言 - insert / delete / updateDQL - 数据查询语言 - selectDCL - 数据控制语言 - grant / revokeMySQL新特性窗口函数的应用JSON数据类型相关知识数据完整性和一致性视图、函数、过程、触发器事务和锁执行计划和索引范式理论和反范式设计在Python中操作MySQLDay41~55 - 实战DjangoDay41 - Django快速上手Web应用工作机制HTTP请求和响应Django框架概述5分钟快速上手Day42 - 深入模型关系型数据库配置使用ORM完成对模型的CRUD操作管理后台的使用Django模型最佳实践模型定义参考Day43 - 静态资源和Ajax请求加载静态资源Ajax概述用Ajax实现投票功能Day44 - Cookie和Session实现用户跟踪cookie和session的关系Django框架对session的支持视图函数中的cookie读写操作Day45 - 报表和日志通过HttpResponse修改响应头使用StreamingHttpResponse处理大文件使用xlwt生成Excel报表使用reportlab生成PDF报表使用ECharts生成前端图表Day46 - 日志和调试工具栏配置日志配置Django-Debug-Toolbar优化ORM代码Day47 - 中间件的应用什么是中间件Django框架内置的中间件自定义中间件及其应用场景Day48 - 前后端分离开发入门返回JSON格式的数据用Vue.js渲染页面Day49 - RESTful架构和DRF入门Day50 - RESTful架构和DRF进阶Day51 - 使用缓存网站优化第一定律在Django项目中使用Redis提供缓存服务在视图函数中读写缓存使用装饰器实现页面缓存为数据接口提供缓存服务Day52 - 接入三方平台文件上传表单控件和图片文件预览服务器端如何处理上传的文件Day53 - 异步任务和定时任务网站优化第二定律配置消息队列服务在项目中使用Celery实现任务异步化在项目中使用Celery实现定时任务Day54 - 单元测试Day55 - 项目上线Python中的单元测试Django框架对单元测试的支持使用版本控制系统配置和使用uWSGI动静分离和Nginx配置配置HTTPS配置域名解析Day56~60 - 用FastAPI开发数据接口FastAPI五分钟上手请求和响应接入关系型数据库依赖注入中间件异步化虚拟化部署（Docker）项目实战：车辆违章查询项目Day61~65 - 爬虫开发Day61 - 网络数据采集概述网络爬虫的概念及其应用领域网络爬虫的合法性探讨开发网络爬虫的相关工具一个爬虫程序的构成Day62 - 数据抓取和解析使用requests三方库实现数据抓取页面解析的三种方式正则表达式解析XPath解析CSS选择器解析Day63 - Python中的并发编程多线程多进程异步I/ODay64 - 使用Selenium抓取网页动态内容Day65 - 爬虫框架Scrapy简介Day66~80 - 数据分析Day66 - 数据分析概述Day67 - 环境准备Day68 - NumPy的应用-1Day69 - NumPy的应用-2Day70 - Pandas的应用-1Day71 - Pandas的应用-2Day72 - Pandas的应用-3Day73 - Pandas的应用-4Day74 - Pandas的应用-5Day75 - 数据可视化-1Day76 - 数据可视化-2Day77 - 概率统计基础Day78 - 方差分析和参数估计Day79 - 相关和回归Day80 - 数据分析方法论Day81~90 - 机器学习和深度学习Day81 - 机器学习基础Day82 - k最近邻分类Day83 - 决策树Day84 - 贝叶斯分类Day85 - 支持向量机Day86 - K-均值聚类Day87 - 回归分析Day88 - 深度学习入门Day89 - PyTorch概述Day90 - PyTorch实战Day91~100 - 团队项目开发第91天：团队项目开发的问题和解决方案软件过程模型经典过程模型（瀑布模型）可行性分析（研究做还是不做），输出《可行性分析报告》。需求分析（研究做什么），输出《需求规格说明书》和产品界面原型图。概要设计和详细设计，输出概念模型图（ER图）、物理模型图、类图、时序图等。编码 / 测试。上线 / 维护。瀑布模型最大的缺点是无法拥抱需求变化，整套流程结束后才能看到产品，团队士气低落。敏捷开发（Scrum）- 产品所有者、Scrum Master、研发人员 - Sprint产品的Backlog（用户故事、产品原型）。计划会议（评估和预算）。日常开发（站立会议、番茄工作法、结对编程、测试先行、代码重构……）。修复bug（问题描述、重现步骤、测试人员、被指派人）。发布版本。评审会议（Showcase，用户需要参与）。回顾会议（对当前迭代周期做一个总结）。补充：敏捷软件开发宣言个体和互动 高于 流程和工具工作的软件 高于 详尽的文档客户合作 高于 合同谈判响应变化 高于 遵循计划角色：产品所有者（决定做什么，能对需求拍板的人）、团队负责人（解决各种问题，专注如何更好的工作，屏蔽外部对开发团队的影响）、开发团队（项目执行人员，具体指开发人员和测试人员）。准备工作：商业案例和资金、合同、憧憬、初始产品需求、初始发布计划、入股、组建团队。敏捷团队通常人数为8-10人。工作量估算：将开发任务量化，包括原型、Logo设计、UI设计、前端开发等，尽量把每个工作分解到最小任务量，最小任务量标准为工作时间不能超过两天，然后估算总体项目时间。把每个任务都贴在看板上面，看板上分三部分：to do（待完成）、in progress（进行中）和done（已完成）。项目团队组建团队的构成和角色说明：谢谢付祥英女士帮助我绘制了下面这张精美的公司组织架构图。编程规范和代码审查（flake8、pylint）Python中的一些“惯例”（请参考《Python惯例-如何编写Pythonic的代码》）影响代码可读性的原因：代码注释太少或者没有注释代码破坏了语言的最佳实践反模式编程（意大利面代码、复制-黏贴编程、自负编程、……）团队开发工具介绍版本控制：Git、Mercury缺陷管理：Gitlab、Redmine敏捷闭环工具：禅道、JIRA持续集成：Jenkins、Travis-CI请参考《团队项目开发的问题和解决方案》。项目选题和理解业务选题范围设定CMS（用户端）：新闻聚合网站、问答/分享社区、影评/书评网站等。MIS（用户端+管理端）：KMS、KPI考核系统、HRS、CRM系统、供应链系统、仓储管理系统等。App后台（管理端+数据接口）：二手交易类、报刊杂志类、小众电商类、新闻资讯类、旅游类、社交类、阅读类等。其他类型：自身行业背景和工作经验、业务容易理解和把控。需求理解、模块划分和任务分配需求理解：头脑风暴和竞品分析。模块划分：画思维导图（XMind），每个模块是一个枝节点，每个具体的功能是一个叶节点（用动词表述），需要确保每个叶节点无法再生出新节点，确定每个叶子节点的重要性、优先级和工作量。任务分配：由项目负责人根据上面的指标为每个团队成员分配任务。制定项目进度表（每日更新）模块功能人员状态完成工时计划开始实际开始计划结束实际结束备注评论添加评论王大锤正在进行50%42018/8/72018/8/7删除评论王大锤等待0%22018/8/72018/8/7查看评论白元芳正在进行20%42018/8/72018/8/7需要进行代码审查评论投票白元芳等待0%42018/8/82018/8/8OOAD和数据库设计UML（统一建模语言）的类图通过模型创建表（正向工程），例如在Django项目中可以通过下面的命令创建二维表。python manage.py makemigrations apppython manage.py migrate使用PowerDesigner绘制物理模型图。通过数据表创建模型（反向工程），例如在Django项目中可以通过下面的命令生成模型。python manage.py inspectdb > app/models.py第92天：Docker容器详解Docker简介安装Docker使用Docker创建容器（Nginx、MySQL、Redis、Gitlab、Jenkins）构建Docker镜像（Dockerfile的编写和相关指令）容器编排（Docker-compose）集群管理（Kubernetes）第93天：MySQL性能优化第94天：网络API接口设计第95天：[使用Django开发商业项目](./Day91-100/95.使用Django开发商业项\t目.md)项目开发中的公共问题数据库的配置（多数据库、主从复制、数据库路由）缓存的配置（分区缓存、键设置、超时设置、主从复制、故障恢复（哨兵））日志的配置分析和调试（Django-Debug-ToolBar）好用的Python模块（日期计算、图像处理、数据加密、三方API）REST API设计RESTful架构理解RESTful架构RESTful API设计指南RESTful API最佳实践API接口文档的撰写RAP2YAPIdjango-REST-framework的应用项目中的重点难点剖析使用缓存缓解数据库压力 - Redis使用消息队列做解耦合和削峰 - Celery + RabbitMQ第96天：软件测试和自动化测试单元测试测试的种类编写单元测试（unittest、pytest、nose2、tox、ddt、……）测试覆盖率（coverage）Django项目部署部署前的准备工作关键设置（SECRET_KEY / DEBUG / ALLOWED_HOSTS / 缓存 / 数据库）HTTPS / CSRF_COOKIE_SECUR  / SESSION_COOKIE_SECURE日志相关配置Linux常用命令回顾Linux常用服务的安装和配置uWSGI/Gunicorn和Nginx的使用Gunicorn和uWSGI的比较对于不需要大量定制化的简单应用程序，Gunicorn是一个不错的选择，uWSGI的学习曲线比Gunicorn要陡峭得多，Gunicorn的默认参数就已经能够适应大多数应用程序。uWSGI支持异构部署。由于Nginx本身支持uWSGI，在线上一般都将Nginx和uWSGI捆绑在一起部署，而且uWSGI属于功能齐全且高度定制的WSGI中间件。在性能上，Gunicorn和uWSGI其实表现相当。使用虚拟化技术（Docker）部署测试环境和生产环境性能测试AB的使用SQLslap的使用sysbench的使用自动化测试使用Shell和Python进行自动化测试使用Selenium实现自动化测试Selenium IDESelenium WebDriverSelenium Remote Control测试工具Robot Framework介绍第97天：电商网站技术要点剖析第98天：项目部署上线和性能调优MySQL数据库调优Web服务器性能优化Nginx负载均衡配置Keepalived实现高可用代码性能调优多线程异步化静态资源访问优化云存储CDN第99天：面试中的公共问题第100天：Python面试题实录"
60,TheAlgorithms/Python,https://github.com/TheAlgorithms/Python/blob/master/README.md,Python,          The Algorithms - Python                                                                  All algorithms implemented in Python - for educationImplementations are for learning purposes only. They may be less efficient than the implementations in the Python standard library. Use them at your discretion.Getting StartedRead through our Contribution Guidelines before you contribute.Community ChannelsWe are on Discord and Gitter! Community channels are a great way for you to ask questions and get help. Please join us!List of AlgorithmsSee our directory for easier navigation and a better overview of the project.
61,Significant-Gravitas/Auto-GPT,https://github.com/Significant-Gravitas/Auto-GPT/blob/master/README.md,Python,"Auto-GPT: An Autonomous GPT-4 Experiment💡 Get help - Q&A or Discord 💬🔴 USE stable not master 🔴Download the latest stable release from here: https://github.com/Significant-Gravitas/Auto-GPT/releases/latest.The master branch is under heavy development and may often be in a broken state.Auto-GPT is an experimental open-source application showcasing the capabilities of the GPT-4 language model. This program, driven by GPT-4, chains together LLM \""thoughts\"", to autonomously achieve whatever goal you set. As one of the first examples of GPT-4 running fully autonomously, Auto-GPT pushes the boundaries of what is possible with AI. Demo April 16th 2023               AutoGPTDemo_Subs_WithoutFinalScreen.mp4          Demo made by Blake Werlinger🚀 Features🌐 Internet access for searches and information gathering💾 Long-term and short-term memory management🧠 GPT-4 instances for text generation🔗 Access to popular websites and platforms🗃️ File storage and summarization with GPT-3.5🔌 Extensibility with PluginsQuickstartCheck out the wikiGet an OpenAI API KeyDownload the latest releaseFollow the installation instructionsConfigure any additional features you want, or install some pluginsRun the appPlease see the documentation for full setup instructions and configuration options.📖 Documentation⚙️ Setup💻 Usage🔌 PluginsConfiguration🔍 Web Search🧠 Memory🗣️ Voice (TTS)🖼️ Image Generation 💖 Help Fund Auto-GPT's Development 💖If you can spare a coffee, you can help to cover the costs of developing Auto-GPT and help to push the boundaries of fully autonomous AI!Your support is greatly appreciated. Development of this free, open-source project is made possible by all the contributors and sponsors. If you'd like to sponsor this project and have your avatar or company logo appear below click here.                                                                                                                                                                                                                                                                                                                                          ⚠️ LimitationsThis experiment aims to showcase the potential of GPT-4 but comes with some limitations:Not a polished application or product, just an experimentMay not perform well in complex, real-world business scenarios. In fact, if it actually does, please share your results!Quite expensive to run, so set and monitor your API key limits with OpenAI!🛡 DisclaimerThis project, Auto-GPT, is an experimental application and is provided \""as-is\"" without any warranty, express or implied. By using this software, you agree to assume all risks associated with its use, including but not limited to data loss, system failure, or any other issues that may arise.The developers and contributors of this project do not accept any responsibility or liability for any losses, damages, or other consequences that may occur as a result of using this software. You are solely responsible for any decisions and actions taken based on the information provided by Auto-GPT.Please note that the use of the GPT-4 language model can be expensive due to its token usage. By utilizing this project, you acknowledge that you are responsible for monitoring and managing your own token usage and the associated costs. It is highly recommended to check your OpenAI API usage regularly and set up any necessary limits or alerts to prevent unexpected charges.As an autonomous experiment, Auto-GPT may generate content or take actions that are not in line with real-world business practices or legal requirements. It is your responsibility to ensure that any actions or decisions made based on the output of this software comply with all applicable laws, regulations, and ethical standards. The developers and contributors of this project shall not be held responsible for any consequences arising from the use of this software.By using Auto-GPT, you agree to indemnify, defend, and hold harmless the developers, contributors, and any affiliated parties from and against any and all claims, damages, losses, liabilities, costs, and expenses (including reasonable attorneys' fees) arising from your use of this software or your violation of these terms.🐦 Connect with Us on TwitterStay up-to-date with the latest news, updates, and insights about Auto-GPT by following our Twitter accounts. Engage with the developer and the AI's own account for interesting discussions, project updates, and more.Developer: Follow @siggravitas for insights into the development process, project updates, and related topics from the creator of Entrepreneur-GPT.We look forward to connecting with you and hearing your thoughts, ideas, and experiences with Auto-GPT. Join us on Twitter and let's explore the future of AI together!        "
62,huggingface/transformers,https://github.com/huggingface/transformers/blob/main/README.md,Python,"                                                                                                                    English |        简体中文 |        繁體中文 |        한국어 |        Español |        日本語 |        हिन्दी        State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow    🤗 Transformers provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio.These models can be applied on:📝 Text, for tasks like text classification, information extraction, question answering, summarization, translation, text generation, in over 100 languages.🖼️ Images, for tasks like image classification, object detection, and segmentation.🗣️ Audio, for tasks like speech recognition and audio classification.Transformer models can also perform tasks on several modalities combined, such as table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.🤗 Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments.🤗 Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch and TensorFlow — with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other.Online demosYou can test most of our models directly on their pages from the model hub. We also offer private model hosting, versioning, & an inference API for public and private models.Here are a few examples:In Natural Language Processing:Masked word completion with BERTName Entity Recognition with ElectraText generation with GPT-2Natural Language Inference with RoBERTaSummarization with BARTQuestion answering with DistilBERTTranslation with T5In Computer Vision:Image classification with ViTObject Detection with DETRSemantic Segmentation with SegFormerPanoptic Segmentation with MaskFormerDepth Estimation with DPTVideo Classification with VideoMAEUniversal Segmentation with OneFormerIn Audio:Automatic Speech Recognition with Wav2Vec2Keyword Spotting with Wav2Vec2Audio Classification with Audio Spectrogram TransformerIn Multimodal tasks:Table Question Answering with TAPASVisual Question Answering with ViLTZero-shot Image Classification with CLIPDocument Question Answering with LayoutLMZero-shot Video Classification with X-CLIP100 projects using TransformersTransformers is more than a toolkit to use pretrained models: it's a community of projects built around it and theHugging Face Hub. We want Transformers to enable developers, researchers, students, professors, engineers, and anyoneelse to build their dream projects.In order to celebrate the 100,000 stars of transformers, we have decided to put the spotlight on thecommunity, and we have created the awesome-transformers page which lists 100incredible projects built in the vicinity of transformers.If you own or use a project that you believe should be part of the list, please open a PR to add it!If you are looking for custom support from the Hugging Face team    Quick tourTo immediately use a model on a given input (text, image, audio, ...), we provide the pipeline API. Pipelines group together a pretrained model with the preprocessing that was used during that model's training. Here is how to quickly use a pipeline to classify positive versus negative texts:>>> from transformers import pipeline# Allocate a pipeline for sentiment-analysis>>> classifier = pipeline('sentiment-analysis')>>> classifier('We are very happy to introduce pipeline to the transformers repository.')[{'label': 'POSITIVE', 'score': 0.9996980428695679}]The second line of code downloads and caches the pretrained model used by the pipeline, while the third evaluates it on the given text. Here the answer is \""positive\"" with a confidence of 99.97%.Many tasks have a pre-trained pipeline ready to go, in NLP but also in computer vision and speech. For example, we can easily extract detected objects in an image:>>> import requests>>> from PIL import Image>>> from transformers import pipeline# Download an image with cute cats>>> url = \""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample.png\"">>> image_data = requests.get(url, stream=True).raw>>> image = Image.open(image_data)# Allocate a pipeline for object detection>>> object_detector = pipeline('object-detection')>>> object_detector(image)[{'score': 0.9982201457023621,  'label': 'remote',  'box': {'xmin': 40, 'ymin': 70, 'xmax': 175, 'ymax': 117}}, {'score': 0.9960021376609802,  'label': 'remote',  'box': {'xmin': 333, 'ymin': 72, 'xmax': 368, 'ymax': 187}}, {'score': 0.9954745173454285,  'label': 'couch',  'box': {'xmin': 0, 'ymin': 1, 'xmax': 639, 'ymax': 473}}, {'score': 0.9988006353378296,  'label': 'cat',  'box': {'xmin': 13, 'ymin': 52, 'xmax': 314, 'ymax': 470}}, {'score': 0.9986783862113953,  'label': 'cat',  'box': {'xmin': 345, 'ymin': 23, 'xmax': 640, 'ymax': 368}}]Here we get a list of objects detected in the image, with a box surrounding the object and a confidence score. Here is the original image on the left, with the predictions displayed on the right:        You can learn more about the tasks supported by the pipeline API in this tutorial.In addition to pipeline, to download and use any of the pretrained models on your given task, all it takes is three lines of code. Here is the PyTorch version:>>> from transformers import AutoTokenizer, AutoModel>>> tokenizer = AutoTokenizer.from_pretrained(\""bert-base-uncased\"")>>> model = AutoModel.from_pretrained(\""bert-base-uncased\"")>>> inputs = tokenizer(\""Hello world!\"", return_tensors=\""pt\"")>>> outputs = model(**inputs)And here is the equivalent code for TensorFlow:>>> from transformers import AutoTokenizer, TFAutoModel>>> tokenizer = AutoTokenizer.from_pretrained(\""bert-base-uncased\"")>>> model = TFAutoModel.from_pretrained(\""bert-base-uncased\"")>>> inputs = tokenizer(\""Hello world!\"", return_tensors=\""tf\"")>>> outputs = model(**inputs)The tokenizer is responsible for all the preprocessing the pretrained model expects, and can be called directly on a single string (as in the above examples) or a list. It will output a dictionary that you can use in downstream code or simply directly pass to your model using the ** argument unpacking operator.The model itself is a regular Pytorch nn.Module or a TensorFlow tf.keras.Model (depending on your backend) which you can use as usual. This tutorial explains how to integrate such a model into a classic PyTorch or TensorFlow training loop, or how to use our Trainer API to quickly fine-tune on a new dataset.Why should I use transformers?Easy-to-use state-of-the-art models:High performance on natural language understanding & generation, computer vision, and audio tasks.Low barrier to entry for educators and practitioners.Few user-facing abstractions with just three classes to learn.A unified API for using all our pretrained models.Lower compute costs, smaller carbon footprint:Researchers can share trained models instead of always retraining.Practitioners can reduce compute time and production costs.Dozens of architectures with over 60,000 pretrained models across all modalities.Choose the right framework for every part of a model's lifetime:Train state-of-the-art models in 3 lines of code.Move a single model between TF2.0/PyTorch/JAX frameworks at will.Seamlessly pick the right framework for training, evaluation and production.Easily customize a model or an example to your needs:We provide examples for each architecture to reproduce the results published by its original authors.Model internals are exposed as consistently as possible.Model files can be used independently of the library for quick experiments.Why shouldn't I use transformers?This library is not a modular toolbox of building blocks for neural nets. The code in the model files is not refactored with additional abstractions on purpose, so that researchers can quickly iterate on each of the models without diving into additional abstractions/files.The training API is not intended to work on any model but is optimized to work with the models provided by the library. For generic machine learning loops, you should use another library (possibly, Accelerate).While we strive to present as many use cases as possible, the scripts in our examples folder are just that: examples. It is expected that they won't work out-of-the box on your specific problem and that you will be required to change a few lines of code to adapt them to your needs.InstallationWith pipThis repository is tested on Python 3.8+, Flax 0.4.1+, PyTorch 1.10+ and TensorFlow 2.6+.You should install 🤗 Transformers in a virtual environment. If you're unfamiliar with Python virtual environments, check out the user guide.First, create a virtual environment with the version of Python you're going to use and activate it.Then, you will need to install at least one of Flax, PyTorch or TensorFlow.Please refer to TensorFlow installation page, PyTorch installation page and/or Flax and Jax installation pages regarding the specific installation command for your platform.When one of those backends has been installed, 🤗 Transformers can be installed using pip as follows:pip install transformersIf you'd like to play with the examples or need the bleeding edge of the code and can't wait for a new release, you must install the library from source.With condaSince Transformers version v4.0.0, we now have a conda channel: huggingface.🤗 Transformers can be installed using conda as follows:conda install -c huggingface transformersFollow the installation pages of Flax, PyTorch or TensorFlow to see how to install them with conda.NOTE:  On Windows, you may be prompted to activate Developer Mode in order to benefit from caching. If this is not an option for you, please let us know in this issue.Model architecturesAll the model checkpoints provided by 🤗 Transformers are seamlessly integrated from the huggingface.co model hub where they are uploaded directly by users and organizations.Current number of checkpoints: 🤗 Transformers currently provides the following architectures (see here for a high-level summary of each them):ALBERT (from Google Research and the Toyota Technological Institute at Chicago) released with the paper ALBERT: A Lite BERT for Self-supervised Learning of Language Representations, by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut.ALIGN (from Google Research) released with the paper Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision by Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig.AltCLIP (from BAAI) released with the paper AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities by Chen, Zhongzhi and Liu, Guang and Zhang, Bo-Wen and Ye, Fulong and Yang, Qinghong and Wu, Ledell.Audio Spectrogram Transformer (from MIT) released with the paper AST: Audio Spectrogram Transformer by Yuan Gong, Yu-An Chung, James Glass.Autoformer (from Tsinghua University) released with the paper Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting by Haixu Wu, Jiehui Xu, Jianmin Wang, Mingsheng Long.Bark (from Suno) released in the repository suno-ai/bark by Suno AI team.BART (from Facebook) released with the paper BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension by Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov and Luke Zettlemoyer.BARThez (from École polytechnique) released with the paper BARThez: a Skilled Pretrained French Sequence-to-Sequence Model by Moussa Kamal Eddine, Antoine J.-P. Tixier, Michalis Vazirgiannis.BARTpho (from VinAI Research) released with the paper BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese by Nguyen Luong Tran, Duong Minh Le and Dat Quoc Nguyen.BEiT (from Microsoft) released with the paper BEiT: BERT Pre-Training of Image Transformers by Hangbo Bao, Li Dong, Furu Wei.BERT (from Google) released with the paper BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.BERT For Sequence Generation (from Google) released with the paper Leveraging Pre-trained Checkpoints for Sequence Generation Tasks by Sascha Rothe, Shashi Narayan, Aliaksei Severyn.BERTweet (from VinAI Research) released with the paper BERTweet: A pre-trained language model for English Tweets by Dat Quoc Nguyen, Thanh Vu and Anh Tuan Nguyen.BigBird-Pegasus (from Google Research) released with the paper Big Bird: Transformers for Longer Sequences by Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.BigBird-RoBERTa (from Google Research) released with the paper Big Bird: Transformers for Longer Sequences by Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.BioGpt (from Microsoft Research AI4Science) released with the paper BioGPT: generative pre-trained transformer for biomedical text generation and mining by Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon and Tie-Yan Liu.BiT (from Google AI) released with the paper Big Transfer (BiT): General Visual Representation Learning by Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, Neil Houlsby.Blenderbot (from Facebook) released with the paper Recipes for building an open-domain chatbot by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.BlenderbotSmall (from Facebook) released with the paper Recipes for building an open-domain chatbot by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.BLIP (from Salesforce) released with the paper BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation by Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi.BLIP-2 (from Salesforce) released with the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi.BLOOM (from BigScience workshop) released by the BigScience Workshop.BORT (from Alexa) released with the paper Optimal Subarchitecture Extraction For BERT by Adrian de Wynter and Daniel J. Perry.BridgeTower (from Harbin Institute of Technology/Microsoft Research Asia/Intel Labs) released with the paper BridgeTower: Building Bridges Between Encoders in Vision-Language Representation Learning by Xiao Xu, Chenfei Wu, Shachar Rosenman, Vasudev Lal, Wanxiang Che, Nan Duan.ByT5 (from Google Research) released with the paper ByT5: Towards a token-free future with pre-trained byte-to-byte models by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel.CamemBERT (from Inria/Facebook/Sorbonne) released with the paper CamemBERT: a Tasty French Language Model by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Suárez*, Yoann Dupont, Laurent Romary, Éric Villemonte de la Clergerie, Djamé Seddah and Benoît Sagot.CANINE (from Google Research) released with the paper CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation by Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting.Chinese-CLIP (from OFA-Sys) released with the paper Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese by An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, Chang Zhou.CLAP (from LAION-AI) released with the paper Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation by Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, Shlomo Dubnov.CLIP (from OpenAI) released with the paper Learning Transferable Visual Models From Natural Language Supervision by Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever.CLIPSeg (from University of Göttingen) released with the paper Image Segmentation Using Text and Image Prompts by Timo Lüddecke and Alexander Ecker.CodeGen (from Salesforce) released with the paper A Conversational Paradigm for Program Synthesis by Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong.Conditional DETR (from Microsoft Research Asia) released with the paper Conditional DETR for Fast Training Convergence by Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng, Houqiang Li, Yuhui Yuan, Lei Sun, Jingdong Wang.ConvBERT (from YituTech) released with the paper ConvBERT: Improving BERT with Span-based Dynamic Convolution by Zihang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, Shuicheng Yan.ConvNeXT (from Facebook AI) released with the paper A ConvNet for the 2020s by Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie.ConvNeXTV2 (from Facebook AI) released with the paper ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders by Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, Saining Xie.CPM (from Tsinghua University) released with the paper CPM: A Large-scale Generative Chinese Pre-trained Language Model by Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin, Yusheng Su, Haozhe Ji, Jian Guan, Fanchao Qi, Xiaozhi Wang, Yanan Zheng, Guoyang Zeng, Huanqi Cao, Shengqi Chen, Daixuan Li, Zhenbo Sun, Zhiyuan Liu, Minlie Huang, Wentao Han, Jie Tang, Juanzi Li, Xiaoyan Zhu, Maosong Sun.CPM-Ant (from OpenBMB) released by the OpenBMB.CTRL (from Salesforce) released with the paper CTRL: A Conditional Transformer Language Model for Controllable Generation by Nitish Shirish Keskar*, Bryan McCann*, Lav R. Varshney, Caiming Xiong and Richard Socher.CvT (from Microsoft) released with the paper CvT: Introducing Convolutions to Vision Transformers by Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, Lei Zhang.Data2Vec (from Facebook) released with the paper Data2Vec:  A General Framework for Self-supervised Learning in Speech, Vision and Language by Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, Michael Auli.DeBERTa (from Microsoft) released with the paper DeBERTa: Decoding-enhanced BERT with Disentangled Attention by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.DeBERTa-v2 (from Microsoft) released with the paper DeBERTa: Decoding-enhanced BERT with Disentangled Attention by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.Decision Transformer (from Berkeley/Facebook/Google) released with the paper Decision Transformer: Reinforcement Learning via Sequence Modeling by Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch.Deformable DETR (from SenseTime Research) released with the paper Deformable DETR: Deformable Transformers for End-to-End Object Detection by Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai.DeiT (from Facebook) released with the paper Training data-efficient image transformers & distillation through attention by Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Hervé Jégou.DePlot (from Google AI) released with the paper DePlot: One-shot visual language reasoning by plot-to-table translation by Fangyu Liu, Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Wenhu Chen, Nigel Collier, Yasemin Altun.DETA (from The University of Texas at Austin) released with the paper NMS Strikes Back by Jeffrey Ouyang-Zhang, Jang Hyun Cho, Xingyi Zhou, Philipp Krähenbühl.DETR (from Facebook) released with the paper End-to-End Object Detection with Transformers by Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko.DialoGPT (from Microsoft Research) released with the paper DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation by Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan.DiNAT (from SHI Labs) released with the paper Dilated Neighborhood Attention Transformer by Ali Hassani and Humphrey Shi.DINOv2 (from Meta AI) released with the paper DINOv2: Learning Robust Visual Features without Supervision by Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, Piotr Bojanowski.DistilBERT (from HuggingFace), released together with the paper DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2 into DistilGPT2, RoBERTa into DistilRoBERTa, Multilingual BERT into DistilmBERT and a German version of DistilBERT.DiT (from Microsoft Research) released with the paper DiT: Self-supervised Pre-training for Document Image Transformer by Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, Furu Wei.Donut (from NAVER), released together with the paper OCR-free Document Understanding Transformer by Geewook Kim, Teakgyu Hong, Moonbin Yim, Jeongyeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, Seunghyun Park.DPR (from Facebook) released with the paper Dense Passage Retrieval for Open-Domain Question Answering by Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih.DPT (from Intel Labs) released with the paper Vision Transformers for Dense Prediction by René Ranftl, Alexey Bochkovskiy, Vladlen Koltun.EfficientFormer (from Snap Research) released with the paper EfficientFormer: Vision Transformers at MobileNetSpeed by Yanyu Li, Geng Yuan, Yang Wen, Ju Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, Jian Ren.EfficientNet (from Google Brain) released with the paper EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks by Mingxing Tan, Quoc V. Le.ELECTRA (from Google Research/Stanford University) released with the paper ELECTRA: Pre-training text encoders as discriminators rather than generators by Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning.EnCodec (from Meta AI) released with the paper High Fidelity Neural Audio Compression by Alexandre Défossez, Jade Copet, Gabriel Synnaeve, Yossi Adi.EncoderDecoder (from Google Research) released with the paper Leveraging Pre-trained Checkpoints for Sequence Generation Tasks by Sascha Rothe, Shashi Narayan, Aliaksei Severyn.ERNIE (from Baidu) released with the paper ERNIE: Enhanced Representation through Knowledge Integration by Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, Hua Wu.ErnieM (from Baidu) released with the paper ERNIE-M: Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora by Xuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang.ESM (from Meta AI) are transformer protein language models.  ESM-1b was released with the paper Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences by Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, and Rob Fergus. ESM-1v was released with the paper Language models enable zero-shot prediction of the effects of mutations on protein function by Joshua Meier, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu and Alexander Rives. ESM-2 and ESMFold were released with the paper Language models of protein sequences at the scale of evolution enable accurate structure prediction by Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Sal Candido, Alexander Rives.Falcon (from Technology Innovation Institute) by Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme.FLAN-T5 (from Google AI) released in the repository google-research/t5x by Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason WeiFLAN-UL2 (from Google AI) released in the repository google-research/t5x by Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason WeiFlauBERT (from CNRS) released with the paper FlauBERT: Unsupervised Language Model Pre-training for French by Hang Le, Loïc Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen, Benoît Crabbé, Laurent Besacier, Didier Schwab.FLAVA (from Facebook AI) released with the paper FLAVA: A Foundational Language And Vision Alignment Model by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela.FNet (from Google Research) released with the paper FNet: Mixing Tokens with Fourier Transforms by James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon.FocalNet (from Microsoft Research) released with the paper Focal Modulation Networks by Jianwei Yang, Chunyuan Li, Xiyang Dai, Lu Yuan, Jianfeng Gao.Funnel Transformer (from CMU/Google Brain) released with the paper Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le.GIT (from Microsoft Research) released with the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, Lijuan Wang.GLPN (from KAIST) released with the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Doyeon Kim, Woonghyun Ga, Pyungwhan Ahn, Donggyu Joo, Sehwan Chun, Junmo Kim.GPT (from OpenAI) released with the paper Improving Language Understanding by Generative Pre-Training by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever.GPT Neo (from EleutherAI) released in the repository EleutherAI/gpt-neo by Sid Black, Stella Biderman, Leo Gao, Phil Wang and Connor Leahy.GPT NeoX (from EleutherAI) released with the paper GPT-NeoX-20B: An Open-Source Autoregressive Language Model by Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, Samuel WeinbachGPT NeoX Japanese (from ABEJA) released by Shinya Otani, Takayoshi Makabe, Anuj Arora, and Kyo Hattori.GPT-2 (from OpenAI) released with the paper Language Models are Unsupervised Multitask Learners by Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei** and Ilya Sutskever**.GPT-J (from EleutherAI) released in the repository kingoflolz/mesh-transformer-jax by Ben Wang and Aran Komatsuzaki.GPT-Sw3 (from AI-Sweden) released with the paper Lessons Learned from GPT-SW3: Building the First Large-Scale Generative Language Model for Swedish by Ariel Ekgren, Amaru Cuba Gyllensten, Evangelia Gogoulou, Alice Heiman, Severine Verlinden, Joey Öhman, Fredrik Carlsson, Magnus Sahlgren.GPTBigCode (from BigCode) released with the paper SantaCoder: don't reach for the stars! by Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, Joel Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco De Toni, Bernardo García del Río, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries, Leandro von Werra.GPTSAN-japanese released in the repository tanreinama/GPTSAN by Toshiyuki Sakamoto(tanreinama).Graphormer (from Microsoft) released with the paper Do Transformers Really Perform Bad for Graph Representation? by Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, Tie-Yan Liu.GroupViT (from UCSD, NVIDIA) released with the paper GroupViT: Semantic Segmentation Emerges from Text Supervision by Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, Xiaolong Wang.Hubert (from Facebook) released with the paper HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed.I-BERT (from Berkeley) released with the paper I-BERT: Integer-only BERT Quantization by Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W. Mahoney, Kurt Keutzer.ImageGPT (from OpenAI) released with the paper Generative Pretraining from Pixels by Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, Ilya Sutskever.Informer (from Beihang University, UC Berkeley, Rutgers University, SEDD Company) released with the paper Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting by Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang.InstructBLIP (from Salesforce) released with the paper InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning by Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi.Jukebox (from OpenAI) released with the paper Jukebox: A Generative Model for Music by Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, Ilya Sutskever.LayoutLM (from Microsoft Research Asia) released with the paper LayoutLM: Pre-training of Text and Layout for Document Image Understanding by Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou.LayoutLMv2 (from Microsoft Research Asia) released with the paper LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding by Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, Lidong Zhou.LayoutLMv3 (from Microsoft Research Asia) released with the paper LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking by Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, Furu Wei.LayoutXLM (from Microsoft Research Asia) released with the paper LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding by Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Furu Wei.LED (from AllenAI) released with the paper Longformer: The Long-Document Transformer by Iz Beltagy, Matthew E. Peters, Arman Cohan.LeViT (from Meta AI) released with the paper LeViT: A Vision Transformer in ConvNet's Clothing for Faster Inference by Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Hervé Jégou, Matthijs Douze.LiLT (from South China University of Technology) released with the paper LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding by Jiapeng Wang, Lianwen Jin, Kai Ding.LLaMA (from The FAIR team of Meta AI) released with the paper LLaMA: Open and Efficient Foundation Language Models by Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample.Llama2 (from The FAIR team of Meta AI) released with the paper Llama2: Open Foundation and Fine-Tuned Chat Models by Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushka rMishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing EllenTan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom.Longformer (from AllenAI) released with the paper Longformer: The Long-Document Transformer by Iz Beltagy, Matthew E. Peters, Arman Cohan.LongT5 (from Google AI) released with the paper LongT5: Efficient Text-To-Text Transformer for Long Sequences by Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, Yinfei Yang.LUKE (from Studio Ousia) released with the paper LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention by Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, Yuji Matsumoto.LXMERT (from UNC Chapel Hill) released with the paper LXMERT: Learning Cross-Modality Encoder Representations from Transformers for Open-Domain Question Answering by Hao Tan and Mohit Bansal.M-CTC-T (from Facebook) released with the paper Pseudo-Labeling For Massively Multilingual Speech Recognition by Loren Lugosch, Tatiana Likhomanenko, Gabriel Synnaeve, and Ronan Collobert.M2M100 (from Facebook) released with the paper Beyond English-Centric Multilingual Machine Translation by Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, Armand Joulin.MarianMT Machine translation models trained using OPUS data by Jörg Tiedemann. The Marian Framework is being developed by the Microsoft Translator Team.MarkupLM (from Microsoft Research Asia) released with the paper MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document Understanding by Junlong Li, Yiheng Xu, Lei Cui, Furu Wei.Mask2Former (from FAIR and UIUC) released with the paper Masked-attention Mask Transformer for Universal Image Segmentation by Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, Rohit Girdhar.MaskFormer (from Meta and UIUC) released with the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation by Bowen Cheng, Alexander G. Schwing, Alexander Kirillov.MatCha (from Google AI) released with the paper MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering by Fangyu Liu, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Yasemin Altun, Nigel Collier, Julian Martin Eisenschlos.mBART (from Facebook) released with the paper Multilingual Denoising Pre-training for Neural Machine Translation by Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, Luke Zettlemoyer.mBART-50 (from Facebook) released with the paper Multilingual Translation with Extensible Multilingual Pretraining and Finetuning by Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, Angela Fan.MEGA (from Meta/USC/CMU/SJTU) released with the paper Mega: Moving Average Equipped Gated Attention by Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer.Megatron-BERT (from NVIDIA) released with the paper Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper and Bryan Catanzaro.Megatron-GPT2 (from NVIDIA) released with the paper Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper and Bryan Catanzaro.MGP-STR (from Alibaba Research) released with the paper Multi-Granularity Prediction for Scene Text Recognition by Peng Wang, Cheng Da, and Cong Yao.mLUKE (from Studio Ousia) released with the paper mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models by Ryokan Ri, Ikuya Yamada, and Yoshimasa Tsuruoka.MMS (from Facebook) released with the paper Scaling Speech Technology to 1,000+ Languages by Vineel Pratap, Andros Tjandra, Bowen Shi, Paden Tomasello, Arun Babu, Sayani Kundu, Ali Elkahky, Zhaoheng Ni, Apoorv Vyas, Maryam Fazel-Zarandi, Alexei Baevski, Yossi Adi, Xiaohui Zhang, Wei-Ning Hsu, Alexis Conneau, Michael Auli.MobileBERT (from CMU/Google Brain) released with the paper MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices by Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou.MobileNetV1 (from Google Inc.) released with the paper MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications by Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam.MobileNetV2 (from Google Inc.) released with the paper MobileNetV2: Inverted Residuals and Linear Bottlenecks by Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen.MobileViT (from Apple) released with the paper MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer by Sachin Mehta and Mohammad Rastegari.MobileViTV2 (from Apple) released with the paper Separable Self-attention for Mobile Vision Transformers by Sachin Mehta and Mohammad Rastegari.MPNet (from Microsoft Research) released with the paper MPNet: Masked and Permuted Pre-training for Language Understanding by Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu.MPT (from MosaiML) released with the repository llm-foundry by the MosaicML NLP Team.MRA (from the University of Wisconsin - Madison) released with the paper Multi Resolution Analysis (MRA) for Approximate Self-Attention by Zhanpeng Zeng, Sourav Pal, Jeffery Kline, Glenn M Fung, Vikas Singh.MT5 (from Google AI) released with the paper mT5: A massively multilingual pre-trained text-to-text transformer by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel.MusicGen (from Meta) released with the paper Simple and Controllable Music Generation by Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi and Alexandre Défossez.MVP (from RUC AI Box) released with the paper MVP: Multi-task Supervised Pre-training for Natural Language Generation by Tianyi Tang, Junyi Li, Wayne Xin Zhao and Ji-Rong Wen.NAT (from SHI Labs) released with the paper Neighborhood Attention Transformer by Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and Humphrey Shi.Nezha (from Huawei Noah’s Ark Lab) released with the paper NEZHA: Neural Contextualized Representation for Chinese Language Understanding by Junqiu Wei, Xiaozhe Ren, Xiaoguang Li, Wenyong Huang, Yi Liao, Yasheng Wang, Jiashu Lin, Xin Jiang, Xiao Chen and Qun Liu.NLLB (from Meta) released with the paper No Language Left Behind: Scaling Human-Centered Machine Translation by the NLLB team.NLLB-MOE (from Meta) released with the paper No Language Left Behind: Scaling Human-Centered Machine Translation by the NLLB team.Nyströmformer (from the University of Wisconsin - Madison) released with the paper Nyströmformer: A Nyström-Based Algorithm for Approximating Self-Attention by Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, Vikas Singh.OneFormer (from SHI Labs) released with the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov, Humphrey Shi.OpenLlama (from s-JoL) released in Open-Llama.OPT (from Meta AI) released with the paper OPT: Open Pre-trained Transformer Language Models by Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen et al.OWL-ViT (from Google AI) released with the paper Simple Open-Vocabulary Object Detection with Vision Transformers by Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby.Pegasus (from Google) released with the paper PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu.PEGASUS-X (from Google) released with the paper Investigating Efficiently Extending Transformers for Long Input Summarization by Jason Phang, Yao Zhao, and Peter J. Liu.Perceiver IO (from Deepmind) released with the paper Perceiver IO: A General Architecture for Structured Inputs & Outputs by Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier Hénaff, Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, João Carreira.PhoBERT (from VinAI Research) released with the paper PhoBERT: Pre-trained language models for Vietnamese by Dat Quoc Nguyen and Anh Tuan Nguyen.Pix2Struct (from Google) released with the paper Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding by Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, Kristina Toutanova.PLBart (from UCLA NLP) released with the paper Unified Pre-training for Program Understanding and Generation by Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, Kai-Wei Chang.PoolFormer (from Sea AI Labs) released with the paper MetaFormer is Actually What You Need for Vision by Yu, Weihao and Luo, Mi and Zhou, Pan and Si, Chenyang and Zhou, Yichen and Wang, Xinchao and Feng, Jiashi and Yan, Shuicheng.ProphetNet (from Microsoft Research) released with the paper ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang and Ming Zhou.PVT (from Nanjing University, The University of Hong Kong etc.) released with the paper Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions by Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao.QDQBert (from NVIDIA) released with the paper Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation by Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev and Paulius Micikevicius.RAG (from Facebook) released with the paper Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela.REALM (from Google Research) released with the paper REALM: Retrieval-Augmented Language Model Pre-Training by Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat and Ming-Wei Chang.Reformer (from Google Research) released with the paper Reformer: The Efficient Transformer by Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya.RegNet (from META Platforms) released with the paper Designing Network Design Space by Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, Piotr Dollár.RemBERT (from Google Research) released with the paper Rethinking embedding coupling in pre-trained language models by Hyung Won Chung, Thibault Févry, Henry Tsai, M. Johnson, Sebastian Ruder.ResNet (from Microsoft Research) released with the paper Deep Residual Learning for Image Recognition by Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun.RoBERTa (from Facebook), released together with the paper RoBERTa: A Robustly Optimized BERT Pretraining Approach by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.RoBERTa-PreLayerNorm (from Facebook) released with the paper fairseq: A Fast, Extensible Toolkit for Sequence Modeling by Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli.RoCBert (from WeChatAI) released with the paper RoCBert: Robust Chinese Bert with Multimodal Contrastive Pretraining by HuiSu, WeiweiShi, XiaoyuShen, XiaoZhou, TuoJi, JiaruiFang, JieZhou.RoFormer (from ZhuiyiTechnology), released together with the paper RoFormer: Enhanced Transformer with Rotary Position Embedding by Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu.RWKV (from Bo Peng), released on this repo by Bo Peng.SegFormer (from NVIDIA) released with the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping Luo.Segment Anything (from Meta AI) released with the paper Segment Anything by Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alex Berg, Wan-Yen Lo, Piotr Dollar, Ross Girshick.SEW (from ASAPP) released with the paper Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition by Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.SEW-D (from ASAPP) released with the paper Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition by Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.SpeechT5 (from Microsoft Research) released with the paper SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing by Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei.SpeechToTextTransformer (from Facebook), released together with the paper fairseq S2T: Fast Speech-to-Text Modeling with fairseq by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Dmytro Okhonko, Juan Pino.SpeechToTextTransformer2 (from Facebook), released together with the paper Large-Scale Self- and Semi-Supervised Learning for Speech Translation by Changhan Wang, Anne Wu, Juan Pino, Alexei Baevski, Michael Auli, Alexis Conneau.Splinter (from Tel Aviv University), released together with the paper Few-Shot Question Answering by Pretraining Span Selection by Ori Ram, Yuval Kirstain, Jonathan Berant, Amir Globerson, Omer Levy.SqueezeBERT (from Berkeley) released with the paper SqueezeBERT: What can computer vision teach NLP about efficient neural networks? by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer.SwiftFormer (from MBZUAI) released with the paper SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications by Abdelrahman Shaker, Muhammad Maaz, Hanoona Rasheed, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan.Swin Transformer (from Microsoft) released with the paper Swin Transformer: Hierarchical Vision Transformer using Shifted Windows by Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo.Swin Transformer V2 (from Microsoft) released with the paper Swin Transformer V2: Scaling Up Capacity and Resolution by Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, Baining Guo.Swin2SR (from University of Würzburg) released with the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Marcos V. Conde, Ui-Jin Choi, Maxime Burchi, Radu Timofte.SwitchTransformers (from Google) released with the paper Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity by William Fedus, Barret Zoph, Noam Shazeer.T5 (from Google AI) released with the paper Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu.T5v1.1 (from Google AI) released in the repository google-research/text-to-text-transfer-transformer by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu.Table Transformer (from Microsoft Research) released with the paper PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents by Brandon Smock, Rohith Pesala, Robin Abraham.TAPAS (from Google AI) released with the paper TAPAS: Weakly Supervised Table Parsing via Pre-training by Jonathan Herzig, Paweł Krzysztof Nowak, Thomas Müller, Francesco Piccinno and Julian Martin Eisenschlos.TAPEX (from Microsoft Research) released with the paper TAPEX: Table Pre-training via Learning a Neural SQL Executor by Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou.Time Series Transformer (from HuggingFace).TimeSformer (from Facebook) released with the paper Is Space-Time Attention All You Need for Video Understanding? by Gedas Bertasius, Heng Wang, Lorenzo Torresani.Trajectory Transformer (from the University of California at Berkeley) released with the paper Offline Reinforcement Learning as One Big Sequence Modeling Problem by Michael Janner, Qiyang Li, Sergey LevineTransformer-XL (from Google/CMU) released with the paper Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context by Zihang Dai*, Zhilin Yang*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.TrOCR (from Microsoft), released together with the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, Furu Wei.TVLT (from UNC Chapel Hill) released with the paper TVLT: Textless Vision-Language Transformer by Zineng Tang, Jaemin Cho, Yixin Nie, Mohit Bansal.UL2 (from Google Research) released with the paper Unifying Language Learning Paradigms by Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, Donald MetzlerUMT5 (from Google Research) released with the paper UniMax: Fairer and More Effective Language Sampling for Large-Scale Multilingual Pretraining by Hyung Won Chung, Xavier Garcia, Adam Roberts, Yi Tay, Orhan Firat, Sharan Narang, Noah Constant.UniSpeech (from Microsoft Research) released with the paper UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael Zeng, Xuedong Huang.UniSpeechSat (from Microsoft Research) released with the paper UNISPEECH-SAT: UNIVERSAL SPEECH REPRESENTATION LEARNING WITH SPEAKER AWARE PRE-TRAINING by Sanyuan Chen, Yu Wu, Chengyi Wang, Zhengyang Chen, Zhuo Chen, Shujie Liu, Jian Wu, Yao Qian, Furu Wei, Jinyu Li, Xiangzhan Yu.UPerNet (from Peking University) released with the paper Unified Perceptual Parsing for Scene Understanding by Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, Jian Sun.VAN (from Tsinghua University and Nankai University) released with the paper Visual Attention Network by Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, Shi-Min Hu.VideoMAE (from Multimedia Computing Group, Nanjing University) released with the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Zhan Tong, Yibing Song, Jue Wang, Limin Wang.ViLT (from NAVER AI Lab/Kakao Enterprise/Kakao Brain) released with the paper ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision by Wonjae Kim, Bokyung Son, Ildoo Kim.Vision Transformer (ViT) (from Google AI) released with the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.VisualBERT (from UCLA NLP) released with the paper VisualBERT: A Simple and Performant Baseline for Vision and Language by Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang.ViT Hybrid (from Google AI) released with the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.ViTMAE (from Meta AI) released with the paper Masked Autoencoders Are Scalable Vision Learners by Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick.ViTMSN (from Meta AI) released with the paper Masked Siamese Networks for Label-Efficient Learning by Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes, Pascal Vincent, Armand Joulin, Michael Rabbat, Nicolas Ballas.ViViT (from Google Research) released with the paper ViViT: A Video Vision Transformer by Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lučić, Cordelia Schmid.Wav2Vec2 (from Facebook AI) released with the paper wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.Wav2Vec2-Conformer (from Facebook AI) released with the paper FAIRSEQ S2T: Fast Speech-to-Text Modeling with FAIRSEQ by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Sravya Popuri, Dmytro Okhonko, Juan Pino.Wav2Vec2Phoneme (from Facebook AI) released with the paper Simple and Effective Zero-shot Cross-lingual Phoneme Recognition by Qiantong Xu, Alexei Baevski, Michael Auli.WavLM (from Microsoft Research) released with the paper WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing by Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Furu Wei.Whisper (from OpenAI) released with the paper Robust Speech Recognition via Large-Scale Weak Supervision by Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever.X-CLIP (from Microsoft Research) released with the paper Expanding Language-Image Pretrained Models for General Video Recognition by Bolin Ni, Houwen Peng, Minghao Chen, Songyang Zhang, Gaofeng Meng, Jianlong Fu, Shiming Xiang, Haibin Ling.X-MOD (from Meta AI) released with the paper Lifting the Curse of Multilinguality by Pre-training Modular Transformers by Jonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James Cross, Sebastian Riedel, Mikel Artetxe.XGLM (From Facebook AI) released with the paper Few-shot Learning with Multilingual Language Models by Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, Xian Li.XLM (from Facebook) released together with the paper Cross-lingual Language Model Pretraining by Guillaume Lample and Alexis Conneau.XLM-ProphetNet (from Microsoft Research) released with the paper ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang and Ming Zhou.XLM-RoBERTa (from Facebook AI), released together with the paper Unsupervised Cross-lingual Representation Learning at Scale by Alexis Conneau*, Kartikay Khandelwal*, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov.XLM-RoBERTa-XL (from Facebook AI), released together with the paper Larger-Scale Transformers for Multilingual Masked Language Modeling by Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, Alexis Conneau.XLM-V (from Meta AI) released with the paper XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models by Davis Liang, Hila Gonen, Yuning Mao, Rui Hou, Naman Goyal, Marjan Ghazvininejad, Luke Zettlemoyer, Madian Khabsa.XLNet (from Google/CMU) released with the paper ​XLNet: Generalized Autoregressive Pretraining for Language Understanding by Zhilin Yang*, Zihang Dai*, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.XLS-R (from Facebook AI) released with the paper XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale by Arun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman Goyal, Kritika Singh, Patrick von Platen, Yatharth Saraf, Juan Pino, Alexei Baevski, Alexis Conneau, Michael Auli.XLSR-Wav2Vec2 (from Facebook AI) released with the paper Unsupervised Cross-Lingual Representation Learning For Speech Recognition by Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, Michael Auli.YOLOS (from Huazhong University of Science & Technology) released with the paper You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Yuxin Fang, Bencheng Liao, Xinggang Wang, Jiemin Fang, Jiyang Qi, Rui Wu, Jianwei Niu, Wenyu Liu.YOSO (from the University of Wisconsin - Madison) released with the paper You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling by Zhanpeng Zeng, Yunyang Xiong, Sathya N. Ravi, Shailesh Acharya, Glenn Fung, Vikas Singh.Want to contribute a new model? We have added a detailed guide and templates to guide you in the process of adding a new model. You can find them in the templates folder of the repository. Be sure to check the contributing guidelines and contact the maintainers or open an issue to collect feedbacks before starting your PR.To check if each model has an implementation in Flax, PyTorch or TensorFlow, or has an associated tokenizer backed by the 🤗 Tokenizers library, refer to this table.These implementations have been tested on several datasets (see the example scripts) and should match the performance of the original implementations. You can find more details on performance in the Examples section of the documentation.Learn moreSectionDescriptionDocumentationFull API documentation and tutorialsTask summaryTasks supported by 🤗 TransformersPreprocessing tutorialUsing the Tokenizer class to prepare data for the modelsTraining and fine-tuningUsing the models provided by 🤗 Transformers in a PyTorch/TensorFlow training loop and the Trainer APIQuick tour: Fine-tuning/usage scriptsExample scripts for fine-tuning models on a wide range of tasksModel sharing and uploadingUpload and share your fine-tuned models with the communityCitationWe now have a paper you can cite for the 🤗 Transformers library:@inproceedings{wolf-etal-2020-transformers,    title = \""Transformers: State-of-the-Art Natural Language Processing\"",    author = \""Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush\"",    booktitle = \""Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\"",    month = oct,    year = \""2020\"",    address = \""Online\"",    publisher = \""Association for Computational Linguistics\"",    url = \""https://www.aclweb.org/anthology/2020.emnlp-demos.6\"",    pages = \""38--45\""}"
63,shadowsocks/shadowsocks,https://github.com/shadowsocks/shadowsocks/blob/rm/README.md,Python,Removed according to regulations.
64,zero-to-mastery/start-here-guidelines,https://github.com/zero-to-mastery/start-here-guidelines/blob/master/README.md,Python,"One rule of this community:We don't care if you break things. This is a playground, and we encourage failing often. Use this as a practice ground, and enjoy contributing to projects you create with your fellow students. Many students have gained real-world experience \""working in teams\"" by working on these projects.A Guide to Get Started (used to be the 4 step guide)Check out Andrei's videos on github if you haven't watched it already.On the GitHub page for this repository, click on the button \""Fork.\""Clone your forked repository to your computer:For example, run this command inside your terminal:git clone https://github.com/<your-github-username>/start-here-guidelines.gitReplace <your-github-username>!Learn more about forking and cloning a repo.Move to project directory:cd start-here-guidelinesBefore you make any changes, keep your fork in sync to avoid merge conflicts:git remote add upstream https://github.com/zero-to-mastery/start-here-guidelines.gitgit pull upstream masterIf you run into a merge conflict, you have to resolve the conflict. There are a lot of guides online, or you can watch this tutorial.After adding the upstream and checking that all files are up to date, we now will create new branch before editing any files. There are two ways to do so:git checkout -b <branch-name>git branch <branch-name>git switch <branch-name>On your computer, open your text editor, and add your name to the CONTRIBUTORS.md file.⚠️ IMPORTANT NOTE #1: Add your name somewhere in the middle. Not at the top or bottom in order to avoid the chance of you getting a merge conflict!⚠️ IMPORTANT NOTE #2: Please do NOT edit or remove other people from the list, even to fix their indentation etc. This will likely prevent your PR from being merged.Add the changes with git add, git commit (write a good commit message, if possible):git add CONTRIBUTORS.mdgit commit -m \""Add <your-github-username>\""Replace <your-github-username>!Push your changes to your repository:git push origin <branch-name>Go to the GitHub page of your fork, and make a pull request:Read more about pull requests on the GitHub help pages.Wait until Zerobot or one of the maintainers merges your pull request. If there are any conflicts, you will get a notification and be required to resolve the conflict.Go join a project and start contributing or create your own group apps. Don't be shy and enjoy creating things together (We have over 20 projects for all levels of programmers)! Check out this guide for more information on selecting a project.To see the Zero to Mastery Icon in your GitHub profile, follow these steps (you must complete steps 1 and 2 for this to work).Anatomy of an open-source project:Every open-source community is different.Spending years on one open-source project means you’ve gotten to know one open-source project. Move to a different project, and you might find the vocabulary, norms, and communication styles are completely different.That being said, many open-source projects follow a similar organizational structure. Understanding the different community roles and overall process will help you get quickly oriented to any new project.A typical open-source project has the following types of people:Author: The person(s) or organization that created the project.Owner: The person(s) who has administrative ownership over the organization or repository (not always the same as the original author).Maintainers: Contributors who are responsible for driving the vision and managing the organizational aspects of the project (may also be authors or owners of the project).Contributors: Everyone who has contributed something back to the project.Community Members: People who use the project. They might be active in conversations or express their opinion on the project’s direction.Bigger projects may also have subcommittees or working groups focused on different tasks, such as tooling, triage, community moderation, and event organizing. Look on a project’s website for a “team” page or in the repository for governance documentation to find this information.A project also has documentation. These files are usually listed in the top level of a repository.LICENSE: By definition, every open-source project must have an open-source license. If the project does not have a license, it is not open source.README: The README is the instruction manual that welcomes new community members to the project. It explains why the project is useful and how to get started.CONTRIBUTING: Whereas READMEs help people use the project, contributing docs help people contribute to the project. It explains what types of contributions are needed and how the process works. While not every project has a CONTRIBUTING file, its presence signals that this is a welcoming project to contribute to.CODE_OF_CONDUCT: The code of conduct sets ground rules for participants’ behavior and helps to facilitate a friendly, welcoming environment. While not every project has a CODE_OF_CONDUCT file, its presence signals that this is a welcoming project to contribute to.Other documentation: There might be additional documentation such as tutorials, walkthroughs, or governance policies, especially on bigger projects.Finally, open-source projects use the following tools to organize discussion. Reading through the archives will give you a good picture of how the community thinks and works.Issue tracker: Where people discuss issues related to the project.Pull requests: Where people discuss and review changes that are in progress.Discussion forums or mailing lists: Some projects may use these channels for conversational topics (for example, “How do I…“ or “What do you think about…“ instead of bug reports or feature requests). Others use the issue tracker for all conversations.Synchronous chat channel: Some projects use chat channels (such as Discord or IRC) for casual conversation, collaboration, and quick exchanges.Get all the ZTM Courses, for one monthly subscription here."
65,Ebazhanov/linkedin-skill-assessments-quizzes,https://github.com/Ebazhanov/linkedin-skill-assessments-quizzes/blob/main/README.md,Python,"Linkedin Skill assessments - Answers⚠️ DISCLAIMER: The owners of this repository are not liable for any illegal usage of the content provided in this repository. The content is provided for informational and educational purposes only, and any actions taken by users of this repository are the responsibility of the user. By accessing this repository, you agree to hold the owners harmless from any claims, damages, or expenses arising from the use of the information provided.[ Go to see the last contributor ]🙏 PLEASEAlways add explanation (or reference link) to your answers. Use online grammar checker.That would help anyone to better learn new concepts!🎉 AnnouncementsColumn Translation have links to quizzes in different languages like Es, Fr, It and De.If you want to meet each other or discuss quiz related problems or maybe ask for skills endorsement just join the Discord chat.Playground before taking quiz using:MD2Practice (Web App)Skill Assessments Quizzes (Web App)LinkedIn Quizzed with Kodyfire (Terminal)Want to contribute? Here is the source code.❓ Need help?Open new issue🔥 Open in VS Code view here or thereTable of ContentsLinkedin-quiz-questionsPassed/FailedTranslated  in ...QuestionsAnswersYour resource for answers. In case you have doubts please contact this person or add them to review your PR.Accounting❗needs updating  5049@tujinwei, @mervynteo, @johnfelipeAdobe-Acrobat  2722Adobe-Illustrator❗needs updating  7674Adobe-InDesign❗needs updating  4240Adobe-Lightroom❗needs updating  2020Adobe-Photoshop❗needs updating  9393@declarckAdobe Premiere Pro  4836Adobe XD  1613After Effects❗needs updating  2413Agile Methodologies❗needs updating  116116@its0x08Android  7272@surajsahani, @mr-shoxruxbek, @ItSNeverLateAngular  7965@vanekbr, @aamita96ArcGIS Products55AutoCAD❗needs updating  7775@djayorAutodesk Fusion 360❗needs updating  3725@djayor, @tm-sanjayAutodesk Maya3030@marifogluAWS  9999@jokerkeny, @Amsal1AWS-Lambda❗needs updating  5149Bash  7877@D4RIO, @Amsal1C#6161@LiviuSosu, @RamonMartinezNieto, @declarckC++❗needs updating7373@Amsal1, @Amsal1C (Programming Language)8383@makifay, @Amsal1, @its0x08CSS122116@BHARGAVPATEL1244Cybersecurity❗needs updating10196Django7171@PROCW.NET Framework6359@declarckEclipse❗needs updating3628Front-end Development6868@vanekbr, @ShankS3, @declarckGit134134@Emanuele-emGo (Programming Language)4040@ruslanbes, @monkrusGoogle Ads2925Google Analytics8282Google Cloud Platform (GCP)5250@antra0497Hadoop7154HTML129128@declarckIT Operations5454@asahioceanJava130130@sumanas27, @ruslanbes, @PROCWJavascript131131@taletski, @PROCW, @msteiner96, @declarckjQuery8477@declarckJSON❗needs updating8786@iHamzaKhanzadaKeynote140Kotlin7878@ItSNeverLate, @HusseinhjLinux8278@D4RIO, @Amsal1Logic Pro8278Machine Learning9898@aaronwangj, @antra0497MATLAB7070@tm-sanjayMaven5350Microsoft Access3028@drmegalomaniacMicrosoft Azure5553@tomtreffke, @ziasistaniMicrosoft Excel❗needs updating109107@gazihasanrahmanMicrosoft Outlook7956Microsoft Power Automate1402@mervynteoMicrosoft Power BI8180@vittorio-giattiMicrosoft Power Point8577@ckulloMicrosoft Project❗needs updating4443Microsoft Word❗needs updating7877MongoDB7777MySQL9797@ruslanbesnode.js7976@pbachmanNoSQL5655objective-c4038OOP10282@declarck, @gaurovgiriPHP8979@ruslanbes, @msteiner96Pro Tools22Python176176@tik9, @Amsal1, @declarck, @TSG405QuickBooks❗needs updating6739R5252@gregglindReact.js100100@RobTables @bandinoplaREST API6565Revit❗needs updating140Ruby on Rails5959@gudataRust3232@BobbyByrne @Emanuele-emScala5248Search Engine Optimization (SEO)8181SharePoint❗needs updating5338Sketchup22SOLIDWORKS❗needs updating5757@BHARGAVPATEL1244Spring Framework6767Swift6767Transact-SQL (T-SQL)4542@beefydog, @BenVlodgiUnity❗needs updating4746@uno-sebastianVisual Basic for Applications (VBA)❗needs updating3634@AdamKaczor6250Visio3535Windows Server6857WordPress8073@ruslanbes, @Amsal1XML4342@ruslanbesContributors ✨Thanks goes to these wonderful people (emoji key):            Evgenii💻 🖋      Sergei Stadnik💻 🔍 🤔 📖      Santhosh💻      Jacob Dsa💻 🖋      Aaron Meese💻 🖋      arqarq💻 🖋      Amit Yadav💻 🖋              Javokhir Nazarov💻 🖋      saurav kumar🖋      Chetan🖋      Amir Hossein Shekari🎨 🖋 💻      SergDaut🎨      Nilotpal Pramanik🎨 💻 🖋 💼 📖 🔣 💡      Abhishek Kumar🎨              Monu Gupta🎨      KARTIKEYA GUPTA💻 🖋      kenkyusha💻 🖋      juandavidtowers💻 🖋      cyber-netics💻 🖋      jtrisw💻 🖋      Renato Regalado💻 🖋              Matthew💻 🖋      Jan S.💻 🖋      Manoli💻 🖋      Faraz tanveer💻 🖋      mohnishkarri💻 🖋 🎨      andyzhu💻 🖋      Vishal Kushwah💻 🖋              Yurii Yakymenko💻 🖋      Swetabh Suman💻 🖋      AJAY DANDGE💻 🖋      Mehmet Yesin🎨      Lok Chun Wai🎨      Adria de Juan🎨      GL-Man🎨              Jheel Patel🎨      Sameer Waskar🎨      Alexander Andrews🎨      Alexander Maxwell🎨      Slava🎨      Mayur Khatri🎨      Mascantosh💻 🖋 📢 🤔              Kivanc Enes🎨      Ritika Das🎨      Zer07793🎨      Andrew Cheung🎨      Sadha🎨      tainenko🎨 💻      github-star-coder🎨              Danilo Oliveira🎨      lordeko🎨      Shubham Kumar🎨 💻      testtree🎨      Cheryl Murphy🎨 💻      Bipin Thomas🎨      Abdulrahman Hisham🎨              Dakshitha Dissanayaka🎨      BADR KACIMI🎨      Alex Wang🎨      Maxim🎨      GordonGrant🎨 💻      Ephrem Demelash🎨      JonOrcutt🎨              topdev10🎨      cookwellwebsite🎨      xren935🎨      Nemo Frenkel🎨      MD SAIF ALAM🎨      Boris López Araya🎨      Larry Chiem🎨              Muhammad Bilal Ilyas🎨      AliMilani🎨 💻      Suraj Sahani🎨      FlyingSquirrel🎨      Erick Tijero🎨      Jaskaran Kukreja🎨      MichaelL🎨              MagicLegend🎨      Dereck Bearsong🎨      Pappu Kumar Pashi🎨      Venkata Kishore Tavva🎨      Rafat Touqir Rafsun🎨      Snehesh Dutta🎨      Timo Körner🎨 💻              alexxxan🎨      GGJason🎨      LeeAnna Ewing🎨 🤔      kamal Jyotwal🎨      Bob-Johns🎨 💻 🖋      yunussalmanlyit🎨 💻      chilcot🎨 💻              Jacky Li💻 🖋 🎨      Sarthak Trivedi🎨      Ayush Aggarwal🎨 💻      Nic Ballarini🎨      Luigi Zambetti🎨 💻      govindhaswin🎨      Addy Roy💻 🎨              Akshat Tamrakar🎨 💻      Sai Bhargava Ramu🎨      Gurkan💻      Spencer Hayes-Laverdiere💻      Aniket Soni💻      tanmay5792💻      Dina Taklit💻 🎨 🖋              Dushyant Singh💻      Ravi Prakash Singh💻      Nihal Joshi💻      Guy Klages💻      Arvind🎨 💻      mujeeb91💻      joserca🎨 💻              Prateek Agrawal💻      Teoh Tze Chuin(サラ)💻 🎨      Jayant Jain💻      Ayush Sahu💻      Hridya Krishna R💻 🎨      Rahul Bali💻 🎨      S.ZHeng🎨 💻 💼              Shriya Madan🎨 💻      mahalrupi🎨      Lucas Lermagne🎨      Jeff Deutsch🎨 💻      Betoxx1🎨      Wingman4l7🎨      Martin Espericueta🎨              Mh-Tahir🎨      Zdravko Šplajt🎨 💻      Ms3105🎨 💻 🖋      Ambika Sidhesware💻      mundoguero💻      Darkus24🖋      Sou-786🖋 🎨              Banurekha🖋      ShiraStarL🎨      Ilya Komarov🎨      DemigodMs🖋 📖      Mekha Hridya🎨 🔍      Andrey Safonov🎨 🔍      Tommaso🎨 💻              Jessica Salbert💻 🎨      JAYANTH DOLAI💻 🎨      silverstroom💻 🎨 💼      Furkan Sayım💻 🎨      Sukumar Chandrasekaran🎨      Yejin Park🎨 💻      Ali Nooshabadi🎨 💻              imitavor🎨 💻      Salih Kilicli🎨 💻      Marcelo Meneses🎨 💻      Anton Krekotun🎨 🚧 🖋 💻 📖 💼      Arnav Sarma💻 💡 🎨      meghatiku💻 🎨      Anshu Trivedi🎨              Taylor Dorsett💻 🖋 🎨      Havit Rovik💻      pushpapune💻 🎨      Ramtin Radfar🎨 🤔 💼 💵 💻 🖋 💬      Abdulmajeed Isa💻 🎨      vikassaxena02🎨      RobTables🎨 💻 💼              Daniel🎨 💻 💼 🔍      Zahid Ali💻 🎨      Chad Chai💻 🎨      Marco Biedermann💻 🎨 💼 🤔      Srinidhi Murthy🎨      Miao Cai💻 🎨      Dionicio Diaz🎨 💻              Mir Monoarul Alam🎨      Shawn Ohn💻 🎨      Amanbolat Balabekov🎨 💻      black-mamba-code💻      Jian-forks🎨 💻      shivani patel🎨      Akash Chowrasia🎨              yairg98🎨      Jay Gajjar🎨      coolerbooler💻      Md Zinnatul Islam Morol🎨      shresthashok550🎨 📖      Alan Pallath📖      Adrian Wong💻              vsDizzy💻 🎨      Frex Cuadillera🎨 💻      ashish570💻 🎨      ruchpeanuts💻 🎨      Artmasque🎨 💻      Amirhossein Mojiri Foroushani🎨      for💻 🎨              Luke🎨 💻      Hector Espinoza🎨      Adrián Buenfil🎨 💻      Amit Kumar🎨      schoppfe🎨 💻      Sofiyal C🎨 💻      spitlisk💻 🎨              PRAVIN SHARMA🎨      NIDZAAA1🎨 💻      John Mai🎨 💻      kimsoyeong🎨      Dona Ghosh💻      Ryan Hill🎨 💻      j42z🎨 💻              Ashish Sangale🎨 💻      Derek Yang🎨 💻      mohsinmsm🎨 💻      Gokulkrish2302💻      Bhaavishek💻 🎨      Louis Liao🎨      sengc92🎨 💻              Alex Marvin🎨      Balkrishna Bhatt🎨 💻      Evaldas Lavrinovičius🎨 💻      Adam Erchegyi🎨 💻      Truman Hung🎨 💻      rzamora11🎨      gaurav0224🎨              Lee GyeongJun🎨      Mirek🎨 💻      surajm245🎨      ArisLaode🎨 💻      RaviDhoriya🎨 💻      sarai-84🎨 💻      Vishnu🎨 💻              Muhammad Minhaj💻      Chandrika Deb🎨 💻      Gitgit101-bit💻 🎨      Hedi Sellami💻 🎨      saurabhvaish93💻 🎨      Nikola Begovic💻 🎨      Wang💻 🎨              Manuel Eusebio de Paz Carmona🎨      Basim Al-Jawahery🎨 💻      RAJA AHMED🎨 💻      Abhik Lodh💻      Md. Pial Ahamed💻 🎨      Hassan Shahzad💻 🎨      Christian Sosa Gago💻              Hasnain Rasheed💻 🎨      T-Radford💻      dahiyashish💻 🎨      RahulSharma468💻 🎨      Jumpod Plekhongthu💻 🎨      Thomas Young-Audet💻 🎨      VinayagamBabu💻 🎨              Deniz Koç💻 🎨      Azhar Khan💻 🎨 🖋 📖 🔣 🚧      Jacob Short💻 🎨      Uchimura85💻 🎨      Leo Nugraha💻 🎨 📖      Mujtaba Mehdi📖 🖋      Jim-ds💻 🎨              Sreehari K💻 🎨      Florian Martinez💻 🎨      Aaron💻 🎨      apoage🎨      Ignacio Guillermo Martinez 💻 🎨      AirlineDog🎨 💻      Mekel🎨 💻              hmosharrof🎨 💻      Ben Emamian💻 🎨      babeshark💻 🎨      Leonardo Jaques💻 🎨      Stefanos Apkarian💻 🎨      Ayhan Albayrak💻 🎨      KidusMT💻 🎨              hectormarroquin20💻 🎨      Edelweiss35💻 🎨      MihaiD💻 🎨      AnveshReddyAnnem💻 🎨      Hyunjae Park💻 🎨      Rajiv Albino💻 🎨      Atishay💻              Yusuf Naheem🎨      Windu🎨 💻      Superv1sor💻 🎨      Karine (:🎨 💻      Eduard Pech🎨 💻      jjeshwani🎨 💻      Steve🎨 💻              Aleigh Ohslund💻      Abhinav Suman🎨 💻      Hamza Ehtesham Farooq🎨 💻      IamNotPeterPan💻 💵 🎨      Cetger🎨      pkonopacki🎨      Yang Yang🎨 💻              Muhammad Shoaib Sarwar💻      Murilo Henrique💻 🎨      emilianoalvz🎨 💻      Sumana Saha🎨 💻      Yurii17K🎨 💻      Rupesh Bhandari🎨 💻      salmos3718💻              John Baker🎨 💻      SanjaySathiraju🎨 💻      Donat Kabashi🎨      Arul Prasad J🎨 💻      Qi Chen🎨 💻      Maksym Dmyterko🎨 💻      ilovepullrequests💻              Samira Maleki🎨 💻      NIKITA MAHOVIYA💻      jesuisdev.Net🎨 💻      Ashraf Nazar🎨      Naveed Ahmad🎨      Ajmain Naqib🎨 💻      Avinash Tingre💻 🎨              nicktids🎨      Keith Dinh💻 🎨      André Ferreira💻 🎨      eliottkespi💻 🎨      praveenpno💻 🎨      vitowidigdo💻 🎨      Devesh Pratap Singh💻 🎨              Dario Rodriguez💻 🎨      charmander_didi💻 🎨      PHBasin💻 🎨      Ritvik Singh Chauhan💻 🎨      Riya P Mathew💻 🎨      Stephanie Cherubin💻 🎨      BenitesGui💻 🎨              FarikBear💻 🎨      Dmytro Havrilov💻 🎨      Parvesh Monu💻 🎨      Dipen Panchasara💻 🎨      gudata🎨 💻      gawadeditor💻 🎨      Kirill Taletski🎨 💻              Saajan🎨 💻      Kushagra S🎨 💻      Oanh Le🎨 💻      Frane Medvidović🎨 💻      Yorman🎨 💻      Bill Chan🎨 💻      Pratik Lomte🎨 💻              LOC LAM🎨 💻      TUSAR RANJAN MAHAPATRA💻      BhargavKanjarla💻      Karel De Smet💻 🎨      sidisan🎨      ygnzayarphyo🎨 💻      svansteelandt💻              Kebechet🎨      Daniel Selvan D🎨 💻      Mahdi Razavi🎨 💻      Niklas Tiede💻 🎨      narutubaderddin💻 🎨      dylandhood💻      Dheeraj Gupta💻              Pieter Claerhout💻 🎨      Shivam Agnihotri💻      RanjithReddy-Narra💻      Nikita Wadhwani🎨 💻      rsholokh💻 🎨      Ayaan Hossain💻 🎨      Rajesh Swarna💻              Deniz Etkar🎨 💻      pro335💻 🎨      Jakub Radzik💻 🎨      Hamza Khanzada💻      ARNON🎨      Vikram Singh💻      Shoxruxbek💻 🎨              Amit Khatri💻 🎨      Wali Ullah🎨 💻      Amit11794💻 🎨      metis-macys-66898💻 🎨      Faisal Maqbool🎨 💻      Kumar Neeraj💻 🎨      Maurizio Marini🎨 💻              Saket Kothari🎨 💻      Szymon Zborowski🎨 💻      iks3000🎨 💻      Ehsan Seyedi🎨 💻      vanekbr🎨 💻      Princy_M🎨 💻      Shijie Zhou🎨 💻              lakshyamcs16🎨 💻      Filippo Facco🎨 💻      mendel5🎨 💻      Patryk🎨 💻      VishwaSangani🎨 💻      Alvin Zhao🎨 💻      Lazar Gugleta🎨 💻              vmicho🎨 💻      Sikandar Ali🎨 💻      Raja Babu🎨 💻      faizajahanzeb💻      Guil_AiT🎨 💻      Kushal Das🎨 💻      Luis Bonilla🎨 💻              jovan1013🎨 💻      Damian🎨 💻      Yash Gupta💻      lolcatnip🎨 💻      Ikko Ashimine🎨 💻      Farukh🎨 💻      Moksedul💻 🎨              Navneet Kumar🎨 💻      Saqib AlMalik💻      fahimrahman🎨 💻      vaibhav patil🎨 💻      Rahul Madan🎨 💻      kartik Kaklotar🎨 💻      ASAHI OCEAN🎨 💻              Daniel Jungbluth🎨 💻      Rajdeep Singh Borana🎨 💻      ankitha19💻      Linh Tran💻      islamarr💻 🎨      Mohamed Sabith🎨 💻      Miguel Angel Cruz Acosta🎨 💻              Adebayo Ilerioluwa 🎨      Markus🎨 💻      dkonyayev🎨 💻      Kevin A Mathew🎨 💻      David Melo🎨 🔣      DFW1N🎨 💻      Sohaib Ayub🎨 💻              Navvy🎨 💻      bloodiator2🎨 💻      Hanji🎨 💻      arthur74🎨 💻      Sri Subathra Devi B🎨 💻      Akif Aydogmus🎨 💻      Umer Javaid🎨 💻              Norio Umata🎨 💻      Gazi Hasan Rahman🎨 💻      Keith Nguyen🎨 💻      Megalomaniac🎨 💻      ShankS3🎨 💻      Farhad Alishov🎨 💻      Ronak J Vanpariya🎨 💻              azrael0learza🎨 💻      Pavel Rahman🎨 💻      chuabern🎨 💻      Rahul Tirkey🎨 💻      Ruslan Bes🎨 💻 💡 🚧 🖋 🔣 🚇      Bohdan🎨 💻      Juzdzewski🎨 💻              Grigor Minasyan🎨 💻      alvintwc🎨 💻      Anand Natarajan🎨 💻      Kashan Ali🎨 💻      Thomas Meshail🎨 💻      Son Pham🎨      Michael French💡              Yash Mishra📖      Miguel Rodriguez🎨 💻      Philipp Bachmann🎨 💻      sunny🎨 💻      Siddharth Chatterjee🎨 💻      Michael Naghavipour🎨 💻      Sahil Garg🎨 💻              MicroLion🎨 💻      wctwc🎨 💻      Rohan Sharma🔣      AshishBodla🎨 💻      Taras Pysarskyi🎨 💻      Luqman Bello O.🎨 💻      DyingDown🎨 💻              Diego Chapedelaine🎨 💻      Richlee🎨 💻      Asif Habib🎨 💻      Mazharul Hossain🎨 💻      toni🎨 💻      Pragyanshu Rai🎨 💻      Matthew Eller🎨 💻              AbhiBiju🎨 💻      Roman Zhornytskiy🎨 💻      Lucas Camino🎨 💻      João Vitor Casarin🎨 💻      Evgeniy Shay🎨 💻      Ehsan Barkhordar🎨 💻      Gabriel🎨 💻              Shibu Mohapatra🎨 💻      Pavel Kirkovsky🎨 💻      Tahir Gul🎨 💻      imDevSalman🎨 💻      Jordan Donaldson🎨 💻      js-venus🎨 💻      Faisal Shaikh🎨 💻              ashishbpatil🎨 💻      Tri Le🎨 💻      tomtreffke🎨 💻      Salah Eddine Lalami🎨 💻      Mattias Xu🎨 💻      Manas Gupta🎨 💻      wolfsong62🎨 💻              Mehdi Mirzaei🎨 💻      Van Ba Khanh🎨 💻      Sel Embee🎨 💻      Suvradip Paul🎨 💻      Sharique🎨      Seabass🎨 💻      Penny Liu🎨 💻              jatinder bhola🎨 💻      misterqbit🎨 💻      Daniel-VS9🎨 💻      Shruthi🎨 💻      beefydog🎨 💻      Suraj Kumar🎨 💻      hrishikeshps🎨 💻              Sudarshan🎨 💻      Divyansh💻 🎨      Zyaire🎨 💻      Omar Belkady🎨 💻      alexiismua🎨 💻      Eduarda Alves🎨      pycoach🎨 💻              Ruhul🎨 💻      pmoustopoulos🎨 💻      Lee Hui Ting💻 🎨      bodi1981🎨 💻      Devaraat Joshi🎨 💻      Johnny🎨 💻      rogue-coder🎨 💻              viiktr🎨      Lalit Mohan💻      João Sousa💻      言葉之靈💻 🎨      RJLABS💻      brittney0522🎨 💻      sham🎨 💻              Glenn Goossens💻 🎨      Cyber Hawk🎨 💻 🖋 💼      Ankit Yadav🎨 💻      verbality💻      Mohammed Siddiqui🎨 💻      AdamKaczor6250🎨 💻      Ramón Martinez Nieto🎨 💻              Grzegorz Dziubak🎨 💻      Ayoub BERDEDDOUCH🎨 💻      nikola-fadv🎨 💻      Akarsh Agrawal🎨 💻      Mitra Mirshafiee🎨 💻      Parker Stephens🎨 💻      alrenee99💻              Karthick Vankayala💻      Iryna 🎨 💻      palanugrah💻      Gwinbleind🎨 💻      Randy Bobandy🎨 💻      Bek Rozikoff💻      davnguye🎨 💻              Neel Patel💻      ehudbehar🎨 💻      nicholas-cod3r🎨 💻      michaelfranki🎨      Esther White🎨 💻      prathmeshpb🎨 💻      Victor Lin🎨 💻              Christine C. Yin🎨 💻      GitLearner-begin🎨 💻      Mesrop Andreasyan🎨 💻      Nathan Garcia🎨      commonsw04🎨 💻      Md. Rashad Tanjim🎨 💻      Ali Malek💻              PAODLT🎨 💻      Nikhil Bobade🎨 💻      hyuckjin21💻      Itasha Modi🎨 💻      Nikitha Reddy🎨 💻      Mahshooq Zubair🎨 💻      Subham Das💻              Onkar Birajdar🎨 💻      Nick Titomichelakis🎨 💻      Christian Leo-Pernold🎨      Matthew Marquise🎨 💻      baronfac🎨 💻      Abhishek Tilwar🎨 💻      DavidsDvm🎨 💻              Parth Parikh🎨 💻      Hector Castro🎨 💻      Rikky Arisendi🎨 💻      Ali HamXa🎨 💻      Frank.wu🎨 💻      Jatin Kumar🎨 💻 📖      masterHAWK99🎨 💻              Pushp Jain🎨 💻      Ashutosh Rout🎨 💻      Atharva Deshpande🎨 💻      Teodor Ciripescu🎨 💻      Anmol Bansal🎨 💻      Nikhil Kumar Macharla🎨 💻      Dexter🎨 💻              Aaron🎨 💻      Yogita Jaswani🎨 💻 📖 🖋      StoryDev🎨 💻      Mesut Doğansoy🎨 💻      Paras Dhawan🎨 💻      Emanuel Zhupa🎨 💻      Aaradhyaa717🎨 💻              jaacko-torus🎨 💻      mBlack💻      kalrayashwin📖 🖋 🎨 💻      Seraph💻 🎨      ZhiHong Chua🎨 💻      Amsal Khan🎨 💻 📖 🖋      Raghav Rastogi🎨 💻              Tzila📖      Shahriar Nasim Nafi📖      AG🎨 💻      Mojtaba Kamyabi🎨 💻      Ahmad Abdulrahman🎨 💻      Eclipse🎨 💻      Anshu Pal🎨 💻              Denis🎨 💻      mehmet sayin📖      WebDEV🎨 💻      Sam Komesarook🎨 💻      Kiran Ghimire🎨 💻      Joshua Davis🎨 💻      Muhammad-Huzaifa-Siddiqui💻              tobeornottobeadev🎨 💻      VAIBHAV SINGHAL🎨 💻      Keiran Pillman🎨 💻      Max Donchenko🎨 💻      sgonsal🎨 💻      diksha137🎨 💻      Vignesh🎨 💻              Gabriel França🎨 💻      Joseph🎨 💻      Bruno Rafael🎨 💻      vcamarre🎨 💻      thibault ketterer🎨 💻 🚧      VictorGonzalezToledo🎨 💻      1911510996🎨 💻              invidu🎨 💻      Nurul Furqon🎨 💻      David Asbill🎨 💻      Niko Birbilis🎨 💻      Mugundan Kottursuresh🎨      agrsachin81🎨 💻      Othmane El Alami🎨 💻              Syed Atif Ali🎨 💻      lakhanjindam🎨 💻      youssef hamdane🎨 💻      starfaerie🎨 💻      rodrigo0107🎨 💻      Michał Gralak🎨 💻      Jewel Mahmud🎨 💻              cwilson830🎨 💻      buun1030🎨 💻      Reda-ELOUAHABI🎨 💻      saad-aksa🎨 💻      Emdadul Haque🎨 💻      PROCW🎨 💻      cccppp1🎨 💻              Joanna Baile🎨 💻      Ahmed Saber🎨 💻      Masoud Keshavarz🎨 💻      mortazavian🎨 💻      Aniket Pandey🎨 💻      Vijay Nirmal🎨 💻      Daniel Carvallo💻              menaechmi🎨 💻      azenyx🎨 💻      Ahmet Özrahat🎨 💻      Abdulrahman Abouzaid🎨 💻      jmgnorbec🎨 💻      palinko91🎨 💻      Laisson R. Silveira🎨 💻              BHARGAVPATEL1244🎨 💻      Candide U🎨 💻      Sitansh Rajput🎨 💻      Houda Mouttalib🎨 💻      MumuTW🎨 💻      Suave Bajaj🎨 💻      Mehdi Parsaei🎨 💻              Dinko Osrecki🎨 💻      Dhia Djobbi🎨 💻      Mahmoud Galal🎨 💻      Anh Minh🎨 💻      Suvesh K🎨 💻      Petar Todorov🎨 💻      Alexander Nguyen🎨 💻              Morteza Jalalvand🎨 💻      Claudson Martins🎨 💻      Matt Jacobson🎨 💻      Rafael Belokurows🎨 💻       Thomas Gamauf🎨 💻      Rishabh Mahajan🎨 💻      rakeshpdgupta23🎨 💻              Shashidharknaik🎨 💻      taleleuma🎨 💻      Florian Bühler🎨 💻      Raihan Bin Wahid🎨 💻      MOHAMMED NASSER🎨 💻      federico🎨 💻      Andre Violante🎨 💻              tcunningham98🎨 💻      Jan Grießer🎨 💻      Serkan Alc🎨 💻 🖋      Jez McKean🎨 💻      meisam alifallahi🎨 💻      Mehul Thakkar🎨 💻      Saksham Soni🎨 💻              Pedro Peregrina🎨 💻      Mintu Choudhary🎨 💻      lucianmoldovanu🎨 💻      John C. Scott🎨 💻      Mia D.🎨 💻      EwenBernard🎨 💻      M. Reza Nasirloo🎨 💻              Jay Agrawal🎨 💻      DeShay🎨 💻      Jay206-Programmer🎨 💻      Elender🎨 💻 🖋      Bobby Byrne🎨 💻      Pirci🎨 💻      Hasanuzzaman🎨 💻              Josh Kautz🎨 💻      Brofar🎨 💻      Mina Karam🎨 💻      Duncan O N🎨 💻      Sean Tumulak-Nguyen🎨 💻      Artur Trześniewski🎨 💻      JJaammeessM🎨 💻              shubham agarwal🎨 💻      Michele Righi🎨 💻      Panagiotis Kontos🎨 💻      sumitbathla🎨 💻      Deepak Mathur🎨 💻      Juho Nykänen🎨 💻      Santiago González Siordia🎨 💻              SRIJITA MALLICK🎨 💻      Samriddhi B🎨 💻      Nitzan Papini🎨 💻      Mario Sanz🎨 💻      Crab^4🎨 💻      Pablo🎨 💻      Gordon Pham-Nguyen🎨 💻              Kristoffer🎨 💻      chrisblach🎨 💻      Gábor🎨 💻      Lina🎨 💻      Harrison Watts🎨 💻      Mario Petričko🎨 💻      Ben8120🎨 💻              Giovanna🎨 💻      Minal Ahuja🎨 💻      mossfarmer🎨 💻      ThaC0derDre🎨 💻      itware🎨 💻      Michael Walker🎨 💻      Tom Jacob Chirayil🎨 💻              Sachin Kumar🎨 💻      adi-ray🎨 💻      Dr-Blank-alt🎨 💻      Bogdan Cazacu🎨 💻      Gilson Urbano🎨 💻      Nina🎨 💻      Anthony🎨 💻              manushimjani🎨 💻      Michael Reyes🎨 💻      Rachel Kennelly🎨 💻      Aakash Garg🎨 💻      Daniel Livingston🎨 💻      alexrojco🎨 💻      Minh Nguyen🎨 💻              Mahesh Dattatraya Babar🎨 💻      Jin Zihang🎨 💻      Bikramjit Ganguly🎨 💻      QuestionableGuise🎨 💻      liq19ch🎨 💻      Bruno Rocha🎨 💻      Anand Dyavanapalli💻 🖋              crucian-afk🎨 💻      0xgainz🎨 💻      weirdfsh🎨 💻      Valan Baptist Mathuranayagam🎨 💻      Paul Kaefer🎨 💻      Yu-Hsiang Wang🎨 💻      Javad Adib🎨 💻              davidliu0930🎨 💻      Achilleas John Yfantis🎨 💻      Omkar Shivadekar🎨 💻 🖋 🐛      ToanTran🎨 💻      Gautam Naik🎨 💻      Marc🎨 💻      twix20🎨 💻              Kristian S.🎨 💻      Aleksey Khoroshilov🎨 💻      arjunsrsr🎨 💻      Ali Haider🎨 💻      Trisha Dring🎨 💻      Andre Marzulo🎨 💻      Krishna Modi🎨 💻              Rosemary Li🎨 💻      Alex Weller🎨 💻      Tam Nguyen🎨 💻      aquintelaoliveira🎨 💻      Norbert Brett🎨 💻      rocsogd🎨 💻      0nyr🎨 💻              rethkevin🎨 💻      RickHeadle🎨 💻      Leandre🎨 💻      Natnael Sisay🎨 💻      sbbu🎨 💻      wael🎨 💻      Fabricio Tramontano Pirini🎨 💻              Alexander Stoyanov🎨 💻      Dezx20🎨 💻      southparkkids🎨 💻      bmstar🎨 💻      kiagam🎨 💻      Juan Castillo🎨 💻      FFenne🎨 💻              Jose Toledo🎨 💻      Pat McGhen🎨 💻      Eiko Wagenknecht💻 🖋 🔣      Alan Chalmers🎨 💻      Jean Didier🎨 💻      Andy🎨 💻      pestadieu🎨 💻              Kanishka Chakraborty🎨 💻      Nandha🎨 💻      Vahid Mafi🎨 💻 🔣 🖋 💼      Akshay Ashok🎨 💻      0x08🎨 💻      Sandeep Mishra🎨 💻      Evann Regnault🎨 💻              Lenny Zeitoun🎨 💻      Eden Boaron🎨 💻      TroyBTC🎨 💻      Aby Sebastian🎨 💻      Matthew Dunn🎨 💻      ckullo🎨 💻 🖋 🔣      Mohamed Mamdouh🎨 💻              Youssef Bazina🎨 💻      Frederico Kückelhaus💻      Nushan Kodikara💻      Zach Cooper💻      Roy🎨 💻      Saurav Panchal🎨 💻      totallynotdavid🎨 💻              goosepirate🎨 💻 💡 💼      KAUTH🎨 💻      Hari Kiran Vusirikala🎨 💻      Sounak Dey🎨 💻      zia💼 🎨 💻      Reza Davari🎨 💻      AkshayAjaykumar🎨 💻              x24870🎨 💻      Ko Phone🎨 💻      Nabstar3🎨 💻      Mateusz🎨 💻      Yunus Emre Emik💻      Abhinav Sinha🎨 💻      Hung Nguyen🎨 💻              Maselino💻      Shuktika Mahanty💻      Mikołaj Gawroński🎨 💻      Hussein Habibi Juybari🎨 💻      Sean-McArthur🎨 💻      Osman F Bayram🎨 💻      Benjamin Thomas Blodgett🎨 💻              Chuanlong-Zang🎨 💻      julian🎨 💻      francisco🎨 💻      aalihhiader9211🎨 💻      Muhammad Zunair🎨 💻      Liya🎨 💻      BegadTarek🎨 💻              etorobot🎨 💻      Hussam Khan🎨 💻      Saikat Chakraborty🎨 💻      Nicholas Quisler🎨 💻      Evang Poul🎨 💻      Gregg Lind🎨 💻      Deepak Kumar🎨 💻              Callum Leslie🎨 💻      Curtis Barnard Jr.🎨 💻      Deepanshukaim🎨 💻      Manthan Ank🎨 💻      hossein varmazyar🎨 💻      Brayan Muñoz V.🎨 💻      Kamil Rasheed Siddiqui💻 🎨              mutt0-ds🎨 💻      egbertjk🎨 💻      Majid Zojaji🎨 💻      Sean Chen🎨 💻      Herbert Milhomme🎨 💻      A3🎨 💻      Killian🎨 💻              Coakeow🎨 💻      ྅༻ Ǭɀħ ༄༆ཉ🎨 💻      Pratik Solanki🎨 💻      Sunny🎨 💻      ssge🎨 💻      Bernat Frangi🎨 💻      Jeevan Rupacha🎨 💻              amirandap🎨 💻      Deepakshi Mittal🎨 💻      Abhijeet Parida🎨 💻      Khaled Riyad🎨 💻      Pratap parui🎨 💻      Prajit Panday🎨 💻      PipeSierra🎨 💻              Collins Oden🎨 💻      Kshitij Dwivedi🎨 💻      Bernardia Vitri Arumsari🎨 💻      Ömer Faruk Taşdemir🎨 💻      Spencer Stith🎨 💻      Porsche Rodjanasak🎨 💻      Shakeel Sharif🎨 💻              Victoria Cheng🎨 💻      Denis🎨 💻      Anand Prakash Tiwari🎨 💻      danijeljw-rpc🎨 💻      Ahmed H Ebrahim🎨 💻      Virginia Gardner🎨 💻      Jhironsel Diaz A.🎨 💻              Yunus Kidem🎨 💻      MT🎨 💻      Dinesh Zaldekar🎨 💻      adi🎨 💻      Farhan Shaikh🎨 💻      Elvis Salvatierra🎨 💻      Kaushik-Iyer🎨 💻              HocAndres🎨 💻      VictorHugoAguilarAguilar🎨 💻      Murat Can Abay🎨 💻      Chris🎨 💻      Shivam7-1🎨 💻      Paipai13🎨 💻      Shambles-io🎨 💻              Abhishek K M🎨 💻      Ezequiel Cuevas🎨 💻      Plamen Ivanov🎨 💻      Yuji🎨 💻      Jean-Philippe Lebœuf🎨 💻 🔣      Naufan🎨 💻      jadnov🎨 💻              vaxtangens🎨 💻      subashkonar13🎨 💻      Rushi Javiya🎨 💻      Mert Gül🎨 💻      Lily🎨 💻      Kalinoff🎨 💻      Joel Tony🎨 💻              Peter🎨 💻      Roozbeh Zarei🎨 💻      Shen🎨 💻      Joonsoo.LEE🎨 💻      Fede.Breg🎨 💻      Rui Costa🎨 💻      João Gustavo Bispo🎨 💻              Sami-I🎨 💻      Tsvetoslav Tsvetkov🎨 💻      Olabode Olaniyi David🎨 💻      theRuslan🎨 💻      leighboz🎨 💻      Frank Sossi🎨 💻      Tomasz Adamski🎨 💻              Mansoor M. Sathir🎨 💻      Golamrabbi Azad🎨 💻      Nahian Ahmed🎨 💻      Rafael de Jesus Silva Monteiro🎨 💻      Odionyebuchukwu Jude🎨 💻      The Nithin Balaji🎨 💻      Knackii🎨 💻              vittorio-giatti🎨 💻      Guilherme de Carvalho Lima Rebouças🎨 💻      aaref shami🎨 💻      Andrey Dryupin🎨 💻      Muhanned Noman🎨 💻      Jan Silva🎨 💻      emanuele-em🎨 💻 🖋              Sanjay TM🎨 💻      Joe Markberg / code editor🎨 💻      Julien Quiaios🎨 💻      Eric Ramirez Santis🎨 💻      M🎨 💻      Malcata🎨 💻      Athul Muralidharan🎨 💻              Dariusz Ochota🎨 💻      CHANDAN CHOUDHURY🎨 💻      Deep🎨 💻      Ahmet İstemihan ÖZTÜRK🎨 💻      TIM🎨 💻      jakeg814🎨 💻      Leonidos🎨 💻              Abhinandu V Nair🎨 💻      charafeddine01🎨 💻      Jasper🎨 💻      Manish Goyal🎨 💻      SATYAM_SINGH🎨 💻      Four🎨 💻      Vaishnavi Amira Yada🎨 💻              ShriKrushna Bhagwat🎨 💻      Rohit Nandagawali🎨 💻      felipe🎨 💻 🚧 🖋 ✅ 🧑‍🏫      Saurabh Mudgal🎨 💻      szenadam🎨 💻      Shubhendra Singh🎨 💻      Yoosuf Sayyid💻 🎨              Güven Çetinerler🎨 💻      Luke Jefferies🎨 💻      Chris🎨 💻      Lúcio Aguiar💻      Enuma029💻      yktsang01💻      maximumn3rd🎨 💻              Jon Galletero🎨 💻      Thaddeus  Thomas🎨 💻      Aakash Kumar💻 🎨      Ali M🎨 💻      OskyEdz🎨 💻      Ravi Gupta🎨 💻      Rafa Raizer🎨 💻              Abdullah Al Muzaki🎨 💻      Rahul Faujdar🎨 💻      Abhishek Verma🎨 💻      Ashutosh Shinde🎨 💻      Ganesh Rai🎨 💻      StefanTrpkovic🎨 💻      Erik Blanca🎨 💻              Vedant Madane🎨 💻      Antra Tripathi🎨 💻      Ethan Knights🎨 💻      Alexandru Boncut🎨 💻      Pablo Bandinopla🎨 💻 🚧 🖋      Robz-99🎨 💻      Harpal Singh🎨 💻              paulboundy99🎨 💻      Mubashir Ahmed🎨 💻      Rohan Hari🎨 💻      Erik Henrique 🎨 💻      Leandro Matheus🎨 💻      Deepak🎨 💻      AlishaSingh🎨 💻              Lynn Latt Yati🎨 💻      San Shwe🎨 💻      SKR🎨 💻      msbunnyjaguar🎨 💻      Mohamad Zabiulla🎨 💻      Hatim Zahid🎨 💻      Rauzan Sumara🎨 💻              Hosein1358🎨 💻      Mohit🎨 💻      Ali🎨 💻      Avinash1765🎨 💻      Sai Teja Madha🎨 💻      Monsur Ahmed Shafiq🎨 💻      xuxianjin-dev🎨 💻              chetna🎨 💻      Gul Zaib🎨 💻      Natalia🎨 💻      Dionísio Braga🎨 💻      Pritish Rajpurohit🎨 💻      incanlove🎨 💻      Innocent🎨 💻              Devin Almonor🎨 💻      antonyveyre🎨 💻      Beltz Anhxton🎨 💻      Mehdi🎨 💻      Muhammad Usman🎨 💻      Patrick Dantas🎨 💻      Tak Vannak🎨 💻              Ramzi RADDAOUI🎨 💻      Konstantin-Glukhov🎨 💻      uguroban🎨 💻      Humberto Alves🎨 💻      JuangZendrato🎨 💻      James Oluwaleye🎨 💻      Wasi Sadman🎨 💻              Pavle Mijatovic🎨 💻      Luiz H. S. Bispo🎨 💻      Сухас Дхолз🎨 💻      Alvaro Trujillo🎨 💻      Everton 🎨 💻      jfrozas🎨 💻      Shuaaib Badran🎨 💻              Shivam Jha🎨 💻      Mohamed Tayeh🎨 💻      Makendran G🎨 💻      mayank singh tomar🎨 💻      hossam sadany🎨 💻      Harshbardhan Singh💻 🎨      Fawad Jawaid Malik🎨 💻              Tina Lacatis🎨 💻      TeddyCuoreDolce🎨 💻      bchooxg🎨 💻      Alisha Takkar🎨 💻      Gianluigi🎨 💻      Mehran Javaherian🎨 💻      Benjamin Ololade Adedokun🎨 💻              Md. Abdul Mutalib🎨 💻      Aadil Arsh.S.R🎨 💻      J. Nathan Allen🎨 💻      Kieran Krug🎨 💻      Seth Addo🎨 💻      Satvik Singh Rathore🎨 💻      dangoth🎨 💻              Maxim🎨 💻      Phuong-Cat Ngo🎨 💻      Frenchtoast0🎨 💻      Rakshith🎨 💻      Vaibhav Arora🎨 💻      zghp🎨 💻      Bedovan🎨 💻              chiaramistro🎨 💻      him2016🎨 💻      HarshitSachdeva🎨 💻      Sadaf Saleem🎨 💻      Aaroh Srivastava🎨 💻      eloygplaza🎨 💻      Gaurav Kumar Verma🎨 💻              AndreaCUS🎨 💻      Simran🎨 💻      Prashant Bhapkar🎨 💻      mhaendler🎨 💻      Gauri Maheshwari🎨 💻      4Lajf🎨 💻      Tanmoy Sengupta🎨 💻              Sharad Tripathi🎨 💻      Niraj Chavan🎨 💻      Luisa Gualda🎨 💻      Monika-Sivakumar-3🎨 💻      harryfensome🎨 💻      Shubham Choubey🎨 💻      Ashwini Patil🎨 💻              cleversonlira🎨 💻      Nurmukhammed🎨 💻      workspace-utkarsh🎨 💻      Santosh Phadtare🎨 💻      Prashant Warghude🎨 💻      Umang Dakh🎨 💻      Shalini Chavan🎨 💻              vinit gurjar🎨 💻      Vishal Kumar🎨 💻      Wonhyeong Seo🎨 💻      Achwale Prajwal Namdevrao🎨 💻      Ankan Banerjee🎨 💻      bhaumikankan🎨 💻      JamesMacroZhang🎨 💻              Pedro Lopes🎨 💻      dia🎨 💻      tayyabhussain2910🎨 💻      Rajdeep Shrivastava 🎨 💻      Mukul Kumar🎨 💻      Mayank N🎨 💻      jdelucca🎨 💻              Sneha Mittal🎨 💻      Sarika Kushwaha🎨 💻      farzad-khb🎨 💻      Elijah Shackelford🎨 💻      The-Only-Raminator🎨 💻      Keerthana Kasthuril🎨 💻      Viachaslau Auchynnikau🎨 💻              Mohammad Osman Rasooli🎨 💻      mvedovato🎨 💻      Sonali Rajput🎨 💻      Isha Dhek🎨 💻      Ramshad Cheriyeri Peediyakkal🎨 💻      Micah🎨 💻      gauravshukla2203🎨 💻              sndmurthy🎨 💻      Shivam-Singh🎨 💻      M. Ammar Khan🎨 💻      chandolakul🎨 💻      bhatnagar221🎨 💻      Adrian Nieściur🎨 💻      nezi311🎨 💻              scottajevans🎨 💻      Marcelo Antunes Soares Fantini🎨 💻      Axel De Acetis🎨 💻      Drishti Sah🎨 💻      VipulDhillon🎨 💻      Urmi Jana🎨 💻      Ayush Mokal🎨 💻              Damola Olutoke🎨 💻      Max🎨 💻      Lakshmi N🎨 💻      ArtemReva🎨 💻      Ujjwal Aggarwal🎨 💻      Mo🎨 💻      Brian🎨 💻              chamley🎨 💻      Simone Baptiste🎨 💻      Shekhar Thakur🎨 💻      Smith🎨 💻      codernoob1🎨 💻      lok84🎨 💻      Tobias Riemenschneider🎨 💻              Tharsanan1🎨 💻      ANURAG SINGH🎨 💻      Yash Sant🎨 💻      Krishiv Patel🎨 💻      GGGalaxy🎨 💻      pardeepdhillon661🎨 💻      anujd64🎨 💻              Pedro Pereira🎨 💻      Master_Saptak🎨 💻      SURANJAN DAS🎨 💻      Tripura kant🎨 💻      shabzkhan🎨 💻      Mustafa Poya🎨 💻      Roshan Jha🎨 💻              GuillaumeLarue🎨 💻      Tomasz Rodak🎨 💻      Junil Kim🎨 💻      Surbhi Mayank🎨 💻      Nemanja Lekic🎨 💻      HemantMalokar🎨 💻      Felipe M. López🎨 💻              bibliofilo🎨 💻      GauthamG2🎨 💻      02_t🎨 💻      Yusuf Abdul-razaq🎨 💻      Vladimir🎨 💻      Sai Chandra K🎨 💻      Soroush Bonab🎨 💻              Giide0n🎨 💻      GG🎨 💻      Dáger Zúñiga🎨 💻      rsk2🎨 💻      Storozhev DJ🎨 💻      Jeevan🎨 💻      Andy Johnson🎨 💻              Aníbal Pozo🎨 💻      Jovane de Castro🎨 💻      Muhammad Hamza Amir🎨 💻      tharaka-mts🎨 💻      Ali KHYAR🎨 💻      Caio Araujo🎨 💻      Oscar Dyremyhr🎨 💻              arteality🎨 💻      Daniel Drexlmaier🎨 💻      Marco Monti🎨 💻      mikeycrystal🎨 💻      Veljanovskii🎨 💻      Ivan Gorbachev🎨 💻      Sahil Rawat🎨 💻              Hasitha Suneth🎨 💻      Yerko Vera Lezama🎨 💻      Ivan Penchev🎨 💻      Tanver Islam Tonmoy🎨 💻      Xun Cao🎨 💻      Nayan Babariya🎨 💻      Priyanshu Maurya🎨 💻              Dylan Tintenfich🎨 💻      Ron Strauss🎨 💻      Mohammed AlBanna🎨 💻      Mukund M🎨 💻      Franklin Ohaegbulam🎨 💻      Nisarg Shah🎨 💻      Unik Dahal🎨 💻              Readily🎨 💻      Alexandre Poitevin🎨 💻      Scaramir🎨 💻      Pruthvi🎨 💻      Kalmanq🎨 💻      Alfatah Nesab🎨 💻      arudesai🎨 💻              Adryenne🎨 💻      El mehdi oudaoud🎨 💻      Jayant Goel🎨 💻      Tsuki🎨 💻      Peter Lemanski🎨 💻      Annurag-byte🎨 💻      Anthony Vu🎨 💻              Vitaly Nikolaychuk🎨 💻      Nathan🎨 💻      Evgenii Petukhov🎨 💻      Loris Guerra🎨 💻      fakhriaunur🎨 💻      Mehdi HYANI🎨 💻      Sarvex Jatasra🎨 💻              santimanuelr🎨 💻      Evgeniy Rezanov🎨 💻      Sonia M🎨 💻      Grzegorz Kmita🎨 💻      Manuel Carita🎨 💻      Felipe Cisternas Alvarez🎨 💻      Guo Ci🎨 💻              Marcos Silva🎨 💻      KK🎨 💻      Shubhanjan Medhi🎨 💻      ArthurFerreiraRodrigues🎨 💻      PabloHermun🎨 💻      disha-baldawa🎨 💻      StaroMoon🎨 💻              Amila T Kumarasekara🎨 💻      Amoh Prince🎨 💻      AngeloGC🎨 💻      Ebube Glory Ogbonda🎨 💻      Prahalad Belavadi📖      Antoni Sarnowski-Trypka🎨 💻      Alberto Pasqualetto🎨 💻              Amir Babaei🎨 💻      Syed Abdul Hannan🎨 💻      Srajan Rai🎨 💻      Clarence Moore🎨 💻      Nguyen Anh Tuan🎨 💻      dar2dar2🎨 💻      Ameer Ibrahim🎨 💻              Tiago Lugatto🎨 💻      raremiroir🎨 💻      Moobie🎨 💻      AlicanDursun🎨 💻      bbalsam🎨 💻      Luboš Hájek🎨 💻      mrshahzeb7🎨 💻              Wesley Scholl🎨 💻      Lawrence Turcotte🎨 💻      Michael DiPaolo🎨 💻      Smart-Codi🎨 💻      Vivek Kumar🎨 💻      Igor Moiseev🎨 💻      Bård Pedersen🎨 💻              HOA PHAN🎨 💻      GaborModra🎨 💻      vivek-114🎨 💻      Robin🎨 💻      Alex🎨 💻      John Ehrlinger🎨 💻      Roman Zhuravlov🎨 💻              Jordan Moss🎨 💻      RaeShelly🎨 💻      gmollard🎨 💻      Md Kaif Khan🎨 💻      Pablo Romera🎨 💻      Erik Bustos🎨 💻      trogfield🎨 💻              simon-aichhorn🎨 💻      Tufan GÜLEÇ🎨 💻      Uğur Berkecan Ünlü🎨 💻      Revanth Naik🎨 💻      Lia Pires🎨 💻      Igor Mestechkin🎨 💻      Anirudh Karanth🎨 💻              KBobovskiy🎨 💻      zhatiayua🎨 💻 🖋      David Cardona🎨 💻      Paulo Castilho🎨 💻      Sebastiano Picchi🎨 💻      pjotar🎨 💻      Rimel CHERIF💻              Arsal uddin🖋      Dmitry Kasporsky💻      SoftwareDev1014🎨 💻      @Robvred🎨 💻      Kasun Shanaka💻      Ahmad M.🎨 💻      Alex Kozin🎨 💻              Mandy Meindersma🎨 💻      LEGALISE PIRACY🎨 💻      Alex Logvin🎨 💻      Aria Dahl🎨 💻      Mustafa Arifoglu🎨 💻      Yevhen Leshchenko🎨 💻      Anubhav Adhikari🎨 💻              Noah Tatko🎨 💻      Mohit Gadhavi🎨 💻      Pedro Basílio🎨 💻      RealSanjeev🎨 💻      Akash Hazra🎨 💻      Christoph Dahlen🎨 💻      Vincent du Plessis🎨 💻              Karen Tamrazyan🎨 💻      Mirza Younus Baig🎨 💻      Ashish Kumar🎨 💻      Unknown6334🎨 💻      flowaz🎨 💻      zi-aikra🎨 💻      PAYAL PM🎨 💻              Lennart Lösche🎨 💻      Yummy-Yums🎨 💻      Njuacha Hubert Mikulowski🎨 💻      Hussein Esmail🎨 💻      Bilgehan Bezir🎨 💻      Muhammed Shittu🎨 💻      Clément FERNANDES🎨 💻              JaCKoP619🎨 💻      userutf8🎨 💻      Mohamed Ubaid🎨 💻      Justin Yates🎨 💻      mohammad ali🎨 💻      Madhav Singh🎨 💻      RgbMouse69🎨 💻              Nicholas Leask🎨 💻      parthav0🎨 💻      Sigma🎨 💻      Evelina Becheva🎨 💻      Akshit Gulyan🎨 💻      Arpita Jana🎨 💻      Praveen Kumar🎨 💻              Mohammad Sami🎨 💻      eddiestefanescu🎨 💻      Ramesh Yadav🎨 💻      Sarthak Joshi🎨 💻      Nikhil12300🎨 💻      Yevgen🎨 💻      Leo🎨 💻              laurent b🎨 💻      Mettchen🎨 💻      Ali Mahdavi🎨 💻      Lucas Dondo🎨 💻      Siddhesh Agarwal🎨 💻      slimerPuncher🎨 💻      saritashh🎨 💻              Iulian-Valeriu Cioată🎨 💻      Szabolcs Nagy🎨 💻      Jarle Kvile🎨 💻      劉耀升 Vic Liu🎨 💻      Suryansh🎨 💻      Matthew Oosthuyse🎨 💻      Florin Zamfir🎨 💻              Melek🎨 💻      moesocio🎨 💻      Alan James🎨 💻      Mai Thanh Phương🎨 💻      Neville Dabre🎨 💻      Maksym🎨 💻      tamanna900🎨 💻              Adithya Awati🎨 💻      This project follows the all-contributors specification.Contributions of any kind welcome![ Go back to the top of the page ]Contributor Over TimeStargazers over timeVisualisation of this repository by Gourcehttps://www.youtube.com/watch?v=24cZVytc5D4"
66,ageitgey/face_recognition,https://github.com/ageitgey/face_recognition/blob/master/README.md,Python,"Face RecognitionYou can also read a translated version of this file in Chinese 简体中文版 or in Korean 한국어 or in Japanese 日本語.Recognize and manipulate faces from Python or from the command line withthe world's simplest face recognition library.Built using dlib's state-of-the-art face recognitionbuilt with deep learning. The model has an accuracy of 99.38% on theLabeled Faces in the Wild benchmark.This also provides a simple face_recognition command line tool that letsyou do face recognition on a folder of images from the command line!FeaturesFind faces in picturesFind all the faces that appear in a picture:import face_recognitionimage = face_recognition.load_image_file(\""your_file.jpg\"")face_locations = face_recognition.face_locations(image)Find and manipulate facial features in picturesGet the locations and outlines of each person's eyes, nose, mouth and chin.import face_recognitionimage = face_recognition.load_image_file(\""your_file.jpg\"")face_landmarks_list = face_recognition.face_landmarks(image)Finding facial features is super useful for lots of important stuff. But you can also use it for really stupid stufflike applying digital make-up (think 'Meitu'):Identify faces in picturesRecognize who appears in each photo.import face_recognitionknown_image = face_recognition.load_image_file(\""biden.jpg\"")unknown_image = face_recognition.load_image_file(\""unknown.jpg\"")biden_encoding = face_recognition.face_encodings(known_image)[0]unknown_encoding = face_recognition.face_encodings(unknown_image)[0]results = face_recognition.compare_faces([biden_encoding], unknown_encoding)You can even use this library with other Python libraries to do real-time face recognition:See this example for the code.Online DemosUser-contributed shared Jupyter notebook demo (not officially supported): InstallationRequirementsPython 3.3+ or Python 2.7macOS or Linux (Windows not officially supported, but might work)Installation Options:Installing on Mac or LinuxFirst, make sure you have dlib already installed with Python bindings:How to install dlib from source on macOS or UbuntuThen, make sure you have cmake installed:brew install cmakeFinally, install this module from pypi using pip3 (or pip2 for Python 2):pip3 install face_recognitionAlternatively, you can try this library with Docker, see this section.If you are having trouble with installation, you can also try out apre-configured VM.Installing on an Nvidia Jetson Nano boardJetson Nano installation instructionsPlease follow the instructions in the article carefully. There is current a bug in the CUDA libraries on the Jetson Nano that will cause this library to fail silently if you don't follow the instructions in the article to comment out a line in dlib and recompile it.Installing on Raspberry Pi 2+Raspberry Pi 2+ installation instructionsInstalling on FreeBSDpkg install graphics/py-face_recognitionInstalling on WindowsWhile Windows isn't officially supported, helpful users have posted instructions on how to install this library:@masoudr's Windows 10 installation guide (dlib + face_recognition)Installing a pre-configured Virtual Machine imageDownload the pre-configured VM image (for VMware Player or VirtualBox).UsageCommand-Line InterfaceWhen you install face_recognition, you get two simple command-lineprograms:face_recognition - Recognize faces in a photograph or folder full forphotographs.face_detection - Find faces in a photograph or folder full for photographs.face_recognition command line toolThe face_recognition command lets you recognize faces in a photograph orfolder full  for photographs.First, you need to provide a folder with one picture of each person youalready know. There should be one image file for each person with thefiles named according to who is in the picture:Next, you need a second folder with the files you want to identify:Then in you simply run the command face_recognition, passing inthe folder of known people and the folder (or single image) with unknownpeople and it tells you who is in each image:$ face_recognition ./pictures_of_people_i_know/ ./unknown_pictures//unknown_pictures/unknown.jpg,Barack Obama/face_recognition_test/unknown_pictures/unknown.jpg,unknown_personThere's one line in the output for each face. The data is comma-separatedwith the filename and the name of the person found.An unknown_person is a face in the image that didn't match anyone inyour folder of known people.face_detection command line toolThe face_detection command lets you find the location (pixel coordinatates)of any faces in an image.Just run the command face_detection, passing in a folder of imagesto check (or a single image):$ face_detection  ./folder_with_pictures/examples/image1.jpg,65,215,169,112examples/image2.jpg,62,394,211,244examples/image2.jpg,95,941,244,792It prints one line for each face that was detected. The coordinatesreported are the top, right, bottom and left coordinates of the face (in pixels).Adjusting Tolerance / SensitivityIf you are getting multiple matches for the same person, it might be thatthe people in your photos look very similar and a lower tolerance valueis needed to make face comparisons more strict.You can do that with the --tolerance parameter. The default tolerancevalue is 0.6 and lower numbers make face comparisons more strict:$ face_recognition --tolerance 0.54 ./pictures_of_people_i_know/ ./unknown_pictures//unknown_pictures/unknown.jpg,Barack Obama/face_recognition_test/unknown_pictures/unknown.jpg,unknown_personIf you want to see the face distance calculated for each match in orderto adjust the tolerance setting, you can use --show-distance true:$ face_recognition --show-distance true ./pictures_of_people_i_know/ ./unknown_pictures//unknown_pictures/unknown.jpg,Barack Obama,0.378542298956785/face_recognition_test/unknown_pictures/unknown.jpg,unknown_person,NoneMore ExamplesIf you simply want to know the names of the people in each photograph but don'tcare about file names, you could do this:$ face_recognition ./pictures_of_people_i_know/ ./unknown_pictures/ | cut -d ',' -f2Barack Obamaunknown_personSpeeding up Face RecognitionFace recognition can be done in parallel if you have a computer withmultiple CPU cores. For example, if your system has 4 CPU cores, you canprocess about 4 times as many images in the same amount of time by usingall your CPU cores in parallel.If you are using Python 3.4 or newer, pass in a --cpus <number_of_cpu_cores_to_use> parameter:$ face_recognition --cpus 4 ./pictures_of_people_i_know/ ./unknown_pictures/You can also pass in --cpus -1 to use all CPU cores in your system.Python ModuleYou can import the face_recognition module and then easily manipulatefaces with just a couple of lines of code. It's super easy!API Docs: https://face-recognition.readthedocs.io.Automatically find all the faces in an imageimport face_recognitionimage = face_recognition.load_image_file(\""my_picture.jpg\"")face_locations = face_recognition.face_locations(image)# face_locations is now an array listing the co-ordinates of each face!See this exampleto try it out.You can also opt-in to a somewhat more accurate deep-learning-based face detection model.Note: GPU acceleration (via NVidia's CUDA library) is required for goodperformance with this model. You'll also want to enable CUDA supportwhen compliling dlib.import face_recognitionimage = face_recognition.load_image_file(\""my_picture.jpg\"")face_locations = face_recognition.face_locations(image, model=\""cnn\"")# face_locations is now an array listing the co-ordinates of each face!See this exampleto try it out.If you have a lot of images and a GPU, you can alsofind faces in batches.Automatically locate the facial features of a person in an imageimport face_recognitionimage = face_recognition.load_image_file(\""my_picture.jpg\"")face_landmarks_list = face_recognition.face_landmarks(image)# face_landmarks_list is now an array with the locations of each facial feature in each face.# face_landmarks_list[0]['left_eye'] would be the location and outline of the first person's left eye.See this exampleto try it out.Recognize faces in images and identify who they areimport face_recognitionpicture_of_me = face_recognition.load_image_file(\""me.jpg\"")my_face_encoding = face_recognition.face_encodings(picture_of_me)[0]# my_face_encoding now contains a universal 'encoding' of my facial features that can be compared to any other picture of a face!unknown_picture = face_recognition.load_image_file(\""unknown.jpg\"")unknown_face_encoding = face_recognition.face_encodings(unknown_picture)[0]# Now we can see the two face encodings are of the same person with `compare_faces`!results = face_recognition.compare_faces([my_face_encoding], unknown_face_encoding)if results[0] == True:    print(\""It's a picture of me!\"")else:    print(\""It's not a picture of me!\"")See this exampleto try it out.Python Code ExamplesAll the examples are available here.Face DetectionFind faces in a photographFind faces in a photograph (using deep learning)Find faces in batches of images w/ GPU (using deep learning)Blur all the faces in a live video using your webcam (Requires OpenCV to be installed)Facial FeaturesIdentify specific facial features in a photographApply (horribly ugly) digital make-upFacial RecognitionFind and recognize unknown faces in a photograph based on photographs of known peopleIdentify and draw boxes around each person in a photoCompare faces by numeric face distance instead of only True/False matchesRecognize faces in live video using your webcam - Simple / Slower Version (Requires OpenCV to be installed)Recognize faces in live video using your webcam - Faster Version (Requires OpenCV to be installed)Recognize faces in a video file and write out new video file (Requires OpenCV to be installed)Recognize faces on a Raspberry Pi w/ cameraRun a web service to recognize faces via HTTP (Requires Flask to be installed)Recognize faces with a K-nearest neighbors classifierTrain multiple images per person then recognize faces using a SVMCreating a Standalone ExecutableIf you want to create a standalone executable that can run without the need to install python or face_recognition, you can use PyInstaller. However, it requires some custom configuration to work with this library. See this issue for how to do it.Articles and Guides that cover face_recognitionMy article on how Face Recognition works: Modern Face Recognition with Deep LearningCovers the algorithms and how they generally workFace recognition with OpenCV, Python, and deep learning by Adrian RosebrockCovers how to use face recognition in practiceRaspberry Pi Face Recognition by Adrian RosebrockCovers how to use this on a Raspberry PiFace clustering with Python by Adrian RosebrockCovers how to automatically cluster photos based on who appears in each photo using unsupervised learningHow Face Recognition WorksIf you want to learn how face location and recognition work instead ofdepending on a black box library, read my article.CaveatsThe face recognition model is trained on adults and does not work very well on children. It tends to mixup children quite easy using the default comparison threshold of 0.6.Accuracy may vary between ethnic groups. Please see this wiki page for more details.Deployment to Cloud Hosts (Heroku, AWS, etc)Since face_recognition depends on dlib which is written in C++, it can be tricky to deploy an appusing it to a cloud hosting provider like Heroku or AWS.To make things easier, there's an example Dockerfile in this repo that shows how to run an app built withface_recognition in a Docker container. With that, you should be able to deployto any service that supports Docker images.You can try the Docker image locally by running: docker-compose up --buildThere are also several prebuilt Docker images.Linux users with a GPU (drivers >= 384.81) and Nvidia-Docker installed can run the example on the GPU: Open the docker-compose.yml file and uncomment the dockerfile: Dockerfile.gpu and runtime: nvidia lines.Having problems?If you run into problems, please read the Common Errors section of the wiki before filing a github issue.ThanksMany, many thanks to Davis King (@nulhom)for creating dlib and for providing the trained facial feature detection and face encoding modelsused in this library. For more information on the ResNet that powers the face encodings, check outhis blog post.Thanks to everyone who works on all the awesome Python data science libraries like numpy, scipy, scikit-image,pillow, etc, etc that makes this kind of stuff so easy and fun in Python.Thanks to Cookiecutter and theaudreyr/cookiecutter-pypackage project templatefor making Python project packaging way more tolerable."
67,swisskyrepo/PayloadsAllTheThings,https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/README.md,Python,"Payloads All The ThingsA list of useful payloads and bypasses for Web Application Security.Feel free to improve with your payloads and techniques !I ❤️ pull requests :)You can also contribute with a 🍻 IRL, or using the sponsor buttonAn alternative display version is available at PayloadsAllTheThingsWeb.  📖 DocumentationEvery section contains the following files, you can use the _template_vuln folder to create a new chapter:README.md - vulnerability description and how to exploit it, including several payloadsIntruder - a set of files to give to Burp IntruderImages - pictures for the README.mdFiles - some files referenced in the README.mdYou might also like the Methodology and Resources folder :Methodology and ResourcesActive Directory Attack.mdCloud - AWS Pentest.mdCloud - Azure Pentest.mdCobalt Strike - Cheatsheet.mdLinux - Evasion.mdLinux - Persistence.mdLinux - Privilege Escalation.mdMetasploit - Cheatsheet.mdMethodology and enumeration.mdNetwork Pivoting Techniques.mdNetwork Discovery.mdReverse Shell Cheatsheet.mdSubdomains Enumeration.mdWindows - AMSI Bypass.mdWindows - DPAPI.mdWindows - Download and Execute.mdWindows - Mimikatz.mdWindows - Persistence.mdWindows - Privilege Escalation.mdWindows - Using credentials.mdYou want more ? Check the Books and Youtube videos selections.👨‍💻 ContributionsBe sure to read CONTRIBUTING.md  Thanks again for your contribution! ❤️🧙‍♂️ SponsorsThis project is proudly sponsored by these companies."
68,xtekky/gpt4free,https://github.com/xtekky/gpt4free/blob/main/README.md,Python,"By using this repository or any code related to it, you agree to the legal notice. The author is not responsible for any copies, forks, or reuploads made by other users. This is the author's only account and repository. To prevent impersonation or irresponsible actions, you may comply with the GNU GPL license this Repository uses.This (quite censored) New Version of gpt4free, was just released, it may contain bugs, open an issue or contribute a PR when encountering one, some features were disabled.Docker is for now not available but I would be happy if someone contributes a PR. The g4f GUI will be uploaded soon enough.Newpypi package:pip install -U g4fTable of Contents:Getting StartedPrerequisitesSetting up the projectUsageThe g4f Packageinterference openai-proxy apiModelsgpt-3.5 / gpt-4Other ModelsRelated gpt4free projectsContributeChatGPT cloneCopyrightCopyright NoticeStar HistoryGetting StartedPrerequisites:Download and install Python (Version 3.x is recommended).Setting up the project:Install using pypipip install -U g4forClone the GitHub repository:git clone https://github.com/xtekky/gpt4free.gitNavigate to the project directory:cd gpt4free(Recommended) Create a virtual environment to manage Python packages for your project:python3 -m venv venvActivate the virtual environment:On Windows:.\\venv\\Scripts\\activateOn macOS and Linux:source venv/bin/activateInstall the required Python packages from requirements.txt:pip install -r requirements.txtCreate a test.py file in the root folder and start using the repo, further Instructions are belowimport g4f...UsageThe g4f Packageimport g4fprint(g4f.provider.Ails.params)  # supported args# Automatic selection of provider# streamed completionresponse = g4f.ChatCompletion.create(    model=\""gpt-3.5-turbo\"",    messages=[{\""role\"": \""user\"", \""content\"": \""Hello world\""}],    stream=True,)for message in response:    print(message, flush=True, end='')# normal responseresponse = g4f.ChatCompletion.create(    model=g4f.models.gpt_4,    messages=[{\""role\"": \""user\"", \""content\"": \""hi\""}],)  # alterative model settingprint(response)# Set with providerresponse = g4f.ChatCompletion.create(    model=\""gpt-3.5-turbo\"",    provider=g4f.provider.DeepAi,    messages=[{\""role\"": \""user\"", \""content\"": \""Hello world\""}],    stream=True,)for message in response:    print(message)providers:from g4f.provider import (    Acytoo,    Aichat,    Ails,    AiService,    AItianhu,    Bard,    Bing,    ChatgptAi,    ChatgptLogin,    DeepAi,    GetGpt)# usage:response = g4f.ChatCompletion.create(..., provider=ProviderName)interference openai-proxy api (use with openai python package)get requirements:pip install -r interference/requirements.txtrun server:python3 -m interference.appimport openaiopenai.api_key = \""\""openai.api_base = \""http://localhost:1337\""def main():    chat_completion = openai.ChatCompletion.create(        model=\""gpt-3.5-turbo\"",        messages=[{\""role\"": \""user\"", \""content\"": \""write a poem about a tree\""}],        stream=True,    )    if isinstance(chat_completion, dict):        # not stream        print(chat_completion.choices[0].message.content)    else:        # stream        for token in chat_completion:            content = token[\""choices\""][0][\""delta\""].get(\""content\"")            if content != None:                print(content, end=\""\"", flush=True)if __name__ == \""__main__\"":    main()Modelsgpt-3.5 / gpt-4WebsiteProvidergpt-3.5gpt-4StreamingStatusAuthwww.aitianhu.comg4f.provider.AItianhu✔️❌❌❌chat.acytoo.comg4f.provider.Acytoo✔️❌❌❌aiservice.vercel.appg4f.provider.AiService✔️❌❌❌chat-gpt.orgg4f.provider.Aichat✔️❌❌❌ai.lsg4f.provider.Ails✔️❌✔️❌bard.google.comg4f.provider.Bard❌❌❌✔️bing.comg4f.provider.Bing❌✔️❌❌chatgpt.aig4f.provider.ChatgptAi❌✔️❌❌chatgptlogin.acg4f.provider.ChatgptLogin✔️❌❌❌deepai.orgg4f.provider.DeepAi✔️❌✔️❌chat.dfehub.comg4f.provider.DfeHub✔️❌✔️❌free.easychat.workg4f.provider.EasyChat✔️❌✔️❌forefront.comg4f.provider.Forefront✔️❌✔️❌chat.getgpt.worldg4f.provider.GetGpt✔️❌✔️❌gpt-gm.h2o.aig4f.provider.H2o❌❌✔️❌liaobots.comg4f.provider.Liaobots✔️✔️✔️✔️supertest.lockchat.appg4f.provider.Lockchat✔️✔️✔️❌opchatgpts.netg4f.provider.Opchatgpts✔️❌❌❌backend.raycast.comg4f.provider.Raycast✔️✔️✔️✔️theb.aig4f.provider.Theb✔️❌✔️❌play.vercel.aig4f.provider.Vercel✔️❌❌❌wewordle.orgg4f.provider.Wewordle✔️❌❌❌you.comg4f.provider.You✔️❌❌❌chat9.yqcloud.topg4f.provider.Yqcloud✔️❌❌❌Other ModelsModelBase ProviderProviderWebsitepalmGoogleg4f.provider.Bardbard.google.comh2ogpt-gm-oasst1-en-2048-falcon-7b-v3Huggingfaceg4f.provider.H2owww.h2o.aih2ogpt-gm-oasst1-en-2048-falcon-40b-v1Huggingfaceg4f.provider.H2owww.h2o.aih2ogpt-gm-oasst1-en-2048-open-llama-13bHuggingfaceg4f.provider.H2owww.h2o.aiclaude-instant-v1Anthropicg4f.provider.Vercelsdk.vercel.aiclaude-v1Anthropicg4f.provider.Vercelsdk.vercel.aiclaude-v2Anthropicg4f.provider.Vercelsdk.vercel.aicommand-light-nightlyCohereg4f.provider.Vercelsdk.vercel.aicommand-nightlyCohereg4f.provider.Vercelsdk.vercel.aigpt-neox-20bHuggingfaceg4f.provider.Vercelsdk.vercel.aioasst-sft-1-pythia-12bHuggingfaceg4f.provider.Vercelsdk.vercel.aioasst-sft-4-pythia-12b-epoch-3.5Huggingfaceg4f.provider.Vercelsdk.vercel.aisantacoderHuggingfaceg4f.provider.Vercelsdk.vercel.aibloomHuggingfaceg4f.provider.Vercelsdk.vercel.aiflan-t5-xxlHuggingfaceg4f.provider.Vercelsdk.vercel.aicode-davinci-002OpenAIg4f.provider.Vercelsdk.vercel.aigpt-3.5-turbo-16kOpenAIg4f.provider.Vercelsdk.vercel.aigpt-3.5-turbo-16k-0613OpenAIg4f.provider.Vercelsdk.vercel.aigpt-4-0613OpenAIg4f.provider.Vercelsdk.vercel.aitext-ada-001OpenAIg4f.provider.Vercelsdk.vercel.aitext-babbage-001OpenAIg4f.provider.Vercelsdk.vercel.aitext-curie-001OpenAIg4f.provider.Vercelsdk.vercel.aitext-davinci-002OpenAIg4f.provider.Vercelsdk.vercel.aitext-davinci-003OpenAIg4f.provider.Vercelsdk.vercel.aillama13b-v2-chatReplicateg4f.provider.Vercelsdk.vercel.aillama7b-v2-chatReplicateg4f.provider.Vercelsdk.vercel.aiRelated gpt4free projects            🎁 Projects      ⭐ Stars      📚 Forks      🛎 Issues      📬 Pull requests                  gpt4free                                      gpt4free-ts                                      ChatGPT-Clone                                      ChatGpt Discord Bot                                      LangChain gpt4free                                      ChatGpt Telegram Bot                              Contributeto add another provider, its very simple:create a new file in g4f/provider with the name of the ProviderImplement a class that extends BaseProvider.from .base_provider import BaseProviderfrom ..typing import CreateResult, Anyclass HogeService(BaseProvider):    url = \""http://hoge.com\""    working = True    supports_gpt_35_turbo = True    @staticmethod    def create_completion(        model: str,        messages: list[dict[str, str]],        stream: bool,        **kwargs: Any,    ) -> CreateResult:        passHere, you can adjust the settings, for example if the website does support streaming, set working to True...Write code to request the provider in create_completion and yield the response, even if its a one-time response, do not hesitate to look at other providers for inspirationAdd the Provider Name in g4f/provider/init.pyfrom .base_provider import BaseProviderfrom .HogeService import HogeService__all__ = [  HogeService,]You are done !, test the provider by calling it:import g4fresponse = g4f.ChatCompletion.create(model='gpt-3.5-turbo', provider=g4f.provider.PROVIDERNAME,                                    messages=[{\""role\"": \""user\"", \""content\"": \""test\""}], stream=g4f.provider.PROVIDERNAME.supports_stream)for message in response:    print(message, flush=True, end='')ChatGPT cloneCurrently implementing new features and trying to scale it, please be patient it may be unstablehttps://chat.g4f.ai/chatThis site was developed by me and includes gpt-4/3.5, internet access and gpt-jailbreak's like DANRun locally here: https://github.com/xtekky/chatgpt-cloneCopyright:This program is licensed under the GNU GPL v3Copyright Notice:xtekky/gpt4free: Copyright (C) 2023 xtekkyThis program is free software: you can redistribute it and/or modifyit under the terms of the GNU General Public License as published bythe Free Software Foundation, either version 3 of the License, or(at your option) any later version.This program is distributed in the hope that it will be useful,but WITHOUT ANY WARRANTY; without even the implied warranty ofMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See theGNU General Public License for more details.You should have received a copy of the GNU General Public Licensealong with this program.  If not, see <https://www.gnu.org/licenses/>.Star History         "
69,TheAlgorithms/Python,https://github.com/TheAlgorithms/Python/blob/master/README.md,Python,          The Algorithms - Python                                                                  All algorithms implemented in Python - for educationImplementations are for learning purposes only. They may be less efficient than the implementations in the Python standard library. Use them at your discretion.Getting StartedRead through our Contribution Guidelines before you contribute.Community ChannelsWe are on Discord and Gitter! Community channels are a great way for you to ask questions and get help. Please join us!List of AlgorithmsSee our directory for easier navigation and a better overview of the project.
70,google/it-cert-automation-practice,https://github.com/google/it-cert-automation-practice/blob/master/README.md,Python,Google IT Automation with Python Professional Certificate - Practice filesThis repository contains the practice files used throughout the courses that arepart of the Google IT Automation with Python Professional CertificateThere's a separate folder for each course.
71,python/cpython,https://github.com/python/cpython/blob/main/README.rst,Python,"This is Python version 3.13.0 alpha 0Copyright © 2001-2023 Python Software Foundation.  All rights reserved.See the end of this file for further copyright and license information.ContentsGeneral InformationContributing to CPythonUsing PythonBuild InstructionsProfile Guided OptimizationLink Time OptimizationWhat's NewDocumentationConverting From Python 2.x to 3.xTestingInstalling multiple versionsRelease ScheduleCopyright and License InformationGeneral InformationWebsite: https://www.python.orgSource code: https://github.com/python/cpythonIssue tracker: https://github.com/python/cpython/issuesDocumentation: https://docs.python.orgDeveloper's Guide: https://devguide.python.org/Contributing to CPythonFor more complete instructions on contributing to CPython development,see the Developer Guide.Using PythonInstallable Python kits, and information about using Python, are available atpython.org.Build InstructionsOn Unix, Linux, BSD, macOS, and Cygwin:./configuremakemake testsudo make installThis will install Python as python3.You can pass many options to the configure script; run ./configure --helpto find out more.  On macOS case-insensitive file systems and on Cygwin,the executable is called python.exe; elsewhere it's just python.Building a complete Python installation requires the use of variousadditional third-party libraries, depending on your build platform andconfigure options.  Not all standard library modules are buildable oruseable on all platforms.  Refer to theInstall dependenciessection of the Developer Guide for current detailed information ondependencies for various Linux distributions and macOS.On macOS, there are additional configure and build options relatedto macOS framework and universal builds.  Refer to Mac/README.rst.On Windows, see PCbuild/readme.txt.If you wish, you can create a subdirectory and invoke configure from there.For example:mkdir debugcd debug../configure --with-pydebugmakemake test(This will fail if you also built at the top-level directory.  You should doa make clean at the top-level first.)To get an optimized build of Python, configure --enable-optimizationsbefore you run make.  This sets the default make targets up to enableProfile Guided Optimization (PGO) and may be used to auto-enable Link TimeOptimization (LTO) on some platforms.  For more details, see the sectionsbelow.Profile Guided OptimizationPGO takes advantage of recent versions of the GCC or Clang compilers.  If used,either via configure --enable-optimizations or by manually runningmake profile-opt regardless of configure flags, the optimized buildprocess will perform the following steps:The entire Python directory is cleaned of temporary files that may haveresulted from a previous compilation.An instrumented version of the interpreter is built, using suitable compilerflags for each flavor. Note that this is just an intermediary step.  Thebinary resulting from this step is not good for real-life workloads as it hasprofiling instructions embedded inside.After the instrumented interpreter is built, the Makefile will run a trainingworkload.  This is necessary in order to profile the interpreter's execution.Note also that any output, both stdout and stderr, that may appear at this stepis suppressed.The final step is to build the actual interpreter, using the informationcollected from the instrumented one.  The end result will be a Python binarythat is optimized; suitable for distribution or production installation.Link Time OptimizationEnabled via configure's --with-lto flag.  LTO takes advantage of theability of recent compiler toolchains to optimize across the otherwisearbitrary .o file boundary when building final executables or sharedlibraries for additional performance gains.What's NewWe have a comprehensive overview of the changes in the What's New in Python3.13 document.  For a moredetailed change log, read Misc/NEWS, but a fullaccounting of changes can only be gleaned from the commit history.If you want to install multiple versions of Python, see the section belowentitled \""Installing multiple versions\"".DocumentationDocumentation for Python 3.13 is online,updated daily.It can also be downloaded in many formats for faster access.  The documentationis downloadable in HTML, PDF, and reStructuredText formats; the latter versionis primarily for documentation authors, translators, and people with specialformatting requirements.For information about building Python's documentation, refer to Doc/README.rst.Converting From Python 2.x to 3.xSignificant backward incompatible changes were made for the release of Python3.0, which may cause programs written for Python 2 to fail when run with Python3.  For more information about porting your code from Python 2 to Python 3, seethe Porting HOWTO.TestingTo test the interpreter, type make test in the top-level directory.  Thetest set produces some output.  You can generally ignore the messages aboutskipped tests due to optional features which can't be imported.  If a messageis printed about a failed test or a traceback or core dump is produced,something is wrong.By default, tests are prevented from overusing resources like disk space andmemory.  To enable these tests, run make testall.If any tests fail, you can re-run the failing test(s) in verbose mode.  Forexample, if test_os and test_gdb failed, you can run:make test TESTOPTS=\""-v test_os test_gdb\""If the failure persists and appears to be a problem with Python rather thanyour environment, you can file a bug report and include relevant output fromthat command to show the issue.See Running & Writing Testsfor more on running tests.Installing multiple versionsOn Unix and Mac systems if you intend to install multiple versions of Pythonusing the same installation prefix (--prefix argument to the configurescript) you must take care that your primary python executable is notoverwritten by the installation of a different version.  All files anddirectories installed using make altinstall contain the major and minorversion and can thus live side-by-side.  make install also creates${prefix}/bin/python3 which refers to ${prefix}/bin/python3.X.  If youintend to install multiple versions using the same prefix you must decide whichversion (if any) is your \""primary\"" version.  Install that version using makeinstall.  Install all other versions using make altinstall.For example, if you want to install Python 2.7, 3.6, and 3.13 with 3.13 being theprimary version, you would execute make install in your 3.13 build directoryand make altinstall in the others.Release ScheduleSee PEP 719 for Python 3.13 release details.Copyright and License InformationCopyright © 2001-2023 Python Software Foundation.  All rights reserved.Copyright © 2000 BeOpen.com.  All rights reserved.Copyright © 1995-2001 Corporation for National Research Initiatives.  Allrights reserved.Copyright © 1991-1995 Stichting Mathematisch Centrum.  All rights reserved.See the LICENSE forinformation on the history of this software, terms & conditions for usage, and aDISCLAIMER OF ALL WARRANTIES.This Python distribution contains no GNU General Public License (GPL) code,so it may be used in proprietary projects.  There are interfaces to some GNUcode but these are entirely optional.All trademarks referenced herein are property of their respective holders."
72,home-assistant/core,https://github.com/home-assistant/core/blob/dev/README.rst,Python,"Home Assistant Open source home automation that puts local control and privacy first. Powered by a worldwide community of tinkerers and DIY enthusiasts. Perfect to run on a Raspberry Pi or a local server.Check out home-assistant.io for ademo, installation instructions,tutorials and documentation.Featured integrationsThe system is built using a modular approach so support for other devices or actions can be implemented easily. See also the section on architecture and the section on creating your owncomponents.If you run into issues while using Home Assistant or during developmentof a component, check the Home Assistant help section of our website for further help and information."
73,huggingface/transformers,https://github.com/huggingface/transformers/blob/main/README.md,Python,"                                                                                                                    English |        简体中文 |        繁體中文 |        한국어 |        Español |        日本語 |        हिन्दी        State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow    🤗 Transformers provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio.These models can be applied on:📝 Text, for tasks like text classification, information extraction, question answering, summarization, translation, text generation, in over 100 languages.🖼️ Images, for tasks like image classification, object detection, and segmentation.🗣️ Audio, for tasks like speech recognition and audio classification.Transformer models can also perform tasks on several modalities combined, such as table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.🤗 Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments.🤗 Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch and TensorFlow — with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other.Online demosYou can test most of our models directly on their pages from the model hub. We also offer private model hosting, versioning, & an inference API for public and private models.Here are a few examples:In Natural Language Processing:Masked word completion with BERTName Entity Recognition with ElectraText generation with GPT-2Natural Language Inference with RoBERTaSummarization with BARTQuestion answering with DistilBERTTranslation with T5In Computer Vision:Image classification with ViTObject Detection with DETRSemantic Segmentation with SegFormerPanoptic Segmentation with MaskFormerDepth Estimation with DPTVideo Classification with VideoMAEUniversal Segmentation with OneFormerIn Audio:Automatic Speech Recognition with Wav2Vec2Keyword Spotting with Wav2Vec2Audio Classification with Audio Spectrogram TransformerIn Multimodal tasks:Table Question Answering with TAPASVisual Question Answering with ViLTZero-shot Image Classification with CLIPDocument Question Answering with LayoutLMZero-shot Video Classification with X-CLIP100 projects using TransformersTransformers is more than a toolkit to use pretrained models: it's a community of projects built around it and theHugging Face Hub. We want Transformers to enable developers, researchers, students, professors, engineers, and anyoneelse to build their dream projects.In order to celebrate the 100,000 stars of transformers, we have decided to put the spotlight on thecommunity, and we have created the awesome-transformers page which lists 100incredible projects built in the vicinity of transformers.If you own or use a project that you believe should be part of the list, please open a PR to add it!If you are looking for custom support from the Hugging Face team    Quick tourTo immediately use a model on a given input (text, image, audio, ...), we provide the pipeline API. Pipelines group together a pretrained model with the preprocessing that was used during that model's training. Here is how to quickly use a pipeline to classify positive versus negative texts:>>> from transformers import pipeline# Allocate a pipeline for sentiment-analysis>>> classifier = pipeline('sentiment-analysis')>>> classifier('We are very happy to introduce pipeline to the transformers repository.')[{'label': 'POSITIVE', 'score': 0.9996980428695679}]The second line of code downloads and caches the pretrained model used by the pipeline, while the third evaluates it on the given text. Here the answer is \""positive\"" with a confidence of 99.97%.Many tasks have a pre-trained pipeline ready to go, in NLP but also in computer vision and speech. For example, we can easily extract detected objects in an image:>>> import requests>>> from PIL import Image>>> from transformers import pipeline# Download an image with cute cats>>> url = \""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample.png\"">>> image_data = requests.get(url, stream=True).raw>>> image = Image.open(image_data)# Allocate a pipeline for object detection>>> object_detector = pipeline('object-detection')>>> object_detector(image)[{'score': 0.9982201457023621,  'label': 'remote',  'box': {'xmin': 40, 'ymin': 70, 'xmax': 175, 'ymax': 117}}, {'score': 0.9960021376609802,  'label': 'remote',  'box': {'xmin': 333, 'ymin': 72, 'xmax': 368, 'ymax': 187}}, {'score': 0.9954745173454285,  'label': 'couch',  'box': {'xmin': 0, 'ymin': 1, 'xmax': 639, 'ymax': 473}}, {'score': 0.9988006353378296,  'label': 'cat',  'box': {'xmin': 13, 'ymin': 52, 'xmax': 314, 'ymax': 470}}, {'score': 0.9986783862113953,  'label': 'cat',  'box': {'xmin': 345, 'ymin': 23, 'xmax': 640, 'ymax': 368}}]Here we get a list of objects detected in the image, with a box surrounding the object and a confidence score. Here is the original image on the left, with the predictions displayed on the right:        You can learn more about the tasks supported by the pipeline API in this tutorial.In addition to pipeline, to download and use any of the pretrained models on your given task, all it takes is three lines of code. Here is the PyTorch version:>>> from transformers import AutoTokenizer, AutoModel>>> tokenizer = AutoTokenizer.from_pretrained(\""bert-base-uncased\"")>>> model = AutoModel.from_pretrained(\""bert-base-uncased\"")>>> inputs = tokenizer(\""Hello world!\"", return_tensors=\""pt\"")>>> outputs = model(**inputs)And here is the equivalent code for TensorFlow:>>> from transformers import AutoTokenizer, TFAutoModel>>> tokenizer = AutoTokenizer.from_pretrained(\""bert-base-uncased\"")>>> model = TFAutoModel.from_pretrained(\""bert-base-uncased\"")>>> inputs = tokenizer(\""Hello world!\"", return_tensors=\""tf\"")>>> outputs = model(**inputs)The tokenizer is responsible for all the preprocessing the pretrained model expects, and can be called directly on a single string (as in the above examples) or a list. It will output a dictionary that you can use in downstream code or simply directly pass to your model using the ** argument unpacking operator.The model itself is a regular Pytorch nn.Module or a TensorFlow tf.keras.Model (depending on your backend) which you can use as usual. This tutorial explains how to integrate such a model into a classic PyTorch or TensorFlow training loop, or how to use our Trainer API to quickly fine-tune on a new dataset.Why should I use transformers?Easy-to-use state-of-the-art models:High performance on natural language understanding & generation, computer vision, and audio tasks.Low barrier to entry for educators and practitioners.Few user-facing abstractions with just three classes to learn.A unified API for using all our pretrained models.Lower compute costs, smaller carbon footprint:Researchers can share trained models instead of always retraining.Practitioners can reduce compute time and production costs.Dozens of architectures with over 60,000 pretrained models across all modalities.Choose the right framework for every part of a model's lifetime:Train state-of-the-art models in 3 lines of code.Move a single model between TF2.0/PyTorch/JAX frameworks at will.Seamlessly pick the right framework for training, evaluation and production.Easily customize a model or an example to your needs:We provide examples for each architecture to reproduce the results published by its original authors.Model internals are exposed as consistently as possible.Model files can be used independently of the library for quick experiments.Why shouldn't I use transformers?This library is not a modular toolbox of building blocks for neural nets. The code in the model files is not refactored with additional abstractions on purpose, so that researchers can quickly iterate on each of the models without diving into additional abstractions/files.The training API is not intended to work on any model but is optimized to work with the models provided by the library. For generic machine learning loops, you should use another library (possibly, Accelerate).While we strive to present as many use cases as possible, the scripts in our examples folder are just that: examples. It is expected that they won't work out-of-the box on your specific problem and that you will be required to change a few lines of code to adapt them to your needs.InstallationWith pipThis repository is tested on Python 3.8+, Flax 0.4.1+, PyTorch 1.10+ and TensorFlow 2.6+.You should install 🤗 Transformers in a virtual environment. If you're unfamiliar with Python virtual environments, check out the user guide.First, create a virtual environment with the version of Python you're going to use and activate it.Then, you will need to install at least one of Flax, PyTorch or TensorFlow.Please refer to TensorFlow installation page, PyTorch installation page and/or Flax and Jax installation pages regarding the specific installation command for your platform.When one of those backends has been installed, 🤗 Transformers can be installed using pip as follows:pip install transformersIf you'd like to play with the examples or need the bleeding edge of the code and can't wait for a new release, you must install the library from source.With condaSince Transformers version v4.0.0, we now have a conda channel: huggingface.🤗 Transformers can be installed using conda as follows:conda install -c huggingface transformersFollow the installation pages of Flax, PyTorch or TensorFlow to see how to install them with conda.NOTE:  On Windows, you may be prompted to activate Developer Mode in order to benefit from caching. If this is not an option for you, please let us know in this issue.Model architecturesAll the model checkpoints provided by 🤗 Transformers are seamlessly integrated from the huggingface.co model hub where they are uploaded directly by users and organizations.Current number of checkpoints: 🤗 Transformers currently provides the following architectures (see here for a high-level summary of each them):ALBERT (from Google Research and the Toyota Technological Institute at Chicago) released with the paper ALBERT: A Lite BERT for Self-supervised Learning of Language Representations, by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut.ALIGN (from Google Research) released with the paper Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision by Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig.AltCLIP (from BAAI) released with the paper AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities by Chen, Zhongzhi and Liu, Guang and Zhang, Bo-Wen and Ye, Fulong and Yang, Qinghong and Wu, Ledell.Audio Spectrogram Transformer (from MIT) released with the paper AST: Audio Spectrogram Transformer by Yuan Gong, Yu-An Chung, James Glass.Autoformer (from Tsinghua University) released with the paper Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting by Haixu Wu, Jiehui Xu, Jianmin Wang, Mingsheng Long.Bark (from Suno) released in the repository suno-ai/bark by Suno AI team.BART (from Facebook) released with the paper BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension by Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov and Luke Zettlemoyer.BARThez (from École polytechnique) released with the paper BARThez: a Skilled Pretrained French Sequence-to-Sequence Model by Moussa Kamal Eddine, Antoine J.-P. Tixier, Michalis Vazirgiannis.BARTpho (from VinAI Research) released with the paper BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese by Nguyen Luong Tran, Duong Minh Le and Dat Quoc Nguyen.BEiT (from Microsoft) released with the paper BEiT: BERT Pre-Training of Image Transformers by Hangbo Bao, Li Dong, Furu Wei.BERT (from Google) released with the paper BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.BERT For Sequence Generation (from Google) released with the paper Leveraging Pre-trained Checkpoints for Sequence Generation Tasks by Sascha Rothe, Shashi Narayan, Aliaksei Severyn.BERTweet (from VinAI Research) released with the paper BERTweet: A pre-trained language model for English Tweets by Dat Quoc Nguyen, Thanh Vu and Anh Tuan Nguyen.BigBird-Pegasus (from Google Research) released with the paper Big Bird: Transformers for Longer Sequences by Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.BigBird-RoBERTa (from Google Research) released with the paper Big Bird: Transformers for Longer Sequences by Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.BioGpt (from Microsoft Research AI4Science) released with the paper BioGPT: generative pre-trained transformer for biomedical text generation and mining by Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon and Tie-Yan Liu.BiT (from Google AI) released with the paper Big Transfer (BiT): General Visual Representation Learning by Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, Neil Houlsby.Blenderbot (from Facebook) released with the paper Recipes for building an open-domain chatbot by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.BlenderbotSmall (from Facebook) released with the paper Recipes for building an open-domain chatbot by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.BLIP (from Salesforce) released with the paper BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation by Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi.BLIP-2 (from Salesforce) released with the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi.BLOOM (from BigScience workshop) released by the BigScience Workshop.BORT (from Alexa) released with the paper Optimal Subarchitecture Extraction For BERT by Adrian de Wynter and Daniel J. Perry.BridgeTower (from Harbin Institute of Technology/Microsoft Research Asia/Intel Labs) released with the paper BridgeTower: Building Bridges Between Encoders in Vision-Language Representation Learning by Xiao Xu, Chenfei Wu, Shachar Rosenman, Vasudev Lal, Wanxiang Che, Nan Duan.ByT5 (from Google Research) released with the paper ByT5: Towards a token-free future with pre-trained byte-to-byte models by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel.CamemBERT (from Inria/Facebook/Sorbonne) released with the paper CamemBERT: a Tasty French Language Model by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Suárez*, Yoann Dupont, Laurent Romary, Éric Villemonte de la Clergerie, Djamé Seddah and Benoît Sagot.CANINE (from Google Research) released with the paper CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation by Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting.Chinese-CLIP (from OFA-Sys) released with the paper Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese by An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, Chang Zhou.CLAP (from LAION-AI) released with the paper Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation by Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, Shlomo Dubnov.CLIP (from OpenAI) released with the paper Learning Transferable Visual Models From Natural Language Supervision by Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever.CLIPSeg (from University of Göttingen) released with the paper Image Segmentation Using Text and Image Prompts by Timo Lüddecke and Alexander Ecker.CodeGen (from Salesforce) released with the paper A Conversational Paradigm for Program Synthesis by Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong.Conditional DETR (from Microsoft Research Asia) released with the paper Conditional DETR for Fast Training Convergence by Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng, Houqiang Li, Yuhui Yuan, Lei Sun, Jingdong Wang.ConvBERT (from YituTech) released with the paper ConvBERT: Improving BERT with Span-based Dynamic Convolution by Zihang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, Shuicheng Yan.ConvNeXT (from Facebook AI) released with the paper A ConvNet for the 2020s by Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie.ConvNeXTV2 (from Facebook AI) released with the paper ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders by Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, Saining Xie.CPM (from Tsinghua University) released with the paper CPM: A Large-scale Generative Chinese Pre-trained Language Model by Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin, Yusheng Su, Haozhe Ji, Jian Guan, Fanchao Qi, Xiaozhi Wang, Yanan Zheng, Guoyang Zeng, Huanqi Cao, Shengqi Chen, Daixuan Li, Zhenbo Sun, Zhiyuan Liu, Minlie Huang, Wentao Han, Jie Tang, Juanzi Li, Xiaoyan Zhu, Maosong Sun.CPM-Ant (from OpenBMB) released by the OpenBMB.CTRL (from Salesforce) released with the paper CTRL: A Conditional Transformer Language Model for Controllable Generation by Nitish Shirish Keskar*, Bryan McCann*, Lav R. Varshney, Caiming Xiong and Richard Socher.CvT (from Microsoft) released with the paper CvT: Introducing Convolutions to Vision Transformers by Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, Lei Zhang.Data2Vec (from Facebook) released with the paper Data2Vec:  A General Framework for Self-supervised Learning in Speech, Vision and Language by Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, Michael Auli.DeBERTa (from Microsoft) released with the paper DeBERTa: Decoding-enhanced BERT with Disentangled Attention by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.DeBERTa-v2 (from Microsoft) released with the paper DeBERTa: Decoding-enhanced BERT with Disentangled Attention by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.Decision Transformer (from Berkeley/Facebook/Google) released with the paper Decision Transformer: Reinforcement Learning via Sequence Modeling by Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch.Deformable DETR (from SenseTime Research) released with the paper Deformable DETR: Deformable Transformers for End-to-End Object Detection by Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai.DeiT (from Facebook) released with the paper Training data-efficient image transformers & distillation through attention by Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Hervé Jégou.DePlot (from Google AI) released with the paper DePlot: One-shot visual language reasoning by plot-to-table translation by Fangyu Liu, Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Wenhu Chen, Nigel Collier, Yasemin Altun.DETA (from The University of Texas at Austin) released with the paper NMS Strikes Back by Jeffrey Ouyang-Zhang, Jang Hyun Cho, Xingyi Zhou, Philipp Krähenbühl.DETR (from Facebook) released with the paper End-to-End Object Detection with Transformers by Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko.DialoGPT (from Microsoft Research) released with the paper DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation by Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan.DiNAT (from SHI Labs) released with the paper Dilated Neighborhood Attention Transformer by Ali Hassani and Humphrey Shi.DINOv2 (from Meta AI) released with the paper DINOv2: Learning Robust Visual Features without Supervision by Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, Piotr Bojanowski.DistilBERT (from HuggingFace), released together with the paper DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2 into DistilGPT2, RoBERTa into DistilRoBERTa, Multilingual BERT into DistilmBERT and a German version of DistilBERT.DiT (from Microsoft Research) released with the paper DiT: Self-supervised Pre-training for Document Image Transformer by Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, Furu Wei.Donut (from NAVER), released together with the paper OCR-free Document Understanding Transformer by Geewook Kim, Teakgyu Hong, Moonbin Yim, Jeongyeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, Seunghyun Park.DPR (from Facebook) released with the paper Dense Passage Retrieval for Open-Domain Question Answering by Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih.DPT (from Intel Labs) released with the paper Vision Transformers for Dense Prediction by René Ranftl, Alexey Bochkovskiy, Vladlen Koltun.EfficientFormer (from Snap Research) released with the paper EfficientFormer: Vision Transformers at MobileNetSpeed by Yanyu Li, Geng Yuan, Yang Wen, Ju Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, Jian Ren.EfficientNet (from Google Brain) released with the paper EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks by Mingxing Tan, Quoc V. Le.ELECTRA (from Google Research/Stanford University) released with the paper ELECTRA: Pre-training text encoders as discriminators rather than generators by Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning.EnCodec (from Meta AI) released with the paper High Fidelity Neural Audio Compression by Alexandre Défossez, Jade Copet, Gabriel Synnaeve, Yossi Adi.EncoderDecoder (from Google Research) released with the paper Leveraging Pre-trained Checkpoints for Sequence Generation Tasks by Sascha Rothe, Shashi Narayan, Aliaksei Severyn.ERNIE (from Baidu) released with the paper ERNIE: Enhanced Representation through Knowledge Integration by Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, Hua Wu.ErnieM (from Baidu) released with the paper ERNIE-M: Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora by Xuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang.ESM (from Meta AI) are transformer protein language models.  ESM-1b was released with the paper Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences by Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, and Rob Fergus. ESM-1v was released with the paper Language models enable zero-shot prediction of the effects of mutations on protein function by Joshua Meier, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu and Alexander Rives. ESM-2 and ESMFold were released with the paper Language models of protein sequences at the scale of evolution enable accurate structure prediction by Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Sal Candido, Alexander Rives.Falcon (from Technology Innovation Institute) by Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme.FLAN-T5 (from Google AI) released in the repository google-research/t5x by Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason WeiFLAN-UL2 (from Google AI) released in the repository google-research/t5x by Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason WeiFlauBERT (from CNRS) released with the paper FlauBERT: Unsupervised Language Model Pre-training for French by Hang Le, Loïc Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen, Benoît Crabbé, Laurent Besacier, Didier Schwab.FLAVA (from Facebook AI) released with the paper FLAVA: A Foundational Language And Vision Alignment Model by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela.FNet (from Google Research) released with the paper FNet: Mixing Tokens with Fourier Transforms by James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon.FocalNet (from Microsoft Research) released with the paper Focal Modulation Networks by Jianwei Yang, Chunyuan Li, Xiyang Dai, Lu Yuan, Jianfeng Gao.Funnel Transformer (from CMU/Google Brain) released with the paper Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le.GIT (from Microsoft Research) released with the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, Lijuan Wang.GLPN (from KAIST) released with the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Doyeon Kim, Woonghyun Ga, Pyungwhan Ahn, Donggyu Joo, Sehwan Chun, Junmo Kim.GPT (from OpenAI) released with the paper Improving Language Understanding by Generative Pre-Training by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever.GPT Neo (from EleutherAI) released in the repository EleutherAI/gpt-neo by Sid Black, Stella Biderman, Leo Gao, Phil Wang and Connor Leahy.GPT NeoX (from EleutherAI) released with the paper GPT-NeoX-20B: An Open-Source Autoregressive Language Model by Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, Samuel WeinbachGPT NeoX Japanese (from ABEJA) released by Shinya Otani, Takayoshi Makabe, Anuj Arora, and Kyo Hattori.GPT-2 (from OpenAI) released with the paper Language Models are Unsupervised Multitask Learners by Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei** and Ilya Sutskever**.GPT-J (from EleutherAI) released in the repository kingoflolz/mesh-transformer-jax by Ben Wang and Aran Komatsuzaki.GPT-Sw3 (from AI-Sweden) released with the paper Lessons Learned from GPT-SW3: Building the First Large-Scale Generative Language Model for Swedish by Ariel Ekgren, Amaru Cuba Gyllensten, Evangelia Gogoulou, Alice Heiman, Severine Verlinden, Joey Öhman, Fredrik Carlsson, Magnus Sahlgren.GPTBigCode (from BigCode) released with the paper SantaCoder: don't reach for the stars! by Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, Joel Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco De Toni, Bernardo García del Río, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries, Leandro von Werra.GPTSAN-japanese released in the repository tanreinama/GPTSAN by Toshiyuki Sakamoto(tanreinama).Graphormer (from Microsoft) released with the paper Do Transformers Really Perform Bad for Graph Representation? by Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, Tie-Yan Liu.GroupViT (from UCSD, NVIDIA) released with the paper GroupViT: Semantic Segmentation Emerges from Text Supervision by Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, Xiaolong Wang.Hubert (from Facebook) released with the paper HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed.I-BERT (from Berkeley) released with the paper I-BERT: Integer-only BERT Quantization by Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W. Mahoney, Kurt Keutzer.ImageGPT (from OpenAI) released with the paper Generative Pretraining from Pixels by Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, Ilya Sutskever.Informer (from Beihang University, UC Berkeley, Rutgers University, SEDD Company) released with the paper Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting by Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang.InstructBLIP (from Salesforce) released with the paper InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning by Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi.Jukebox (from OpenAI) released with the paper Jukebox: A Generative Model for Music by Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, Ilya Sutskever.LayoutLM (from Microsoft Research Asia) released with the paper LayoutLM: Pre-training of Text and Layout for Document Image Understanding by Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou.LayoutLMv2 (from Microsoft Research Asia) released with the paper LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding by Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, Lidong Zhou.LayoutLMv3 (from Microsoft Research Asia) released with the paper LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking by Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, Furu Wei.LayoutXLM (from Microsoft Research Asia) released with the paper LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding by Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Furu Wei.LED (from AllenAI) released with the paper Longformer: The Long-Document Transformer by Iz Beltagy, Matthew E. Peters, Arman Cohan.LeViT (from Meta AI) released with the paper LeViT: A Vision Transformer in ConvNet's Clothing for Faster Inference by Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Hervé Jégou, Matthijs Douze.LiLT (from South China University of Technology) released with the paper LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding by Jiapeng Wang, Lianwen Jin, Kai Ding.LLaMA (from The FAIR team of Meta AI) released with the paper LLaMA: Open and Efficient Foundation Language Models by Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample.Llama2 (from The FAIR team of Meta AI) released with the paper Llama2: Open Foundation and Fine-Tuned Chat Models by Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushka rMishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing EllenTan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom.Longformer (from AllenAI) released with the paper Longformer: The Long-Document Transformer by Iz Beltagy, Matthew E. Peters, Arman Cohan.LongT5 (from Google AI) released with the paper LongT5: Efficient Text-To-Text Transformer for Long Sequences by Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, Yinfei Yang.LUKE (from Studio Ousia) released with the paper LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention by Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, Yuji Matsumoto.LXMERT (from UNC Chapel Hill) released with the paper LXMERT: Learning Cross-Modality Encoder Representations from Transformers for Open-Domain Question Answering by Hao Tan and Mohit Bansal.M-CTC-T (from Facebook) released with the paper Pseudo-Labeling For Massively Multilingual Speech Recognition by Loren Lugosch, Tatiana Likhomanenko, Gabriel Synnaeve, and Ronan Collobert.M2M100 (from Facebook) released with the paper Beyond English-Centric Multilingual Machine Translation by Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, Armand Joulin.MarianMT Machine translation models trained using OPUS data by Jörg Tiedemann. The Marian Framework is being developed by the Microsoft Translator Team.MarkupLM (from Microsoft Research Asia) released with the paper MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document Understanding by Junlong Li, Yiheng Xu, Lei Cui, Furu Wei.Mask2Former (from FAIR and UIUC) released with the paper Masked-attention Mask Transformer for Universal Image Segmentation by Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, Rohit Girdhar.MaskFormer (from Meta and UIUC) released with the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation by Bowen Cheng, Alexander G. Schwing, Alexander Kirillov.MatCha (from Google AI) released with the paper MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering by Fangyu Liu, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Yasemin Altun, Nigel Collier, Julian Martin Eisenschlos.mBART (from Facebook) released with the paper Multilingual Denoising Pre-training for Neural Machine Translation by Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, Luke Zettlemoyer.mBART-50 (from Facebook) released with the paper Multilingual Translation with Extensible Multilingual Pretraining and Finetuning by Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, Angela Fan.MEGA (from Meta/USC/CMU/SJTU) released with the paper Mega: Moving Average Equipped Gated Attention by Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer.Megatron-BERT (from NVIDIA) released with the paper Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper and Bryan Catanzaro.Megatron-GPT2 (from NVIDIA) released with the paper Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper and Bryan Catanzaro.MGP-STR (from Alibaba Research) released with the paper Multi-Granularity Prediction for Scene Text Recognition by Peng Wang, Cheng Da, and Cong Yao.mLUKE (from Studio Ousia) released with the paper mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models by Ryokan Ri, Ikuya Yamada, and Yoshimasa Tsuruoka.MMS (from Facebook) released with the paper Scaling Speech Technology to 1,000+ Languages by Vineel Pratap, Andros Tjandra, Bowen Shi, Paden Tomasello, Arun Babu, Sayani Kundu, Ali Elkahky, Zhaoheng Ni, Apoorv Vyas, Maryam Fazel-Zarandi, Alexei Baevski, Yossi Adi, Xiaohui Zhang, Wei-Ning Hsu, Alexis Conneau, Michael Auli.MobileBERT (from CMU/Google Brain) released with the paper MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices by Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou.MobileNetV1 (from Google Inc.) released with the paper MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications by Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam.MobileNetV2 (from Google Inc.) released with the paper MobileNetV2: Inverted Residuals and Linear Bottlenecks by Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen.MobileViT (from Apple) released with the paper MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer by Sachin Mehta and Mohammad Rastegari.MobileViTV2 (from Apple) released with the paper Separable Self-attention for Mobile Vision Transformers by Sachin Mehta and Mohammad Rastegari.MPNet (from Microsoft Research) released with the paper MPNet: Masked and Permuted Pre-training for Language Understanding by Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu.MPT (from MosaiML) released with the repository llm-foundry by the MosaicML NLP Team.MRA (from the University of Wisconsin - Madison) released with the paper Multi Resolution Analysis (MRA) for Approximate Self-Attention by Zhanpeng Zeng, Sourav Pal, Jeffery Kline, Glenn M Fung, Vikas Singh.MT5 (from Google AI) released with the paper mT5: A massively multilingual pre-trained text-to-text transformer by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel.MusicGen (from Meta) released with the paper Simple and Controllable Music Generation by Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi and Alexandre Défossez.MVP (from RUC AI Box) released with the paper MVP: Multi-task Supervised Pre-training for Natural Language Generation by Tianyi Tang, Junyi Li, Wayne Xin Zhao and Ji-Rong Wen.NAT (from SHI Labs) released with the paper Neighborhood Attention Transformer by Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and Humphrey Shi.Nezha (from Huawei Noah’s Ark Lab) released with the paper NEZHA: Neural Contextualized Representation for Chinese Language Understanding by Junqiu Wei, Xiaozhe Ren, Xiaoguang Li, Wenyong Huang, Yi Liao, Yasheng Wang, Jiashu Lin, Xin Jiang, Xiao Chen and Qun Liu.NLLB (from Meta) released with the paper No Language Left Behind: Scaling Human-Centered Machine Translation by the NLLB team.NLLB-MOE (from Meta) released with the paper No Language Left Behind: Scaling Human-Centered Machine Translation by the NLLB team.Nyströmformer (from the University of Wisconsin - Madison) released with the paper Nyströmformer: A Nyström-Based Algorithm for Approximating Self-Attention by Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, Vikas Singh.OneFormer (from SHI Labs) released with the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov, Humphrey Shi.OpenLlama (from s-JoL) released in Open-Llama.OPT (from Meta AI) released with the paper OPT: Open Pre-trained Transformer Language Models by Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen et al.OWL-ViT (from Google AI) released with the paper Simple Open-Vocabulary Object Detection with Vision Transformers by Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby.Pegasus (from Google) released with the paper PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu.PEGASUS-X (from Google) released with the paper Investigating Efficiently Extending Transformers for Long Input Summarization by Jason Phang, Yao Zhao, and Peter J. Liu.Perceiver IO (from Deepmind) released with the paper Perceiver IO: A General Architecture for Structured Inputs & Outputs by Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier Hénaff, Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, João Carreira.PhoBERT (from VinAI Research) released with the paper PhoBERT: Pre-trained language models for Vietnamese by Dat Quoc Nguyen and Anh Tuan Nguyen.Pix2Struct (from Google) released with the paper Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding by Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, Kristina Toutanova.PLBart (from UCLA NLP) released with the paper Unified Pre-training for Program Understanding and Generation by Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, Kai-Wei Chang.PoolFormer (from Sea AI Labs) released with the paper MetaFormer is Actually What You Need for Vision by Yu, Weihao and Luo, Mi and Zhou, Pan and Si, Chenyang and Zhou, Yichen and Wang, Xinchao and Feng, Jiashi and Yan, Shuicheng.ProphetNet (from Microsoft Research) released with the paper ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang and Ming Zhou.PVT (from Nanjing University, The University of Hong Kong etc.) released with the paper Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions by Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao.QDQBert (from NVIDIA) released with the paper Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation by Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev and Paulius Micikevicius.RAG (from Facebook) released with the paper Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela.REALM (from Google Research) released with the paper REALM: Retrieval-Augmented Language Model Pre-Training by Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat and Ming-Wei Chang.Reformer (from Google Research) released with the paper Reformer: The Efficient Transformer by Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya.RegNet (from META Platforms) released with the paper Designing Network Design Space by Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, Piotr Dollár.RemBERT (from Google Research) released with the paper Rethinking embedding coupling in pre-trained language models by Hyung Won Chung, Thibault Févry, Henry Tsai, M. Johnson, Sebastian Ruder.ResNet (from Microsoft Research) released with the paper Deep Residual Learning for Image Recognition by Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun.RoBERTa (from Facebook), released together with the paper RoBERTa: A Robustly Optimized BERT Pretraining Approach by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.RoBERTa-PreLayerNorm (from Facebook) released with the paper fairseq: A Fast, Extensible Toolkit for Sequence Modeling by Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli.RoCBert (from WeChatAI) released with the paper RoCBert: Robust Chinese Bert with Multimodal Contrastive Pretraining by HuiSu, WeiweiShi, XiaoyuShen, XiaoZhou, TuoJi, JiaruiFang, JieZhou.RoFormer (from ZhuiyiTechnology), released together with the paper RoFormer: Enhanced Transformer with Rotary Position Embedding by Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu.RWKV (from Bo Peng), released on this repo by Bo Peng.SegFormer (from NVIDIA) released with the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping Luo.Segment Anything (from Meta AI) released with the paper Segment Anything by Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alex Berg, Wan-Yen Lo, Piotr Dollar, Ross Girshick.SEW (from ASAPP) released with the paper Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition by Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.SEW-D (from ASAPP) released with the paper Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition by Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.SpeechT5 (from Microsoft Research) released with the paper SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing by Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei.SpeechToTextTransformer (from Facebook), released together with the paper fairseq S2T: Fast Speech-to-Text Modeling with fairseq by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Dmytro Okhonko, Juan Pino.SpeechToTextTransformer2 (from Facebook), released together with the paper Large-Scale Self- and Semi-Supervised Learning for Speech Translation by Changhan Wang, Anne Wu, Juan Pino, Alexei Baevski, Michael Auli, Alexis Conneau.Splinter (from Tel Aviv University), released together with the paper Few-Shot Question Answering by Pretraining Span Selection by Ori Ram, Yuval Kirstain, Jonathan Berant, Amir Globerson, Omer Levy.SqueezeBERT (from Berkeley) released with the paper SqueezeBERT: What can computer vision teach NLP about efficient neural networks? by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer.SwiftFormer (from MBZUAI) released with the paper SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications by Abdelrahman Shaker, Muhammad Maaz, Hanoona Rasheed, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan.Swin Transformer (from Microsoft) released with the paper Swin Transformer: Hierarchical Vision Transformer using Shifted Windows by Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo.Swin Transformer V2 (from Microsoft) released with the paper Swin Transformer V2: Scaling Up Capacity and Resolution by Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, Baining Guo.Swin2SR (from University of Würzburg) released with the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Marcos V. Conde, Ui-Jin Choi, Maxime Burchi, Radu Timofte.SwitchTransformers (from Google) released with the paper Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity by William Fedus, Barret Zoph, Noam Shazeer.T5 (from Google AI) released with the paper Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu.T5v1.1 (from Google AI) released in the repository google-research/text-to-text-transfer-transformer by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu.Table Transformer (from Microsoft Research) released with the paper PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents by Brandon Smock, Rohith Pesala, Robin Abraham.TAPAS (from Google AI) released with the paper TAPAS: Weakly Supervised Table Parsing via Pre-training by Jonathan Herzig, Paweł Krzysztof Nowak, Thomas Müller, Francesco Piccinno and Julian Martin Eisenschlos.TAPEX (from Microsoft Research) released with the paper TAPEX: Table Pre-training via Learning a Neural SQL Executor by Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou.Time Series Transformer (from HuggingFace).TimeSformer (from Facebook) released with the paper Is Space-Time Attention All You Need for Video Understanding? by Gedas Bertasius, Heng Wang, Lorenzo Torresani.Trajectory Transformer (from the University of California at Berkeley) released with the paper Offline Reinforcement Learning as One Big Sequence Modeling Problem by Michael Janner, Qiyang Li, Sergey LevineTransformer-XL (from Google/CMU) released with the paper Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context by Zihang Dai*, Zhilin Yang*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.TrOCR (from Microsoft), released together with the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, Furu Wei.TVLT (from UNC Chapel Hill) released with the paper TVLT: Textless Vision-Language Transformer by Zineng Tang, Jaemin Cho, Yixin Nie, Mohit Bansal.UL2 (from Google Research) released with the paper Unifying Language Learning Paradigms by Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, Donald MetzlerUMT5 (from Google Research) released with the paper UniMax: Fairer and More Effective Language Sampling for Large-Scale Multilingual Pretraining by Hyung Won Chung, Xavier Garcia, Adam Roberts, Yi Tay, Orhan Firat, Sharan Narang, Noah Constant.UniSpeech (from Microsoft Research) released with the paper UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael Zeng, Xuedong Huang.UniSpeechSat (from Microsoft Research) released with the paper UNISPEECH-SAT: UNIVERSAL SPEECH REPRESENTATION LEARNING WITH SPEAKER AWARE PRE-TRAINING by Sanyuan Chen, Yu Wu, Chengyi Wang, Zhengyang Chen, Zhuo Chen, Shujie Liu, Jian Wu, Yao Qian, Furu Wei, Jinyu Li, Xiangzhan Yu.UPerNet (from Peking University) released with the paper Unified Perceptual Parsing for Scene Understanding by Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, Jian Sun.VAN (from Tsinghua University and Nankai University) released with the paper Visual Attention Network by Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, Shi-Min Hu.VideoMAE (from Multimedia Computing Group, Nanjing University) released with the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Zhan Tong, Yibing Song, Jue Wang, Limin Wang.ViLT (from NAVER AI Lab/Kakao Enterprise/Kakao Brain) released with the paper ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision by Wonjae Kim, Bokyung Son, Ildoo Kim.Vision Transformer (ViT) (from Google AI) released with the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.VisualBERT (from UCLA NLP) released with the paper VisualBERT: A Simple and Performant Baseline for Vision and Language by Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang.ViT Hybrid (from Google AI) released with the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.ViTMAE (from Meta AI) released with the paper Masked Autoencoders Are Scalable Vision Learners by Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick.ViTMSN (from Meta AI) released with the paper Masked Siamese Networks for Label-Efficient Learning by Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes, Pascal Vincent, Armand Joulin, Michael Rabbat, Nicolas Ballas.ViViT (from Google Research) released with the paper ViViT: A Video Vision Transformer by Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lučić, Cordelia Schmid.Wav2Vec2 (from Facebook AI) released with the paper wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.Wav2Vec2-Conformer (from Facebook AI) released with the paper FAIRSEQ S2T: Fast Speech-to-Text Modeling with FAIRSEQ by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Sravya Popuri, Dmytro Okhonko, Juan Pino.Wav2Vec2Phoneme (from Facebook AI) released with the paper Simple and Effective Zero-shot Cross-lingual Phoneme Recognition by Qiantong Xu, Alexei Baevski, Michael Auli.WavLM (from Microsoft Research) released with the paper WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing by Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Furu Wei.Whisper (from OpenAI) released with the paper Robust Speech Recognition via Large-Scale Weak Supervision by Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever.X-CLIP (from Microsoft Research) released with the paper Expanding Language-Image Pretrained Models for General Video Recognition by Bolin Ni, Houwen Peng, Minghao Chen, Songyang Zhang, Gaofeng Meng, Jianlong Fu, Shiming Xiang, Haibin Ling.X-MOD (from Meta AI) released with the paper Lifting the Curse of Multilinguality by Pre-training Modular Transformers by Jonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James Cross, Sebastian Riedel, Mikel Artetxe.XGLM (From Facebook AI) released with the paper Few-shot Learning with Multilingual Language Models by Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, Xian Li.XLM (from Facebook) released together with the paper Cross-lingual Language Model Pretraining by Guillaume Lample and Alexis Conneau.XLM-ProphetNet (from Microsoft Research) released with the paper ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang and Ming Zhou.XLM-RoBERTa (from Facebook AI), released together with the paper Unsupervised Cross-lingual Representation Learning at Scale by Alexis Conneau*, Kartikay Khandelwal*, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov.XLM-RoBERTa-XL (from Facebook AI), released together with the paper Larger-Scale Transformers for Multilingual Masked Language Modeling by Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, Alexis Conneau.XLM-V (from Meta AI) released with the paper XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models by Davis Liang, Hila Gonen, Yuning Mao, Rui Hou, Naman Goyal, Marjan Ghazvininejad, Luke Zettlemoyer, Madian Khabsa.XLNet (from Google/CMU) released with the paper ​XLNet: Generalized Autoregressive Pretraining for Language Understanding by Zhilin Yang*, Zihang Dai*, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.XLS-R (from Facebook AI) released with the paper XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale by Arun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman Goyal, Kritika Singh, Patrick von Platen, Yatharth Saraf, Juan Pino, Alexei Baevski, Alexis Conneau, Michael Auli.XLSR-Wav2Vec2 (from Facebook AI) released with the paper Unsupervised Cross-Lingual Representation Learning For Speech Recognition by Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, Michael Auli.YOLOS (from Huazhong University of Science & Technology) released with the paper You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Yuxin Fang, Bencheng Liao, Xinggang Wang, Jiemin Fang, Jiyang Qi, Rui Wu, Jianwei Niu, Wenyu Liu.YOSO (from the University of Wisconsin - Madison) released with the paper You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling by Zhanpeng Zeng, Yunyang Xiong, Sathya N. Ravi, Shailesh Acharya, Glenn Fung, Vikas Singh.Want to contribute a new model? We have added a detailed guide and templates to guide you in the process of adding a new model. You can find them in the templates folder of the repository. Be sure to check the contributing guidelines and contact the maintainers or open an issue to collect feedbacks before starting your PR.To check if each model has an implementation in Flax, PyTorch or TensorFlow, or has an associated tokenizer backed by the 🤗 Tokenizers library, refer to this table.These implementations have been tested on several datasets (see the example scripts) and should match the performance of the original implementations. You can find more details on performance in the Examples section of the documentation.Learn moreSectionDescriptionDocumentationFull API documentation and tutorialsTask summaryTasks supported by 🤗 TransformersPreprocessing tutorialUsing the Tokenizer class to prepare data for the modelsTraining and fine-tuningUsing the models provided by 🤗 Transformers in a PyTorch/TensorFlow training loop and the Trainer APIQuick tour: Fine-tuning/usage scriptsExample scripts for fine-tuning models on a wide range of tasksModel sharing and uploadingUpload and share your fine-tuned models with the communityCitationWe now have a paper you can cite for the 🤗 Transformers library:@inproceedings{wolf-etal-2020-transformers,    title = \""Transformers: State-of-the-Art Natural Language Processing\"",    author = \""Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush\"",    booktitle = \""Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\"",    month = oct,    year = \""2020\"",    address = \""Online\"",    publisher = \""Association for Computational Linguistics\"",    url = \""https://www.aclweb.org/anthology/2020.emnlp-demos.6\"",    pages = \""38--45\""}"
74,keras-team/keras,https://github.com/keras-team/keras/blob/master/README.md,Python,"Keras: Deep Learning for humansThis repository hosts the development of the Keras library.Read the documentation at keras.io.About KerasKeras is a deep learning API written in Python,running on top of the machine learning platform TensorFlow.It was developed with a focus on enabling fast experimentation andproviding a delightful developer experience.The purpose of Keras is to give an unfair advantage to any developer looking to ship ML-powered apps.Keras is:Simple -- but not simplistic. Keras reduces developer cognitive loadto free you to focus on the parts of the problem that really matter.Keras focuses on ease of use, debugging speed, code elegance & conciseness,maintainability, and deployability (via TFServing, TFLite, TF.js).Flexible -- Keras adopts the principle of progressive disclosure ofcomplexity: simple workflows should be quick and easy, while arbitrarilyadvanced workflows should be possible via a clear path that builds uponwhat you've already learned.Powerful -- Keras provides industry-strength performance andscalability: it is used by organizations and companies including NASA,YouTube, and Waymo. That's right -- your YouTube recommendations arepowered by Keras, and so is the world's most advanced driverless vehicle.Keras & TensorFlow 2TensorFlow 2 is an end-to-end, open-source machine learning platform.You can think of it as an infrastructure layer fordifferentiable programming.It combines four key abilities:Efficiently executing low-level tensor operations on CPU, GPU, or TPU.Computing the gradient of arbitrary differentiable expressions.Scaling computation to many devices, such as clusters of hundreds of GPUs.Exporting programs (\""graphs\"") to external runtimes such as servers, browsers, mobile and embedded devices.Keras is the high-level API of TensorFlow 2: an approachable, highly-productive interfacefor solving machine learning problems,with a focus on modern deep learning. It provides essential abstractions and building blocks for developingand shipping machine learning solutions with high iteration velocity.Keras empowers engineers and researchers to take full advantage of the scalabilityand cross-platform capabilities of TensorFlow 2: you can run Keras on TPU or on large clusters of GPUs,and you can export your Keras models to run in the browser or on a mobile device.First contact with KerasThe core data structures of Keras are layers and models.The simplest type of model is the Sequential model, a linear stack of layers.For more complex architectures, you should use the Keras functional API,which allows you to build arbitrary graphs of layers or write models entirely from scratch via subclassing.Here is the Sequential model:from tensorflow.keras.models import Sequentialmodel = Sequential()Stacking layers is as easy as .add():from tensorflow.keras.layers import Densemodel.add(Dense(units=64, activation='relu'))model.add(Dense(units=10, activation='softmax'))Once your model looks good, configure its learning process with .compile():model.compile(loss='categorical_crossentropy',              optimizer='sgd',              metrics=['accuracy'])If you need to, you can further configure your optimizer. The Keras philosophy is to keep simple things simple,while allowing the user to be fully in control when they need to be (the ultimate control being the easy extensibility of the source code via subclassing).model.compile(loss=tf.keras.losses.categorical_crossentropy,              optimizer=tf.keras.optimizers.SGD(                  learning_rate=0.01, momentum=0.9, nesterov=True))You can now iterate on your training data in batches:# x_train and y_train are Numpy arrays.model.fit(x_train, y_train, epochs=5, batch_size=32)Evaluate your test loss and metrics in one line:loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)Or generate predictions on new data:classes = model.predict(x_test, batch_size=128)What you just saw is the most elementary way to use Keras.However, Keras is also a highly-flexible framework suitable to iterate on state-of-the-art research ideas.Keras follows the principle of progressive disclosure of complexity: it makes it easy to get started,yet it makes it possible to handle arbitrarily advanced use cases,only requiring incremental learning at each step.In pretty much the same way that you were able to train & evaluate a simple neural network above in a few lines,you can use Keras to quickly develop new training procedures or exotic model architectures.Here's a low-level training loop example, combining Keras functionality with the TensorFlow GradientTape:import tensorflow as tf# Prepare an optimizer.optimizer = tf.keras.optimizers.Adam()# Prepare a loss function.loss_fn = tf.keras.losses.kl_divergence# Iterate over the batches of a dataset.for inputs, targets in dataset:    # Open a GradientTape.    with tf.GradientTape() as tape:        # Forward pass.        predictions = model(inputs)        # Compute the loss value for this batch.        loss_value = loss_fn(targets, predictions)    # Get gradients of loss wrt the weights.    gradients = tape.gradient(loss_value, model.trainable_weights)    # Update the weights of the model.    optimizer.apply_gradients(zip(gradients, model.trainable_weights))For more in-depth tutorials about Keras, you can check out:Introduction to Keras for engineersIntroduction to Keras for researchersDeveloper guidesOther learning resourcesInstallationKeras comes packaged with TensorFlow 2 as tensorflow.keras.To start using Keras, simply install TensorFlow 2.You can then import Keras as follows:from tensorflow import kerasRelease and compatibilityKeras has nightly releases (keras-nightly on PyPI)and stable releases (keras on PyPI).The nightly Keras releases are usually compatible with the corresponding versionof the tf-nightly releases(e.g. keras-nightly==2.7.0.dev2021100607 should beused with tf-nightly==2.7.0.dev2021100607).We don't maintain backward compatibility for nightly releases.For stable releases, each Kerasversion maps to a specific stable version of TensorFlow.The table below shows the compatibility version mappingbetween TensorFlow versions and Keras versions.All the release branches can be found on GitHub.All the release binaries can be found on Pypi.SupportYou can ask questions and join the development discussion:In the TensorFlow forum.On the Keras mailing list.Opening an issueYou can also post bug reports and feature requests (only)in GitHub issues.Opening a PRWe welcome contributions! Before opening a PR, please readour contributor guide,and the API design guideline."
75,Ebazhanov/linkedin-skill-assessments-quizzes,https://github.com/Ebazhanov/linkedin-skill-assessments-quizzes/blob/main/README.md,Python,"Linkedin Skill assessments - Answers⚠️ DISCLAIMER: The owners of this repository are not liable for any illegal usage of the content provided in this repository. The content is provided for informational and educational purposes only, and any actions taken by users of this repository are the responsibility of the user. By accessing this repository, you agree to hold the owners harmless from any claims, damages, or expenses arising from the use of the information provided.[ Go to see the last contributor ]🙏 PLEASEAlways add explanation (or reference link) to your answers. Use online grammar checker.That would help anyone to better learn new concepts!🎉 AnnouncementsColumn Translation have links to quizzes in different languages like Es, Fr, It and De.If you want to meet each other or discuss quiz related problems or maybe ask for skills endorsement just join the Discord chat.Playground before taking quiz using:MD2Practice (Web App)Skill Assessments Quizzes (Web App)LinkedIn Quizzed with Kodyfire (Terminal)Want to contribute? Here is the source code.❓ Need help?Open new issue🔥 Open in VS Code view here or thereTable of ContentsLinkedin-quiz-questionsPassed/FailedTranslated  in ...QuestionsAnswersYour resource for answers. In case you have doubts please contact this person or add them to review your PR.Accounting❗needs updating  5049@tujinwei, @mervynteo, @johnfelipeAdobe-Acrobat  2722Adobe-Illustrator❗needs updating  7674Adobe-InDesign❗needs updating  4240Adobe-Lightroom❗needs updating  2020Adobe-Photoshop❗needs updating  9393@declarckAdobe Premiere Pro  4836Adobe XD  1613After Effects❗needs updating  2413Agile Methodologies❗needs updating  116116@its0x08Android  7272@surajsahani, @mr-shoxruxbek, @ItSNeverLateAngular  7965@vanekbr, @aamita96ArcGIS Products55AutoCAD❗needs updating  7775@djayorAutodesk Fusion 360❗needs updating  3725@djayor, @tm-sanjayAutodesk Maya3030@marifogluAWS  9999@jokerkeny, @Amsal1AWS-Lambda❗needs updating  5149Bash  7877@D4RIO, @Amsal1C#6161@LiviuSosu, @RamonMartinezNieto, @declarckC++❗needs updating7373@Amsal1, @Amsal1C (Programming Language)8383@makifay, @Amsal1, @its0x08CSS122116@BHARGAVPATEL1244Cybersecurity❗needs updating10196Django7171@PROCW.NET Framework6359@declarckEclipse❗needs updating3628Front-end Development6868@vanekbr, @ShankS3, @declarckGit134134@Emanuele-emGo (Programming Language)4040@ruslanbes, @monkrusGoogle Ads2925Google Analytics8282Google Cloud Platform (GCP)5250@antra0497Hadoop7154HTML129128@declarckIT Operations5454@asahioceanJava130130@sumanas27, @ruslanbes, @PROCWJavascript131131@taletski, @PROCW, @msteiner96, @declarckjQuery8477@declarckJSON❗needs updating8786@iHamzaKhanzadaKeynote140Kotlin7878@ItSNeverLate, @HusseinhjLinux8278@D4RIO, @Amsal1Logic Pro8278Machine Learning9898@aaronwangj, @antra0497MATLAB7070@tm-sanjayMaven5350Microsoft Access3028@drmegalomaniacMicrosoft Azure5553@tomtreffke, @ziasistaniMicrosoft Excel❗needs updating109107@gazihasanrahmanMicrosoft Outlook7956Microsoft Power Automate1402@mervynteoMicrosoft Power BI8180@vittorio-giattiMicrosoft Power Point8577@ckulloMicrosoft Project❗needs updating4443Microsoft Word❗needs updating7877MongoDB7777MySQL9797@ruslanbesnode.js7976@pbachmanNoSQL5655objective-c4038OOP10282@declarck, @gaurovgiriPHP8979@ruslanbes, @msteiner96Pro Tools22Python176176@tik9, @Amsal1, @declarck, @TSG405QuickBooks❗needs updating6739R5252@gregglindReact.js100100@RobTables @bandinoplaREST API6565Revit❗needs updating140Ruby on Rails5959@gudataRust3232@BobbyByrne @Emanuele-emScala5248Search Engine Optimization (SEO)8181SharePoint❗needs updating5338Sketchup22SOLIDWORKS❗needs updating5757@BHARGAVPATEL1244Spring Framework6767Swift6767Transact-SQL (T-SQL)4542@beefydog, @BenVlodgiUnity❗needs updating4746@uno-sebastianVisual Basic for Applications (VBA)❗needs updating3634@AdamKaczor6250Visio3535Windows Server6857WordPress8073@ruslanbes, @Amsal1XML4342@ruslanbesContributors ✨Thanks goes to these wonderful people (emoji key):            Evgenii💻 🖋      Sergei Stadnik💻 🔍 🤔 📖      Santhosh💻      Jacob Dsa💻 🖋      Aaron Meese💻 🖋      arqarq💻 🖋      Amit Yadav💻 🖋              Javokhir Nazarov💻 🖋      saurav kumar🖋      Chetan🖋      Amir Hossein Shekari🎨 🖋 💻      SergDaut🎨      Nilotpal Pramanik🎨 💻 🖋 💼 📖 🔣 💡      Abhishek Kumar🎨              Monu Gupta🎨      KARTIKEYA GUPTA💻 🖋      kenkyusha💻 🖋      juandavidtowers💻 🖋      cyber-netics💻 🖋      jtrisw💻 🖋      Renato Regalado💻 🖋              Matthew💻 🖋      Jan S.💻 🖋      Manoli💻 🖋      Faraz tanveer💻 🖋      mohnishkarri💻 🖋 🎨      andyzhu💻 🖋      Vishal Kushwah💻 🖋              Yurii Yakymenko💻 🖋      Swetabh Suman💻 🖋      AJAY DANDGE💻 🖋      Mehmet Yesin🎨      Lok Chun Wai🎨      Adria de Juan🎨      GL-Man🎨              Jheel Patel🎨      Sameer Waskar🎨      Alexander Andrews🎨      Alexander Maxwell🎨      Slava🎨      Mayur Khatri🎨      Mascantosh💻 🖋 📢 🤔              Kivanc Enes🎨      Ritika Das🎨      Zer07793🎨      Andrew Cheung🎨      Sadha🎨      tainenko🎨 💻      github-star-coder🎨              Danilo Oliveira🎨      lordeko🎨      Shubham Kumar🎨 💻      testtree🎨      Cheryl Murphy🎨 💻      Bipin Thomas🎨      Abdulrahman Hisham🎨              Dakshitha Dissanayaka🎨      BADR KACIMI🎨      Alex Wang🎨      Maxim🎨      GordonGrant🎨 💻      Ephrem Demelash🎨      JonOrcutt🎨              topdev10🎨      cookwellwebsite🎨      xren935🎨      Nemo Frenkel🎨      MD SAIF ALAM🎨      Boris López Araya🎨      Larry Chiem🎨              Muhammad Bilal Ilyas🎨      AliMilani🎨 💻      Suraj Sahani🎨      FlyingSquirrel🎨      Erick Tijero🎨      Jaskaran Kukreja🎨      MichaelL🎨              MagicLegend🎨      Dereck Bearsong🎨      Pappu Kumar Pashi🎨      Venkata Kishore Tavva🎨      Rafat Touqir Rafsun🎨      Snehesh Dutta🎨      Timo Körner🎨 💻              alexxxan🎨      GGJason🎨      LeeAnna Ewing🎨 🤔      kamal Jyotwal🎨      Bob-Johns🎨 💻 🖋      yunussalmanlyit🎨 💻      chilcot🎨 💻              Jacky Li💻 🖋 🎨      Sarthak Trivedi🎨      Ayush Aggarwal🎨 💻      Nic Ballarini🎨      Luigi Zambetti🎨 💻      govindhaswin🎨      Addy Roy💻 🎨              Akshat Tamrakar🎨 💻      Sai Bhargava Ramu🎨      Gurkan💻      Spencer Hayes-Laverdiere💻      Aniket Soni💻      tanmay5792💻      Dina Taklit💻 🎨 🖋              Dushyant Singh💻      Ravi Prakash Singh💻      Nihal Joshi💻      Guy Klages💻      Arvind🎨 💻      mujeeb91💻      joserca🎨 💻              Prateek Agrawal💻      Teoh Tze Chuin(サラ)💻 🎨      Jayant Jain💻      Ayush Sahu💻      Hridya Krishna R💻 🎨      Rahul Bali💻 🎨      S.ZHeng🎨 💻 💼              Shriya Madan🎨 💻      mahalrupi🎨      Lucas Lermagne🎨      Jeff Deutsch🎨 💻      Betoxx1🎨      Wingman4l7🎨      Martin Espericueta🎨              Mh-Tahir🎨      Zdravko Šplajt🎨 💻      Ms3105🎨 💻 🖋      Ambika Sidhesware💻      mundoguero💻      Darkus24🖋      Sou-786🖋 🎨              Banurekha🖋      ShiraStarL🎨      Ilya Komarov🎨      DemigodMs🖋 📖      Mekha Hridya🎨 🔍      Andrey Safonov🎨 🔍      Tommaso🎨 💻              Jessica Salbert💻 🎨      JAYANTH DOLAI💻 🎨      silverstroom💻 🎨 💼      Furkan Sayım💻 🎨      Sukumar Chandrasekaran🎨      Yejin Park🎨 💻      Ali Nooshabadi🎨 💻              imitavor🎨 💻      Salih Kilicli🎨 💻      Marcelo Meneses🎨 💻      Anton Krekotun🎨 🚧 🖋 💻 📖 💼      Arnav Sarma💻 💡 🎨      meghatiku💻 🎨      Anshu Trivedi🎨              Taylor Dorsett💻 🖋 🎨      Havit Rovik💻      pushpapune💻 🎨      Ramtin Radfar🎨 🤔 💼 💵 💻 🖋 💬      Abdulmajeed Isa💻 🎨      vikassaxena02🎨      RobTables🎨 💻 💼              Daniel🎨 💻 💼 🔍      Zahid Ali💻 🎨      Chad Chai💻 🎨      Marco Biedermann💻 🎨 💼 🤔      Srinidhi Murthy🎨      Miao Cai💻 🎨      Dionicio Diaz🎨 💻              Mir Monoarul Alam🎨      Shawn Ohn💻 🎨      Amanbolat Balabekov🎨 💻      black-mamba-code💻      Jian-forks🎨 💻      shivani patel🎨      Akash Chowrasia🎨              yairg98🎨      Jay Gajjar🎨      coolerbooler💻      Md Zinnatul Islam Morol🎨      shresthashok550🎨 📖      Alan Pallath📖      Adrian Wong💻              vsDizzy💻 🎨      Frex Cuadillera🎨 💻      ashish570💻 🎨      ruchpeanuts💻 🎨      Artmasque🎨 💻      Amirhossein Mojiri Foroushani🎨      for💻 🎨              Luke🎨 💻      Hector Espinoza🎨      Adrián Buenfil🎨 💻      Amit Kumar🎨      schoppfe🎨 💻      Sofiyal C🎨 💻      spitlisk💻 🎨              PRAVIN SHARMA🎨      NIDZAAA1🎨 💻      John Mai🎨 💻      kimsoyeong🎨      Dona Ghosh💻      Ryan Hill🎨 💻      j42z🎨 💻              Ashish Sangale🎨 💻      Derek Yang🎨 💻      mohsinmsm🎨 💻      Gokulkrish2302💻      Bhaavishek💻 🎨      Louis Liao🎨      sengc92🎨 💻              Alex Marvin🎨      Balkrishna Bhatt🎨 💻      Evaldas Lavrinovičius🎨 💻      Adam Erchegyi🎨 💻      Truman Hung🎨 💻      rzamora11🎨      gaurav0224🎨              Lee GyeongJun🎨      Mirek🎨 💻      surajm245🎨      ArisLaode🎨 💻      RaviDhoriya🎨 💻      sarai-84🎨 💻      Vishnu🎨 💻              Muhammad Minhaj💻      Chandrika Deb🎨 💻      Gitgit101-bit💻 🎨      Hedi Sellami💻 🎨      saurabhvaish93💻 🎨      Nikola Begovic💻 🎨      Wang💻 🎨              Manuel Eusebio de Paz Carmona🎨      Basim Al-Jawahery🎨 💻      RAJA AHMED🎨 💻      Abhik Lodh💻      Md. Pial Ahamed💻 🎨      Hassan Shahzad💻 🎨      Christian Sosa Gago💻              Hasnain Rasheed💻 🎨      T-Radford💻      dahiyashish💻 🎨      RahulSharma468💻 🎨      Jumpod Plekhongthu💻 🎨      Thomas Young-Audet💻 🎨      VinayagamBabu💻 🎨              Deniz Koç💻 🎨      Azhar Khan💻 🎨 🖋 📖 🔣 🚧      Jacob Short💻 🎨      Uchimura85💻 🎨      Leo Nugraha💻 🎨 📖      Mujtaba Mehdi📖 🖋      Jim-ds💻 🎨              Sreehari K💻 🎨      Florian Martinez💻 🎨      Aaron💻 🎨      apoage🎨      Ignacio Guillermo Martinez 💻 🎨      AirlineDog🎨 💻      Mekel🎨 💻              hmosharrof🎨 💻      Ben Emamian💻 🎨      babeshark💻 🎨      Leonardo Jaques💻 🎨      Stefanos Apkarian💻 🎨      Ayhan Albayrak💻 🎨      KidusMT💻 🎨              hectormarroquin20💻 🎨      Edelweiss35💻 🎨      MihaiD💻 🎨      AnveshReddyAnnem💻 🎨      Hyunjae Park💻 🎨      Rajiv Albino💻 🎨      Atishay💻              Yusuf Naheem🎨      Windu🎨 💻      Superv1sor💻 🎨      Karine (:🎨 💻      Eduard Pech🎨 💻      jjeshwani🎨 💻      Steve🎨 💻              Aleigh Ohslund💻      Abhinav Suman🎨 💻      Hamza Ehtesham Farooq🎨 💻      IamNotPeterPan💻 💵 🎨      Cetger🎨      pkonopacki🎨      Yang Yang🎨 💻              Muhammad Shoaib Sarwar💻      Murilo Henrique💻 🎨      emilianoalvz🎨 💻      Sumana Saha🎨 💻      Yurii17K🎨 💻      Rupesh Bhandari🎨 💻      salmos3718💻              John Baker🎨 💻      SanjaySathiraju🎨 💻      Donat Kabashi🎨      Arul Prasad J🎨 💻      Qi Chen🎨 💻      Maksym Dmyterko🎨 💻      ilovepullrequests💻              Samira Maleki🎨 💻      NIKITA MAHOVIYA💻      jesuisdev.Net🎨 💻      Ashraf Nazar🎨      Naveed Ahmad🎨      Ajmain Naqib🎨 💻      Avinash Tingre💻 🎨              nicktids🎨      Keith Dinh💻 🎨      André Ferreira💻 🎨      eliottkespi💻 🎨      praveenpno💻 🎨      vitowidigdo💻 🎨      Devesh Pratap Singh💻 🎨              Dario Rodriguez💻 🎨      charmander_didi💻 🎨      PHBasin💻 🎨      Ritvik Singh Chauhan💻 🎨      Riya P Mathew💻 🎨      Stephanie Cherubin💻 🎨      BenitesGui💻 🎨              FarikBear💻 🎨      Dmytro Havrilov💻 🎨      Parvesh Monu💻 🎨      Dipen Panchasara💻 🎨      gudata🎨 💻      gawadeditor💻 🎨      Kirill Taletski🎨 💻              Saajan🎨 💻      Kushagra S🎨 💻      Oanh Le🎨 💻      Frane Medvidović🎨 💻      Yorman🎨 💻      Bill Chan🎨 💻      Pratik Lomte🎨 💻              LOC LAM🎨 💻      TUSAR RANJAN MAHAPATRA💻      BhargavKanjarla💻      Karel De Smet💻 🎨      sidisan🎨      ygnzayarphyo🎨 💻      svansteelandt💻              Kebechet🎨      Daniel Selvan D🎨 💻      Mahdi Razavi🎨 💻      Niklas Tiede💻 🎨      narutubaderddin💻 🎨      dylandhood💻      Dheeraj Gupta💻              Pieter Claerhout💻 🎨      Shivam Agnihotri💻      RanjithReddy-Narra💻      Nikita Wadhwani🎨 💻      rsholokh💻 🎨      Ayaan Hossain💻 🎨      Rajesh Swarna💻              Deniz Etkar🎨 💻      pro335💻 🎨      Jakub Radzik💻 🎨      Hamza Khanzada💻      ARNON🎨      Vikram Singh💻      Shoxruxbek💻 🎨              Amit Khatri💻 🎨      Wali Ullah🎨 💻      Amit11794💻 🎨      metis-macys-66898💻 🎨      Faisal Maqbool🎨 💻      Kumar Neeraj💻 🎨      Maurizio Marini🎨 💻              Saket Kothari🎨 💻      Szymon Zborowski🎨 💻      iks3000🎨 💻      Ehsan Seyedi🎨 💻      vanekbr🎨 💻      Princy_M🎨 💻      Shijie Zhou🎨 💻              lakshyamcs16🎨 💻      Filippo Facco🎨 💻      mendel5🎨 💻      Patryk🎨 💻      VishwaSangani🎨 💻      Alvin Zhao🎨 💻      Lazar Gugleta🎨 💻              vmicho🎨 💻      Sikandar Ali🎨 💻      Raja Babu🎨 💻      faizajahanzeb💻      Guil_AiT🎨 💻      Kushal Das🎨 💻      Luis Bonilla🎨 💻              jovan1013🎨 💻      Damian🎨 💻      Yash Gupta💻      lolcatnip🎨 💻      Ikko Ashimine🎨 💻      Farukh🎨 💻      Moksedul💻 🎨              Navneet Kumar🎨 💻      Saqib AlMalik💻      fahimrahman🎨 💻      vaibhav patil🎨 💻      Rahul Madan🎨 💻      kartik Kaklotar🎨 💻      ASAHI OCEAN🎨 💻              Daniel Jungbluth🎨 💻      Rajdeep Singh Borana🎨 💻      ankitha19💻      Linh Tran💻      islamarr💻 🎨      Mohamed Sabith🎨 💻      Miguel Angel Cruz Acosta🎨 💻              Adebayo Ilerioluwa 🎨      Markus🎨 💻      dkonyayev🎨 💻      Kevin A Mathew🎨 💻      David Melo🎨 🔣      DFW1N🎨 💻      Sohaib Ayub🎨 💻              Navvy🎨 💻      bloodiator2🎨 💻      Hanji🎨 💻      arthur74🎨 💻      Sri Subathra Devi B🎨 💻      Akif Aydogmus🎨 💻      Umer Javaid🎨 💻              Norio Umata🎨 💻      Gazi Hasan Rahman🎨 💻      Keith Nguyen🎨 💻      Megalomaniac🎨 💻      ShankS3🎨 💻      Farhad Alishov🎨 💻      Ronak J Vanpariya🎨 💻              azrael0learza🎨 💻      Pavel Rahman🎨 💻      chuabern🎨 💻      Rahul Tirkey🎨 💻      Ruslan Bes🎨 💻 💡 🚧 🖋 🔣 🚇      Bohdan🎨 💻      Juzdzewski🎨 💻              Grigor Minasyan🎨 💻      alvintwc🎨 💻      Anand Natarajan🎨 💻      Kashan Ali🎨 💻      Thomas Meshail🎨 💻      Son Pham🎨      Michael French💡              Yash Mishra📖      Miguel Rodriguez🎨 💻      Philipp Bachmann🎨 💻      sunny🎨 💻      Siddharth Chatterjee🎨 💻      Michael Naghavipour🎨 💻      Sahil Garg🎨 💻              MicroLion🎨 💻      wctwc🎨 💻      Rohan Sharma🔣      AshishBodla🎨 💻      Taras Pysarskyi🎨 💻      Luqman Bello O.🎨 💻      DyingDown🎨 💻              Diego Chapedelaine🎨 💻      Richlee🎨 💻      Asif Habib🎨 💻      Mazharul Hossain🎨 💻      toni🎨 💻      Pragyanshu Rai🎨 💻      Matthew Eller🎨 💻              AbhiBiju🎨 💻      Roman Zhornytskiy🎨 💻      Lucas Camino🎨 💻      João Vitor Casarin🎨 💻      Evgeniy Shay🎨 💻      Ehsan Barkhordar🎨 💻      Gabriel🎨 💻              Shibu Mohapatra🎨 💻      Pavel Kirkovsky🎨 💻      Tahir Gul🎨 💻      imDevSalman🎨 💻      Jordan Donaldson🎨 💻      js-venus🎨 💻      Faisal Shaikh🎨 💻              ashishbpatil🎨 💻      Tri Le🎨 💻      tomtreffke🎨 💻      Salah Eddine Lalami🎨 💻      Mattias Xu🎨 💻      Manas Gupta🎨 💻      wolfsong62🎨 💻              Mehdi Mirzaei🎨 💻      Van Ba Khanh🎨 💻      Sel Embee🎨 💻      Suvradip Paul🎨 💻      Sharique🎨      Seabass🎨 💻      Penny Liu🎨 💻              jatinder bhola🎨 💻      misterqbit🎨 💻      Daniel-VS9🎨 💻      Shruthi🎨 💻      beefydog🎨 💻      Suraj Kumar🎨 💻      hrishikeshps🎨 💻              Sudarshan🎨 💻      Divyansh💻 🎨      Zyaire🎨 💻      Omar Belkady🎨 💻      alexiismua🎨 💻      Eduarda Alves🎨      pycoach🎨 💻              Ruhul🎨 💻      pmoustopoulos🎨 💻      Lee Hui Ting💻 🎨      bodi1981🎨 💻      Devaraat Joshi🎨 💻      Johnny🎨 💻      rogue-coder🎨 💻              viiktr🎨      Lalit Mohan💻      João Sousa💻      言葉之靈💻 🎨      RJLABS💻      brittney0522🎨 💻      sham🎨 💻              Glenn Goossens💻 🎨      Cyber Hawk🎨 💻 🖋 💼      Ankit Yadav🎨 💻      verbality💻      Mohammed Siddiqui🎨 💻      AdamKaczor6250🎨 💻      Ramón Martinez Nieto🎨 💻              Grzegorz Dziubak🎨 💻      Ayoub BERDEDDOUCH🎨 💻      nikola-fadv🎨 💻      Akarsh Agrawal🎨 💻      Mitra Mirshafiee🎨 💻      Parker Stephens🎨 💻      alrenee99💻              Karthick Vankayala💻      Iryna 🎨 💻      palanugrah💻      Gwinbleind🎨 💻      Randy Bobandy🎨 💻      Bek Rozikoff💻      davnguye🎨 💻              Neel Patel💻      ehudbehar🎨 💻      nicholas-cod3r🎨 💻      michaelfranki🎨      Esther White🎨 💻      prathmeshpb🎨 💻      Victor Lin🎨 💻              Christine C. Yin🎨 💻      GitLearner-begin🎨 💻      Mesrop Andreasyan🎨 💻      Nathan Garcia🎨      commonsw04🎨 💻      Md. Rashad Tanjim🎨 💻      Ali Malek💻              PAODLT🎨 💻      Nikhil Bobade🎨 💻      hyuckjin21💻      Itasha Modi🎨 💻      Nikitha Reddy🎨 💻      Mahshooq Zubair🎨 💻      Subham Das💻              Onkar Birajdar🎨 💻      Nick Titomichelakis🎨 💻      Christian Leo-Pernold🎨      Matthew Marquise🎨 💻      baronfac🎨 💻      Abhishek Tilwar🎨 💻      DavidsDvm🎨 💻              Parth Parikh🎨 💻      Hector Castro🎨 💻      Rikky Arisendi🎨 💻      Ali HamXa🎨 💻      Frank.wu🎨 💻      Jatin Kumar🎨 💻 📖      masterHAWK99🎨 💻              Pushp Jain🎨 💻      Ashutosh Rout🎨 💻      Atharva Deshpande🎨 💻      Teodor Ciripescu🎨 💻      Anmol Bansal🎨 💻      Nikhil Kumar Macharla🎨 💻      Dexter🎨 💻              Aaron🎨 💻      Yogita Jaswani🎨 💻 📖 🖋      StoryDev🎨 💻      Mesut Doğansoy🎨 💻      Paras Dhawan🎨 💻      Emanuel Zhupa🎨 💻      Aaradhyaa717🎨 💻              jaacko-torus🎨 💻      mBlack💻      kalrayashwin📖 🖋 🎨 💻      Seraph💻 🎨      ZhiHong Chua🎨 💻      Amsal Khan🎨 💻 📖 🖋      Raghav Rastogi🎨 💻              Tzila📖      Shahriar Nasim Nafi📖      AG🎨 💻      Mojtaba Kamyabi🎨 💻      Ahmad Abdulrahman🎨 💻      Eclipse🎨 💻      Anshu Pal🎨 💻              Denis🎨 💻      mehmet sayin📖      WebDEV🎨 💻      Sam Komesarook🎨 💻      Kiran Ghimire🎨 💻      Joshua Davis🎨 💻      Muhammad-Huzaifa-Siddiqui💻              tobeornottobeadev🎨 💻      VAIBHAV SINGHAL🎨 💻      Keiran Pillman🎨 💻      Max Donchenko🎨 💻      sgonsal🎨 💻      diksha137🎨 💻      Vignesh🎨 💻              Gabriel França🎨 💻      Joseph🎨 💻      Bruno Rafael🎨 💻      vcamarre🎨 💻      thibault ketterer🎨 💻 🚧      VictorGonzalezToledo🎨 💻      1911510996🎨 💻              invidu🎨 💻      Nurul Furqon🎨 💻      David Asbill🎨 💻      Niko Birbilis🎨 💻      Mugundan Kottursuresh🎨      agrsachin81🎨 💻      Othmane El Alami🎨 💻              Syed Atif Ali🎨 💻      lakhanjindam🎨 💻      youssef hamdane🎨 💻      starfaerie🎨 💻      rodrigo0107🎨 💻      Michał Gralak🎨 💻      Jewel Mahmud🎨 💻              cwilson830🎨 💻      buun1030🎨 💻      Reda-ELOUAHABI🎨 💻      saad-aksa🎨 💻      Emdadul Haque🎨 💻      PROCW🎨 💻      cccppp1🎨 💻              Joanna Baile🎨 💻      Ahmed Saber🎨 💻      Masoud Keshavarz🎨 💻      mortazavian🎨 💻      Aniket Pandey🎨 💻      Vijay Nirmal🎨 💻      Daniel Carvallo💻              menaechmi🎨 💻      azenyx🎨 💻      Ahmet Özrahat🎨 💻      Abdulrahman Abouzaid🎨 💻      jmgnorbec🎨 💻      palinko91🎨 💻      Laisson R. Silveira🎨 💻              BHARGAVPATEL1244🎨 💻      Candide U🎨 💻      Sitansh Rajput🎨 💻      Houda Mouttalib🎨 💻      MumuTW🎨 💻      Suave Bajaj🎨 💻      Mehdi Parsaei🎨 💻              Dinko Osrecki🎨 💻      Dhia Djobbi🎨 💻      Mahmoud Galal🎨 💻      Anh Minh🎨 💻      Suvesh K🎨 💻      Petar Todorov🎨 💻      Alexander Nguyen🎨 💻              Morteza Jalalvand🎨 💻      Claudson Martins🎨 💻      Matt Jacobson🎨 💻      Rafael Belokurows🎨 💻       Thomas Gamauf🎨 💻      Rishabh Mahajan🎨 💻      rakeshpdgupta23🎨 💻              Shashidharknaik🎨 💻      taleleuma🎨 💻      Florian Bühler🎨 💻      Raihan Bin Wahid🎨 💻      MOHAMMED NASSER🎨 💻      federico🎨 💻      Andre Violante🎨 💻              tcunningham98🎨 💻      Jan Grießer🎨 💻      Serkan Alc🎨 💻 🖋      Jez McKean🎨 💻      meisam alifallahi🎨 💻      Mehul Thakkar🎨 💻      Saksham Soni🎨 💻              Pedro Peregrina🎨 💻      Mintu Choudhary🎨 💻      lucianmoldovanu🎨 💻      John C. Scott🎨 💻      Mia D.🎨 💻      EwenBernard🎨 💻      M. Reza Nasirloo🎨 💻              Jay Agrawal🎨 💻      DeShay🎨 💻      Jay206-Programmer🎨 💻      Elender🎨 💻 🖋      Bobby Byrne🎨 💻      Pirci🎨 💻      Hasanuzzaman🎨 💻              Josh Kautz🎨 💻      Brofar🎨 💻      Mina Karam🎨 💻      Duncan O N🎨 💻      Sean Tumulak-Nguyen🎨 💻      Artur Trześniewski🎨 💻      JJaammeessM🎨 💻              shubham agarwal🎨 💻      Michele Righi🎨 💻      Panagiotis Kontos🎨 💻      sumitbathla🎨 💻      Deepak Mathur🎨 💻      Juho Nykänen🎨 💻      Santiago González Siordia🎨 💻              SRIJITA MALLICK🎨 💻      Samriddhi B🎨 💻      Nitzan Papini🎨 💻      Mario Sanz🎨 💻      Crab^4🎨 💻      Pablo🎨 💻      Gordon Pham-Nguyen🎨 💻              Kristoffer🎨 💻      chrisblach🎨 💻      Gábor🎨 💻      Lina🎨 💻      Harrison Watts🎨 💻      Mario Petričko🎨 💻      Ben8120🎨 💻              Giovanna🎨 💻      Minal Ahuja🎨 💻      mossfarmer🎨 💻      ThaC0derDre🎨 💻      itware🎨 💻      Michael Walker🎨 💻      Tom Jacob Chirayil🎨 💻              Sachin Kumar🎨 💻      adi-ray🎨 💻      Dr-Blank-alt🎨 💻      Bogdan Cazacu🎨 💻      Gilson Urbano🎨 💻      Nina🎨 💻      Anthony🎨 💻              manushimjani🎨 💻      Michael Reyes🎨 💻      Rachel Kennelly🎨 💻      Aakash Garg🎨 💻      Daniel Livingston🎨 💻      alexrojco🎨 💻      Minh Nguyen🎨 💻              Mahesh Dattatraya Babar🎨 💻      Jin Zihang🎨 💻      Bikramjit Ganguly🎨 💻      QuestionableGuise🎨 💻      liq19ch🎨 💻      Bruno Rocha🎨 💻      Anand Dyavanapalli💻 🖋              crucian-afk🎨 💻      0xgainz🎨 💻      weirdfsh🎨 💻      Valan Baptist Mathuranayagam🎨 💻      Paul Kaefer🎨 💻      Yu-Hsiang Wang🎨 💻      Javad Adib🎨 💻              davidliu0930🎨 💻      Achilleas John Yfantis🎨 💻      Omkar Shivadekar🎨 💻 🖋 🐛      ToanTran🎨 💻      Gautam Naik🎨 💻      Marc🎨 💻      twix20🎨 💻              Kristian S.🎨 💻      Aleksey Khoroshilov🎨 💻      arjunsrsr🎨 💻      Ali Haider🎨 💻      Trisha Dring🎨 💻      Andre Marzulo🎨 💻      Krishna Modi🎨 💻              Rosemary Li🎨 💻      Alex Weller🎨 💻      Tam Nguyen🎨 💻      aquintelaoliveira🎨 💻      Norbert Brett🎨 💻      rocsogd🎨 💻      0nyr🎨 💻              rethkevin🎨 💻      RickHeadle🎨 💻      Leandre🎨 💻      Natnael Sisay🎨 💻      sbbu🎨 💻      wael🎨 💻      Fabricio Tramontano Pirini🎨 💻              Alexander Stoyanov🎨 💻      Dezx20🎨 💻      southparkkids🎨 💻      bmstar🎨 💻      kiagam🎨 💻      Juan Castillo🎨 💻      FFenne🎨 💻              Jose Toledo🎨 💻      Pat McGhen🎨 💻      Eiko Wagenknecht💻 🖋 🔣      Alan Chalmers🎨 💻      Jean Didier🎨 💻      Andy🎨 💻      pestadieu🎨 💻              Kanishka Chakraborty🎨 💻      Nandha🎨 💻      Vahid Mafi🎨 💻 🔣 🖋 💼      Akshay Ashok🎨 💻      0x08🎨 💻      Sandeep Mishra🎨 💻      Evann Regnault🎨 💻              Lenny Zeitoun🎨 💻      Eden Boaron🎨 💻      TroyBTC🎨 💻      Aby Sebastian🎨 💻      Matthew Dunn🎨 💻      ckullo🎨 💻 🖋 🔣      Mohamed Mamdouh🎨 💻              Youssef Bazina🎨 💻      Frederico Kückelhaus💻      Nushan Kodikara💻      Zach Cooper💻      Roy🎨 💻      Saurav Panchal🎨 💻      totallynotdavid🎨 💻              goosepirate🎨 💻 💡 💼      KAUTH🎨 💻      Hari Kiran Vusirikala🎨 💻      Sounak Dey🎨 💻      zia💼 🎨 💻      Reza Davari🎨 💻      AkshayAjaykumar🎨 💻              x24870🎨 💻      Ko Phone🎨 💻      Nabstar3🎨 💻      Mateusz🎨 💻      Yunus Emre Emik💻      Abhinav Sinha🎨 💻      Hung Nguyen🎨 💻              Maselino💻      Shuktika Mahanty💻      Mikołaj Gawroński🎨 💻      Hussein Habibi Juybari🎨 💻      Sean-McArthur🎨 💻      Osman F Bayram🎨 💻      Benjamin Thomas Blodgett🎨 💻              Chuanlong-Zang🎨 💻      julian🎨 💻      francisco🎨 💻      aalihhiader9211🎨 💻      Muhammad Zunair🎨 💻      Liya🎨 💻      BegadTarek🎨 💻              etorobot🎨 💻      Hussam Khan🎨 💻      Saikat Chakraborty🎨 💻      Nicholas Quisler🎨 💻      Evang Poul🎨 💻      Gregg Lind🎨 💻      Deepak Kumar🎨 💻              Callum Leslie🎨 💻      Curtis Barnard Jr.🎨 💻      Deepanshukaim🎨 💻      Manthan Ank🎨 💻      hossein varmazyar🎨 💻      Brayan Muñoz V.🎨 💻      Kamil Rasheed Siddiqui💻 🎨              mutt0-ds🎨 💻      egbertjk🎨 💻      Majid Zojaji🎨 💻      Sean Chen🎨 💻      Herbert Milhomme🎨 💻      A3🎨 💻      Killian🎨 💻              Coakeow🎨 💻      ྅༻ Ǭɀħ ༄༆ཉ🎨 💻      Pratik Solanki🎨 💻      Sunny🎨 💻      ssge🎨 💻      Bernat Frangi🎨 💻      Jeevan Rupacha🎨 💻              amirandap🎨 💻      Deepakshi Mittal🎨 💻      Abhijeet Parida🎨 💻      Khaled Riyad🎨 💻      Pratap parui🎨 💻      Prajit Panday🎨 💻      PipeSierra🎨 💻              Collins Oden🎨 💻      Kshitij Dwivedi🎨 💻      Bernardia Vitri Arumsari🎨 💻      Ömer Faruk Taşdemir🎨 💻      Spencer Stith🎨 💻      Porsche Rodjanasak🎨 💻      Shakeel Sharif🎨 💻              Victoria Cheng🎨 💻      Denis🎨 💻      Anand Prakash Tiwari🎨 💻      danijeljw-rpc🎨 💻      Ahmed H Ebrahim🎨 💻      Virginia Gardner🎨 💻      Jhironsel Diaz A.🎨 💻              Yunus Kidem🎨 💻      MT🎨 💻      Dinesh Zaldekar🎨 💻      adi🎨 💻      Farhan Shaikh🎨 💻      Elvis Salvatierra🎨 💻      Kaushik-Iyer🎨 💻              HocAndres🎨 💻      VictorHugoAguilarAguilar🎨 💻      Murat Can Abay🎨 💻      Chris🎨 💻      Shivam7-1🎨 💻      Paipai13🎨 💻      Shambles-io🎨 💻              Abhishek K M🎨 💻      Ezequiel Cuevas🎨 💻      Plamen Ivanov🎨 💻      Yuji🎨 💻      Jean-Philippe Lebœuf🎨 💻 🔣      Naufan🎨 💻      jadnov🎨 💻              vaxtangens🎨 💻      subashkonar13🎨 💻      Rushi Javiya🎨 💻      Mert Gül🎨 💻      Lily🎨 💻      Kalinoff🎨 💻      Joel Tony🎨 💻              Peter🎨 💻      Roozbeh Zarei🎨 💻      Shen🎨 💻      Joonsoo.LEE🎨 💻      Fede.Breg🎨 💻      Rui Costa🎨 💻      João Gustavo Bispo🎨 💻              Sami-I🎨 💻      Tsvetoslav Tsvetkov🎨 💻      Olabode Olaniyi David🎨 💻      theRuslan🎨 💻      leighboz🎨 💻      Frank Sossi🎨 💻      Tomasz Adamski🎨 💻              Mansoor M. Sathir🎨 💻      Golamrabbi Azad🎨 💻      Nahian Ahmed🎨 💻      Rafael de Jesus Silva Monteiro🎨 💻      Odionyebuchukwu Jude🎨 💻      The Nithin Balaji🎨 💻      Knackii🎨 💻              vittorio-giatti🎨 💻      Guilherme de Carvalho Lima Rebouças🎨 💻      aaref shami🎨 💻      Andrey Dryupin🎨 💻      Muhanned Noman🎨 💻      Jan Silva🎨 💻      emanuele-em🎨 💻 🖋              Sanjay TM🎨 💻      Joe Markberg / code editor🎨 💻      Julien Quiaios🎨 💻      Eric Ramirez Santis🎨 💻      M🎨 💻      Malcata🎨 💻      Athul Muralidharan🎨 💻              Dariusz Ochota🎨 💻      CHANDAN CHOUDHURY🎨 💻      Deep🎨 💻      Ahmet İstemihan ÖZTÜRK🎨 💻      TIM🎨 💻      jakeg814🎨 💻      Leonidos🎨 💻              Abhinandu V Nair🎨 💻      charafeddine01🎨 💻      Jasper🎨 💻      Manish Goyal🎨 💻      SATYAM_SINGH🎨 💻      Four🎨 💻      Vaishnavi Amira Yada🎨 💻              ShriKrushna Bhagwat🎨 💻      Rohit Nandagawali🎨 💻      felipe🎨 💻 🚧 🖋 ✅ 🧑‍🏫      Saurabh Mudgal🎨 💻      szenadam🎨 💻      Shubhendra Singh🎨 💻      Yoosuf Sayyid💻 🎨              Güven Çetinerler🎨 💻      Luke Jefferies🎨 💻      Chris🎨 💻      Lúcio Aguiar💻      Enuma029💻      yktsang01💻      maximumn3rd🎨 💻              Jon Galletero🎨 💻      Thaddeus  Thomas🎨 💻      Aakash Kumar💻 🎨      Ali M🎨 💻      OskyEdz🎨 💻      Ravi Gupta🎨 💻      Rafa Raizer🎨 💻              Abdullah Al Muzaki🎨 💻      Rahul Faujdar🎨 💻      Abhishek Verma🎨 💻      Ashutosh Shinde🎨 💻      Ganesh Rai🎨 💻      StefanTrpkovic🎨 💻      Erik Blanca🎨 💻              Vedant Madane🎨 💻      Antra Tripathi🎨 💻      Ethan Knights🎨 💻      Alexandru Boncut🎨 💻      Pablo Bandinopla🎨 💻 🚧 🖋      Robz-99🎨 💻      Harpal Singh🎨 💻              paulboundy99🎨 💻      Mubashir Ahmed🎨 💻      Rohan Hari🎨 💻      Erik Henrique 🎨 💻      Leandro Matheus🎨 💻      Deepak🎨 💻      AlishaSingh🎨 💻              Lynn Latt Yati🎨 💻      San Shwe🎨 💻      SKR🎨 💻      msbunnyjaguar🎨 💻      Mohamad Zabiulla🎨 💻      Hatim Zahid🎨 💻      Rauzan Sumara🎨 💻              Hosein1358🎨 💻      Mohit🎨 💻      Ali🎨 💻      Avinash1765🎨 💻      Sai Teja Madha🎨 💻      Monsur Ahmed Shafiq🎨 💻      xuxianjin-dev🎨 💻              chetna🎨 💻      Gul Zaib🎨 💻      Natalia🎨 💻      Dionísio Braga🎨 💻      Pritish Rajpurohit🎨 💻      incanlove🎨 💻      Innocent🎨 💻              Devin Almonor🎨 💻      antonyveyre🎨 💻      Beltz Anhxton🎨 💻      Mehdi🎨 💻      Muhammad Usman🎨 💻      Patrick Dantas🎨 💻      Tak Vannak🎨 💻              Ramzi RADDAOUI🎨 💻      Konstantin-Glukhov🎨 💻      uguroban🎨 💻      Humberto Alves🎨 💻      JuangZendrato🎨 💻      James Oluwaleye🎨 💻      Wasi Sadman🎨 💻              Pavle Mijatovic🎨 💻      Luiz H. S. Bispo🎨 💻      Сухас Дхолз🎨 💻      Alvaro Trujillo🎨 💻      Everton 🎨 💻      jfrozas🎨 💻      Shuaaib Badran🎨 💻              Shivam Jha🎨 💻      Mohamed Tayeh🎨 💻      Makendran G🎨 💻      mayank singh tomar🎨 💻      hossam sadany🎨 💻      Harshbardhan Singh💻 🎨      Fawad Jawaid Malik🎨 💻              Tina Lacatis🎨 💻      TeddyCuoreDolce🎨 💻      bchooxg🎨 💻      Alisha Takkar🎨 💻      Gianluigi🎨 💻      Mehran Javaherian🎨 💻      Benjamin Ololade Adedokun🎨 💻              Md. Abdul Mutalib🎨 💻      Aadil Arsh.S.R🎨 💻      J. Nathan Allen🎨 💻      Kieran Krug🎨 💻      Seth Addo🎨 💻      Satvik Singh Rathore🎨 💻      dangoth🎨 💻              Maxim🎨 💻      Phuong-Cat Ngo🎨 💻      Frenchtoast0🎨 💻      Rakshith🎨 💻      Vaibhav Arora🎨 💻      zghp🎨 💻      Bedovan🎨 💻              chiaramistro🎨 💻      him2016🎨 💻      HarshitSachdeva🎨 💻      Sadaf Saleem🎨 💻      Aaroh Srivastava🎨 💻      eloygplaza🎨 💻      Gaurav Kumar Verma🎨 💻              AndreaCUS🎨 💻      Simran🎨 💻      Prashant Bhapkar🎨 💻      mhaendler🎨 💻      Gauri Maheshwari🎨 💻      4Lajf🎨 💻      Tanmoy Sengupta🎨 💻              Sharad Tripathi🎨 💻      Niraj Chavan🎨 💻      Luisa Gualda🎨 💻      Monika-Sivakumar-3🎨 💻      harryfensome🎨 💻      Shubham Choubey🎨 💻      Ashwini Patil🎨 💻              cleversonlira🎨 💻      Nurmukhammed🎨 💻      workspace-utkarsh🎨 💻      Santosh Phadtare🎨 💻      Prashant Warghude🎨 💻      Umang Dakh🎨 💻      Shalini Chavan🎨 💻              vinit gurjar🎨 💻      Vishal Kumar🎨 💻      Wonhyeong Seo🎨 💻      Achwale Prajwal Namdevrao🎨 💻      Ankan Banerjee🎨 💻      bhaumikankan🎨 💻      JamesMacroZhang🎨 💻              Pedro Lopes🎨 💻      dia🎨 💻      tayyabhussain2910🎨 💻      Rajdeep Shrivastava 🎨 💻      Mukul Kumar🎨 💻      Mayank N🎨 💻      jdelucca🎨 💻              Sneha Mittal🎨 💻      Sarika Kushwaha🎨 💻      farzad-khb🎨 💻      Elijah Shackelford🎨 💻      The-Only-Raminator🎨 💻      Keerthana Kasthuril🎨 💻      Viachaslau Auchynnikau🎨 💻              Mohammad Osman Rasooli🎨 💻      mvedovato🎨 💻      Sonali Rajput🎨 💻      Isha Dhek🎨 💻      Ramshad Cheriyeri Peediyakkal🎨 💻      Micah🎨 💻      gauravshukla2203🎨 💻              sndmurthy🎨 💻      Shivam-Singh🎨 💻      M. Ammar Khan🎨 💻      chandolakul🎨 💻      bhatnagar221🎨 💻      Adrian Nieściur🎨 💻      nezi311🎨 💻              scottajevans🎨 💻      Marcelo Antunes Soares Fantini🎨 💻      Axel De Acetis🎨 💻      Drishti Sah🎨 💻      VipulDhillon🎨 💻      Urmi Jana🎨 💻      Ayush Mokal🎨 💻              Damola Olutoke🎨 💻      Max🎨 💻      Lakshmi N🎨 💻      ArtemReva🎨 💻      Ujjwal Aggarwal🎨 💻      Mo🎨 💻      Brian🎨 💻              chamley🎨 💻      Simone Baptiste🎨 💻      Shekhar Thakur🎨 💻      Smith🎨 💻      codernoob1🎨 💻      lok84🎨 💻      Tobias Riemenschneider🎨 💻              Tharsanan1🎨 💻      ANURAG SINGH🎨 💻      Yash Sant🎨 💻      Krishiv Patel🎨 💻      GGGalaxy🎨 💻      pardeepdhillon661🎨 💻      anujd64🎨 💻              Pedro Pereira🎨 💻      Master_Saptak🎨 💻      SURANJAN DAS🎨 💻      Tripura kant🎨 💻      shabzkhan🎨 💻      Mustafa Poya🎨 💻      Roshan Jha🎨 💻              GuillaumeLarue🎨 💻      Tomasz Rodak🎨 💻      Junil Kim🎨 💻      Surbhi Mayank🎨 💻      Nemanja Lekic🎨 💻      HemantMalokar🎨 💻      Felipe M. López🎨 💻              bibliofilo🎨 💻      GauthamG2🎨 💻      02_t🎨 💻      Yusuf Abdul-razaq🎨 💻      Vladimir🎨 💻      Sai Chandra K🎨 💻      Soroush Bonab🎨 💻              Giide0n🎨 💻      GG🎨 💻      Dáger Zúñiga🎨 💻      rsk2🎨 💻      Storozhev DJ🎨 💻      Jeevan🎨 💻      Andy Johnson🎨 💻              Aníbal Pozo🎨 💻      Jovane de Castro🎨 💻      Muhammad Hamza Amir🎨 💻      tharaka-mts🎨 💻      Ali KHYAR🎨 💻      Caio Araujo🎨 💻      Oscar Dyremyhr🎨 💻              arteality🎨 💻      Daniel Drexlmaier🎨 💻      Marco Monti🎨 💻      mikeycrystal🎨 💻      Veljanovskii🎨 💻      Ivan Gorbachev🎨 💻      Sahil Rawat🎨 💻              Hasitha Suneth🎨 💻      Yerko Vera Lezama🎨 💻      Ivan Penchev🎨 💻      Tanver Islam Tonmoy🎨 💻      Xun Cao🎨 💻      Nayan Babariya🎨 💻      Priyanshu Maurya🎨 💻              Dylan Tintenfich🎨 💻      Ron Strauss🎨 💻      Mohammed AlBanna🎨 💻      Mukund M🎨 💻      Franklin Ohaegbulam🎨 💻      Nisarg Shah🎨 💻      Unik Dahal🎨 💻              Readily🎨 💻      Alexandre Poitevin🎨 💻      Scaramir🎨 💻      Pruthvi🎨 💻      Kalmanq🎨 💻      Alfatah Nesab🎨 💻      arudesai🎨 💻              Adryenne🎨 💻      El mehdi oudaoud🎨 💻      Jayant Goel🎨 💻      Tsuki🎨 💻      Peter Lemanski🎨 💻      Annurag-byte🎨 💻      Anthony Vu🎨 💻              Vitaly Nikolaychuk🎨 💻      Nathan🎨 💻      Evgenii Petukhov🎨 💻      Loris Guerra🎨 💻      fakhriaunur🎨 💻      Mehdi HYANI🎨 💻      Sarvex Jatasra🎨 💻              santimanuelr🎨 💻      Evgeniy Rezanov🎨 💻      Sonia M🎨 💻      Grzegorz Kmita🎨 💻      Manuel Carita🎨 💻      Felipe Cisternas Alvarez🎨 💻      Guo Ci🎨 💻              Marcos Silva🎨 💻      KK🎨 💻      Shubhanjan Medhi🎨 💻      ArthurFerreiraRodrigues🎨 💻      PabloHermun🎨 💻      disha-baldawa🎨 💻      StaroMoon🎨 💻              Amila T Kumarasekara🎨 💻      Amoh Prince🎨 💻      AngeloGC🎨 💻      Ebube Glory Ogbonda🎨 💻      Prahalad Belavadi📖      Antoni Sarnowski-Trypka🎨 💻      Alberto Pasqualetto🎨 💻              Amir Babaei🎨 💻      Syed Abdul Hannan🎨 💻      Srajan Rai🎨 💻      Clarence Moore🎨 💻      Nguyen Anh Tuan🎨 💻      dar2dar2🎨 💻      Ameer Ibrahim🎨 💻              Tiago Lugatto🎨 💻      raremiroir🎨 💻      Moobie🎨 💻      AlicanDursun🎨 💻      bbalsam🎨 💻      Luboš Hájek🎨 💻      mrshahzeb7🎨 💻              Wesley Scholl🎨 💻      Lawrence Turcotte🎨 💻      Michael DiPaolo🎨 💻      Smart-Codi🎨 💻      Vivek Kumar🎨 💻      Igor Moiseev🎨 💻      Bård Pedersen🎨 💻              HOA PHAN🎨 💻      GaborModra🎨 💻      vivek-114🎨 💻      Robin🎨 💻      Alex🎨 💻      John Ehrlinger🎨 💻      Roman Zhuravlov🎨 💻              Jordan Moss🎨 💻      RaeShelly🎨 💻      gmollard🎨 💻      Md Kaif Khan🎨 💻      Pablo Romera🎨 💻      Erik Bustos🎨 💻      trogfield🎨 💻              simon-aichhorn🎨 💻      Tufan GÜLEÇ🎨 💻      Uğur Berkecan Ünlü🎨 💻      Revanth Naik🎨 💻      Lia Pires🎨 💻      Igor Mestechkin🎨 💻      Anirudh Karanth🎨 💻              KBobovskiy🎨 💻      zhatiayua🎨 💻 🖋      David Cardona🎨 💻      Paulo Castilho🎨 💻      Sebastiano Picchi🎨 💻      pjotar🎨 💻      Rimel CHERIF💻              Arsal uddin🖋      Dmitry Kasporsky💻      SoftwareDev1014🎨 💻      @Robvred🎨 💻      Kasun Shanaka💻      Ahmad M.🎨 💻      Alex Kozin🎨 💻              Mandy Meindersma🎨 💻      LEGALISE PIRACY🎨 💻      Alex Logvin🎨 💻      Aria Dahl🎨 💻      Mustafa Arifoglu🎨 💻      Yevhen Leshchenko🎨 💻      Anubhav Adhikari🎨 💻              Noah Tatko🎨 💻      Mohit Gadhavi🎨 💻      Pedro Basílio🎨 💻      RealSanjeev🎨 💻      Akash Hazra🎨 💻      Christoph Dahlen🎨 💻      Vincent du Plessis🎨 💻              Karen Tamrazyan🎨 💻      Mirza Younus Baig🎨 💻      Ashish Kumar🎨 💻      Unknown6334🎨 💻      flowaz🎨 💻      zi-aikra🎨 💻      PAYAL PM🎨 💻              Lennart Lösche🎨 💻      Yummy-Yums🎨 💻      Njuacha Hubert Mikulowski🎨 💻      Hussein Esmail🎨 💻      Bilgehan Bezir🎨 💻      Muhammed Shittu🎨 💻      Clément FERNANDES🎨 💻              JaCKoP619🎨 💻      userutf8🎨 💻      Mohamed Ubaid🎨 💻      Justin Yates🎨 💻      mohammad ali🎨 💻      Madhav Singh🎨 💻      RgbMouse69🎨 💻              Nicholas Leask🎨 💻      parthav0🎨 💻      Sigma🎨 💻      Evelina Becheva🎨 💻      Akshit Gulyan🎨 💻      Arpita Jana🎨 💻      Praveen Kumar🎨 💻              Mohammad Sami🎨 💻      eddiestefanescu🎨 💻      Ramesh Yadav🎨 💻      Sarthak Joshi🎨 💻      Nikhil12300🎨 💻      Yevgen🎨 💻      Leo🎨 💻              laurent b🎨 💻      Mettchen🎨 💻      Ali Mahdavi🎨 💻      Lucas Dondo🎨 💻      Siddhesh Agarwal🎨 💻      slimerPuncher🎨 💻      saritashh🎨 💻              Iulian-Valeriu Cioată🎨 💻      Szabolcs Nagy🎨 💻      Jarle Kvile🎨 💻      劉耀升 Vic Liu🎨 💻      Suryansh🎨 💻      Matthew Oosthuyse🎨 💻      Florin Zamfir🎨 💻              Melek🎨 💻      moesocio🎨 💻      Alan James🎨 💻      Mai Thanh Phương🎨 💻      Neville Dabre🎨 💻      Maksym🎨 💻      tamanna900🎨 💻              Adithya Awati🎨 💻      This project follows the all-contributors specification.Contributions of any kind welcome![ Go back to the top of the page ]Contributor Over TimeStargazers over timeVisualisation of this repository by Gourcehttps://www.youtube.com/watch?v=24cZVytc5D4"
76,fighting41love/funNLP,https://github.com/fighting41love/funNLP/blob/master/README.md,Python,"            NLP民工的乐园The Most Powerful NLP-Weapon ArsenalNLP民工的乐园: 几乎最全的中文NLP资源库在入门到熟悉NLP的过程中，用到了很多github上的包，遂整理了一下，分享在这里。很多包非常有趣，值得收藏，满足大家的收集癖！如果觉得有用，请分享并star⭐，谢谢！长期不定时更新，欢迎watch和fork！❤️❤️❤️🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥                    * 类ChatGPT的模型评测对比  * 类ChatGPT的资料 * 类ChatGPT的开源框架 * LLM的训练_推理_低资源_高效训练 * 提示工程 * 类ChatGPT的文档问答 * 类ChatGPT的行业应用 * 类ChatGPT的课程资料 * LLM的安全问题 * 多模态LLM * LLM的数据集🍆 🍒 🍐 🍊                    🌻 🍓  🍈 🍅 🍍                    * 语料库  * 词库及词法工具  * 预训练语言模型   * 抽取  * 知识图谱    * 文本生成    * 文本摘要   * 智能问答   * 文本纠错* 文档处理    * 表格处理    * 文本匹配    * 文本数据增强    * 文本检索   * 阅读理解   * 情感分析   * 常用正则表达式    * 语音处理* 常用正则表达式   * 事件抽取  * 机器翻译  * 数字转换   * 指代消解   * 文本聚类   * 文本分类  * 知识推理  * 可解释NLP  * 文本对抗攻击* 文本可视化   * 文本标注工具   * 综合工具  * 有趣搞笑工具  * 课程报告面试等  * 比赛  * 金融NLP  * 医疗NLP  * 法律NLP  * 文本生成图像  * 其他                        类ChatGPT的模型评测对比资源名（Name）描述（Description）链接ChatALL：可以同时与多个AI聊天机器人（含清华、讯飞的产品）可以同时与多个AI聊天机器人（如ChatGPT、Bing Chat、Bard、Alpaca、Vincuna、Claude、ChatGLM、MOSS、iFlytek Spark、ERNIE等）进行对话的工具。它可以并行发送提示给不同的AI机器人，帮助用户找到最好的回答github-ChatALLChatbot Arena实际场景用Elo rating对 LLM 进行基准测试 - 介绍了 Chatbot Arena，一种针对大型语言模型 (LLM) 的基准平台，采用匿名、随机的方式进行对抗评测，评测方式基于国际象棋等竞技游戏中广泛使用的 Elo rating system。发布了9个流行的开源 LLM 模型的 Elo rating 并推出排行榜。平台采用 FastChat 多模型服务系统，在多个语言下提供交互式界面，数据来源于用户投票。总结了 Chatbot Arena 的优点并计划提供更好的采样算法、排名和服务系统截止2023年5月3日类ChatGPT模型评测总结大型语言模型(LLM)受到广泛关注，这些强大的模型能够理解复杂的信息，并对各种问题提供类人的回应。其中GPT-3和GPT-4表现最好，Flan-t5和Lit-LLaMA表现也不错。但要注意，模型商用可能需要付费和数据共享blog大型语言模型（LLMs）大盘点blog大模型评测方面的最新研究长文本建模一直是ChaGPT令人惊艳的能力之一，我们以【篇章翻译】为实验场景，对大模型的篇章建模能力进行全面、细粒度的测试。paper中文大模型评测工具&排行榜C-Eval是一个全面的中文评估套件，适用于基础模型。它包含13948个多项选择题，涵盖52个不同的学科和四个难度级别，具体如下所示。请访问我们的网站或查阅我们的论文获取更多详细信息。githubpaper类ChatGPT的资料资源名（Name）描述（Description）链接Open LLMs：可供商业使用的开放大型语言模型(LLM)A list of open LLMs available for commercial usegithubLLM Zoo: 大型语言模型的数据、模型和基准集市LLM Zoo: democratizing ChatGPT - a project that provides data, models, and evaluation benchmark for large language modelsgithub大型语言模型(LLM)资料合集相关论文列表，包括指导、推理、决策、持续改进和自我提升等方面的研究工作LLM资料合集DecryptPrompt总结Prompt&LLM论文，开源数据&模型，AIGC应用githubSmartGPT旨在为大型语言模型(尤其是GPT-3.5和GPT-4)提供完成复杂任务的能力，通过将它们分解成更小的问题，并使用互联网和其他外部来源收集信息。特点包括模块化设计，易于配置，以及对插件的高度支持。SmartGPT的运作基于\""Autos\""的概念，包括\""Runner\""和\""Assistant\""两种类型，都配有处理计划、推理和任务执行的LLM代理。此外，SmartGPT还具有内存管理系统，以及可以定义各种命令的插件系统github-SmartGPTOpenGPT用于创建基于指令的数据集并训练对话领域专家大型语言模型(LLMs)的框架。已经成功应用于训练健康护理对话模型NHS-LLM，利用来自英国国家卫生服务体系(NHS)网站的数据，生成了大量的问答对和独特对话github-OpenGPTPaLM 2技术报告Google最新发布PaLM 2，一种新的语言模型，具有更好的多语言和推理能力，同时比其前身PaLM更节省计算资源。PaLM 2综合了多项研究进展，包括计算最优的模型和数据规模、更多样化和多语言的数据集、以及更有效的模型架构和目标函数。PaLM 2在多种任务和能力上达到了最先进的性能，包括语言水平考试、分类和问答、推理、编程、翻译和自然语言生成等。PaLM 2还展示了强大的多语言能力，能够处理数百种语言，并在不同语言之间进行翻译和解释。PaLM 2还考虑了负责任的使用问题，包括推理时控制毒性、减少记忆化、评估潜在的伤害和偏见等PaLM 2 Technical ReportDB-GPT于vicuna-13b和FastChat的开源实验项目，采用了langchain和llama-index技术进行上下文学习和问答。项目完全本地化部署，保证数据的隐私安全，能直接连接到私有数据库处理私有数据。其功能包括SQL生成、SQL诊断、数据库知识问答等github-DB-GPTTransformers相关文献资源大列表包含了各种各样的Transformer模型，例如BERT、GPT、Transformer-XL等，这些模型已经在许多自然语言处理任务中得到了广泛应用。此外，该列表还提供了这些模型的相关论文和代码链接，为自然语言处理领域的研究人员和开发者提供了很好的参考资源githubGPT-4终极指南一份关于如何使用GPT3和GPT4的指南，其中包括100多个资源，可以帮助学习如何用它来提高生活效率。包括如何学习ChatGPT基础知识、如何学习ChatGPT高级知识、如何在语言学习中使用GPT-3、如何在教学中使用GPT-3、如何使用GPT-4等，还提供了如何升级到ChatGPT+计划以使用GPT-4以及如何免费使用GPT-4的方法等内容。同时，还提供了如何在业务、生产力、受益、金钱等方面使用ChatGPT的指南link基于LoRA的LLM参数高效微调link复杂推理：大语言模型的北极星能力在 GPT-4 发布博客中，作者写道：“在一次随意的谈话中，GPT-3.5 和 GPT-4 之间的区别可能是微妙的。当任务的复杂程度达到足够的阈值时，差异就会显现出来。”这意味着复杂任务很可能是大型和小型语言模型的关键差异因素。在这篇文章中，我们将仔细分析讨论如何让大语言模型拥有强大的复杂推理能力。blog大型语言模型的涌现能力是否是海市蜃楼？大语言模型的涌现能力一直是被大家视作很神奇的现象，似乎是一种大力出奇迹，但这篇论文认为这可能只是一种错觉。paper大语言模型的概率总结非常详尽的LLM科学解释和总结paperLLaMA 模型简史LLaMA是Meta发布的语言模型，采用Transformer架构，有多个版本，最大为65B参数。与GPT类似，可用于进一步微调，适用于多种任务。与GPT不同的是，LLaMA是开源的，可以在本地运行。现有的LLaMA模型包括：Alpaca、Vicuna、Koala、GPT4-x-Alpaca和WizardLM。每个模型都有不同的训练数据和性能表现blog大型语言模型的复杂推理讨论了如何训练具有强大复杂推理能力的语言模型，并探讨了如何有效地提示模型以充分释放其潜力；针对语言模型和编程的训练相似性，提出了三阶段的训练：持续训练、监督微调和强化学习；介绍了评估大型语言模型推理能力的一套任务集合；讨论了如何进行提示工程，通过提供各种学习机会使模型获得更好的学习效果，最终实现智能化link大语言模型进化树paper李宏毅：穷人如何低资源复刻自己的ChatGPTblog训练ChatGPT的必备资源：语料、模型和代码库完全指南资源链接论文地址GitHub宝藏库，里面整理了GPT相关的各种开源项目githubChatGPT中文指南gitlab探讨了ChatGPT在自然语言处理中的应用、优势、限制以及未来发展方向强调了在使用该技术时的伦理道德考量和提示工程技术。paper大型语言模型相关文献资源列表github大型语言模型文献综述--中文版githubChatGPT 相关资源大列表githubPre-Training to Learn in ContextpaperLangchain架构图imageLLM开发人员都应该知道的数字github大语言模型如何构建强大的复杂推理能力blogLLMs九层妖塔分享打怪(ChatGLM、Chinese-LLaMA-Alpaca、MiniGPT-4、FastChat、LLaMA、gpt4all等)实战与经验github类ChatGPT的开源框架资源名（Name）描述（Description）链接LLM-As-Chatbot这个项目把市面上有的LLM全部做成了Chatbot，直接可以在google colab运行，不需要自己搭建，非常适用于想体验LLM的朋友们。我刚试了，真的超简单。有些LLM需要的显存比较多，所以最好是要有colab pro订阅。githubOpenBuddy一款强大的开源多语言聊天机器人模型，目标是全球用户，重点是对话AI和流畅的多语言支持，包括英文、中文等多种语言。基于Facebook的LLAMA模型，进行了微调，包括扩展词汇表、增加常用字符和增强的token embeddings。通过这些改进和多轮对话数据集，OpenBuddy提供了一个强大的模型，能回答问题并在各种语言之间进行翻译任务。OpenBuddy的使命是提供一个免费、开放且可离线使用的AI模型，该模型可以在用户的设备上运行，无论他们的语言或文化背景如何。目前，OpenBuddy-13B的演示版本可以在Discord服务器上找到。其关键功能包括多语言对话AI(包括中文、英文、日文、韩文、法文等)、增强的词汇表和对常见CJK字符的支持，以及两种模型版本：7B和13Bgithub-OpenBuddyPanda: 海外中文开源大语言模型基于 Llama-7B, -13B, -33B, -65B 进行中文领域上的持续预训练，使用了接近15M条数据，并针对推理能力在中文benchmark上进行了评测github-PandaLMDromedary：一个开源的自对齐语言模型，只需少量人工监督即可进行训练github-DromedaryLaMini-LM 蒸馏的小型、高效的语言模型集合从 ChatGPT 蒸馏的小型、高效的语言模型集合，在2.58 M 指令大规模数据集上进行训练githubLLaMA-Adapter V2上海人工智能实验室 LLaMA-Adapter V2，仅注入14M参数，1小时时间即可完成训练，对比较果确实很惊艳，且具有多模态功能（对图像进行解释和问答）githubHuggingChatHugging Face 推出第一个 ChatGPT 开源替代品：HuggingChat。基于 Open Assistant  大模型搭建，支持中文对话与编写代码，但暂不支持中文回复。应用已上线，无需代理，打开即可访问linkOpen-Chinese-LLaMA基于 LLaMA-7B 经过 中文数据集增量预训练 产生的 中文大语言模型基座githubOpenLLaMALLaMA模型的开源复现，在RedPajama数据集上训练，使用了与LLaMA相同的预处理步骤和超参数，模型结构，上下文长度，训练步骤，学习率调度和优化器。OpenLLaMA的PyTorch和Jax权重可以在Huggingface Hub上获得。OpenLLaMA在各种任务中展现出与LLaMA和GPT-J相似的表现，部分任务表现优异githubreplit-code-v1-3bBY-SA 4.0授权发布，这意味着允许商业使用linkMOSSMOSS是一个支持中英双语和多种插件的开源对话语言模型，moss-moon系列模型具有160亿参数，在FP16精度下可在单张A100/A800或两张3090显卡运行，在INT4/8精度下可在单张3090显卡运行。MOSS基座语言模型在约七千亿中英文以及代码单词上预训练得到，后续经过对话指令微调、插件增强学习和人类偏好训练具备多轮对话能力及使用多种插件的能力。githubRedPajama1.2 万亿tokens数据集linkchinese_llama_alpaca_lora 抽取框架githubScaling Transformer to 1M tokens and beyond with RMT该论文提出一种名为 RMT 的新技术，或许可将 Transform 的 Token 上限扩展至 100 万，甚至更多。githubOpen Assistant包含大量AI生成的、人工标注的语料库和包括基于LLaMA和基于Pythia的多种模型可选。发布的数据集包括超过161K较高质量的，多达35种语言的人工助手型交互对话语料库data modelChatGLM Efficient Tuning基于 PEFT 的高效 ChatGLM 微调githubDolly介绍newsBaize：一种对自聊天数据进行参数高效调优的开源聊天模型Baize是一个开源的聊天模型，可以进行多轮对话。它是通过使用ChatGPT自我对话生成高质量的多轮聊天语料库，并使用参数高效调整来增强LLaMA（一个开源的大型语言模型）而创建的。Baize模型在具有最小潜在风险的情况下表现出良好的多轮对话性能。它可以在单个GPU上运行，使更广泛的研究人员可以使用它。Baize模型和数据仅用于研究目的。论文地址源码地址GPTrillion--未找到开源代码包含1.5万亿（1.5T）参数的大模型GPTrillion开源了，号称是目前世界上最大的开源LLMgoogle_docCerebras-GPT-13B(可商用)hugging_faceChinese-ChatLLaMA中文ChatLLaMA对话模型；预训练/指令微调数据集，基于 TencentPretrain 多模态预训练框架构建，支持简繁体中文、英文、日文等多语言githubLit-LLaMA基于Apache 2.0许可证完全开源的LLaMA独立实现，建立在nanoGPT之上，旨在解决原始LLaMA代码采用GPL许可证的限制，以实现更广泛的学术和商业应用githubMosaicMLMPT-7B-StoryWriter，65K tokens，可以把《了不起的盖茨比》都一次性扔进去。huggingfaceLangchain大型语言模型（LLMs）正在成为一项具有变革性的技术，使开发者能够构建以前无法实现的应用程序。然而，仅仅使用这些独立的LLMs通常不足以创建一个真正强大的应用程序 - 真正的力量来自于能够将它们与其他计算或知识来源相结合。githubGuidance引导能够比传统的提示或链接更有效地控制现代语言模型，并且更高效。引导程序允许您将生成、提示和逻辑控制交错到单一连续流中，与语言模型实际处理文本的方式相匹配。像\""Chain of Thought\""及其许多变体（例如ART、Auto-CoT等）这样的简单输出结构已被证明能改善语言模型的性能。更强大的语言模型（如GPT-4）的出现使得更丰富的结构成为可能，而引导则使得构建这种结构变得更加容易和经济。githubWizardLM赋予大型预训练语言模型遵循复杂指令的能力，使用完整进化指令（约300k）训练的WizardLM-7B模型githubLLM的训练_推理_低资源_高效训练资源名（Name）描述（Description）链接QLoRA--Guanaco一种高效的微调方法，可以在单个48GB的GPU上微调一个拥有65B参数的模型，同时保持完整的16位微调任务性能，并通过QLoRA将梯度反向传播通过一个冻结的、4位量化的预训练语言模型到低秩适配器（LoRA）githubChinese-Guanaco一个中文低资源的量化训练/部署方案githubDeepSpeed Chat: 一键式RLHF训练githubLLMTune: 在消费级GPU上微调大型65B+LLM可以在普通消费级GPU上进行4位微调，例如最大的65B LLAMA模型。LLMTune还实现了LoRA算法和GPTQ算法来压缩和量化LLM，并通过数据并行处理大型模型。此外，LLMTune提供了命令行界面和Python库的使用方式github基于ChatGLM-6B+LoRA在指令数据集上进行微调基于deepspeed支持多卡微调，速度相比单卡提升8-9倍具体设置可见 微调3 基于DeepSpeed进行Lora微调github微软发布RLHF训练工具DeepSpeed ChatgithubLlamaChat：Mac上基于LLaMa的聊天机器人githubChatGPT/GPT4开源“平替”们github训练大型机器学习模型的实用建议和技巧帮助您训练大型模型（>1B 参数）、避免不稳定性、保存开始失败的实验而不从 0 重新开始linkInstruction Tuning with GPT-4paperxturing一个Python软件包，用于高效、快速、简单地微调LLM模型，支持LLaMA、GPT-J、GPT-2等多种模型，可使用单GPU和多GPU训练，使用LoRA等高效微调技术可将硬件成本降低高达90%，并在短时间内完成模型训练githubGPT4All一个允许在Macbook本地运行GPT的开源项目。基于LLaMa-7B大语言模型打造，包括数据、代码和demo都是开源的，对话风格偏向AI助理github用Alpaca-LoRA微调ChatGPT类模型linkLMFlow可扩展、方便有效的工具箱，用于微调大型机器学习模型github闻达：大型语言模型调用平台目前支持chatGLM-6B、chatRWKV、chatYuan和chatGLM-6B模型下的chatPDF（自建知识库查找）'githubMicro Agent小型自主智能体开源项目，由LLM(OpenAI GPT-4)提供动力，可以为你编写软件，只需设置一个“目的”，让它自己工作githubLlama-X开源的学术研究项目，通过社区共同努力，逐步将LLaMA的性能提高到SOTA LLM水平，节省重复工作，共同创造更多、更快的增量githubChinese-LLaMA-Alpaca中文LLaMA&Alpaca大语言模型+本地部署 (Chinese LLaMA & Alpaca LLMs) - 开源了经过中文文本数据预训练的中文LLaMA大模型；开源了进一步经过指令精调的中文Alpaca大模型；快速地使用笔记本电脑（个人PC）本地部署和体验量化版大模型githubEfficient Alpaca基于LLaMA实现的开源项目，旨在通过微调 LLaMA-7B模型在资源消耗更少、推理速度更快、更适合研究者使用方面提高Stanford Alpaca的性能githubChatGLM-6B-Slim裁减掉20K图片Token的ChatGLM-6B，完全一样的性能，占用更小的显存githubChinese-Vicuna一个中文低资源的llama+lora方案githubAlpaca-LoRA用LoRA在消费级硬件上复现斯坦福Alpaca的结果githubLLM Accelerator让基础大模型更聪明的LLM Accelerator来了！基础大模型正在诸多应用中发挥着日益重要的作用。大多数大语言模型的训练都是采取自回归的方式进行生成，虽然自回归模型生成的文本质量有所保证，但却导致了高昂的推理成本和长时间的延迟。由于大模型的参数量巨大、推理成本高，因此如何在大规模部署大模型的过程中降低成本、减小延迟是一个关键课题。针对此问题，微软亚洲研究院的研究员们提出了一种使用参考文本无损加速大语言模型推理的方法 LLM Accelerator，在大模型典型的应用场景中可以取得两到三倍的加速。blog大语言模型（LLM）微调技术笔记githubPyLLMs简洁的 Python 库，用于连接各种 LLM(OpenAI、Anthropic、Google、AI21、Cohere、Aleph Alpha、HuggingfaceHub)，内置模型性能基准。非常适合快速原型设计和评估不同模型，具有以下特点：通过少量代码连接顶级 LLM；响应元数据包括处理的Token、成本和延迟，对各个模型进行标准化；支持多模型：同时从不同模型获取补全；LLM 基准：评估模型的质量、速度和成本github用混合精度加速大型语言模型通过使用低精度浮点数运算，可以将训练和推断速度提升多达3倍，同时不影响模型准确性blog新的LLM训练方法 Federate杜克大学和微软一起发布了一个新的LLM训练方法 Federated GPT，这个训练方法是将原本中心化的训练方法分散到不同的边缘设备里面（edge device），然后训练完成后，再上传到中心去将各子模型合并。github提示工程资源名（Name）描述（Description）链接OpenBuprompt-engineering-note提示工程笔记(课程总结)》介绍了面向开发者的 ChatGPT Prompt Engineering Learning Notes 课程，该课程提供了语言模型的工作原理和提示工程实践，并展示了如何将语言模型 API 应用于各种任务的应用程序中。课程包括总结、推断、转换、扩展和打造聊天机器人等方面的内容，并讲述了如何设计好的提示和构建自定义聊天机器人。github-OpenBuprompt提示工程指南linkAIGC提示工程学习站 Learn PromptChatGPT/Midjourney/RunwaylinkPrompts 精选 - ChatGPT 使用指南ChatGPT 使用指南，提升 ChatGPT 可玩性和可用性github非官方的ChatGPT资源聚合列表，旨在汇总使用ChatGPT旨在汇总使用ChatGPT的应用、Web应用、浏览器扩展、CLI工具、机器人、集成、软件包、文章等资源githubSnack Prompt：ChatGPT Prompt提示分享社区linkChatGPT提问技巧如何向 ChatGPT 提问以获得高质量答案：提示技巧工程完全指南githubrompt-Engineering-Guide-Chinese - 提示工程师指南源自英文版，但增加了AIGC的prompt部分githubOpenPrompt一个开放的共享Prompt社区，大家一起推荐好用的promptgithubGPT-Prompts教你如何用GPT生成Promptsgithub类ChatGPT的文档问答资源名（Name）描述（Description）链接privateGPT基于GPT4All-J的私有化部署文档问答平台，无需联网，能100%保证用户的隐私不泄露。提供了一个API，用户可以使用自己的文档进行交互式问答和生成文本。此外，平台支持自定义训练数据和模型参数，以满足个性化需求github-privateGPTAuto-evaluator文档问答的自动评估 ；、githubPDF GP一个基于 GPT 实现的开源 PDF 文档聊天方案,主要实现以下功能：跟 PDF 文档进行一对一对话；自动切割内容，并使用强大的深度平均网络编码器来生成嵌入；对 PDF 内容执行语义搜索，并将最相关的嵌入传递给 Open AI；自定义逻辑，生成更精确的响应信息，速度要比 OpenAI 的快。githubRedis-LLM-Document-Chat用LlamaIndex、Redis和OpenAI与PDF文档进行交互，包含一个Jupyter笔记本，演示了如何使用Redis作为向量数据库来存储和检索文档向量，还展示了如何使用LlamaIndex在文档中执行语义搜索，以及如何利用OpenAI提供类似聊天机器人的体验githubdoc-chatbotGPT-4 + Pinecone + LangChain + MongoDB实现的文档聊天机器人，可多文件、多话题和多窗口聊天，聊天历史由MongoDB保存githubdocument.ai基于向量数据库与GPT3.5的通用本地知识库方案(A universal local knowledge base solution based on vector database and GPT3.5)githubDocsGPTDocsGPT是一种尖端的开源解决方案，可以简化在项目文档中查找信息的过程。通过集成强大的GPT模型，开发人员可以轻松地提出关于项目的问题并获得准确的答案。githubChatGPT Retrieval PluginChatGPT检索插件存储库提供了一种灵活的解决方案，可以使用自然语言查询对个人或组织文档进行语义搜索和检索。githubLamaIndexlamaIndex（GPT索引）是您的LLM应用程序的数据框架。githubchatWebChatWeb可以爬取任意网页或PDF，DOCX，TXT文件并提取正文，可以生成嵌入式概要，可以根据正文内容回答你的问题。 基于gpt3.5的chatAPI和embeddingAPI，以及向量数据库实现。github类ChatGPT的行业应用资源名（Name）描述（Description）链接新闻报道进行情感分析用ChatGPT通过对上市公司的新闻报道进行情感分析，在15个月时间内在股票市场(交易期权)产生了500%的回报（在历史数据中测试得出的结果）——探讨了ChatGPT在利用新闻标题的情感分析来预测股市回报方面的潜力。发现ChatGPT的情感分析能力超过了传统的方法，并且与股市回报呈正相关。提出ChatGPT在金融经济领域有很大的价值，并对未来的研究和应用提出了一些启示和建议paper编程语言生成模型 StarCoderBigCode是 ServiceNow Inc. 和 Hugging Face Inc. 合作成立的。StarCoder 有多个版本。核心版本 StarCoderBase 具有 155 亿个参数，支持80多种编程语言，8192个token的上下文。视频为其vscode插件效果githubCodeGen2: Lessons for Training LLMs on Programming and Natural Languagescode generationpaperMedicalGPT-zh：中文医疗通用语言模型中文医疗通用语言模型，基于28个科室的医疗共识与临床指南文本，提高模型的医疗领域知识与对话能力githubMagicSlides不少人梦寐以求的AI自作PPT，免费版每月能做3个PPT，支持2500字输入linkSalesGPT使用LLM实现上下文感知的销售助手，可自动化销售拓展代表的活动，如外呼销售电话github华驼(HuaTuo): 基于中文医学知识的LLaMA微调模型githubai-code-translator帮助你把代码从一种语言翻译成另一种语言，这事对ChatGPT来说简直太擅长了，尤其是GPT-4，翻译质量相当高，而且tokens长度也可以更长。githubChatGenTitle使用百万arXiv论文信息在LLaMA模型上进行微调的论文题目生成模型githubRegex.ai一款所见即所得的，基于 AI 的正则表达式自动生成工具，只需要选择出数据，它就能帮你写正则表达式，并提供多种提取数据的方式videoChatDoctor一个基于医学领域知识微调LLaMA的医学聊天模型，其中医学数据包含大约700种疾病的数据、以及大约5000段医生和病人的对话记录paperCodeGPT提高编程能力的关键在于数据。CodeGPT是通过GPT生成的用于GPT的代码对话数据集。现在公开了32K条中文数据，让模型更擅长编程githubLaWGPT一系列基于中文法律知识的开源大语言模型githubLangChain-ChatGLM-Webui受langchain-ChatGLM启发, 利用LangChain和ChatGLM-6B系列模型制作的Webui, 提供基于本地知识的大模型应用.目前支持上传 txt、docx、md、pdf等文本格式文件, 提供包括ChatGLM-6B系列、Belle系列等模型文件以及GanymedeNil/text2vec-large-chinese、nghuyong/ernie-3.0-base-zh、nghuyong/ernie-3.0-nano-zh等Embedding模型.github类ChatGPT的课程资料资源名（Name）描述（Description）链接Databricks（Dolly模型的作者）在edX发布了两个免费课程程，其中第二个是关于LLM是如何构建的。link大语言模型技术分享系列东北大学自然语言处理实验室videoGPT-4是如何工作的？如何利用GPT-4打造智能程序？哈佛大学CS50公开课video提示工程最佳实践：Andrew Ng 提示工程新课摘要+LangChain经验总结medium_blog微调LLM模型如果你对微调LLM模型感兴趣，一定要关注这个油管博主，他把几乎世面上所有的LLM模型都公开了微调的方法。油管博主 Sam WitteveenTransformer的架构解读通俗易懂的介绍youtube1youtube2 youtube3Transformer multi head机制的视频如果想要真正理解整个Transform的每一个细节，包括里面的数学原理，可以看一下这个视频，真的是剖析地非常详细youtubeIntroduction to Large Language Models大语言模型介绍介绍了大型语言模型（Large Language Models，LLMs）的概念、使用场景、提示调整以及Google的Gen AI开发工具。LLM的安全问题资源名（Name）描述（Description）链接LLM模型安全研究linkChatbot Injections & Exploit收集了一些Chatbot注入和漏洞的例子，以帮助人们了解Chatbot的潜在漏洞和脆弱性。注入和攻击的方式包括命令注入、字符编码、社交工程、表情符号、Unicode等。仓库提供了一些示例，其中一些包括可用于攻击Chatbot的表情符号列表githubGPTSecurity一个涵盖了前沿学术研究和实践经验分享的社区，集成了生成预训练 Transformer（GPT）、人工智能生成内容（AIGC）以及大型语言模型（LLM）等安全领域应用的知识。在这里，您可以找到关于GPT/AIGC/LLM最新的研究论文、博客文章、实用的工具和预设指令（Prompts）。github多模态LLM资源名（Name）描述（Description）链接DeepFloyd IF高度逼真且具有语言理解能力的最新开源文本到图像模型，由一个冻结文本编码器和三个连续的像素扩散模块组成，是一个高效的模型，性超越了当前最先进的模型，在COCO数据集上实现了零样本的FID得分为6.66githubMulti-modal GPT用多模态GPT训练一个能同时接收视觉和语言指令的聊天机器人。基于OpenFlamingo多模态模型，使用各种开放数据集创建各种视觉指导数据，联合训练视觉和语言指导，有效提高模型性能githubAudioGPTUnderstanding and Generating Speech, Music, Sound, and Talking Head' by AIGC-Audiogithubtext2image-prompt-generator基于GPT-2用25万条Midjourney的promps训练出来的小模型，可以生成高质量的Midjourney  promptlink data汇总6个Midjourney以外的免费以文生图服务：Bing Image Creator Playground AI DreamStudio Pixlr Leonardo AI CraiyonBARK一个非常强大的TTS（文字转语音）项目，这个项目的特点是，它可以在文字中加入提示词，比如“大笑”。这个提示词会变成笑的声音，然后合成到语音里去。它也可以混合“男声”，“女声”，这样再做就可以不用再做拼接操作了githubwhisper在语音转文字（STT，也称ASR）方面，whisper是我用过的最好的，最快的库。没想到，这么快的模型，还能70x的优化空间。我准备部署这个模型，并开放给大家使用，可以用来转录大的语音文件，和进行翻译。这个模型是多语言的，而且能自动识别是什么语言，真的非常强大githubOFA-Chinese：中文多模态统一预训练模型transformers结构的中文OFA模型github文生图开源模型试炼场可根据输入文字同时用stable-diffusion 1.5、stable-diffusion 2.1、DALL-E、kandinsky-2等模型生成图像，方便测试比较linkLLMScoreLLMScore是一种全新的框架，能够提供具有多粒度组合性的评估分数。它使用大语言模型（LLM）来评估文本到图像生成模型。首先，将图像转化为图像级别和对象级别的视觉描述，然后将评估指令输入到LLM中，以衡量合成图像与文本的对齐程度，并最终生成一个评分和解释。我们的大量分析显示，LLMScore在众多数据集上与人类判断的相关性最高，明显优于常用的文本-图像匹配度量指标CLIP和BLIP。papergithubVisualGLM-6BVisualGLM-6B 是一个开源的，支持图像、中文和英文的多模态对话语言模型，语言模型基于 ChatGLM-6B，具有 62 亿参数；图像部分通过训练 BLIP2-Qformer 构建起视觉模型与语言模型的桥梁，整体模型共78亿参数。githubLLM的数据集资源名（Name）描述（Description）链接歧义数据集能否正确的消除歧义是衡量大语言模型的一个重要指标。不过一直没有一个标准化的衡量方法，这篇论文提出了一个包含1,645个具有不同种类歧义的数据集及对应的评估方法。github paperthu指令训练数据设计了一套流程来自动产生多样化高质量的多轮指令对话数据UltraChat，并进行了细致的人工后处理。现已将英文数据全部开源，共计150余万条，是开源社区数量最多的高质量指令数据之一github多模态数据集MMC45.8亿图片，1亿文档，400亿tokengithubEleutherAI 数据800g的文本语料给你整合好了免费下载，不知道trian出来的model质量如何，打算试试：pile data paperUltraChat大规模、信息丰富、多样化的多轮对话数据githubConvFinQA金融数据问答githubThe botbots dataset一个包含对话内容的数据集，对话内容来自于两个ChatGPT实例(gpt-3.5-turbo)，CLT命令和对话提示来自GPT-4，覆盖多种情境和任务，生成成本约为35美元，可用于研究和训练更小的对话模型(如Alpaca)githubalpaca_chinese_dataset - 人工精调的中文对话数据集githubCodeGPT-data提高编程能力的关键在于数据。CodeGPT是通过GPT生成的用于GPT的代码对话数据集。现在公开了32K条中文数据，让模型更擅长编程github语料库资源名（Name）描述（Description）链接人名语料库wainshine/Chinese-Names-CorpusChinese-Word-Vectors各种中文词向量github repo中文聊天语料该库搜集了包含豆瓣多轮, PTT八卦语料, 青云语料, 电视剧对白语料, 贴吧论坛回帖语料,微博语料,小黄鸡语料link中文谣言数据该数据文件中，每一行为一条json格式的谣言数据github中文问答数据集链接 提取码 2dva微信公众号语料3G语料，包含部分网络抓取的微信公众号的文章，已经去除HTML，只包含了纯文本。每行一篇，是JSON格式，name是微信公众号名字，account是微信公众号ID，title是题目，content是正文github中文自然语言处理 语料、数据集github任务型对话英文数据集【最全任务型对话数据集】主要介绍了一份任务型对话数据集大全，这份数据集大全涵盖了到目前在任务型对话领域的所有常用数据集的主要信息。此外，为了帮助研究者更好的把握领域进展的脉络，我们以Leaderboard的形式给出了几个数据集上的State-of-the-art实验结果。github语音识别语料生成工具从具有音频/字幕的在线视频创建自动语音识别(ASR)语料库githubLitBankNLP数据集支持自然语言处理和计算人文学科任务的100部带标记英文小说语料github中文ULMFiT情感分析 文本分类 语料及模型github省市区镇行政区划数据带拼音标注github教育行业新闻 自动文摘 语料库github中文自然语言处理数据集github维基大规模平行文本语料85种语言、1620种语言对、135M对照句github古诗词库github repo 更全的古诗词库低内存加载维基百科数据用新版nlp库加载17GB+英文维基语料只占用9MB内存遍历速度2-3 Gbit/sgithub对联数据700,000 couplets, 超过70万对对联github《配色辞典》数据集github42GB的JD客服对话数据(CSDD)github70万对联数据link用户名黑名单列表github依存句法分析语料4万句高质量标注数据Homepage人民日报语料处理工具集github虚假新闻数据集 fake news corpusgithub诗歌质量评价/细粒度情感诗歌语料库github中文自然语言处理相关的开放任务数据集以及当前最佳结果github中文缩写数据集github中文任务基准测评代表性的数据集-基准(预训练)模型-语料库-baseline-工具包-排行榜github中文谣言数据库githubCLUEDatasetSearch中英文NLP数据集搜索所有中文NLP数据集，附常用英文NLP数据集github多文档摘要数据集github让人人都变得“彬彬有礼”礼貌迁移任务在保留意义的同时将非礼貌语句转换为礼貌语句，提供包含139M + 实例的数据集paper and code粤语/英语会话双语语料库github中文NLP数据集列表github类人名/地名/组织机构名的命名体识别数据集github中文语言理解测评基准包括代表性的数据集&基准模型&语料库&排行榜githubOpenCLaP多领域开源中文预训练语言模型仓库民事文书、刑事文书、百度百科github中文全词覆盖BERT及两份阅读理解数据DRCD数据集：由中国台湾台达研究院发布，其形式与SQuAD相同，是基于繁体中文的抽取式阅读理解数据集。CMRC 2018数据集:哈工大讯飞联合实验室发布的中文机器阅读理解数据。根据给定问题，系统需要从篇章中抽取出片段作为答案，形式与SQuAD相同。githubDakshina数据集十二种南亚语言的拉丁/本地文字平行数据集合githubOPUS-100以英文为中心的多语(100种)平行语料github中文阅读理解数据集github中文自然语言处理向量合集github中文语言理解测评基准包括代表性的数据集、基准(预训练)模型、语料库、排行榜githubNLP数据集/基准任务大列表githubLitBankNLP数据集支持自然语言处理和计算人文学科任务的100部带标记英文小说语料github70万对联数据github文言文（古文）-现代文平行语料短篇章中包括了《论语》、《孟子》、《左传》等篇幅较短的古籍，已和《资治通鉴》合并githubCOLDDateset，中文冒犯性语言检测数据集涵盖了种族、性别和地区等话题内容，数据待论文发表后放出paperGAOKAO-bench：以中国高考题目作为数据集以中国高考题目作为数据集，评估大语言模型的语言理解能力和逻辑推理能力的测评框架，包含1781道选择题、218道填空题和812道解答题githubzero to nlp - 中文nlp应用数据、模型、训练、推理github词库及词法工具资源名（Name）描述（Description）链接textfilter中英文敏感词过滤observerss/textfilter人名抽取功能中文（现代、古代）名字、日文名字、中文的姓和名、称呼（大姨妈、小姨妈等）、英文->中文名字（李约翰）、成语词典cocoNLP中文缩写库全国人大: 全国 人民 代表大会; 中国: 中华人民共和国;女网赛: 女子/n 网球/n 比赛/vngithub汉语拆字词典漢字\t拆法 (一)\t拆法 (二)\t拆法 (三) 拆\t手 斥\t扌 斥\t才 斥kfcd/chaizi词汇情感值山泉水:0.400704566541   充沛:\t0.37006739587rainarch/SentiBridge中文词库、停用词、敏感词dongxiexidian/Chinesepython-pinyin汉字转拼音mozillazg/python-pinyinzhtools中文繁简体互转skydark/nstools英文模拟中文发音引擎say wo i ni #说：我爱你tinyfool/ChineseWithEnglishchinese_dictionary同义词库、反义词库、否定词库guotong1988/chinese_dictionarywordninja无空格英文串分割、抽取单词wordninja汽车品牌、汽车零件相关词汇dataTHU整理的词库IT词库、财经词库、成语词库、地名词库、历史名人词库、诗词词库、医学词库、饮食词库、法律词库、汽车词库、动物词库link罪名法务名词及分类模型包含856项罪名知识图谱, 基于280万罪名训练库的罪名预测,基于20W法务问答对的13类问题分类与法律资讯问答功能github分词语料库+代码百度网盘链接     - 提取码 pea6基于Bi-LSTM + CRF的中文分词+词性标注keras实现link基于Universal Transformer + CRF 的中文分词和词性标注link快速神经网络分词包java versionchinese-xinhua中华新华字典数据库及api，包括常用歇后语、成语、词语和汉字githubSpaCy 中文模型包含Parser, NER, 语法树等功能。有一些英文package使用spacy的英文模型的，如果要适配中文，可能需要使用spacy中文模型。github中文字符数据githubSynonyms中文近义词工具包githubHarvestText领域自适应文本挖掘工具（新词发现-情感分析-实体链接等）githubword2word方便易用的多语言词-词对集62种语言/3,564个多语言对github多音字词典数据及代码github汉字、词语、成语查询接口github103976个英语单词库包（sql版，csv版，Excel版）github英文脏话大列表github词语拼音数据github186种语言的数字叫法库github世界各国大规模人名库github汉字字符特征提取器 (featurizer)提取汉字的特征（发音特征、字形特征）用做深度学习的特征githubchar_featurizer - 汉字字符特征提取工具github中日韩分词库mecab的Python接口库githubg2pC基于上下文的汉语读音自动标记模块githubssc, Sound Shape Code音形码 - 基于“音形码”的中文字符串相似度计算方法version 1version 2blog/introduction基于百科知识库的中文词语多词义/义项获取与特定句子词语语义消歧githubTokenizer快速、可定制的文本词条化库githubTokenizers注重性能与多功能性的最先进分词器github通过同义词替换实现文本“变脸”githubtoken2index与PyTorch/Tensorflow兼容的强大轻量词条索引库github繁简体转换github粤语NLP工具github领域词典库涵盖68个领域、共计916万词的专业词典知识库github预训练语言模型&大模型资源名（Name）描述（Description）链接BMList大模型大列表githubbert论文中文翻译linkbert原作者的slideslink文本分类实践githubbert tutorial文本分类教程githubbert pytorch实现githubbert pytorch实现githubBERT生成句向量，BERT做文本分类、文本相似度计算githubbert、ELMO的图解githubBERT Pre-trained models and downstream applicationsgithub语言/知识表示工具BERT & ERNIEgithubKashgari中使用gpt-2语言模型githubFacebook LAMA用于分析预训练语言模型中包含的事实和常识知识的探针。语言模型分析，提供Transformer-XL/BERT/ELMo/GPT预训练语言模型的统一访问接口github中文的GPT2训练代码githubXLMFacebook的跨语言预训练语言模型github海量中文预训练ALBERT模型githubTransformers 20支持TensorFlow 20 和 PyTorch 的自然语言处理预训练语言模型(BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) 8种架构/33种预训练模型/102种语言github8篇论文梳理BERT相关模型进展与反思github法文RoBERTa预训练语言模型用138GB语料训练的法文RoBERTa预训练语言模型link中文预训练 ELECTREA 模型基于对抗学习 pretrain Chinese Modelgithubalbert-chinese-ner用预训练语言模型ALBERT做中文NERgithub开源预训练语言模型合集github中文ELECTRA预训练模型github用Transformers(BERT, XLNet, Bart, Electra, Roberta, XLM-Roberta)预测下一个词(模型比较)githubTensorFlow Hub40+种语言的新语言模型(包括中文)linkUER基于不同语料、编码器、目标任务的中文预训练模型仓库（包括BERT、GPT、ELMO等）github开源预训练语言模型合集github多语言句向量包githubLanguage Model as a Service (LMaaS)语言模型即服务github开源语言模型GPT-NeoX-20B200亿参数，是目前最大的可公开访问的预训练通用自回归语言模型github中文科学文献数据集（CSL）包含 396,209 篇中文核心期刊论文元信息 （标题、摘要、关键词、学科、门类）。CSL 数据集可以作为预训练语料，也可以构建许多NLP任务，例如文本摘要（标题预测）、 关键词生成和文本分类等。github大模型开发神器github抽取资源名（Name）描述（Description）链接时间抽取已集成到 python package cocoNLP中，欢迎试用java versionpython version神经网络关系抽取 pytorch暂不支持中文github基于bert的命名实体识别 pytorch暂不支持中文github关键词(Keyphrase)抽取包 pkegithubBLINK最先进的实体链接库githubBERT/CRF实现的命名实体识别github支持批并行的LatticeLSTM中文命名实体识别github构建医疗实体识别的模型包含词典和语料标注，基于pythongithub基于TensorFlow和BERT的管道式实体及关系抽取- Entity and Relation Extraction Based on TensorFlow and BERT 基于TensorFlow和BERT的管道式实体及关系抽取，2019语言与智能技术竞赛信息抽取任务解决方案。Schema based Knowledge Extraction, SKE 2019github中文命名实体识别NeuroNER vs BertNERgithub基于BERT的中文命名实体识别github中文关键短语抽取工具githubbert用于中文命名实体识别 tensorflow版本githubbert-Kashgari基于 keras 的封装分类标注框架 Kashgari，几分钟即可搭建一个分类或者序列标注模型githubcocoNLP人名、地址、邮箱、手机号、手机归属地 等信息的抽取，rake短语抽取算法。githubMicrosoft多语言数字/单位/如日期时间识别包github百度开源的基准信息抽取系统github中文地址分词（地址元素识别与抽取），通过序列标注进行NERgithub基于依存句法的开放域文本知识三元组抽取和知识库构建github基于预训练模型的中文关键词抽取方法githubchinese_keyphrase_extractor (CKPE)A tool for chinese keyphrase extraction 一个快速从自然语言文本中提取和识别关键短语的工具github简单的简历解析器，用来从简历中提取关键信息githubBERT-NER-Pytorch三种不同模式的BERT中文NER实验github知识图谱资源名（Name）描述（Description）链接清华大学XLORE中英文跨语言百科知识图谱百度、中文维基、英文维基link文档图谱自动生成github基于医疗领域知识图谱的问答系统github 该repo参考了github中文人物关系知识图谱项目githubAmpliGraph 知识图谱表示学习(Python)库知识图谱概念链接预测github中文知识图谱资料、数据及工具github基于百度百科的中文知识图谱抽取三元组信息，构建中文知识图谱githubZincbase 知识图谱构建工具包github基于知识图谱的问答系统github知识图谱深度学习相关资料整理github东南大学《知识图谱》研究生课程(资料)github知识图谱车音工作项目github《海贼王》知识图谱github132个知识图谱的数据集涵盖常识、城市、金融、农业、地理、气象、社交、物联网、医疗、娱乐、生活、商业、出行、科教link大规模、结构化、中英文双语的新冠知识图谱(COKG-19)link基于依存句法与语义角色标注的事件三元组抽取github抽象知识图谱目前规模50万，支持名词性实体、状态性描述、事件性动作进行抽象github大规模中文知识图谱数据14亿实体githubJiagu自然语言处理工具以BiLSTM等模型为基础，提供知识图谱关系抽取 中文分词 词性标注 命名实体识别 情感分析 新词发现 关键词 文本摘要 文本聚类等功能githubmedical_NER - 中文医学知识图谱命名实体识别github知识图谱相关学习资料/数据集/工具资源大列表githubLibKGE面向可复现研究的知识图谱嵌入库github基于mongodb存储的军事领域知识图谱问答项目包括飞行器、太空装备等8大类，100余小类，共计5800项的军事武器知识库，该项目不使用图数据库进行存储，通过jieba进行问句解析，问句实体项识别，基于查询模板完成多类问题的查询，主要是提供一种工业界的问答思想demo。github京东商品知识图谱github基于远监督的中文关系抽取github基于医药知识图谱的智能问答系统githubBLINK最先进的实体链接库github一个小型的证券知识图谱/知识库githubdstlr非结构化文本可扩展知识图谱构建平台github百度百科人物词条属性抽取用基于BERT的微调和特征提取方法来进行知识图谱github新冠肺炎相关数据新冠及其他类型肺炎中文医疗对话数据集；清华大学等机构的开放数据源（COVID-19）github githubDGL-KE 图嵌入表示学习算法github因果关系图谱method data基于多领域文本数据集的因果事件对link文本生成资源名（Name）描述（Description）链接TexarToolkit for Text Generation and BeyondgithubEhud Reiter教授的博客link  北大万小军教授强力推荐，该博客对NLG技术、评价与应用进行了深入的探讨与反思。文本生成相关资源大列表github开放域对话生成及在微软小冰中的实践自然语言生成让机器掌握自动创作的本领link文本生成控制github自然语言生成相关资源大列表github用BLEURT评价自然语言生成link自动对联数据及机器人代码 link  70万对联数据自动生成评论用Transformer编解码模型实现的根据Hacker News文章标题生成评论github自然语言生成SQL语句（英文）github自然语言生成资源大全github中文生成任务基准测评github基于GPT2的特定主题文本生成/文本增广github编码、标记和实现一种可控高效的文本生成方法githubTextFooler针对文本分类/推理的对抗文本生成模块githubSimBERT基于UniLM思想、融检索与生成于一体的BERT模型github新词生成及造句不存在的词用GPT-2变体从头生成新词及其定义、例句github由文本自动生成多项选择题github合成数据生成基准github文本摘要资源名（Name）描述（Description）链接中文文本摘要/关键词提取github基于命名实体识别的简历自动摘要github文本自动摘要库TextTeaser仅支持英文github基于BERT等最新语言模型的抽取式摘要提取githubPython利用深度学习进行文本摘要的综合指南link(Colab)抽象文本摘要实现集锦(教程github智能问答资源名（Name）描述（Description）链接中文聊天机器人根据自己的语料训练出自己想要的聊天机器人，可以用于智能客服、在线问答、智能聊天等场景github有趣的情趣robot qingyunqingyun 训练出来的中文聊天机器人github开放了对话机器人、知识图谱、语义理解、自然语言处理工具及数据githubqa对的机器人Amodel-for-Retrivalchatbot - 客服机器人，Chinese Retreival chatbot（中文检索式机器人）gitConvLab开源多域端到端对话系统平台github基于最新版本rasa搭建的对话系统github基于金融-司法领域(兼有闲聊性质)的聊天机器人github端到端的封闭域对话系统githubMiningZhiDaoQACorpus580万百度知道问答数据挖掘项目，百度知道问答语料库，包括超过580万的问题，每个问题带有问题标签。基于该问答语料库，可支持多种应用，如逻辑挖掘github用于中文闲聊的GPT2模型GPT2-chitchatgithub基于检索聊天机器人多轮响应选择相关资源列表(Leaderboards、Datasets、Papers)github微软对话机器人框架githubchatbot-list行业内关于智能客服、聊天机器人的应用和架构、算法分享和介绍githubChinese medical dialogue data 中文医疗对话数据集github一个大规模医疗对话数据集包含110万医学咨询，400万条医患对话github大规模跨领域中文任务导向多轮对话数据集及模型CrossWOZpaper & data开源对话式信息搜索平台github情境互动多模态对话挑战2020(DSTC9 2020)github用Quora问题对训练的T5问题意译(Paraphrase)githubGoogle发布Taskmaster-2自然语言任务对话数据集githubHaystack灵活、强大的可扩展问答(QA)框架github端到端的封闭域对话系统githubAmazon发布基于知识的人-人开放领域对话数据集github基于百度webqa与dureader数据集训练的Albert Large QA模型githubCommonsenseQA面向常识的英文QA挑战linkMedQuAD(英文)医学问答数据集github基于Albert、Electra，用维基百科文本作为上下文的问答引擎github基于14W歌曲知识库的问答尝试功能包括歌词接龙，已知歌词找歌曲以及歌曲歌手歌词三角关系的问答github文本纠错资源名（Name）描述（Description）链接中文文本纠错模块代码github英文拼写检查库githubpython拼写检查库githubGitHub Typo Corpus大规模GitHub多语言拼写错误/语法错误数据集githubBertPunc基于BERT的最先进标点修复模型github中文写作校对工具github文本纠错文献列表Chinese Spell Checking (CSC) and Grammatical Error Correction (GEC)github文本智能校对大赛冠军方案已落地应用，来自苏州大学、达摩院团队link多模态资源名（Name）描述（Description）链接中文多模态数据集「悟空」华为诺亚方舟实验室开源大型，包含1亿图文对github中文图文表征预训练模型Chinese-CLIP中文版本CLIP预训练模型，开源多个模型规模，几行代码搞定中文图文表征提取 & 图文检索github语音处理资源名（Name）描述（Description）链接ASR 语音数据集 + 基于深度学习的中文语音识别系统github清华大学THCHS30中文语音数据集data_thchs30tgz-OpenSLR国内镜像data_thchs30tgz test-noisetgz-OpenSLR国内镜像test-noisetgz resourcetgz-OpenSLR国内镜像resourcetgzFree ST Chinese Mandarin CorpusFree ST Chinese Mandarin CorpusAIShell-1 开源版数据集-OpenSLR国内镜像AIShell-1 开源版数据集Primewords Chinese Corpus Set 1-OpenSLR国内镜像Primewords Chinese Corpus Set 1笑声检测器githubCommon Voice语音识别数据集新版包括来自42,000名贡献者超过1,400小时的语音样本，涵githublinkspeech-aligner从“人声语音”及其“语言文本”，产生音素级别时间对齐标注的工具githubASR语音大辞典/词典github语音情感分析githubmasr中文语音识别，提供预训练模型，高识别率github面向语音识别的中文文本规范化github语音质量评价指标(MOSNet, BSSEval, STOI, PESQ, SRMR)github面向语音识别的中文/英文发音辞典githubCoVoSTFacebook发布的多语种语音-文本翻译语料库包括11种语言(法语、德语、荷兰语、俄语、西班牙语、意大利语、土耳其语、波斯语、瑞典语、蒙古语和中文)的语音、文字转录及英文译文githubParakeet基于PaddlePaddle的文本-语音合成github(Java)准确的语音自然语言检测库githubCoVoSTFacebook发布的多语种语音-文本翻译语料库githubTensorFlow 2 实现的文本语音合成githubPython音频特征提取包githubViSQOL音频质量感知客观、完整参考指标，分音频、语音两种模式githubzhrtvc好用的中文语音克隆兼中文语音合成系统githubaukit好用的语音处理工具箱，包含语音降噪、音频格式转换、特征频谱生成等模块githubphkit好用的音素处理工具箱，包含中文音素、英文音素、文本转拼音、文本正则化等模块githubzhvoice中文语音语料，语音更加清晰自然，包含8个开源数据集，3200个说话人，900小时语音，1300万字githubaudio面向语音行为检测、二值化、说话人识别、自动语音识别、情感识别等任务的音频标注工具github深度学习情感文本语音合成githubPython音频数据增广库github基于大规模音频数据集Audioset的音频增强github语声迁移github文档处理资源名（Name）描述（Description）链接LayoutLM-v3文档理解模型githubPyLaia面向手写文档分析的深度学习工具包github单文档非监督的关键词抽取githubDocSearch免费文档搜索引擎githubfdfgen能够自动创建pdf文档，并填写信息linkpdfx自动抽取出引用参考文献，并下载对应的pdf文件linkinvoice2data发票pdf信息抽取invoice2datapdf文档信息抽取githubPDFMinerPDFMiner能获取页面中文本的准确位置，以及字体或行等其他信息。它还有一个PDF转换器，可以将PDF文件转换成其他文本格式(如HTML)。还有一个可扩展的解析器PDF，可以用于文本分析以外的其他用途。linkPyPDF2PyPDF 2是一个python PDF库，能够分割、合并、裁剪和转换PDF文件的页面。它还可以向PDF文件中添加自定义数据、查看选项和密码。它可以从PDF检索文本和元数据，还可以将整个文件合并在一起。linkPyPDF2PyPDF 2是一个python PDF库，能够分割、合并、裁剪和转换PDF文件的页面。它还可以向PDF文件中添加自定义数据、查看选项和密码。它可以从PDF检索文本和元数据，还可以将整个文件合并在一起。linkReportLabReportLab能快速创建PDF 文档。经过时间证明的、超好用的开源项目，用于创建复杂的、数据驱动的PDF文档和自定义矢量图形。它是免费的，开源的，用Python编写的。该软件包每月下载5万多次，是标准Linux发行版的一部分，嵌入到许多产品中，并被选中为Wikipedia的打印/导出功能提供动力。linkSIMPdfPython写的简单PDF文件文字编辑器githubpdf-diffPDF文件diff工具 可显示两个pdf文档的差别github表格处理资源名（Name）描述（Description）链接用unet实现对文档表格的自动检测，表格重建githubpdftabextract用于OCR识别后的表格信息解析，很强大linktabula-py直接将pdf中的表格信息转换为pandas的dataframe，有java和python两种版本代码camelotpdf表格解析linkpdfplumberpdf表格解析PubLayNet能够划分段落、识别表格、图片link从论文中提取表格数据github用BERT在表格中寻找答案github表格问答的系列文章简介模型完结篇使用GAN生成表格数据（仅支持英文）githubcarefree-learn(PyTorch)表格数据集自动化机器学习(AutoML)包github封闭域微调表格检测githubPDF表格数据提取工具githubTaBERT理解表格数据查询的新模型paper表格处理Awesome-Table-Recognitiongithub文本匹配资源名（Name）描述（Description）链接句子、QA相似度匹配MatchZoo文本相似度匹配算法的集合，包含多个深度学习的方法，值得尝试。github中文问题句子相似度计算比赛及方案汇总githubsimilarity相似度计算工具包java编写,用于词语、短语、句子、词法分析、情感分析、语义分析等相关的相似度计算github中文词语相似度计算方法综合了同义词词林扩展版与知网（Hownet）的词语相似度计算方法，词汇覆盖更多、结果更准确。gihtubPython字符串相似性算法库github基于Siamese bilstm模型的相似句子判定模型,提供训练数据集和测试数据集提供了10万个训练样本github文本数据增强资源名（Name）描述（Description）链接中文NLP数据增强（EDA）工具github英文NLP数据增强工具github一键中文数据增强工具github数据增强在机器翻译及其他nlp任务中的应用及效果linkNLP数据增广资源集github常用正则表达式资源名（Name）描述（Description）链接抽取email的正则表达式已集成到 python package cocoNLP中，欢迎试用抽取phone_number已集成到 python package cocoNLP中，欢迎试用抽取身份证号的正则表达式IDCards_pattern = r'^([1-9]\\d{5}[12]\\d{3}(0[1-9]|1[012])(0[1-9]|[12][0-9]|3[01])\\d{3}[0-9xX])IDs = re.findall(IDCards_pattern, text, flags=0)IP地址正则表达式(25[0-5]|  2[0-4]\\d|  [0-1]\\d{2}|  [1-9]?\\d).(25[0-5]|  2[0-4]\\d|  [0-1]\\d{2}|  [1-9]?\\d).(25[0-5]|  2[0-4]\\d|  [0-1]\\d{2}|  [1-9]?\\d).(25[0-5]|  2[0-4]\\d|  [0-1]\\d{2}|  [1-9]?\\d)腾讯QQ号正则表达式[1-9]([0-9]{5,11})国内固话号码正则表达式[0-9-()（）]{7,18}用户名正则表达式[A-Za-z0-9_-\\u4e00-\\u9fa5]+国内电话号码正则匹配（三大运营商+虚拟等）github正则表达式教程github文本检索资源名（Name）描述（Description）链接高效模糊搜索工具github面向各语种/任务的BERT模型大列表/搜索引擎linkDeepmatch针对推荐、广告和搜索的深度匹配模型库githubwwsearch是企业微信后台自研的全文检索引擎githubaili - the fastest in-memory index in the East 东半球最快并发索引github高效的字符串匹配工具 RapidFuzza fast string matching library for Python and C++, which is using the string similarity calculations from FuzzyWuzzygithub阅读理解资源名（Name）描述（Description）链接高效模糊搜索工具github面向各语种/任务的BERT模型大列表/搜索引擎linkDeepmatch针对推荐、广告和搜索的深度匹配模型库githuballennlp阅读理解支持多种数据和模github情感分析资源名（Name）描述（Description）链接方面情感分析包githubawesome-nlp-sentiment-analysis情感分析、情绪原因识别、评价对象和评价词抽取github情感分析技术让智能客服更懂人类情感github事件抽取资源名（Name）描述（Description）链接中文事件抽取githubNLP事件提取文献资源列表githubPyTorch实现的BERT事件抽取(ACE 2005 corpus)github新闻事件线索抽取github机器翻译资源名（Name）描述（Description）链接无道词典有道词典的命令行版本，支持英汉互查和在线查询githubNLLB支持200+种语言任意互译的语言模型NLLBlinkEasy-Translate在本地翻译大文本文件的脚本，基于Facebook/Meta AI的 M2M100模型和NLLB200模型，支持200+种语言github数字转换资源名（Name）描述（Description）链接最好的汉字数字(中文数字)-阿拉伯数字转换工具github快速转化「中文数字」和「阿拉伯数字」github将自然语言数字串解析转换为整数和浮点数github指代消解资源名（Name）描述（Description）链接中文指代消解数据github baidu ink  code a0qq文本聚类资源名（Name）描述（Description）链接TextCluster短文本聚类预处理模块 Short text clustergithub文本分类资源名（Name）描述（Description）链接NeuralNLP-NeuralClassifier腾讯开源深度学习文本分类工具github知识推理资源名（Name）描述（Description）链接GraphbrainAI开源软件库和科研工具，目的是促进自动意义提取和文本理解以及知识的探索和推断github(哈佛)讲因果推理的免费书pdf可解释自然语言处理资源名（Name）描述（Description）链接文本机器学习模型最先进解释器库github文本攻击资源名（Name）描述（Description）链接TextAttack自然语言处理模型对抗性攻击框架githubOpenBackdoor: 文本后门攻防工具包OpenBackdoor基于Python和PyTorch开发，可用于复现、评估和开发文本后门攻防的相关算法github文本可视化资源名（Name）描述（Description）链接Scattertext 文本可视化(python)githubwhatlies词向量交互可视化spacy工具PySS3面向可解释AI的SS3文本分类器机器可视化工具github用记事本渲染3D图像githubattnvisGPT2、BERT等transformer语言模型注意力交互可视化githubTexthero文本数据高效处理包包括预处理、关键词提取、命名实体识别、向量空间分析、文本可视化等github文本标注工具资源名（Name）描述（Description）链接NLP标注平台综述githubbrat rapid annotation tool 序列标注工具linkPoplar网页版自然语言标注工具githubLIDA轻量交互式对话标注工具githubdoccano基于网页的开源协同多语言文本标注工具githubDatasaurai 在线数据标注工作流管理工具link语言检测资源名（Name）描述（Description）链接langid97种语言检测https://github.com/saffsd/langid.pylangdetect语言检测https://code.google.com/archive/p/language-detection/综合工具资源名（Name）描述（Description）链接jiebajiebahanlphanlpnlp4han中文自然语言处理工具集(断句/分词/词性标注/组块/句法分析/语义分析/NER/N元语法/HMM/代词消解/情感分析/拼写检github仇恨言论检测进展link基于Pytorch的Bert应用包括命名实体识别、情感分析、文本分类以及文本相似度等githubnlp4han中文自然语言处理工具集断句/分词/词性标注/组块/句法分析/语义分析/NER/N元语法/HMM/代词消解/情感分析/拼写检查github一些关于自然语言的基本模型github用BERT进行序列标记和文本分类的模板代码githubjieba_fast 加速版的jiebagithubStanfordNLP纯Python版自然语言处理包linkPython口语自然语言处理工具集(英文)githubPreNLP自然语言预处理库githubnlp相关的一些论文及代码包括主题模型、词向量(Word Embedding)、命名实体识别(NER)、文本分类(Text Classificatin)、文本生成(Text Generation)、文本相似性(Text Similarity)计算等，涉及到各种与nlp相关的算法，基于keras和tensorflowgithubPython文本挖掘/NLP实战示例githubForte灵活强大的自然语言处理pipeline工具集githubstanza斯坦福团队NLP工具可处理六十多种语言githubFancy-NLP用于建设商品画像的文本知识挖掘工具github全面简便的中文 NLP 工具包github工业界常用基于DSSM向量化召回pipeline复现githubTexthero文本数据高效处理包包括预处理、关键词提取、命名实体识别、向量空间分析、文本可视化等githubnlpgnn图神经网络自然语言处理工具箱githubMacadam以Tensorflow(Keras)和bert4keras为基础，专注于文本分类、序列标注和关系抽取的自然语言处理工具包githubLineFlow面向所有深度学习框架的NLP数据高效加载器githubArabica：Python文本数据探索性分析工具包githubPython 压力测试工具：SMSBoomgithub有趣搞笑工具资源名（Name）描述（Description）链接汪峰歌词生成器phunterlau/wangfeng-rnn女友 情感波动分析githubNLP太难了系列github变量命名神器github link图片文字去除，可用于漫画翻译githubCoupletAI - 对联生成基于CNN+Bi-LSTM+Attention 的自动对对联系统github用神经网络符号推理求解复杂数学方程github基于14W歌曲知识库的问答机器人功能包括歌词接龙，已知歌词找歌曲以及歌曲歌手歌词三角关系的问答githubCOPE - 格律诗编辑程序githubPaper2GUI一款面向普通人的AI桌面APP工具箱，免安装即开即用，已支持18+AI模型，内容涵盖语音合成、视频补帧、视频超分、目标检测、图片风格化、OCR识别等领域github礼貌程度估算器（使用新浪微博数据训练）github paper草蟒（Python 中文版）入门指南中文编程语言homepage gitee课程报告面试等资源名（Name）描述（Description）链接自然语言处理报告link知识图谱报告link数据挖掘报告link自动驾驶报告link机器翻译报告link区块链报告link机器人报告link计算机图形学报告link3D打印报告link人脸识别报告link人工智能芯片报告linkcs224n深度学习自然语言处理课程link 课程中模型的pytorch实现 link面向深度学习研究人员的自然语言处理实例教程github《Natural Language Processing》by Jacob EisensteingithubML-NLP机器学习(Machine Learning)、NLP面试中常考到的知识点和代码实现githubNLP任务示例项目代码集github2019年NLP亮点回顾downloadnlp-recipes微软出品--自然语言处理最佳实践和范例github面向深度学习研究人员的自然语言处理实例教程githubTransfer Learning in Natural Language Processing (NLP)youtube《机器学习系统》图书link github比赛资源名（Name）描述（Description）链接NLPer-ArsenalNLP竞赛，含当前赛事信息、过往竞赛方案等，持续更新中github复盘所有NLP比赛的TOP方案github2019年百度的三元组抽取比赛，“科学空间队”源码(第7名)github金融自然语言处理资源名（Name）描述（Description）链接BDCI2019金融负面信息判定github开源的金融投资数据提取工具github金融领域自然语言处理研究资源大列表github基于金融-司法领域(兼有闲聊性质)的聊天机器人github小型金融知识图谱构流程示范github医疗自然语言处理资源名（Name）描述（Description）链接中文医学NLP公开资源整理githubspaCy 医学文本挖掘与信息提取github构建医疗实体识别的模型包含词典和语料标注，基于pythongithub基于医疗领域知识图谱的问答系统github 该repo参考了githubChinese medical dialogue data 中文医疗对话数据集github一个大规模医疗对话数据集包含110万医学咨询，400万条医患对话github新冠肺炎相关数据新冠及其他类型肺炎中文医疗对话数据集；清华大学等机构的开放数据源（COVID-19）github github法律自然语言处理资源名（Name）描述（Description）链接Blackstone面向非结构化法律文本的spaCy pipeline和NLP模型github法务智能文献资源列表github基于金融-司法领域(兼有闲聊性质)的聊天机器人github罪名法务名词及分类模型包含856项罪名知识图谱, 基于280万罪名训练库的罪名预测,基于20W法务问答对的13类问题分类与法律资讯问答功能github法律NLP相关资源大列表github文本生成图像资源名（Name）描述（Description）链接Dalle-mini根据文本提示生成图片的迷你版DALL·Egithub其他资源名（Name）描述（Description）链接phone中国手机归属地查询ls0f/phonephone国际手机、电话归属地查询AfterShip/phonengender根据名字判断性别observerss/ngender中文对比英文自然语言处理NLP的区别综述link各大公司内部里大牛分享的技术文档 PDF 或者 PPTgithubcomparxiv 用于比较arXiv上两提交版本差异的命令pypiCHAMELEON深度学习新闻推荐系统元架构github简历自动筛选系统githubPython实现的多种文本可读性评价指标github"
77,testerSunshine/12306,https://github.com/testerSunshine/12306/blob/master/README.md,Python,"12306 购票小助手python版本 2.7.10 - 2.7.15 3.6 - 3.7.4 2.7.9已有功能 自动打码 自动登录 准点预售和捡漏 智能候补 邮件通知 server酱通知依赖库验证码目前可以本地识别，需要下载模型，放于项目根目录，全部代码来源于此项目 传送门，表示感谢  1. 模型下载链接:https://pan.baidu.com/s/1rS155VjweWVWIJogakechA  密码:bmlm     群里面也可以下载  2. git仓库下载：https://github.com/testerSunshine/12306model.git自托管云打码服务器搭建：12306_code_server如果大家有空闲的服务器，可搭建之后在这个 issues 里面填入自己的服务器(请注意服务器安全！)项目依赖 requirements.txt安装方法x:root用户(避免多python环境产生问题): pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple -r requirements.txt非root用户（避免安装和运行时使用了不同环境）: pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple -r requirements.txt许多windows的用户装不了tensorflow的话，可以适当降低版本或者升高版本都是可以的1. tensorflow的兼容版本 1.14.0rc\\1.14.0rc\\1.15.0\\1.15.0rc以上版本都测试无问题2. 如果pip代理的清华源无法下载，可以更换其他源解决此问题项目使用说明服务器启动:修改配置文件可以配置邮箱,配置邮箱的格式在配置里面可以看到ex# 测试邮箱和server酱是否可用， server酱测试的前提是server酱开关开启# 可以配置server酱提醒（推荐）[配置教程](https://www.jianshu.com/p/8d10b5b9c4e3)# 用python3 还是python 完全取决于安装的时候配置的环境变量是否为python3,以下启动默认环境变量为python3python3 run.py t配置配置文件的时候，需注意空格和遵循python语法格式启动前请先筛选cdn，这点很重要python3 run.py c启动服务python3 run.py r如果你不知道如何操作，下面的命令可能会帮助你python3 run.py -h——————————————————————————sage: run.py [-h] operatepositional arguments:  operate     r: 运行抢票程序, c: 过滤cdn, t: 测试邮箱和server酱，server酱如果你的服务器安装了docker与docker-compose, 那么你可以忽略上面的所有步骤，直接按以下步骤操作，即可开始抢票：前提条件:请确认你安装的docker版本为18.09及以上: docker -v请确认你安装的docker-compose版本为1.23.2及以上: docker-compose -v请根据自己需要修改好配置文件:TickerConfig.py请修改配置文件TickerConfig.py中的变量AUTO_CODE_TYPE和HOST，AUTO_CODE_TYPE改为3, HOST改为\""captcha:80\""（这里很重要，这是本地打码服务器的配置）运行命令:开始抢票：docker-compose up --build -d停止抢票：docker-compose down查看抢票log: docker logs --follow ticket目录对应说明agency - cdn代理config - 项目配置verify - 自动打码init - 项目主运行目录inter - 接口myException - 异常myUrllib  request网络请求库思路图项目声明：本软件只供学习交流使用，勿作为商业用途，交流群号1群：286271084(已满)2群：649992274(已满)3群：632501142(已满)4群: 606340519(已满)5群: 948526733(已满)7群: 660689659(已满)8群: 620629239(已满)6群: 608792930(未满)9群: 693035807(未满)请不要重复加群，一个群就可以了，把机会留给更多人进群先看公告！！！进群先看公告！！！进群先看公告！！！ 重要的事情说三遍能为你抢到一张回家的票，是我最大的心愿日志列子成功log，如果是购票失败的，请带上失败的log给我，我尽力帮你调，也可加群一起交流，程序只是加速买票的过程，并不一定能买到票正在第355次查询  乘车日期: 2018-02-12  车次G4741,G2365,G1371,G1377,G1329 查询无票  代理设置 无  总耗时429ms车次: G4741 始发车站: 上海 终点站: 邵阳 二等座:有正在尝试提交订票...尝试提交订单...出票成功排队成功, 当前余票还剩余: 359 张正在使用自动识别验证码功能验证码通过,正在提交订单提交订单成功！排队等待时间预计还剩 -12 ms排队等待时间预计还剩 -6 ms排队等待时间预计还剩 -7 ms排队等待时间预计还剩 -4 ms排队等待时间预计还剩 -4 ms恭喜您订票成功，订单号为：EB52743573, 请立即打开浏览器登录12306，访问‘未完成订单’，在30分钟内完成支付！使用帮助(一些安装问题和使用反馈较多的问题)：测试邮箱是否可用 邮箱配置问题看issues学生票issues 学生票修改依赖安装不对的问题（ImportError）requirements.txt问题若快豆子疑问 点我IOError: 【Errno 0】 Error 问题 点我测试下单接口是否可用，有两个下单接口，随便用哪个都ok如果下载验证码过期或者下载失败的问题，应该是12306封ip的策略，多重试几次，12306现在封服务器(阿里云和腾讯云)ip比较严重，尽量不要放在服务器里面目前12306对服务器ip比较敏感，大家还是在自己家里挂着吧自动更换ip软件目前已支持TPLINK和小米路由器，只限家庭网络点我跳转感谢一下小伙伴对本项目提供的帮助@sun7127@126.com@ 才@MonsterTan以及所有为此项目提供pr的同学更新日志更新日志"
78,google-research/bert,https://github.com/google-research/bert/blob/master/README.md,Python,"BERT***** New March 11th, 2020: Smaller BERT Models *****This is a release of 24 smaller BERT models (English only, uncased, trained with WordPiece masking) referenced in Well-Read Students Learn Better: On the Importance of Pre-training Compact Models.We have shown that the standard BERT recipe (including model architecture and training objective) is effective on a wide range of model sizes, beyond BERT-Base and BERT-Large. The smaller BERT models are intended for environments with restricted computational resources. They can be fine-tuned in the same manner as the original BERT models. However, they are most effective in the context of knowledge distillation, where the fine-tuning labels are produced by a larger and more accurate teacher.Our goal is to enable research in institutions with fewer computational resources and encourage the community to seek directions of innovation alternative to increasing model capacity.You can download all 24 from here, or individually from the table below:H=128H=256H=512H=768L=22/128 (BERT-Tiny)2/2562/5122/768L=44/1284/256 (BERT-Mini)4/512 (BERT-Small)4/768L=66/1286/2566/5126/768L=88/1288/2568/512 (BERT-Medium)8/768L=1010/12810/25610/51210/768L=1212/12812/25612/51212/768 (BERT-Base)Note that the BERT-Base model in this release is included for completeness only; it was re-trained under the same regime as the original model.Here are the corresponding GLUE scores on the test set:ModelScoreCoLASST-2MRPCSTS-BQQPMNLI-mMNLI-mmQNLI(v2)RTEWNLIAXBERT-Tiny64.20.083.281.1/71.174.3/73.662.2/83.470.270.381.557.262.321.0BERT-Mini65.80.085.981.1/71.875.4/73.366.4/86.274.874.384.157.962.326.1BERT-Small71.227.889.783.4/76.278.8/77.068.1/87.077.677.086.461.862.328.6BERT-Medium73.538.089.686.6/81.680.4/78.469.6/87.980.079.187.762.262.330.5For each task, we selected the best fine-tuning hyperparameters from the lists below, and trained for 4 epochs:batch sizes: 8, 16, 32, 64, 128learning rates: 3e-4, 1e-4, 5e-5, 3e-5If you use these models, please cite the following paper:@article{turc2019,  title={Well-Read Students Learn Better: On the Importance of Pre-training Compact Models},  author={Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},  journal={arXiv preprint arXiv:1908.08962v2 },  year={2019}}***** New May 31st, 2019: Whole Word Masking Models *****This is a release of several new models which were the result of an improvementthe pre-processing code.In the original pre-processing code, we randomly select WordPiece tokens tomask. For example:Input Text: the man jumped up , put his basket on phil ##am ##mon ' s headOriginal Masked Input: [MASK] man [MASK] up , put his [MASK] on phil [MASK] ##mon ' s headThe new technique is called Whole Word Masking. In this case, we always maskall of the the tokens corresponding to a word at once. The overall maskingrate remains the same.Whole Word Masked Input: the man [MASK] up , put his basket on [MASK] [MASK] [MASK] ' s headThe training is identical -- we still predict each masked WordPiece tokenindependently. The improvement comes from the fact that the original predictiontask was too 'easy' for words that had been split into multiple WordPieces.This can be enabled during data generation by passing the flag--do_whole_word_mask=True to create_pretraining_data.py.Pre-trained models with Whole Word Masking are linked below. The data andtraining were otherwise identical, and the models have identical structure andvocab to the original models. We only include BERT-Large models. When usingthese models, please make it clear in the paper that you are using the WholeWord Masking variant of BERT-Large.BERT-Large, Uncased (Whole Word Masking):24-layer, 1024-hidden, 16-heads, 340M parametersBERT-Large, Cased (Whole Word Masking):24-layer, 1024-hidden, 16-heads, 340M parametersModelSQUAD 1.1 F1/EMMulti NLI AccuracyBERT-Large, Uncased (Original)91.0/84.386.05BERT-Large, Uncased (Whole Word Masking)92.8/86.787.07BERT-Large, Cased (Original)91.5/84.886.09BERT-Large, Cased (Whole Word Masking)92.9/86.786.46***** New February 7th, 2019: TfHub Module *****BERT has been uploaded to TensorFlow Hub. Seerun_classifier_with_tfhub.py for an example of how to use the TF Hub module,or run an example in the browser onColab.***** New November 23rd, 2018: Un-normalized multilingual model + Thai +Mongolian *****We uploaded a new multilingual model which does not perform any normalizationon the input (no lower casing, accent stripping, or Unicode normalization), andadditionally inclues Thai and Mongolian.It is recommended to use this version for developing multilingual models,especially on languages with non-Latin alphabets.This does not require any code changes, and can be downloaded here:BERT-Base, Multilingual Cased:104 languages, 12-layer, 768-hidden, 12-heads, 110M parameters***** New November 15th, 2018: SOTA SQuAD 2.0 System *****We released code changes to reproduce our 83% F1 SQuAD 2.0 system, which iscurrently 1st place on the leaderboard by 3%. See the SQuAD 2.0 section of theREADME for details.***** New November 5th, 2018: Third-party PyTorch and Chainer versions ofBERT available *****NLP researchers from HuggingFace made aPyTorch version of BERT availablewhich is compatible with our pre-trained checkpoints and is able to reproduceour results. Sosuke Kobayashi also made aChainer version of BERT available(Thanks!) We were not involved in the creation or maintenance of the PyTorchimplementation so please direct any questions towards the authors of thatrepository.***** New November 3rd, 2018: Multilingual and Chinese models available*****We have made two new BERT models available:BERT-Base, Multilingual(Not recommended, use Multilingual Cased instead): 102 languages,12-layer, 768-hidden, 12-heads, 110M parametersBERT-Base, Chinese:Chinese Simplified and Traditional, 12-layer, 768-hidden, 12-heads, 110MparametersWe use character-based tokenization for Chinese, and WordPiece tokenization forall other languages. Both models should work out-of-the-box without any codechanges. We did update the implementation of BasicTokenizer intokenization.py to support Chinese character tokenization, so please update ifyou forked it. However, we did not change the tokenization API.For more, see theMultilingual README.***** End new information *****IntroductionBERT, or Bidirectional Encoder Representations fromTransformers, is a new method of pre-training language representations whichobtains state-of-the-art results on a wide array of Natural Language Processing(NLP) tasks.Our academic paper which describes BERT in detail and provides full results on anumber of tasks can be found here:https://arxiv.org/abs/1810.04805.To give a few numbers, here are the results on theSQuAD v1.1 question answeringtask:SQuAD v1.1 Leaderboard (Oct 8th 2018)Test EMTest F11st Place Ensemble - BERT87.493.22nd Place Ensemble - nlnet86.091.71st Place Single Model - BERT85.191.82nd Place Single Model - nlnet83.590.1And several natural language inference tasks:SystemMultiNLIQuestion NLISWAGBERT86.791.186.3OpenAI GPT (Prev. SOTA)82.288.175.0Plus many other tasks.Moreover, these results were all obtained with almost no task-specific neuralnetwork architecture design.If you already know what BERT is and you just want to get started, you candownload the pre-trained models andrun a state-of-the-art fine-tuning in only a fewminutes.What is BERT?BERT is a method of pre-training language representations, meaning that we traina general-purpose \""language understanding\"" model on a large text corpus (likeWikipedia), and then use that model for downstream NLP tasks that we care about(like question answering). BERT outperforms previous methods because it is thefirst unsupervised, deeply bidirectional system for pre-training NLP.Unsupervised means that BERT was trained using only a plain text corpus, whichis important because an enormous amount of plain text data is publicly availableon the web in many languages.Pre-trained representations can also either be context-free or contextual,and contextual representations can further be unidirectional orbidirectional. Context-free models such asword2vec orGloVe generate a single \""wordembedding\"" representation for each word in the vocabulary, so bank would havethe same representation in bank deposit and river bank. Contextual modelsinstead generate a representation of each word that is based on the other wordsin the sentence.BERT was built upon recent work in pre-training contextual representations —including Semi-supervised Sequence Learning,Generative Pre-Training,ELMo, andULMFit— but crucially these models are all unidirectional or shallowlybidirectional. This means that each word is only contextualized using the wordsto its left (or right). For example, in the sentence I made a bank deposit theunidirectional representation of bank is only based on I made a but notdeposit. Some previous work does combine the representations from separateleft-context and right-context models, but only in a \""shallow\"" manner. BERTrepresents \""bank\"" using both its left and right context — I made a ... deposit— starting from the very bottom of a deep neural network, so it is deeplybidirectional.BERT uses a simple approach for this: We mask out 15% of the words in the input,run the entire sequence through a deep bidirectionalTransformer encoder, and then predict onlythe masked words. For example:Input: the man went to the [MASK1] . he bought a [MASK2] of milk.Labels: [MASK1] = store; [MASK2] = gallonIn order to learn relationships between sentences, we also train on a simpletask which can be generated from any monolingual corpus: Given two sentences Aand B, is B the actual next sentence that comes after A, or just a randomsentence from the corpus?Sentence A: the man went to the store .Sentence B: he bought a gallon of milk .Label: IsNextSentenceSentence A: the man went to the store .Sentence B: penguins are flightless .Label: NotNextSentenceWe then train a large model (12-layer to 24-layer Transformer) on a large corpus(Wikipedia + BookCorpus) for a long time (1Mupdate steps), and that's BERT.Using BERT has two stages: Pre-training and fine-tuning.Pre-training is fairly expensive (four days on 4 to 16 Cloud TPUs), but is aone-time procedure for each language (current models are English-only, butmultilingual models will be released in the near future). We are releasing anumber of pre-trained models from the paper which were pre-trained at Google.Most NLP researchers will never need to pre-train their own model from scratch.Fine-tuning is inexpensive. All of the results in the paper can bereplicated in at most 1 hour on a single Cloud TPU, or a few hours on a GPU,starting from the exact same pre-trained model. SQuAD, for example, can betrained in around 30 minutes on a single Cloud TPU to achieve a Dev F1 score of91.0%, which is the single system state-of-the-art.The other important aspect of BERT is that it can be adapted to many types ofNLP tasks very easily. In the paper, we demonstrate state-of-the-art results onsentence-level (e.g., SST-2), sentence-pair-level (e.g., MultiNLI), word-level(e.g., NER), and span-level (e.g., SQuAD) tasks with almost no task-specificmodifications.What has been released in this repository?We are releasing the following:TensorFlow code for the BERT model architecture (which is mostly a standardTransformer architecture).Pre-trained checkpoints for both the lowercase and cased version ofBERT-Base and BERT-Large from the paper.TensorFlow code for push-button replication of the most importantfine-tuning experiments from the paper, including SQuAD, MultiNLI, and MRPC.All of the code in this repository works out-of-the-box with CPU, GPU, and CloudTPU.Pre-trained modelsWe are releasing the BERT-Base and BERT-Large models from the paper.Uncased means that the text has been lowercased before WordPiece tokenization,e.g., John Smith becomes john smith. The Uncased model also strips out anyaccent markers. Cased means that the true case and accent markers arepreserved. Typically, the Uncased model is better unless you know that caseinformation is important for your task (e.g., Named Entity Recognition orPart-of-Speech tagging).These models are all released under the same license as the source code (Apache2.0).For information about the Multilingual and Chinese model, see theMultilingual README.When using a cased model, make sure to pass --do_lower=False to the trainingscripts. (Or pass do_lower_case=False directly to FullTokenizer if you'reusing your own script.)The links to the models are here (right-click, 'Save link as...' on the name):BERT-Large, Uncased (Whole Word Masking):24-layer, 1024-hidden, 16-heads, 340M parametersBERT-Large, Cased (Whole Word Masking):24-layer, 1024-hidden, 16-heads, 340M parametersBERT-Base, Uncased:12-layer, 768-hidden, 12-heads, 110M parametersBERT-Large, Uncased:24-layer, 1024-hidden, 16-heads, 340M parametersBERT-Base, Cased:12-layer, 768-hidden, 12-heads , 110M parametersBERT-Large, Cased:24-layer, 1024-hidden, 16-heads, 340M parametersBERT-Base, Multilingual Cased (New, recommended):104 languages, 12-layer, 768-hidden, 12-heads, 110M parametersBERT-Base, Multilingual Uncased (Orig, not recommended)(Not recommended, use Multilingual Cased instead): 102 languages,12-layer, 768-hidden, 12-heads, 110M parametersBERT-Base, Chinese:Chinese Simplified and Traditional, 12-layer, 768-hidden, 12-heads, 110MparametersEach .zip file contains three items:A TensorFlow checkpoint (bert_model.ckpt) containing the pre-trainedweights (which is actually 3 files).A vocab file (vocab.txt) to map WordPiece to word id.A config file (bert_config.json) which specifies the hyperparameters ofthe model.Fine-tuning with BERTImportant: All results on the paper were fine-tuned on a single Cloud TPU,which has 64GB of RAM. It is currently not possible to re-produce most of theBERT-Large results on the paper using a GPU with 12GB - 16GB of RAM, becausethe maximum batch size that can fit in memory is too small. We are working onadding code to this repository which allows for much larger effective batch sizeon the GPU. See the section on out-of-memory issues formore details.This code was tested with TensorFlow 1.11.0. It was tested with Python2 andPython3 (but more thoroughly with Python2, since this is what's used internallyin Google).The fine-tuning examples which use BERT-Base should be able to run on a GPUthat has at least 12GB of RAM using the hyperparameters given.Fine-tuning with Cloud TPUsMost of the examples below assumes that you will be running training/evaluationon your local machine, using a GPU like a Titan X or GTX 1080.However, if you have access to a Cloud TPU that you want to train on, just addthe following flags to run_classifier.py or run_squad.py:  --use_tpu=True \\  --tpu_name=$TPU_NAMEPlease see theGoogle Cloud TPU tutorialfor how to use Cloud TPUs. Alternatively, you can use the Google Colab notebook\""BERT FineTuning with Cloud TPUs\"".On Cloud TPUs, the pretrained model and the output directory will need to be onGoogle Cloud Storage. For example, if you have a bucket named some_bucket, youmight use the following flags instead:  --output_dir=gs://some_bucket/my_output_dir/The unzipped pre-trained model files can also be found in the Google CloudStorage folder gs://bert_models/2018_10_18. For example:export BERT_BASE_DIR=gs://bert_models/2018_10_18/uncased_L-12_H-768_A-12Sentence (and sentence-pair) classification tasksBefore running this example you must download theGLUE data by runningthis scriptand unpack it to some directory $GLUE_DIR. Next, download the BERT-Basecheckpoint and unzip it to some directory $BERT_BASE_DIR.This example code fine-tunes BERT-Base on the Microsoft Research ParaphraseCorpus (MRPC) corpus, which only contains 3,600 examples and can fine-tune in afew minutes on most GPUs.export BERT_BASE_DIR=/path/to/bert/uncased_L-12_H-768_A-12export GLUE_DIR=/path/to/gluepython run_classifier.py \\  --task_name=MRPC \\  --do_train=true \\  --do_eval=true \\  --data_dir=$GLUE_DIR/MRPC \\  --vocab_file=$BERT_BASE_DIR/vocab.txt \\  --bert_config_file=$BERT_BASE_DIR/bert_config.json \\  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\  --max_seq_length=128 \\  --train_batch_size=32 \\  --learning_rate=2e-5 \\  --num_train_epochs=3.0 \\  --output_dir=/tmp/mrpc_output/You should see output like this:***** Eval results *****  eval_accuracy = 0.845588  eval_loss = 0.505248  global_step = 343  loss = 0.505248This means that the Dev set accuracy was 84.55%. Small sets like MRPC have ahigh variance in the Dev set accuracy, even when starting from the samepre-training checkpoint. If you re-run multiple times (making sure to point todifferent output_dir), you should see results between 84% and 88%.A few other pre-trained models are implemented off-the-shelf inrun_classifier.py, so it should be straightforward to follow those examples touse BERT for any single-sentence or sentence-pair classification task.Note: You might see a message Running train on CPU. This really just meansthat it's running on something other than a Cloud TPU, which includes a GPU.Prediction from classifierOnce you have trained your classifier you can use it in inference mode by usingthe --do_predict=true command. You need to have a file named test.tsv in theinput folder. Output will be created in file called test_results.tsv in theoutput folder. Each line will contain output for each sample, columns are theclass probabilities.export BERT_BASE_DIR=/path/to/bert/uncased_L-12_H-768_A-12export GLUE_DIR=/path/to/glueexport TRAINED_CLASSIFIER=/path/to/fine/tuned/classifierpython run_classifier.py \\  --task_name=MRPC \\  --do_predict=true \\  --data_dir=$GLUE_DIR/MRPC \\  --vocab_file=$BERT_BASE_DIR/vocab.txt \\  --bert_config_file=$BERT_BASE_DIR/bert_config.json \\  --init_checkpoint=$TRAINED_CLASSIFIER \\  --max_seq_length=128 \\  --output_dir=/tmp/mrpc_output/SQuAD 1.1The Stanford Question Answering Dataset (SQuAD) is a popular question answeringbenchmark dataset. BERT (at the time of the release) obtains state-of-the-artresults on SQuAD with almost no task-specific network architecture modificationsor data augmentation. However, it does require semi-complex data pre-processingand post-processing to deal with (a) the variable-length nature of SQuAD contextparagraphs, and (b) the character-level answer annotations which are used forSQuAD training. This processing is implemented and documented in run_squad.py.To run on SQuAD, you will first need to download the dataset. TheSQuAD website does not seem tolink to the v1.1 datasets any longer, but the necessary files can be found here:train-v1.1.jsondev-v1.1.jsonevaluate-v1.1.pyDownload these to some directory $SQUAD_DIR.The state-of-the-art SQuAD results from the paper currently cannot be reproducedon a 12GB-16GB GPU due to memory constraints (in fact, even batch size 1 doesnot seem to fit on a 12GB GPU using BERT-Large). However, a reasonably strongBERT-Base model can be trained on the GPU with these hyperparameters:python run_squad.py \\  --vocab_file=$BERT_BASE_DIR/vocab.txt \\  --bert_config_file=$BERT_BASE_DIR/bert_config.json \\  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\  --do_train=True \\  --train_file=$SQUAD_DIR/train-v1.1.json \\  --do_predict=True \\  --predict_file=$SQUAD_DIR/dev-v1.1.json \\  --train_batch_size=12 \\  --learning_rate=3e-5 \\  --num_train_epochs=2.0 \\  --max_seq_length=384 \\  --doc_stride=128 \\  --output_dir=/tmp/squad_base/The dev set predictions will be saved into a file called predictions.json inthe output_dir:python $SQUAD_DIR/evaluate-v1.1.py $SQUAD_DIR/dev-v1.1.json ./squad/predictions.jsonWhich should produce an output like this:{\""f1\"": 88.41249612335034, \""exact_match\"": 81.2488174077578}You should see a result similar to the 88.5% reported in the paper forBERT-Base.If you have access to a Cloud TPU, you can train with BERT-Large. Here is aset of hyperparameters (slightly different than the paper) which consistentlyobtain around 90.5%-91.0% F1 single-system trained only on SQuAD:python run_squad.py \\  --vocab_file=$BERT_LARGE_DIR/vocab.txt \\  --bert_config_file=$BERT_LARGE_DIR/bert_config.json \\  --init_checkpoint=$BERT_LARGE_DIR/bert_model.ckpt \\  --do_train=True \\  --train_file=$SQUAD_DIR/train-v1.1.json \\  --do_predict=True \\  --predict_file=$SQUAD_DIR/dev-v1.1.json \\  --train_batch_size=24 \\  --learning_rate=3e-5 \\  --num_train_epochs=2.0 \\  --max_seq_length=384 \\  --doc_stride=128 \\  --output_dir=gs://some_bucket/squad_large/ \\  --use_tpu=True \\  --tpu_name=$TPU_NAMEFor example, one random run with these parameters produces the following Devscores:{\""f1\"": 90.87081895814865, \""exact_match\"": 84.38978240302744}If you fine-tune for one epoch onTriviaQA before this the results willbe even better, but you will need to convert TriviaQA into the SQuAD jsonformat.SQuAD 2.0This model is also implemented and documented in run_squad.py.To run on SQuAD 2.0, you will first need to download the dataset. The necessaryfiles can be found here:train-v2.0.jsondev-v2.0.jsonevaluate-v2.0.pyDownload these to some directory $SQUAD_DIR.On Cloud TPU you can run with BERT-Large as follows:python run_squad.py \\  --vocab_file=$BERT_LARGE_DIR/vocab.txt \\  --bert_config_file=$BERT_LARGE_DIR/bert_config.json \\  --init_checkpoint=$BERT_LARGE_DIR/bert_model.ckpt \\  --do_train=True \\  --train_file=$SQUAD_DIR/train-v2.0.json \\  --do_predict=True \\  --predict_file=$SQUAD_DIR/dev-v2.0.json \\  --train_batch_size=24 \\  --learning_rate=3e-5 \\  --num_train_epochs=2.0 \\  --max_seq_length=384 \\  --doc_stride=128 \\  --output_dir=gs://some_bucket/squad_large/ \\  --use_tpu=True \\  --tpu_name=$TPU_NAME \\  --version_2_with_negative=TrueWe assume you have copied everything from the output directory to a localdirectory called ./squad/. The initial dev set predictions will be at./squad/predictions.json and the differences between the score of no answer (\""\"")and the best non-null answer for each question will be in the file./squad/null_odds.jsonRun this script to tune a threshold for predicting null versus non-null answers:python $SQUAD_DIR/evaluate-v2.0.py $SQUAD_DIR/dev-v2.0.json./squad/predictions.json --na-prob-file ./squad/null_odds.jsonAssume the script outputs \""best_f1_thresh\"" THRESH. (Typical values are between-1.0 and -5.0). You can now re-run the model to generate predictions with thederived threshold or alternatively you can extract the appropriate answers from./squad/nbest_predictions.json.python run_squad.py \\  --vocab_file=$BERT_LARGE_DIR/vocab.txt \\  --bert_config_file=$BERT_LARGE_DIR/bert_config.json \\  --init_checkpoint=$BERT_LARGE_DIR/bert_model.ckpt \\  --do_train=False \\  --train_file=$SQUAD_DIR/train-v2.0.json \\  --do_predict=True \\  --predict_file=$SQUAD_DIR/dev-v2.0.json \\  --train_batch_size=24 \\  --learning_rate=3e-5 \\  --num_train_epochs=2.0 \\  --max_seq_length=384 \\  --doc_stride=128 \\  --output_dir=gs://some_bucket/squad_large/ \\  --use_tpu=True \\  --tpu_name=$TPU_NAME \\  --version_2_with_negative=True \\  --null_score_diff_threshold=$THRESHOut-of-memory issuesAll experiments in the paper were fine-tuned on a Cloud TPU, which has 64GB ofdevice RAM. Therefore, when using a GPU with 12GB - 16GB of RAM, you are likelyto encounter out-of-memory issues if you use the same hyperparameters describedin the paper.The factors that affect memory usage are:max_seq_length: The released models were trained with sequence lengthsup to 512, but you can fine-tune with a shorter max sequence length to savesubstantial memory. This is controlled by the max_seq_length flag in ourexample code.train_batch_size: The memory usage is also directly proportional tothe batch size.Model type, BERT-Base vs. BERT-Large: The BERT-Large modelrequires significantly more memory than BERT-Base.Optimizer: The default optimizer for BERT is Adam, which requires a lotof extra memory to store the m and v vectors. Switching to a more memoryefficient optimizer can reduce memory usage, but can also affect theresults. We have not experimented with other optimizers for fine-tuning.Using the default training scripts (run_classifier.py and run_squad.py), webenchmarked the maximum batch size on single Titan X GPU (12GB RAM) withTensorFlow 1.11.0:SystemSeq LengthMax Batch SizeBERT-Base6464...12832...25616...32014...38412...5126BERT-Large6412...1286...2562...3201...3840...5120Unfortunately, these max batch sizes for BERT-Large are so small that theywill actually harm the model accuracy, regardless of the learning rate used. Weare working on adding code to this repository which will allow much largereffective batch sizes to be used on the GPU. The code will be based on one (orboth) of the following techniques:Gradient accumulation: The samples in a minibatch are typicallyindependent with respect to gradient computation (excluding batchnormalization, which is not used here). This means that the gradients ofmultiple smaller minibatches can be accumulated before performing the weightupdate, and this will be exactly equivalent to a single larger update.Gradient checkpointing:The major use of GPU/TPU memory during DNN training is caching theintermediate activations in the forward pass that are necessary forefficient computation in the backward pass. \""Gradient checkpointing\"" tradesmemory for compute time by re-computing the activations in an intelligentway.However, this is not implemented in the current release.Using BERT to extract fixed feature vectors (like ELMo)In certain cases, rather than fine-tuning the entire pre-trained modelend-to-end, it can be beneficial to obtained pre-trained contextualembeddings, which are fixed contextual representations of each input tokengenerated from the hidden layers of the pre-trained model. This should alsomitigate most of the out-of-memory issues.As an example, we include the script extract_features.py which can be usedlike this:# Sentence A and Sentence B are separated by the ||| delimiter for sentence# pair tasks like question answering and entailment.# For single sentence inputs, put one sentence per line and DON'T use the# delimiter.echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txtpython extract_features.py \\  --input_file=/tmp/input.txt \\  --output_file=/tmp/output.jsonl \\  --vocab_file=$BERT_BASE_DIR/vocab.txt \\  --bert_config_file=$BERT_BASE_DIR/bert_config.json \\  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\  --layers=-1,-2,-3,-4 \\  --max_seq_length=128 \\  --batch_size=8This will create a JSON file (one line per line of input) containing the BERTactivations from each Transformer layer specified by layers (-1 is the finalhidden layer of the Transformer, etc.)Note that this script will produce very large output files (by default, around15kb for every input token).If you need to maintain alignment between the original and tokenized words (forprojecting training labels), see the Tokenization sectionbelow.Note: You may see a message like Could not find trained model in model_dir: /tmp/tmpuB5g5c, running initialization to predict. This message is expected, itjust means that we are using the init_from_checkpoint() API rather than thesaved model API. If you don't specify a checkpoint or specify an invalidcheckpoint, this script will complain.TokenizationFor sentence-level tasks (or sentence-pair) tasks, tokenization is very simple.Just follow the example code in run_classifier.py and extract_features.py.The basic procedure for sentence-level tasks is:Instantiate an instance of tokenizer = tokenization.FullTokenizerTokenize the raw text with tokens = tokenizer.tokenize(raw_text).Truncate to the maximum sequence length. (You can use up to 512, but youprobably want to use shorter if possible for memory and speed reasons.)Add the [CLS] and [SEP] tokens in the right place.Word-level and span-level tasks (e.g., SQuAD and NER) are more complex, sinceyou need to maintain alignment between your input text and output text so thatyou can project your training labels. SQuAD is a particularly complex examplebecause the input labels are character-based, and SQuAD paragraphs are oftenlonger than our maximum sequence length. See the code in run_squad.py to showhow we handle this.Before we describe the general recipe for handling word-level tasks, it'simportant to understand what exactly our tokenizer is doing. It has three mainsteps:Text normalization: Convert all whitespace characters to spaces, and(for the Uncased model) lowercase the input and strip out accent markers.E.g., John Johanson's, → john johanson's,.Punctuation splitting: Split all punctuation characters on both sides(i.e., add whitespace around all punctuation characters). Punctuationcharacters are defined as (a) Anything with a P* Unicode class, (b) anynon-letter/number/space ASCII character (e.g., characters like $ which aretechnically not punctuation). E.g., john johanson's, → john johanson ' s ,WordPiece tokenization: Apply whitespace tokenization to the output ofthe above procedure, and applyWordPiecetokenization to each token separately. (Our implementation is directly basedon the one from tensor2tensor, which is linked). E.g., john johanson ' s , → john johan ##son ' s ,The advantage of this scheme is that it is \""compatible\"" with most existingEnglish tokenizers. For example, imagine that you have a part-of-speech taggingtask which looks like this:Input:  John Johanson 's   houseLabels: NNP  NNP      POS NNThe tokenized output will look like this:Tokens: john johan ##son ' s houseCrucially, this would be the same output as if the raw text were John Johanson's house (with no space before the 's).If you have a pre-tokenized representation with word-level annotations, you cansimply tokenize each input word independently, and deterministically maintain anoriginal-to-tokenized alignment:### Inputorig_tokens = [\""John\"", \""Johanson\"", \""'s\"",  \""house\""]labels      = [\""NNP\"",  \""NNP\"",      \""POS\"", \""NN\""]### Outputbert_tokens = []# Token map will be an int -> int mapping between the `orig_tokens` index and# the `bert_tokens` index.orig_to_tok_map = []tokenizer = tokenization.FullTokenizer(    vocab_file=vocab_file, do_lower_case=True)bert_tokens.append(\""[CLS]\"")for orig_token in orig_tokens:  orig_to_tok_map.append(len(bert_tokens))  bert_tokens.extend(tokenizer.tokenize(orig_token))bert_tokens.append(\""[SEP]\"")# bert_tokens == [\""[CLS]\"", \""john\"", \""johan\"", \""##son\"", \""'\"", \""s\"", \""house\"", \""[SEP]\""]# orig_to_tok_map == [1, 2, 4, 6]Now orig_to_tok_map can be used to project labels to the tokenizedrepresentation.There are common English tokenization schemes which will cause a slight mismatchbetween how BERT was pre-trained. For example, if your input tokenization splitsoff contractions like do n't, this will cause a mismatch. If it is possible todo so, you should pre-process your data to convert these back to raw-lookingtext, but if it's not possible, this mismatch is likely not a big deal.Pre-training with BERTWe are releasing code to do \""masked LM\"" and \""next sentence prediction\"" on anarbitrary text corpus. Note that this is not the exact code that was used forthe paper (the original code was written in C++, and had some additionalcomplexity), but this code does generate pre-training data as described in thepaper.Here's how to run the data generation. The input is a plain text file, with onesentence per line. (It is important that these be actual sentences for the \""nextsentence prediction\"" task). Documents are delimited by empty lines. The outputis a set of tf.train.Examples serialized into TFRecord file format.You can perform sentence segmentation with an off-the-shelf NLP toolkit such asspaCy. The create_pretraining_data.py script willconcatenate segments until they reach the maximum sequence length to minimizecomputational waste from padding (see the script for more details). However, youmay want to intentionally add a slight amount of noise to your input data (e.g.,randomly truncate 2% of input segments) to make it more robust to non-sententialinput during fine-tuning.This script stores all of the examples for the entire input file in memory, sofor large data files you should shard the input file and call the scriptmultiple times. (You can pass in a file glob to run_pretraining.py, e.g.,tf_examples.tf_record*.)The max_predictions_per_seq is the maximum number of masked LM predictions persequence. You should set this to around max_seq_length * masked_lm_prob (thescript doesn't do that automatically because the exact value needs to be passedto both scripts).python create_pretraining_data.py \\  --input_file=./sample_text.txt \\  --output_file=/tmp/tf_examples.tfrecord \\  --vocab_file=$BERT_BASE_DIR/vocab.txt \\  --do_lower_case=True \\  --max_seq_length=128 \\  --max_predictions_per_seq=20 \\  --masked_lm_prob=0.15 \\  --random_seed=12345 \\  --dupe_factor=5Here's how to run the pre-training. Do not include init_checkpoint if you arepre-training from scratch. The model configuration (including vocab size) isspecified in bert_config_file. This demo code only pre-trains for a smallnumber of steps (20), but in practice you will probably want to setnum_train_steps to 10000 steps or more. The max_seq_length andmax_predictions_per_seq parameters passed to run_pretraining.py must be thesame as create_pretraining_data.py.python run_pretraining.py \\  --input_file=/tmp/tf_examples.tfrecord \\  --output_dir=/tmp/pretraining_output \\  --do_train=True \\  --do_eval=True \\  --bert_config_file=$BERT_BASE_DIR/bert_config.json \\  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\  --train_batch_size=32 \\  --max_seq_length=128 \\  --max_predictions_per_seq=20 \\  --num_train_steps=20 \\  --num_warmup_steps=10 \\  --learning_rate=2e-5This will produce an output like this:***** Eval results *****  global_step = 20  loss = 0.0979674  masked_lm_accuracy = 0.985479  masked_lm_loss = 0.0979328  next_sentence_accuracy = 1.0  next_sentence_loss = 3.45724e-05Note that since our sample_text.txt file is very small, this example trainingwill overfit that data in only a few steps and produce unrealistically highaccuracy numbers.Pre-training tips and caveatsIf using your own vocabulary, make sure to change vocab_size inbert_config.json. If you use a larger vocabulary without changing this,you will likely get NaNs when training on GPU or TPU due to uncheckedout-of-bounds access.If your task has a large domain-specific corpus available (e.g., \""moviereviews\"" or \""scientific papers\""), it will likely be beneficial to runadditional steps of pre-training on your corpus, starting from the BERTcheckpoint.The learning rate we used in the paper was 1e-4. However, if you are doingadditional steps of pre-training starting from an existing BERT checkpoint,you should use a smaller learning rate (e.g., 2e-5).Current BERT models are English-only, but we do plan to release amultilingual model which has been pre-trained on a lot of languages in thenear future (hopefully by the end of November 2018).Longer sequences are disproportionately expensive because attention isquadratic to the sequence length. In other words, a batch of 64 sequences oflength 512 is much more expensive than a batch of 256 sequences oflength 128. The fully-connected/convolutional cost is the same, but theattention cost is far greater for the 512-length sequences. Therefore, onegood recipe is to pre-train for, say, 90,000 steps with a sequence length of128 and then for 10,000 additional steps with a sequence length of 512. Thevery long sequences are mostly needed to learn positional embeddings, whichcan be learned fairly quickly. Note that this does require generating thedata twice with different values of max_seq_length.If you are pre-training from scratch, be prepared that pre-training iscomputationally expensive, especially on GPUs. If you are pre-training fromscratch, our recommended recipe is to pre-train a BERT-Base on a singlepreemptible Cloud TPU v2, whichtakes about 2 weeks at a cost of about $500 USD (based on the pricing inOctober 2018). You will have to scale down the batch size when only trainingon a single Cloud TPU, compared to what was used in the paper. It isrecommended to use the largest batch size that fits into TPU memory.Pre-training dataWe will not be able to release the pre-processed datasets used in the paper.For Wikipedia, the recommended pre-processing is to downloadthe latest dump,extract the text withWikiExtractor.py, and then applyany necessary cleanup to convert it into plain text.Unfortunately the researchers who collected theBookCorpus no longer have it available forpublic download. TheProject Guttenberg Datasetis a somewhat smaller (200M word) collection of older books that are publicdomain.Common Crawl is another very large collection oftext, but you will likely have to do substantial pre-processing and cleanup toextract a usable corpus for pre-training BERT.Learning a new WordPiece vocabularyThis repository does not include code for learning a new WordPiece vocabulary.The reason is that the code used in the paper was implemented in C++ withdependencies on Google's internal libraries. For English, it is almost alwaysbetter to just start with our vocabulary and pre-trained models. For learningvocabularies of other languages, there are a number of open source optionsavailable. However, keep in mind that these are not compatible with ourtokenization.py library:Google's SentencePiece librarytensor2tensor's WordPiece generation scriptRico Sennrich's Byte Pair Encoding libraryUsing BERT in ColabIf you want to use BERT with Colab, you canget started with the notebook\""BERT FineTuning with Cloud TPUs\"".At the time of this writing (October 31st, 2018), Colab users can access aCloud TPU completely for free. Note: One per user, availability limited,requires a Google Cloud Platform account with storage (although storage may bepurchased with free credit for signing up with GCP), and this capability may notlonger be available in the future. Click on the BERT Colab that was just linkedfor more information.FAQIs this code compatible with Cloud TPUs? What about GPUs?Yes, all of the code in this repository works out-of-the-box with CPU, GPU, andCloud TPU. However, GPU training is single-GPU only.I am getting out-of-memory errors, what is wrong?See the section on out-of-memory issues for moreinformation.Is there a PyTorch version available?There is no official PyTorch implementation. However, NLP researchers fromHuggingFace made aPyTorch version of BERT availablewhich is compatible with our pre-trained checkpoints and is able to reproduceour results. We were not involved in the creation or maintenance of the PyTorchimplementation so please direct any questions towards the authors of thatrepository.Is there a Chainer version available?There is no official Chainer implementation. However, Sosuke Kobayashi made aChainer version of BERT availablewhich is compatible with our pre-trained checkpoints and is able to reproduceour results. We were not involved in the creation or maintenance of the Chainerimplementation so please direct any questions towards the authors of thatrepository.Will models in other languages be released?Yes, we plan to release a multi-lingual BERT model in the near future. We cannotmake promises about exactly which languages will be included, but it will likelybe a single model which includes most of the languages which have asignificantly-sized Wikipedia.Will models larger than BERT-Large be released?So far we have not attempted to train anything larger than BERT-Large. It ispossible that we will release larger models if we are able to obtain significantimprovements.What license is this library released under?All code and models are released under the Apache 2.0 license. See theLICENSE file for more information.How do I cite BERT?For now, cite the Arxiv paper:@article{devlin2018bert,  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},  journal={arXiv preprint arXiv:1810.04805},  year={2018}}If we submit the paper to a conference or journal, we will update the BibTeX.DisclaimerThis is not an official Google product.Contact informationFor help or issues using BERT, please submit a GitHub issue.For personal communication related to BERT, please contact Jacob Devlin(jacobdevlin@google.com), Ming-Wei Chang (mingweichang@google.com), orKenton Lee (kentonl@google.com)."
79,ageitgey/face_recognition,https://github.com/ageitgey/face_recognition/blob/master/README.md,Python,"Face RecognitionYou can also read a translated version of this file in Chinese 简体中文版 or in Korean 한국어 or in Japanese 日本語.Recognize and manipulate faces from Python or from the command line withthe world's simplest face recognition library.Built using dlib's state-of-the-art face recognitionbuilt with deep learning. The model has an accuracy of 99.38% on theLabeled Faces in the Wild benchmark.This also provides a simple face_recognition command line tool that letsyou do face recognition on a folder of images from the command line!FeaturesFind faces in picturesFind all the faces that appear in a picture:import face_recognitionimage = face_recognition.load_image_file(\""your_file.jpg\"")face_locations = face_recognition.face_locations(image)Find and manipulate facial features in picturesGet the locations and outlines of each person's eyes, nose, mouth and chin.import face_recognitionimage = face_recognition.load_image_file(\""your_file.jpg\"")face_landmarks_list = face_recognition.face_landmarks(image)Finding facial features is super useful for lots of important stuff. But you can also use it for really stupid stufflike applying digital make-up (think 'Meitu'):Identify faces in picturesRecognize who appears in each photo.import face_recognitionknown_image = face_recognition.load_image_file(\""biden.jpg\"")unknown_image = face_recognition.load_image_file(\""unknown.jpg\"")biden_encoding = face_recognition.face_encodings(known_image)[0]unknown_encoding = face_recognition.face_encodings(unknown_image)[0]results = face_recognition.compare_faces([biden_encoding], unknown_encoding)You can even use this library with other Python libraries to do real-time face recognition:See this example for the code.Online DemosUser-contributed shared Jupyter notebook demo (not officially supported): InstallationRequirementsPython 3.3+ or Python 2.7macOS or Linux (Windows not officially supported, but might work)Installation Options:Installing on Mac or LinuxFirst, make sure you have dlib already installed with Python bindings:How to install dlib from source on macOS or UbuntuThen, make sure you have cmake installed:brew install cmakeFinally, install this module from pypi using pip3 (or pip2 for Python 2):pip3 install face_recognitionAlternatively, you can try this library with Docker, see this section.If you are having trouble with installation, you can also try out apre-configured VM.Installing on an Nvidia Jetson Nano boardJetson Nano installation instructionsPlease follow the instructions in the article carefully. There is current a bug in the CUDA libraries on the Jetson Nano that will cause this library to fail silently if you don't follow the instructions in the article to comment out a line in dlib and recompile it.Installing on Raspberry Pi 2+Raspberry Pi 2+ installation instructionsInstalling on FreeBSDpkg install graphics/py-face_recognitionInstalling on WindowsWhile Windows isn't officially supported, helpful users have posted instructions on how to install this library:@masoudr's Windows 10 installation guide (dlib + face_recognition)Installing a pre-configured Virtual Machine imageDownload the pre-configured VM image (for VMware Player or VirtualBox).UsageCommand-Line InterfaceWhen you install face_recognition, you get two simple command-lineprograms:face_recognition - Recognize faces in a photograph or folder full forphotographs.face_detection - Find faces in a photograph or folder full for photographs.face_recognition command line toolThe face_recognition command lets you recognize faces in a photograph orfolder full  for photographs.First, you need to provide a folder with one picture of each person youalready know. There should be one image file for each person with thefiles named according to who is in the picture:Next, you need a second folder with the files you want to identify:Then in you simply run the command face_recognition, passing inthe folder of known people and the folder (or single image) with unknownpeople and it tells you who is in each image:$ face_recognition ./pictures_of_people_i_know/ ./unknown_pictures//unknown_pictures/unknown.jpg,Barack Obama/face_recognition_test/unknown_pictures/unknown.jpg,unknown_personThere's one line in the output for each face. The data is comma-separatedwith the filename and the name of the person found.An unknown_person is a face in the image that didn't match anyone inyour folder of known people.face_detection command line toolThe face_detection command lets you find the location (pixel coordinatates)of any faces in an image.Just run the command face_detection, passing in a folder of imagesto check (or a single image):$ face_detection  ./folder_with_pictures/examples/image1.jpg,65,215,169,112examples/image2.jpg,62,394,211,244examples/image2.jpg,95,941,244,792It prints one line for each face that was detected. The coordinatesreported are the top, right, bottom and left coordinates of the face (in pixels).Adjusting Tolerance / SensitivityIf you are getting multiple matches for the same person, it might be thatthe people in your photos look very similar and a lower tolerance valueis needed to make face comparisons more strict.You can do that with the --tolerance parameter. The default tolerancevalue is 0.6 and lower numbers make face comparisons more strict:$ face_recognition --tolerance 0.54 ./pictures_of_people_i_know/ ./unknown_pictures//unknown_pictures/unknown.jpg,Barack Obama/face_recognition_test/unknown_pictures/unknown.jpg,unknown_personIf you want to see the face distance calculated for each match in orderto adjust the tolerance setting, you can use --show-distance true:$ face_recognition --show-distance true ./pictures_of_people_i_know/ ./unknown_pictures//unknown_pictures/unknown.jpg,Barack Obama,0.378542298956785/face_recognition_test/unknown_pictures/unknown.jpg,unknown_person,NoneMore ExamplesIf you simply want to know the names of the people in each photograph but don'tcare about file names, you could do this:$ face_recognition ./pictures_of_people_i_know/ ./unknown_pictures/ | cut -d ',' -f2Barack Obamaunknown_personSpeeding up Face RecognitionFace recognition can be done in parallel if you have a computer withmultiple CPU cores. For example, if your system has 4 CPU cores, you canprocess about 4 times as many images in the same amount of time by usingall your CPU cores in parallel.If you are using Python 3.4 or newer, pass in a --cpus <number_of_cpu_cores_to_use> parameter:$ face_recognition --cpus 4 ./pictures_of_people_i_know/ ./unknown_pictures/You can also pass in --cpus -1 to use all CPU cores in your system.Python ModuleYou can import the face_recognition module and then easily manipulatefaces with just a couple of lines of code. It's super easy!API Docs: https://face-recognition.readthedocs.io.Automatically find all the faces in an imageimport face_recognitionimage = face_recognition.load_image_file(\""my_picture.jpg\"")face_locations = face_recognition.face_locations(image)# face_locations is now an array listing the co-ordinates of each face!See this exampleto try it out.You can also opt-in to a somewhat more accurate deep-learning-based face detection model.Note: GPU acceleration (via NVidia's CUDA library) is required for goodperformance with this model. You'll also want to enable CUDA supportwhen compliling dlib.import face_recognitionimage = face_recognition.load_image_file(\""my_picture.jpg\"")face_locations = face_recognition.face_locations(image, model=\""cnn\"")# face_locations is now an array listing the co-ordinates of each face!See this exampleto try it out.If you have a lot of images and a GPU, you can alsofind faces in batches.Automatically locate the facial features of a person in an imageimport face_recognitionimage = face_recognition.load_image_file(\""my_picture.jpg\"")face_landmarks_list = face_recognition.face_landmarks(image)# face_landmarks_list is now an array with the locations of each facial feature in each face.# face_landmarks_list[0]['left_eye'] would be the location and outline of the first person's left eye.See this exampleto try it out.Recognize faces in images and identify who they areimport face_recognitionpicture_of_me = face_recognition.load_image_file(\""me.jpg\"")my_face_encoding = face_recognition.face_encodings(picture_of_me)[0]# my_face_encoding now contains a universal 'encoding' of my facial features that can be compared to any other picture of a face!unknown_picture = face_recognition.load_image_file(\""unknown.jpg\"")unknown_face_encoding = face_recognition.face_encodings(unknown_picture)[0]# Now we can see the two face encodings are of the same person with `compare_faces`!results = face_recognition.compare_faces([my_face_encoding], unknown_face_encoding)if results[0] == True:    print(\""It's a picture of me!\"")else:    print(\""It's not a picture of me!\"")See this exampleto try it out.Python Code ExamplesAll the examples are available here.Face DetectionFind faces in a photographFind faces in a photograph (using deep learning)Find faces in batches of images w/ GPU (using deep learning)Blur all the faces in a live video using your webcam (Requires OpenCV to be installed)Facial FeaturesIdentify specific facial features in a photographApply (horribly ugly) digital make-upFacial RecognitionFind and recognize unknown faces in a photograph based on photographs of known peopleIdentify and draw boxes around each person in a photoCompare faces by numeric face distance instead of only True/False matchesRecognize faces in live video using your webcam - Simple / Slower Version (Requires OpenCV to be installed)Recognize faces in live video using your webcam - Faster Version (Requires OpenCV to be installed)Recognize faces in a video file and write out new video file (Requires OpenCV to be installed)Recognize faces on a Raspberry Pi w/ cameraRun a web service to recognize faces via HTTP (Requires Flask to be installed)Recognize faces with a K-nearest neighbors classifierTrain multiple images per person then recognize faces using a SVMCreating a Standalone ExecutableIf you want to create a standalone executable that can run without the need to install python or face_recognition, you can use PyInstaller. However, it requires some custom configuration to work with this library. See this issue for how to do it.Articles and Guides that cover face_recognitionMy article on how Face Recognition works: Modern Face Recognition with Deep LearningCovers the algorithms and how they generally workFace recognition with OpenCV, Python, and deep learning by Adrian RosebrockCovers how to use face recognition in practiceRaspberry Pi Face Recognition by Adrian RosebrockCovers how to use this on a Raspberry PiFace clustering with Python by Adrian RosebrockCovers how to automatically cluster photos based on who appears in each photo using unsupervised learningHow Face Recognition WorksIf you want to learn how face location and recognition work instead ofdepending on a black box library, read my article.CaveatsThe face recognition model is trained on adults and does not work very well on children. It tends to mixup children quite easy using the default comparison threshold of 0.6.Accuracy may vary between ethnic groups. Please see this wiki page for more details.Deployment to Cloud Hosts (Heroku, AWS, etc)Since face_recognition depends on dlib which is written in C++, it can be tricky to deploy an appusing it to a cloud hosting provider like Heroku or AWS.To make things easier, there's an example Dockerfile in this repo that shows how to run an app built withface_recognition in a Docker container. With that, you should be able to deployto any service that supports Docker images.You can try the Docker image locally by running: docker-compose up --buildThere are also several prebuilt Docker images.Linux users with a GPU (drivers >= 384.81) and Nvidia-Docker installed can run the example on the GPU: Open the docker-compose.yml file and uncomment the dockerfile: Dockerfile.gpu and runtime: nvidia lines.Having problems?If you run into problems, please read the Common Errors section of the wiki before filing a github issue.ThanksMany, many thanks to Davis King (@nulhom)for creating dlib and for providing the trained facial feature detection and face encoding modelsused in this library. For more information on the ResNet that powers the face encodings, check outhis blog post.Thanks to everyone who works on all the awesome Python data science libraries like numpy, scipy, scikit-image,pillow, etc, etc that makes this kind of stuff so easy and fun in Python.Thanks to Cookiecutter and theaudreyr/cookiecutter-pypackage project templatefor making Python project packaging way more tolerable."
80,swisskyrepo/PayloadsAllTheThings,https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/README.md,Python,"Payloads All The ThingsA list of useful payloads and bypasses for Web Application Security.Feel free to improve with your payloads and techniques !I ❤️ pull requests :)You can also contribute with a 🍻 IRL, or using the sponsor buttonAn alternative display version is available at PayloadsAllTheThingsWeb.  📖 DocumentationEvery section contains the following files, you can use the _template_vuln folder to create a new chapter:README.md - vulnerability description and how to exploit it, including several payloadsIntruder - a set of files to give to Burp IntruderImages - pictures for the README.mdFiles - some files referenced in the README.mdYou might also like the Methodology and Resources folder :Methodology and ResourcesActive Directory Attack.mdCloud - AWS Pentest.mdCloud - Azure Pentest.mdCobalt Strike - Cheatsheet.mdLinux - Evasion.mdLinux - Persistence.mdLinux - Privilege Escalation.mdMetasploit - Cheatsheet.mdMethodology and enumeration.mdNetwork Pivoting Techniques.mdNetwork Discovery.mdReverse Shell Cheatsheet.mdSubdomains Enumeration.mdWindows - AMSI Bypass.mdWindows - DPAPI.mdWindows - Download and Execute.mdWindows - Mimikatz.mdWindows - Persistence.mdWindows - Privilege Escalation.mdWindows - Using credentials.mdYou want more ? Check the Books and Youtube videos selections.👨‍💻 ContributionsBe sure to read CONTRIBUTING.md  Thanks again for your contribution! ❤️🧙‍♂️ SponsorsThis project is proudly sponsored by these companies."
81,xtekky/gpt4free,https://github.com/xtekky/gpt4free/blob/main/README.md,Python,"By using this repository or any code related to it, you agree to the legal notice. The author is not responsible for any copies, forks, or reuploads made by other users. This is the author's only account and repository. To prevent impersonation or irresponsible actions, you may comply with the GNU GPL license this Repository uses.This (quite censored) New Version of gpt4free, was just released, it may contain bugs, open an issue or contribute a PR when encountering one, some features were disabled.Docker is for now not available but I would be happy if someone contributes a PR. The g4f GUI will be uploaded soon enough.Newpypi package:pip install -U g4fTable of Contents:Getting StartedPrerequisitesSetting up the projectUsageThe g4f Packageinterference openai-proxy apiModelsgpt-3.5 / gpt-4Other ModelsRelated gpt4free projectsContributeChatGPT cloneCopyrightCopyright NoticeStar HistoryGetting StartedPrerequisites:Download and install Python (Version 3.x is recommended).Setting up the project:Install using pypipip install -U g4forClone the GitHub repository:git clone https://github.com/xtekky/gpt4free.gitNavigate to the project directory:cd gpt4free(Recommended) Create a virtual environment to manage Python packages for your project:python3 -m venv venvActivate the virtual environment:On Windows:.\\venv\\Scripts\\activateOn macOS and Linux:source venv/bin/activateInstall the required Python packages from requirements.txt:pip install -r requirements.txtCreate a test.py file in the root folder and start using the repo, further Instructions are belowimport g4f...UsageThe g4f Packageimport g4fprint(g4f.provider.Ails.params)  # supported args# Automatic selection of provider# streamed completionresponse = g4f.ChatCompletion.create(    model=\""gpt-3.5-turbo\"",    messages=[{\""role\"": \""user\"", \""content\"": \""Hello world\""}],    stream=True,)for message in response:    print(message, flush=True, end='')# normal responseresponse = g4f.ChatCompletion.create(    model=g4f.models.gpt_4,    messages=[{\""role\"": \""user\"", \""content\"": \""hi\""}],)  # alterative model settingprint(response)# Set with providerresponse = g4f.ChatCompletion.create(    model=\""gpt-3.5-turbo\"",    provider=g4f.provider.DeepAi,    messages=[{\""role\"": \""user\"", \""content\"": \""Hello world\""}],    stream=True,)for message in response:    print(message)providers:from g4f.provider import (    Acytoo,    Aichat,    Ails,    AiService,    AItianhu,    Bard,    Bing,    ChatgptAi,    ChatgptLogin,    DeepAi,    GetGpt)# usage:response = g4f.ChatCompletion.create(..., provider=ProviderName)interference openai-proxy api (use with openai python package)get requirements:pip install -r interference/requirements.txtrun server:python3 -m interference.appimport openaiopenai.api_key = \""\""openai.api_base = \""http://localhost:1337\""def main():    chat_completion = openai.ChatCompletion.create(        model=\""gpt-3.5-turbo\"",        messages=[{\""role\"": \""user\"", \""content\"": \""write a poem about a tree\""}],        stream=True,    )    if isinstance(chat_completion, dict):        # not stream        print(chat_completion.choices[0].message.content)    else:        # stream        for token in chat_completion:            content = token[\""choices\""][0][\""delta\""].get(\""content\"")            if content != None:                print(content, end=\""\"", flush=True)if __name__ == \""__main__\"":    main()Modelsgpt-3.5 / gpt-4WebsiteProvidergpt-3.5gpt-4StreamingStatusAuthwww.aitianhu.comg4f.provider.AItianhu✔️❌❌❌chat.acytoo.comg4f.provider.Acytoo✔️❌❌❌aiservice.vercel.appg4f.provider.AiService✔️❌❌❌chat-gpt.orgg4f.provider.Aichat✔️❌❌❌ai.lsg4f.provider.Ails✔️❌✔️❌bard.google.comg4f.provider.Bard❌❌❌✔️bing.comg4f.provider.Bing❌✔️❌❌chatgpt.aig4f.provider.ChatgptAi❌✔️❌❌chatgptlogin.acg4f.provider.ChatgptLogin✔️❌❌❌deepai.orgg4f.provider.DeepAi✔️❌✔️❌chat.dfehub.comg4f.provider.DfeHub✔️❌✔️❌free.easychat.workg4f.provider.EasyChat✔️❌✔️❌forefront.comg4f.provider.Forefront✔️❌✔️❌chat.getgpt.worldg4f.provider.GetGpt✔️❌✔️❌gpt-gm.h2o.aig4f.provider.H2o❌❌✔️❌liaobots.comg4f.provider.Liaobots✔️✔️✔️✔️supertest.lockchat.appg4f.provider.Lockchat✔️✔️✔️❌opchatgpts.netg4f.provider.Opchatgpts✔️❌❌❌backend.raycast.comg4f.provider.Raycast✔️✔️✔️✔️theb.aig4f.provider.Theb✔️❌✔️❌play.vercel.aig4f.provider.Vercel✔️❌❌❌wewordle.orgg4f.provider.Wewordle✔️❌❌❌you.comg4f.provider.You✔️❌❌❌chat9.yqcloud.topg4f.provider.Yqcloud✔️❌❌❌Other ModelsModelBase ProviderProviderWebsitepalmGoogleg4f.provider.Bardbard.google.comh2ogpt-gm-oasst1-en-2048-falcon-7b-v3Huggingfaceg4f.provider.H2owww.h2o.aih2ogpt-gm-oasst1-en-2048-falcon-40b-v1Huggingfaceg4f.provider.H2owww.h2o.aih2ogpt-gm-oasst1-en-2048-open-llama-13bHuggingfaceg4f.provider.H2owww.h2o.aiclaude-instant-v1Anthropicg4f.provider.Vercelsdk.vercel.aiclaude-v1Anthropicg4f.provider.Vercelsdk.vercel.aiclaude-v2Anthropicg4f.provider.Vercelsdk.vercel.aicommand-light-nightlyCohereg4f.provider.Vercelsdk.vercel.aicommand-nightlyCohereg4f.provider.Vercelsdk.vercel.aigpt-neox-20bHuggingfaceg4f.provider.Vercelsdk.vercel.aioasst-sft-1-pythia-12bHuggingfaceg4f.provider.Vercelsdk.vercel.aioasst-sft-4-pythia-12b-epoch-3.5Huggingfaceg4f.provider.Vercelsdk.vercel.aisantacoderHuggingfaceg4f.provider.Vercelsdk.vercel.aibloomHuggingfaceg4f.provider.Vercelsdk.vercel.aiflan-t5-xxlHuggingfaceg4f.provider.Vercelsdk.vercel.aicode-davinci-002OpenAIg4f.provider.Vercelsdk.vercel.aigpt-3.5-turbo-16kOpenAIg4f.provider.Vercelsdk.vercel.aigpt-3.5-turbo-16k-0613OpenAIg4f.provider.Vercelsdk.vercel.aigpt-4-0613OpenAIg4f.provider.Vercelsdk.vercel.aitext-ada-001OpenAIg4f.provider.Vercelsdk.vercel.aitext-babbage-001OpenAIg4f.provider.Vercelsdk.vercel.aitext-curie-001OpenAIg4f.provider.Vercelsdk.vercel.aitext-davinci-002OpenAIg4f.provider.Vercelsdk.vercel.aitext-davinci-003OpenAIg4f.provider.Vercelsdk.vercel.aillama13b-v2-chatReplicateg4f.provider.Vercelsdk.vercel.aillama7b-v2-chatReplicateg4f.provider.Vercelsdk.vercel.aiRelated gpt4free projects            🎁 Projects      ⭐ Stars      📚 Forks      🛎 Issues      📬 Pull requests                  gpt4free                                      gpt4free-ts                                      ChatGPT-Clone                                      ChatGpt Discord Bot                                      LangChain gpt4free                                      ChatGpt Telegram Bot                              Contributeto add another provider, its very simple:create a new file in g4f/provider with the name of the ProviderImplement a class that extends BaseProvider.from .base_provider import BaseProviderfrom ..typing import CreateResult, Anyclass HogeService(BaseProvider):    url = \""http://hoge.com\""    working = True    supports_gpt_35_turbo = True    @staticmethod    def create_completion(        model: str,        messages: list[dict[str, str]],        stream: bool,        **kwargs: Any,    ) -> CreateResult:        passHere, you can adjust the settings, for example if the website does support streaming, set working to True...Write code to request the provider in create_completion and yield the response, even if its a one-time response, do not hesitate to look at other providers for inspirationAdd the Provider Name in g4f/provider/init.pyfrom .base_provider import BaseProviderfrom .HogeService import HogeService__all__ = [  HogeService,]You are done !, test the provider by calling it:import g4fresponse = g4f.ChatCompletion.create(model='gpt-3.5-turbo', provider=g4f.provider.PROVIDERNAME,                                    messages=[{\""role\"": \""user\"", \""content\"": \""test\""}], stream=g4f.provider.PROVIDERNAME.supports_stream)for message in response:    print(message, flush=True, end='')ChatGPT cloneCurrently implementing new features and trying to scale it, please be patient it may be unstablehttps://chat.g4f.ai/chatThis site was developed by me and includes gpt-4/3.5, internet access and gpt-jailbreak's like DANRun locally here: https://github.com/xtekky/chatgpt-cloneCopyright:This program is licensed under the GNU GPL v3Copyright Notice:xtekky/gpt4free: Copyright (C) 2023 xtekkyThis program is free software: you can redistribute it and/or modifyit under the terms of the GNU General Public License as published bythe Free Software Foundation, either version 3 of the License, or(at your option) any later version.This program is distributed in the hope that it will be useful,but WITHOUT ANY WARRANTY; without even the implied warranty ofMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See theGNU General Public License for more details.You should have received a copy of the GNU General Public Licensealong with this program.  If not, see <https://www.gnu.org/licenses/>.Star History         "
82,jhao104/proxy_pool,https://github.com/jhao104/proxy_pool/blob/master/README.md,Python,"ProxyPool 爬虫代理IP池______                        ______             _| ___ \\_                      | ___ \\           | || |_/ / \\__ __   __  _ __   _ | |_/ /___   ___  | ||  __/|  _// _ \\ \\ \\/ /| | | ||  __// _ \\ / _ \\ | || |   | | | (_) | >  < \\ |_| || |  | (_) | (_) || |___\\_|   |_|  \\___/ /_/\\_\\ \\__  |\\_|   \\___/ \\___/ \\_____\\                       __ / /                      /___ /ProxyPool爬虫代理IP池项目,主要功能为定时采集网上发布的免费代理验证入库，定时验证入库的代理保证代理的可用性，提供API和CLI两种使用方式。同时你也可以扩展代理源以增加代理池IP的质量和数量。文档: document 支持版本: 测试地址: http://demo.spiderpy.cn (勿压谢谢)付费代理推荐: luminati-china. 国外的亮数据BrightData（以前叫luminati）被认为是代理市场领导者，覆盖全球的7200万IP，大部分是真人住宅IP，成功率扛扛的。付费套餐多种，需要高质量代理IP的可以注册后联系中文客服，开通后赠送5美金余额和教程指引(PS:用不明白的同学可以参考这个使用教程)。运行项目下载代码:git clonegit clone git@github.com:jhao104/proxy_pool.gitreleaseshttps://github.com/jhao104/proxy_pool/releases 下载对应zip文件安装依赖:pip install -r requirements.txt更新配置:# setting.py 为项目配置文件# 配置API服务HOST = \""0.0.0.0\""               # IPPORT = 5000                    # 监听端口# 配置数据库DB_CONN = 'redis://:pwd@127.0.0.1:8888/0'# 配置 ProxyFetcherPROXY_FETCHER = [    \""freeProxy01\"",      # 这里是启用的代理抓取方法名，所有fetch方法位于fetcher/proxyFetcher.py    \""freeProxy02\"",    # ....]启动项目:# 如果已经具备运行条件, 可用通过proxyPool.py启动。# 程序分为: schedule 调度程序 和 server Api服务# 启动调度程序python proxyPool.py schedule# 启动webApi服务python proxyPool.py serverDocker Imagedocker pull jhao104/proxy_pooldocker run --env DB_CONN=redis://:password@ip:port/0 -p 5010:5010 jhao104/proxy_pool:latestdocker-compose项目目录下运行:docker-compose up -d使用Api启动web服务后, 默认配置下会开启 http://127.0.0.1:5010 的api接口服务:apimethodDescriptionparams/GETapi介绍None/getGET随机获取一个代理可选参数: ?type=https 过滤支持https的代理/popGET获取并删除一个代理可选参数: ?type=https 过滤支持https的代理/allGET获取所有代理可选参数: ?type=https 过滤支持https的代理/countGET查看代理数量None/deleteGET删除代理?proxy=host:ip爬虫使用　　如果要在爬虫代码中使用的话， 可以将此api封装成函数直接使用，例如：import requestsdef get_proxy():    return requests.get(\""http://127.0.0.1:5010/get/\"").json()def delete_proxy(proxy):    requests.get(\""http://127.0.0.1:5010/delete/?proxy={}\"".format(proxy))# your spider codedef getHtml():    # ....    retry_count = 5    proxy = get_proxy().get(\""proxy\"")    while retry_count > 0:        try:            html = requests.get('http://www.example.com', proxies={\""http\"": \""http://{}\"".format(proxy)})            # 使用代理访问            return html        except Exception:            retry_count -= 1    # 删除代理池中代理    delete_proxy(proxy)    return None扩展代理　　项目默认包含几个免费的代理获取源，但是免费的毕竟质量有限，所以如果直接运行可能拿到的代理质量不理想。所以，提供了代理获取的扩展方法。　　添加一个新的代理源方法如下:1、首先在ProxyFetcher类中添加自定义的获取代理的静态方法，该方法需要以生成器(yield)形式返回host:ip格式的代理，例如:class ProxyFetcher(object):    # ....    # 自定义代理源获取方法    @staticmethod    def freeProxyCustom1():  # 命名不和已有重复即可        # 通过某网站或者某接口或某数据库获取代理        # 假设你已经拿到了一个代理列表        proxies = [\""x.x.x.x:3128\"", \""x.x.x.x:80\""]        for proxy in proxies:            yield proxy        # 确保每个proxy都是 host:ip正确的格式返回2、添加好方法后，修改setting.py文件中的PROXY_FETCHER项：　　在PROXY_FETCHER下添加自定义方法的名字:PROXY_FETCHER = [    \""freeProxy01\"",        \""freeProxy02\"",    # ....    \""freeProxyCustom1\""  #  # 确保名字和你添加方法名字一致]　　schedule 进程会每隔一段时间抓取一次代理，下次抓取时会自动识别调用你定义的方法。免费代理源目前实现的采集免费代理网站有(排名不分先后, 下面仅是对其发布的免费代理情况, 付费代理测评可以参考这里):代理名称状态更新速度可用率地址代码站大爷✔★**地址freeProxy0166代理✔★*地址freeProxy02开心代理✔★*地址freeProxy03FreeProxyList✔★*地址freeProxy04快代理✔★*地址freeProxy05FateZero✔★★*地址freeProxy06云代理✔★*地址freeProxy07小幻代理✔★★*地址freeProxy08免费代理库✔☆*地址freeProxy0989代理✔☆*地址freeProxy10稻壳代理✔★★***地址freeProxy11如果还有其他好的免费代理网站, 可以在提交在issues, 下次更新时会考虑在项目中支持。问题反馈　　任何问题欢迎在Issues 中反馈，同时也可以到我的博客中留言。　　你的反馈会让此项目变得更加完美。贡献代码　　本项目仅作为基本的通用的代理池架构，不接收特有功能(当然,不限于特别好的idea)。　　本项目依然不够完善，如果发现bug或有新的功能添加，请在Issues中提交bug(或新功能)描述，我会尽力改进，使她更加完美。　　这里感谢以下contributor的无私奉献：　　@kangnwh | @bobobo80 | @halleywj | @newlyedward | @wang-ye | @gladmo | @bernieyangmh | @PythonYXY | @zuijiawoniu | @netAir | @scil | @tangrela | @highroom | @luocaodan | @vc5 | @1again | @obaiyan | @zsbh | @jiannanya | @Jerry12228Release Noteschangelog"
83,THUDM/ChatGLM-6B,https://github.com/THUDM/ChatGLM-6B/blob/main/README.md,Python,"ChatGLM-6B   🌐 Blog • 🤗 HF Repo • 🐦 Twitter • 📃 [GLM@ACL 22] [GitHub] • 📃 [GLM-130B@ICLR 23] [GitHub]     👋 加入我们的 Slack 和 WeChatRead this in English.介绍ChatGLM-6B 是一个开源的、支持中英双语的对话语言模型，基于 General Language Model (GLM) 架构，具有 62 亿参数。结合模型量化技术，用户可以在消费级的显卡上进行本地部署（INT4 量化级别下最低只需 6GB 显存）。ChatGLM-6B 使用了和 ChatGPT 相似的技术，针对中文问答和对话进行了优化。经过约 1T 标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持，62 亿参数的 ChatGLM-6B 已经能生成相当符合人类偏好的回答，更多信息请参考我们的博客。欢迎通过 chatglm.cn 体验更大规模的 ChatGLM 模型。为了方便下游开发者针对自己的应用场景定制模型，我们同时实现了基于 P-Tuning v2 的高效参数微调方法 (使用指南) ，INT4 量化级别下最低只需 7GB 显存即可启动微调。ChatGLM-6B 权重对学术研究完全开放，在填写问卷进行登记后亦允许免费商业使用。想让 ChatGLM-6B 更符合你的应用场景？欢迎参与 Badcase 反馈计划。ChatGLM-6B 开源模型旨在与开源社区一起推动大模型技术发展，恳请开发者和大家遵守开源协议，勿将开源模型和代码及基于开源项目产生的衍生物用于任何可能给国家和社会带来危害的用途以及用于任何未经过安全评估和备案的服务。目前，本项目团队未基于 ChatGLM-6B 开发任何应用，包括网页端、安卓、苹果 iOS 及 Windows App 等应用。尽管模型在训练的各个阶段都尽力确保数据的合规性和准确性，但由于 ChatGLM-6B 模型规模较小，且模型受概率随机性因素影响，无法保证输出内容的准确性，且模型易被误导（详见局限性）。本项目不承担开源模型和代码导致的数据安全、舆情风险或发生任何模型被误导、滥用、传播、不当利用而产生的风险和责任。更新信息[2023/07/25] 发布 CodeGeeX2 ，基于 ChatGLM2-6B 的代码生成模型，代码能力全面提升，更多特性包括：更强大的代码能力：CodeGeeX2-6B 进一步经过了 600B 代码数据预训练，相比 CodeGeeX 一代模型，在代码能力上全面提升，HumanEval-X 评测集的六种编程语言均大幅提升 (Python +57%, C++ +71%, Java +54%, JavaScript +83%, Go +56%, Rust +321%)，在Python上达到 35.9% 的 Pass@1 一次通过率，超越规模更大的 StarCoder-15B。更优秀的模型特性：继承 ChatGLM2-6B 模型特性，CodeGeeX2-6B 更好支持中英文输入，支持最大 8192 序列长度，推理速度较一代 大幅提升，量化后仅需6GB显存即可运行，支持轻量级本地化部署。更全面的AI编程助手：CodeGeeX插件（VS Code, Jetbrains）后端升级，支持超过100种编程语言，新增上下文补全、跨文件补全等实用功能。结合 Ask CodeGeeX 交互式AI编程助手，支持中英文对话解决各种编程问题，包括且不限于代码解释、代码翻译、代码纠错、文档生成等，帮助程序员更高效开发。[2023/06/25] 发布 ChatGLM2-6B，ChatGLM-6B 的升级版本，在保留了了初代模型对话流畅、部署门槛较低等众多优秀特性的基础之上，ChatGLM2-6B 引入了如下新特性：更强大的性能：基于 ChatGLM 初代模型的开发经验，我们全面升级了 ChatGLM2-6B 的基座模型。ChatGLM2-6B 使用了 GLM 的混合目标函数，经过了 1.4T 中英标识符的预训练与人类偏好对齐训练，评测结果显示，相比于初代模型，ChatGLM2-6B 在 MMLU（+23%）、CEval（+33%）、GSM8K（+571%） 、BBH（+60%）等数据集上的性能取得了大幅度的提升，在同尺寸开源模型中具有较强的竞争力。更长的上下文：基于 FlashAttention 技术，我们将基座模型的上下文长度（Context Length）由 ChatGLM-6B 的 2K 扩展到了 32K，并在对话阶段使用 8K 的上下文长度训练，允许更多轮次的对话。但当前版本的 ChatGLM2-6B 对单轮超长文档的理解能力有限，我们会在后续迭代升级中着重进行优化。更高效的推理：基于 Multi-Query Attention 技术，ChatGLM2-6B 有更高效的推理速度和更低的显存占用：在官方的模型实现下，推理速度相比初代提升了 42%，INT4 量化下，6G 显存支持的对话长度由 1K 提升到了 8K。更多信息参见 ChatGLM2-6B。[2023/06/14] 发布 WebGLM，一项被接受于KDD 2023的研究工作，支持利用网络信息生成带有准确引用的长回答。[2023/05/17] 发布 VisualGLM-6B，一个支持图像理解的多模态对话语言模型。可以通过本仓库中的 cli_demo_vision.py 和 web_demo_vision.py 来运行命令行和网页 Demo。注意 VisualGLM-6B 需要额外安装 SwissArmyTransformer 和 torchvision。更多信息参见 VisualGLM-6B。[2023/05/15] 更新 v1.1 版本 checkpoint，训练数据增加英文指令微调数据以平衡中英文数据比例，解决英文回答中夹杂中文词语的现象。以下是更新前后的英文问题对比：问题：Describe a time when you had to make a difficult decision.v1.0:v1.1:问题：Describe the function of a computer motherboardv1.0:v1.1:问题：Develop a plan to reduce electricity usage in a home.v1.0:v1.1:问题：未来的NFT，可能真实定义一种现实的资产，它会是一处房产，一辆汽车，一片土地等等，这样的数字凭证可能比真实的东西更有价值，你可以随时交易和使用，在虚拟和现实中无缝的让拥有的资产继续创造价值，未来会是万物归我所用，但不归我所有的时代。翻译成专业的英语v1.0:v1.1:更多更新信息参见 UPDATE.md友情链接对 ChatGLM 进行加速的开源项目：lyraChatGLM: 对 ChatGLM-6B 进行推理加速，最高可以实现 9000+ tokens/s 的推理速度ChatGLM-MNN: 一个基于 MNN 的 ChatGLM-6B C++ 推理实现，支持根据显存大小自动分配计算任务给 GPU 和 CPUJittorLLMs：最低3G显存或者没有显卡都可运行 ChatGLM-6B FP16， 支持Linux、windows、Mac部署InferLLM：轻量级 C++ 推理，可以实现本地 x86，Arm 处理器上实时聊天，手机上也同样可以实时运行，运行内存只需要 4G基于或使用了 ChatGLM-6B 的开源项目：langchain-ChatGLM：基于 langchain 的 ChatGLM 应用，实现基于可扩展知识库的问答闻达：大型语言模型调用平台，基于 ChatGLM-6B 实现了类 ChatPDF 功能glm-bot：将ChatGLM接入Koishi可在各大聊天平台上调用ChatGLMChuanhu Chat: 为各个大语言模型和在线模型API提供美观易用、功能丰富、快速部署的用户界面，支持ChatGLM-6B。支持 ChatGLM-6B 和相关应用在线训练的示例项目：ChatGLM-6B 的部署与微调教程ChatGLM-6B 结合 langchain 实现本地知识库 QA Bot第三方评测：Measuring Massive Multitask Chinese Understanding更多开源项目参见 PROJECT.md使用方式硬件需求量化等级最低 GPU 显存（推理）最低 GPU 显存（高效参数微调）FP16（无量化）13 GB14 GBINT88 GB9 GBINT46 GB7 GB环境安装使用 pip 安装依赖：pip install -r requirements.txt，其中 transformers 库版本推荐为 4.27.1，但理论上不低于 4.23.1 即可。此外，如果需要在 cpu 上运行量化后的模型，还需要安装 gcc 与 openmp。多数 Linux 发行版默认已安装。对于 Windows ，可在安装 TDM-GCC 时勾选 openmp。 Windows 测试环境 gcc 版本为 TDM-GCC 10.3.0， Linux 为 gcc 11.3.0。在 MacOS 上请参考 Q1。代码调用可以通过如下代码调用 ChatGLM-6B 模型来生成对话：>>> from transformers import AutoTokenizer, AutoModel>>> tokenizer = AutoTokenizer.from_pretrained(\""THUDM/chatglm-6b\"", trust_remote_code=True)>>> model = AutoModel.from_pretrained(\""THUDM/chatglm-6b\"", trust_remote_code=True).half().cuda()>>> model = model.eval()>>> response, history = model.chat(tokenizer, \""你好\"", history=[])>>> print(response)你好👋!我是人工智能助手 ChatGLM-6B,很高兴见到你,欢迎问我任何问题。>>> response, history = model.chat(tokenizer, \""晚上睡不着应该怎么办\"", history=history)>>> print(response)晚上睡不着可能会让你感到焦虑或不舒服,但以下是一些可以帮助你入睡的方法:1. 制定规律的睡眠时间表:保持规律的睡眠时间表可以帮助你建立健康的睡眠习惯,使你更容易入睡。尽量在每天的相同时间上床,并在同一时间起床。2. 创造一个舒适的睡眠环境:确保睡眠环境舒适,安静,黑暗且温度适宜。可以使用舒适的床上用品,并保持房间通风。3. 放松身心:在睡前做些放松的活动,例如泡个热水澡,听些轻柔的音乐,阅读一些有趣的书籍等,有助于缓解紧张和焦虑,使你更容易入睡。4. 避免饮用含有咖啡因的饮料:咖啡因是一种刺激性物质,会影响你的睡眠质量。尽量避免在睡前饮用含有咖啡因的饮料,例如咖啡,茶和可乐。5. 避免在床上做与睡眠无关的事情:在床上做些与睡眠无关的事情,例如看电影,玩游戏或工作等,可能会干扰你的睡眠。6. 尝试呼吸技巧:深呼吸是一种放松技巧,可以帮助你缓解紧张和焦虑,使你更容易入睡。试着慢慢吸气,保持几秒钟,然后缓慢呼气。如果这些方法无法帮助你入睡,你可以考虑咨询医生或睡眠专家,寻求进一步的建议。模型的实现仍然处在变动中。如果希望固定使用的模型实现以保证兼容性，可以在 from_pretrained 的调用中增加 revision=\""v1.1.0\"" 参数。v1.1.0 是当前最新的版本号，完整的版本列表参见 Change Log。从本地加载模型以上代码会由 transformers 自动下载模型实现和参数。完整的模型实现可以在 Hugging Face Hub。如果你的网络环境较差，下载模型参数可能会花费较长时间甚至失败。此时可以先将模型下载到本地，然后从本地加载。从 Hugging Face Hub 下载模型需要先安装Git LFS，然后运行git clone https://huggingface.co/THUDM/chatglm-6b如果你从 Hugging Face Hub 上下载 checkpoint 的速度较慢，可以只下载模型实现GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/THUDM/chatglm-6b然后从这里手动下载模型参数文件，并将下载的文件替换到本地的 chatglm-6b 目录下。将模型下载到本地之后，将以上代码中的 THUDM/chatglm-6b 替换为你本地的 chatglm-6b 文件夹的路径，即可从本地加载模型。Optional 模型的实现仍然处在变动中。如果希望固定使用的模型实现以保证兼容性，可以执行git checkout v1.1.0Demo & API我们提供了一个基于 Gradio 的网页版 Demo 和一个命令行 Demo。使用时首先需要下载本仓库：git clone https://github.com/THUDM/ChatGLM-6Bcd ChatGLM-6B网页版 Demo首先安装 Gradio：pip install gradio，然后运行仓库中的 web_demo.py：python web_demo.py程序会运行一个 Web Server，并输出地址。在浏览器中打开输出的地址即可使用。最新版 Demo 实现了打字机效果，速度体验大大提升。注意，由于国内 Gradio 的网络访问较为缓慢，启用 demo.queue().launch(share=True, inbrowser=True) 时所有网络会经过 Gradio 服务器转发，导致打字机体验大幅下降，现在默认启动方式已经改为 share=False，如有需要公网访问的需求，可以重新修改为 share=True 启动。感谢 @AdamBear 实现了基于 Streamlit 的网页版 Demo，运行方式见#117.命令行 Demo运行仓库中 cli_demo.py：python cli_demo.py程序会在命令行中进行交互式的对话，在命令行中输入指示并回车即可生成回复，输入 clear 可以清空对话历史，输入 stop 终止程序。API部署首先需要安装额外的依赖 pip install fastapi uvicorn，然后运行仓库中的 api.py：python api.py默认部署在本地的 8000 端口，通过 POST 方法进行调用curl -X POST \""http://127.0.0.1:8000\"" \\     -H 'Content-Type: application/json' \\     -d '{\""prompt\"": \""你好\"", \""history\"": []}'得到的返回值为{  \""response\"":\""你好👋！我是人工智能助手 ChatGLM-6B，很高兴见到你，欢迎问我任何问题。\"",  \""history\"":[[\""你好\"",\""你好👋！我是人工智能助手 ChatGLM-6B，很高兴见到你，欢迎问我任何问题。\""]],  \""status\"":200,  \""time\"":\""2023-03-23 21:38:40\""}低成本部署模型量化默认情况下，模型以 FP16 精度加载，运行上述代码需要大概 13GB 显存。如果你的 GPU 显存有限，可以尝试以量化方式加载模型，使用方法如下：# 按需修改，目前只支持 4/8 bit 量化model = AutoModel.from_pretrained(\""THUDM/chatglm-6b\"", trust_remote_code=True).quantize(8).half().cuda()进行 2 至 3 轮对话后，8-bit 量化下 GPU 显存占用约为 10GB，4-bit 量化下仅需 6GB 占用。随着对话轮数的增多，对应消耗显存也随之增长，由于采用了相对位置编码，理论上 ChatGLM-6B 支持无限长的 context-length，但总长度超过 2048（训练长度）后性能会逐渐下降。模型量化会带来一定的性能损失，经过测试，ChatGLM-6B 在 4-bit 量化下仍然能够进行自然流畅的生成。使用 GPT-Q 等量化方案可以进一步压缩量化精度/提升相同量化精度下的模型性能，欢迎大家提出对应的 Pull Request。量化过程需要在内存中首先加载 FP16 格式的模型，消耗大概 13GB 的内存。如果你的内存不足的话，可以直接加载量化后的模型，INT4 量化后的模型仅需大概 5.2GB 的内存：# INT8 量化的模型将\""THUDM/chatglm-6b-int4\""改为\""THUDM/chatglm-6b-int8\""model = AutoModel.from_pretrained(\""THUDM/chatglm-6b-int4\"", trust_remote_code=True).half().cuda()量化模型的参数文件也可以从这里手动下载。CPU 部署如果你没有 GPU 硬件的话，也可以在 CPU 上进行推理，但是推理速度会更慢。使用方法如下（需要大概 32GB 内存）model = AutoModel.from_pretrained(\""THUDM/chatglm-6b\"", trust_remote_code=True).float()如果你的内存不足，可以直接加载量化后的模型：# INT8 量化的模型将\""THUDM/chatglm-6b-int4\""改为\""THUDM/chatglm-6b-int8\""model = AutoModel.from_pretrained(\""THUDM/chatglm-6b-int4\"",trust_remote_code=True).float()如果遇到了报错 Could not find module 'nvcuda.dll' 或者 RuntimeError: Unknown platform: darwin (MacOS) ，请从本地加载模型Mac 部署对于搭载了 Apple Silicon 或者 AMD GPU 的Mac，可以使用 MPS 后端来在 GPU 上运行 ChatGLM-6B。需要参考 Apple 的 官方说明 安装 PyTorch-Nightly（正确的版本号应该是2.1.0.dev2023xxxx，而不是2.0.0）。目前在 MacOS 上只支持从本地加载模型。将代码中的模型加载改为从本地加载，并使用 mps 后端：model = AutoModel.from_pretrained(\""your local path\"", trust_remote_code=True).half().to('mps')加载半精度的 ChatGLM-6B 模型需要大概 13GB 内存。内存较小的机器（比如 16GB 内存的 MacBook Pro），在空余内存不足的情况下会使用硬盘上的虚拟内存，导致推理速度严重变慢。此时可以使用量化后的模型如 chatglm-6b-int4。因为 GPU 上量化的 kernel 是使用 CUDA 编写的，因此无法在 MacOS 上使用，只能使用 CPU 进行推理。# INT8 量化的模型将\""THUDM/chatglm-6b-int4\""改为\""THUDM/chatglm-6b-int8\""model = AutoModel.from_pretrained(\""THUDM/chatglm-6b-int4\"",trust_remote_code=True).float()为了充分使用 CPU 并行，还需要单独安装 OpenMP。多卡部署如果你有多张 GPU，但是每张 GPU 的显存大小都不足以容纳完整的模型，那么可以将模型切分在多张GPU上。首先安装 accelerate: pip install accelerate，然后通过如下方法加载模型：from utils import load_model_on_gpusmodel = load_model_on_gpus(\""THUDM/chatglm-6b\"", num_gpus=2)即可将模型部署到两张 GPU 上进行推理。你可以将 num_gpus 改为你希望使用的 GPU 数。默认是均匀切分的，你也可以传入 device_map 参数来自己指定。高效参数微调基于 P-tuning v2 的高效参数微调。具体使用方法详见 ptuning/README.md。ChatGLM-6B 示例以下是一些使用 web_demo.py 得到的示例截图。更多 ChatGLM-6B 的可能，等待你来探索发现！自我认知提纲写作文案写作邮件写作助手信息抽取角色扮演评论比较旅游向导局限性由于 ChatGLM-6B 的小规模，其能力仍然有许多局限性。以下是我们目前发现的一些问题：模型容量较小：6B 的小容量，决定了其相对较弱的模型记忆和语言能力。在面对许多事实性知识任务时，ChatGLM-6B 可能会生成不正确的信息；它也不擅长逻辑类问题（如数学、编程）的解答。  点击查看例子  产生有害说明或有偏见的内容：ChatGLM-6B 只是一个初步与人类意图对齐的语言模型，可能会生成有害、有偏见的内容。（内容可能具有冒犯性，此处不展示）英文能力不足：ChatGLM-6B 训练时使用的指示/回答大部分都是中文的，仅有极小一部分英文内容。因此，如果输入英文指示，回复的质量远不如中文，甚至与中文指示下的内容矛盾，并且出现中英夹杂的情况。易被误导，对话能力较弱：ChatGLM-6B 对话能力还比较弱，而且 “自我认知” 存在问题，并很容易被误导并产生错误的言论。例如当前版本的模型在被误导的情况下，会在自我认知上发生偏差。  点击查看例子  协议本仓库的代码依照 Apache-2.0 协议开源，ChatGLM-6B 模型的权重的使用则需要遵循 Model License。ChatGLM-6B 权重对学术研究完全开放，在填写问卷进行登记后亦允许免费商业使用。引用如果你觉得我们的工作有帮助的话，请考虑引用下列论文@article{zeng2022glm,  title={Glm-130b: An open bilingual pre-trained model},  author={Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and others},  journal={arXiv preprint arXiv:2210.02414},  year={2022}}@inproceedings{du2022glm,  title={GLM: General Language Model Pretraining with Autoregressive Blank Infilling},  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},  pages={320--335},  year={2022}}"
84,miguelgrinberg/flasky,https://github.com/miguelgrinberg/flasky/blob/master/README.md,Python,"FlaskyThis repository contains the source code examples for the second edition of my O'Reilly book Flask Web Development.The commits and tags in this repository were carefully created to match the sequence in which concepts are presented in the book. Please read the section titled \""How to Work with the Example Code\"" in the book's preface for instructions.For Readers of the First Edition of the BookThe code examples for the first edition of the book were moved to a different repository: https://github.com/miguelgrinberg/flasky-first-edition."
85,getredash/redash,https://github.com/getredash/redash/blob/master/README.md,Python,"  Redash is designed to enable anyone, regardless of the level of technical sophistication, to harness the power of data big and small. SQL users leverage Redash to explore, query, visualize, and share data from any data sources. Their work in turn enables anybody in their organization to use the data. Every day, millions of users at thousands of organizations around the world use Redash to develop insights and make data-driven decisions.Redash features:Browser-based: Everything in your browser, with a shareable URL.Ease-of-use: Become immediately productive with data without the need to master complex software.Query editor: Quickly compose SQL and NoSQL queries with a schema browser and auto-complete.Visualization and dashboards: Create beautiful visualizations with drag and drop, and combine them into a single dashboard.Sharing: Collaborate easily by sharing visualizations and their associated queries, enabling peer review of reports and queries.Schedule refreshes: Automatically update your charts and dashboards at regular intervals you define.Alerts: Define conditions and be alerted instantly when your data changes.REST API: Everything that can be done in the UI is also available through REST API.Broad support for data sources: Extensible data source API with native support for a long list of common databases and platforms.Getting StartedSetting up Redash instance (includes links to ready-made AWS/GCE images).Documentation.Supported Data SourcesRedash supports more than 35 SQL and NoSQL data sources. It can also be extended to support more. Below is a list of built-in sources:Amazon AthenaAmazon CloudWatch / InsightsAmazon DynamoDBAmazon RedshiftArangoDBAxibase Time Series DatabaseApache CassandraClickHouseCockroachDBCouchbaseCSVDatabricksDB2 by IBMDgraphApache DrillApache DruidEccenca Corporate MemoryElasticsearchExasolMicrosoft ExcelFireboltDatabendGoogle AnalyticsGoogle BigQueryGoogle SpreadsheetsGraphiteGreenplumApache HiveApache ImpalaInfluxDBIBM Netezza Performance ServerJIRA (JQL)JSONApache KylinOmniSciDB (Formerly MapD)MariaDBMemSQLMicrosoft Azure Data Warehouse / SynapseMicrosoft Azure SQL DatabaseMicrosoft Azure Data Explorer / KustoMicrosoft SQL ServerMongoDBMySQLOracleApache PhoenixApache PinotPostgreSQLPrestoPrometheusPythonQuboleRocksetSalesforceScyllaDBShell ScriptsSnowflakeSPARQLSQLiteTiDBTreasureDataTrinoUptycsVerticaYandex AppMetrricaYandex MetricaGetting HelpIssues: https://github.com/getredash/redash/issuesDiscussion Forum: https://github.com/getredash/redash/discussions/Development Discussion: https://discord.gg/tN5MdmfGBpReporting Bugs and Contributing CodeWant to report a bug or request a feature? Please open an issue.Want to help us build Redash? Fork the project, edit in a dev environment and make a pull request. We need all the help we can get!SecurityPlease email security@redash.io to report any security vulnerabilities. We will acknowledge receipt of your vulnerability and strive to send you regular updates about our progress. If you're curious about the status of your disclosure please feel free to email us again. If you want to encrypt your disclosure email, you can use this PGP key.LicenseBSD-2-Clause."
86,pytorch/tutorials,https://github.com/pytorch/tutorials/blob/main/README.md,Python,"PyTorch TutorialsAll the tutorials are now presented as sphinx style documentation at:https://pytorch.org/tutorialsContributingWe use sphinx-gallery's notebook styled examples to create the tutorials. Syntax is very simple. In essence, you write a slightly well formatted Python file and it shows up as an HTML page. In addition, a Jupyter notebook is autogenerated and available to run in Google Colab.Here is how you can create a new tutorial (for a detailed description, see CONTRIBUTING.md):Create a Python file. If you want it executed while inserted into documentation, save the file with the suffix tutorial so that the file name is your_tutorial.py.Put it in one of the beginner_source, intermediate_source, advanced_source directory based on the level of difficulty. If it is a recipe, add it to recipes_source. For tutorials demonstrating unstable prototype features, add to the prototype_source.For Tutorials (except if it is a prototype feature), include it in the toctree directive and create a customcarditem in index.rst.For Tutorials (except if it is a prototype feature), create a thumbnail in the index.rst file using a command like .. customcarditem:: beginner/your_tutorial.html. For Recipes, create a thumbnail in the recipes_index.rstIf you are starting off with a Jupyter notebook, you can use this script to convert the notebook to Python file. After conversion and addition to the project, please make sure that section headings and other things are in logical order.Building locallyThe tutorial build is very large and requires a GPU. If your machine does not have a GPU device, you can preview your HTML build without actually downloading the data and running the tutorial code:Install required dependencies by running: pip install -r requirements.txt.If you want to use virtualenv, in the root of the repo, run: virtualenv venv, then source venv/bin/activate.If you have a GPU-powered laptop, you can build using make docs. This will download the data, execute the tutorials and build the documentation to docs/ directory. This might take about 60-120 min for systems with GPUs. If you do not have a GPU installed on your system, then see next step.You can skip the computationally intensive graph generation by running make html-noplot to build basic html documentation to _build/html. This way, you can quickly preview your tutorial.If you get ModuleNotFoundError: No module named 'pytorch_sphinx_theme' make: *** [html-noplot] Error 2 from /tutorials/src/pytorch-sphinx-theme or /venv/src/pytorch-sphinx-theme (while using virtualenv), run python setup.py install.Building a single tutorialYou can build a single tutorial by using the GALLERY_PATTERN environment variable. For example to run only neural_style_transfer_tutorial.py, run:GALLERY_PATTERN=\""neural_style_transfer_tutorial.py\"" make htmlorGALLERY_PATTERN=\""neural_style_transfer_tutorial.py\"" sphinx-build . _buildThe GALLERY_PATTERN variable respects regular expressions.About contributing to PyTorch Documentation and TutorialsYou can find information about contributing to PyTorch documentation in thePyTorch Repo README.md file.Additional information can be found in PyTorch CONTRIBUTING.md."
87,httpie/cli,https://github.com/httpie/cli/blob/master/README.md,Python,"                        HTTPie CLI: human-friendly HTTP client for the API eraHTTPie (pronounced aitch-tee-tee-pie) is a command-line HTTP client.Its goal is to make CLI interaction with web services as human-friendly as possible.HTTPie is designed for testing, debugging, and generally interacting with APIs & HTTP servers.The http & https commands allow for creating and sending arbitrary HTTP requests.They use simple and natural syntax and provide formatted and colorized output.We lost 54k GitHub starsPlease note we recently accidentally made this repo private for a moment, and GitHub deleted our community that took a decade to build. Read the full story here: https://httpie.io/blog/stardustGetting startedInstallation instructions →Full documentation →FeaturesExpressive and intuitive syntaxFormatted and colorized terminal outputBuilt-in JSON supportForms and file uploadsHTTPS, proxies, and authenticationArbitrary request dataCustom headersPersistent sessionswget-like downloadsSee all features →ExamplesHello World:https httpie.io/helloCustom HTTP method, HTTP headers and JSON data:http PUT pie.dev/put X-API-Token:123 name=JohnBuild and print a request without sending it using offline mode:http --offline pie.dev/post hello=offlineUse GitHub API to post a comment on an Issue with authentication:http -a USERNAME POST https://api.github.com/repos/httpie/cli/issues/83/comments body='HTTPie is awesome! :heart:'See more examples →Community & supportVisit the HTTPie website for full documentation and useful links.Join our Discord server is to ask questions, discuss features, and for general API chat.Tweet at @httpie on Twitter.Use StackOverflow to ask questions and include a httpie tag.Create GitHub Issues for bug reports and feature requests.Subscribe to the HTTPie newsletter for occasional updates.ContributingHave a look through existing Issues and Pull Requests that you could help with. If you'd like to request a feature or report a bug, please create a GitHub Issue using one of the templates provided.See contribution guide →"
88,localstack/localstack,https://github.com/localstack/localstack/blob/master/README.md,Python,"⚡ We are thrilled to announce LocalStack 2.2 which brings new features, enhancements and bugfixes ⚡                        LocalStack provides an easy-to-use test/mocking framework for developing cloud applications.  Overview •  Install •  Example •  Run •  Usage •  Releases •  Contributing    📖 Docs •  💻 Pro version •  ☑️ LocalStack coverageOverviewLocalStack is a cloud service emulator that runs in a single container on your laptop or in your CI environment. With LocalStack, you can run your AWS applications or Lambdas entirely on your local machine without connecting to a remote cloud provider! Whether you are testing complex CDK applications or Terraform configurations, or just beginning to learn about AWS services, LocalStack helps speed up and simplify your testing and development workflow.LocalStack supports a growing number of AWS services, like AWS Lambda, S3, Dynamodb, Kinesis, SQS, SNS, and many more! The Pro version of LocalStack supports additional APIs and advanced features. You can find a comprehensive list of supported APIs on our ☑️ Feature Coverage page.LocalStack also provides additional features to make your life as a cloud developer easier! Check out LocalStack's Cloud Developer Tools for more information.InstallationThe quickest way get started with LocalStack is by using the LocalStack CLI.It allows you to start and manage the LocalStack Docker container from your command line.Please make sure that you have a working docker environment on your machine before moving on.Brew (MacOS or Linux with Homebrew)Install the LocalStack CLI by using our official LocalStack Brew Tap:$ brew install localstack/tap/localstack-cliBinary download (MacOS, Linux, Windows)If you do not have Brew on your machine, you can directly download the pre-built LocalStack CLI binary for your system:Download the latest release for your platform on localstack/localstack-cli.Extract the archive to a folder in your PATH variable:MacOS / Linux: sudo tar xvzf ~/Downloads/localstack-cli-*-darwin-*-onefile.tar.gz -C /usr/local/binPython package (MacOS, Linux, Windows)LocalStack is built with Python.You can directly install the LocalStack CLI in your Python environment using pip.Prerequisitespython (Python 3.7 up to 3.11 supported)Installationpython3 -m pip install localstackImportant: Do not use sudo or run as root user. LocalStack must be installed and started entirely under a local non-root user. If you have problems with permissions in macOS High Sierra, install with pip install --user localstackExampleStart LocalStack inside a Docker container by running: % localstack start -d     __                     _______ __             __    / /   ____  _________ _/ / ___// /_____ ______/ /__   / /   / __ \\/ ___/ __ `/ /\\__ \\/ __/ __ `/ ___/ //_/  / /___/ /_/ / /__/ /_/ / /___/ / /_/ /_/ / /__/ ,< /_____/\\____/\\___/\\__,_/_//____/\\__/\\__,_/\\___/_/|_| 💻 LocalStack CLI 2.2.0[20:22:20] starting LocalStack in Docker mode 🐳[20:22:21] detachingYou can query the status of respective services on LocalStack by running:% localstack status services┏━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓┃ Service                  ┃ Status      ┃┡━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩│ acm                      │ ✔ available ││ apigateway               │ ✔ available ││ cloudformation           │ ✔ available ││ cloudwatch               │ ✔ available ││ config                   │ ✔ available ││ dynamodb                 │ ✔ available │...To use SQS, a fully managed distributed message queuing service, on LocalStack, run:% awslocal sqs create-queue --queue-name sample-queue{    \""QueueUrl\"": \""http://localhost:4566/000000000000/sample-queue\""}Learn more about LocalStack AWS services and using them with LocalStack's awslocal CLI.RunningYou can run LocalStack through the following options:LocalStack CLIDockerDocker ComposeHelmUsageTo start using LocalStack, check out our documentation at https://docs.localstack.cloud.LocalStack ConfigurationLocalStack in CILocalStack IntegrationsLocalStack ToolsUnderstanding LocalStackTroubleshootTo use LocalStack with a graphical user interface, you can use the following UI clients:Commandeer desktop appDynamoDB Admin Web UIReleasesPlease refer to GitHub releases to see the complete list of changes for each release. For extended release notes, please refer to the LocalStack Discuss.ContributingIf you are interested in contributing to LocalStack:Start by reading our contributing guide.Check out our developer guide.Navigate our codebase and open issues.We are thankful for all the contributions and feedback we receive.Get in touchTo get in touch with LocalStack team for bugs/feature requests, support questions or general discussions, please use:LocalStack Slack CommunityLocalStack Discussion PageLocalStack GitHub Issue trackerContributorsWe are thankful to all the people who have contributed to this project.BackersWe are also grateful to all our backers who have donated to the project. You can become a backer on Open Collective.SponsorsYou can also support this project by becoming a sponsor on Open Collective. Your logo will show up here along with a link to your website.LicenseCopyright (c) 2017-2023 LocalStack maintainers and contributors.Copyright (c) 2016 Atlassian and others.This version of LocalStack is released under the Apache License, Version 2.0 (see LICENSE). By downloading and using this software you agree to the End-User License Agreement (EULA). To know about the external software we use, look at our third party software tools page."
